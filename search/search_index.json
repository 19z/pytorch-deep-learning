{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\u4ece\u200b\u96f6\u5230\u200b\u7cbe\u901a\u200b\uff1aPyTorch \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u8bad\u7ec3\u8425","text":"<p>\u200b\u6b22\u8fce\u200b\u6765\u5230\u200b\u4e92\u8054\u7f51\u200b\u4e0a\u200b\u5b66\u4e60\u200b PyTorch \u200b\u7684\u200b\u7b2c\u4e8c\u200b\u4f73\u200b\u5730\u70b9\u200b\uff08\u200b\u7b2c\u4e00\u200b\u4f73\u200b\u5730\u70b9\u200b\u662f\u200b PyTorch \u200b\u5b98\u65b9\u200b\u6587\u6863\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u662f\u200b \u200b\u5b66\u4e60\u200b PyTorch \u200b\u8fdb\u884c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff1a\u200b\u4ece\u200b\u96f6\u5230\u200b\u7cbe\u901a\u200b\u8bfe\u7a0b\u200b \u200b\u7684\u200b\u5728\u7ebf\u200b\u4e66\u7c4d\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u5c06\u200b\u6559\u6388\u200b\u60a8\u200b\u4f7f\u7528\u200b PyTorch\uff08\u200b\u4e00\u79cd\u200b\u7528\u200b Python \u200b\u7f16\u5199\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\uff09\u200b\u8fdb\u884c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u7840\u77e5\u8bc6\u200b\u3002</p> <p>\u200b\u8bfe\u7a0b\u200b\u4ee5\u200b\u89c6\u9891\u200b\u4e3a\u200b\u57fa\u7840\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u89c6\u9891\u200b\u5185\u5bb9\u200b\u57fa\u4e8e\u200b\u8fd9\u672c\u200b\u5728\u7ebf\u200b\u4e66\u7c4d\u200b\u3002</p> <p>\u200b\u5b8c\u6574\u200b\u7684\u200b\u4ee3\u7801\u200b\u548c\u200b\u8d44\u6e90\u200b\u8bf7\u200b\u53c2\u89c1\u200b \u200b\u8bfe\u7a0b\u200b GitHub\u3002</p> <p>\u200b\u60a8\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e0b\u9762\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p>"},{"location":"#pytorch-20","title":"\u672c\u200b\u8bfe\u7a0b\u200b\u662f\u5426\u200b\u6db5\u76d6\u200b PyTorch 2.0\uff1f","text":"<p>\u200b\u662f\u200b\u7684\u200b\u3002PyTorch 2.0 \u200b\u662f\u200b\u5bf9\u200b\u4e4b\u524d\u200b\u7248\u672c\u200b\u7684\u200b PyTorch \u200b\u7684\u200b\u9644\u52a0\u200b\u53d1\u5e03\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u5728\u200b PyTorch \u200b\u73b0\u6709\u200b\u57fa\u7ebf\u200b\u529f\u80fd\u200b\u4e4b\u4e0a\u200b\u6dfb\u52a0\u200b\u4e86\u200b\u65b0\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e13\u6ce8\u200b\u4e8e\u200b PyTorch \u200b\u7684\u200b\u57fa\u7ebf\u200b\u529f\u80fd\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u60a8\u200b\u662f\u200b\u521d\u5b66\u8005\u200b\uff0c\u200b\u60f3\u8981\u200b\u8fdb\u5165\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b/\u200b\u4eba\u5de5\u667a\u80fd\u200b\u9886\u57df\u200b\uff09\u3002</p> <p>\u200b\u4e00\u65e6\u200b\u60a8\u200b\u638c\u63e1\u200b\u4e86\u200b PyTorch \u200b\u7684\u200b\u57fa\u7840\u77e5\u8bc6\u200b\uff0c\u200b\u5347\u7ea7\u200b\u5230\u200b PyTorch 2.0 \u200b\u662f\u200b\u5f88\u5feb\u200b\u7684\u200b\uff0c\u200b\u672c\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u6709\u200b\u4e00\u4e2a\u200b \u200b\u6559\u7a0b\u200b \u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u65b0\u200b\u529f\u80fd\u200b\u3002</p>"},{"location":"#_1","title":"\u72b6\u6001","text":"<p>\u200b\u8bfe\u7a0b\u200b\u5df2\u200b\u5728\u200b ZTM Academy\u200b\u4e0a\u7ebf\u200b\uff01</p> <ul> <li>\u200b\u6700\u540e\u200b\u66f4\u65b0\u200b\uff1a2023\u200b\u5e74\u200b4\u200b\u6708\u200b16\u200b\u65e5\u200b</li> <li>\u200b\u5df2\u200b\u5b8c\u6210\u200b\u7ae0\u8282\u200b\u89c6\u9891\u200b\uff1a00, 01, 02, 03, 04, 05, 06, 07, 08, 09\uff08\u200b\u6240\u6709\u200b\u7ae0\u8282\u200b\uff01\uff09</li> <li>\u200b\u76ee\u524d\u200b\u6b63\u5728\u200b\u5236\u4f5c\u200b\uff1aPyTorch 2.0 \u200b\u6559\u7a0b\u200b</li> <li>\u200b\u67e5\u770b\u200b\u8bfe\u7a0b\u200b\u8fdb\u5ea6\u200b\uff1aGitHub \u200b\u9879\u76ee\u200b</li> </ul> <p>\u200b\u83b7\u53d6\u200b\u66f4\u65b0\u200b\uff1a \u200b\u5173\u6ce8\u200b <code>pytorch-deep-learning</code> \u200b\u4ed3\u5e93\u200b\u65e5\u5fd7\u200b\u6216\u200b\u8ba2\u9605\u200b\u90ae\u4ef6\u200b\u3002</p>"},{"location":"#_2","title":"\u8bfe\u7a0b\u200b\u6750\u6599\u200b/\u200b\u5927\u7eb2","text":"<ul> <li>\ud83d\udcbb GitHub \u200b\u4ee3\u7801\u200b\uff1a \u200b\u6240\u6709\u200b\u8bfe\u7a0b\u200b\u6750\u6599\u200b\u5747\u200b\u53ef\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u514d\u8d39\u200b\u83b7\u53d6\u200b\u3002</li> <li>\ud83c\udfa5 \u200b\u524d\u4e94\u200b\u90e8\u5206\u200b\u5728\u200b YouTube\uff1a \u200b\u901a\u8fc7\u200b\u89c2\u770b\u200b \u200b\u524d\u200b 25 \u200b\u5c0f\u65f6\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4e00\u5929\u200b\u5185\u200b\u5b66\u4f1a\u200b PyTorch\u3002</li> <li>\ud83d\udd2c \u200b\u8bfe\u7a0b\u200b\u91cd\u70b9\u200b\uff1a \u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\u3002</li> <li>\ud83c\udfc3\u200d\u2642\ufe0f \u200b\u6559\u5b66\u98ce\u683c\u200b\uff1a https://sive.rs/kimo</li> <li>\ud83e\udd14 \u200b\u63d0\u95ee\u200b\uff1a \u200b\u67e5\u770b\u200b\u8bfe\u7a0b\u200b GitHub \u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b \u200b\u4ee5\u200b\u83b7\u53d6\u200b\u73b0\u6709\u200b\u95ee\u9898\u200b\u6216\u200b\u63d0\u51fa\u200b\u60a8\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</li> </ul> \u200b\u7ae0\u8282\u200b \u200b\u5185\u5bb9\u200b\u6982\u8ff0\u200b \u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u5e7b\u706f\u7247\u200b 00 - PyTorch \u200b\u57fa\u7840\u200b \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u8bb8\u591a\u200b\u57fa\u672c\u200b PyTorch \u200b\u64cd\u4f5c\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 01 - PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b \u200b\u63d0\u4f9b\u200b\u89e3\u51b3\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\u548c\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6846\u67b6\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 02 - PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b\u200b \u200b\u4f7f\u7528\u200b 01 \u200b\u7ae0\u8282\u200b\u7684\u200b PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u89e3\u51b3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 03 - PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b \u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b 01 &amp; 02 \u200b\u7ae0\u8282\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u89e3\u51b3\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 04 - PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u5982\u4f55\u200b\u5c06\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u52a0\u8f7d\u200b\u5230\u200b PyTorch \u200b\u4e2d\u200b\uff1f\u200b\u672c\u200b\u7ae0\u8282\u200b\u8fd8\u200b\u5c06\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u5757\u5316\u200b\u4ee3\u7801\u200b\uff08\u200b\u5728\u200b 05 \u200b\u7ae0\u8282\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\uff09\u200b\u5960\u5b9a\u200b\u57fa\u7840\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 05 - PyTorch \u200b\u6a21\u5757\u5316\u200b PyTorch \u200b\u8bbe\u8ba1\u200b\u4e3a\u200b\u6a21\u5757\u5316\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u7684\u200b\u5185\u5bb9\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7cfb\u5217\u200b Python \u200b\u811a\u672c\u200b\uff08\u200b\u8fd9\u200b\u662f\u200b\u60a8\u200b\u5728\u200b\u91ce\u5916\u200b\u7ecf\u5e38\u200b\u770b\u5230\u200b\u7684\u200b PyTorch \u200b\u4ee3\u7801\u200b\uff09\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 06 - PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u91c7\u7528\u200b\u4e00\u4e2a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8c03\u6574\u200b\u4e3a\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 07 - \u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b 1\uff1aPyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6784\u5efa\u200b\u4e86\u200b\u8bb8\u591a\u200b\u6a21\u578b\u200b...\u200b\u8ddf\u8e2a\u200b\u5b83\u4eec\u200b\u7684\u200b\u8fdb\u5c55\u200b\u4e0d\u662f\u200b\u5f88\u200b\u597d\u200b\u5417\u200b\uff1f \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 08 - \u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b 2\uff1aPyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b PyTorch \u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u4e2d\u200b\u6700\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u590d\u73b0\u200b\u4e00\u7bc7\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bba\u6587\u200b\u6765\u200b\u770b\u770b\u200b\u4e3a\u4ec0\u4e48\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b 09 - \u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b 3\uff1a\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6784\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5de5\u4f5c\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b...\u200b\u5982\u4f55\u200b\u8ba9\u200b\u5176\u4ed6\u4eba\u200b\u4f7f\u7528\u200b\u5b83\u200b\uff1f\u200b\u63d0\u793a\u200b\uff1a\u200b\u5c06\u200b\u5176\u200b\u90e8\u7f72\u200b\u5230\u200b\u4e92\u8054\u7f51\u200b\u4e0a\u200b\u3002 \u200b\u524d\u5f80\u200b\u7ec3\u4e60\u200b &amp; \u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b \u200b\u524d\u5f80\u200b\u5e7b\u706f\u7247\u200b PyTorch \u200b\u989d\u5916\u200b\u8d44\u6e90\u200b \u200b\u672c\u200b\u8bfe\u7a0b\u200b\u6db5\u76d6\u200b\u4e86\u200b\u5927\u91cf\u200b\u7684\u200b PyTorch \u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4f46\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u5e7f\u9614\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u60a8\u200b\u5c06\u200b\u627e\u5230\u200b\u63a8\u8350\u200b\u7684\u200b\u4e66\u7c4d\u200b\u548c\u200b\u8d44\u6e90\u200b\uff1aPyTorch \u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff0cML \u200b\u5de5\u7a0b\u200b\uff0cNLP\uff08\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff09\uff0c\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u6570\u636e\u200b\uff0c\u200b\u5982\u4f55\u200b\u627e\u5230\u200b\u6570\u636e\u200b\u96c6\u7b49\u200b\u3002 - - PyTorch \u200b\u901f\u67e5\u8868\u200b \u200b\u5bf9\u200b PyTorch \u200b\u7684\u200b\u4e00\u4e9b\u200b\u4e3b\u8981\u200b\u529f\u80fd\u200b\u8fdb\u884c\u200b\u5feb\u901f\u200b\u6982\u89c8\u200b\uff0c\u200b\u5e76\u200b\u9644\u4e0a\u200b\u8bfe\u7a0b\u200b\u548c\u200b PyTorch \u200b\u6587\u6863\u200b\u4e2d\u200b\u66f4\u200b\u591a\u200b\u8d44\u6e90\u200b\u7684\u200b\u94fe\u63a5\u200b\u3002 - - PyTorch \u200b\u4e09\u5927\u200b\u5e38\u89c1\u200b\u9519\u8bef\u200b \u200b\u6982\u8ff0\u200b PyTorch \u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e09\u4e2a\u200b\u9519\u8bef\u200b\uff08\u200b\u5f62\u72b6\u200b\u3001\u200b\u8bbe\u5907\u200b\u548c\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b\uff09\uff0c\u200b\u5b83\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u53d1\u751f\u200b\u7684\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u4fee\u590d\u200b\u5b83\u4eec\u200b\u3002 - - \u200b\u5feb\u901f\u200b PyTorch 2.0 \u200b\u6559\u7a0b\u200b \u200b\u5bf9\u200b PyTorch 2.0 \u200b\u7684\u200b\u5feb\u901f\u200b\u4ecb\u7ecd\u200b\uff0c\u200b\u65b0\u200b\u529f\u80fd\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u9644\u4e0a\u200b\u66f4\u200b\u591a\u200b\u5b66\u4e60\u200b\u8d44\u6e90\u200b\u7684\u200b\u94fe\u63a5\u200b\u3002 - -"},{"location":"#_3","title":"\u5173\u4e8e\u200b\u672c\u200b\u8bfe\u7a0b","text":""},{"location":"#_4","title":"\u672c\u200b\u8bfe\u7a0b\u200b\u9002\u5408\u200b\u8c01\u200b\uff1f","text":"<p>\u200b\u4f60\u200b\uff1a \u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u3001\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6216\u200b\u4eba\u5de5\u667a\u80fd\u200b\u9886\u57df\u200b\u7684\u200b\u65b0\u624b\u200b\uff0c\u200b\u5e0c\u671b\u200b\u5b66\u4e60\u200b PyTorch\u3002</p> <p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\uff1a \u200b\u4ee5\u200b\u5b9e\u8df5\u200b\u4e3a\u4e3b\u200b\u3001\u200b\u4ee3\u7801\u200b\u4f18\u5148\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6559\u6388\u200b\u4f60\u200b PyTorch \u200b\u4ee5\u53ca\u200b\u8bb8\u591a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u3001\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u4eba\u5de5\u667a\u80fd\u200b\u6982\u5ff5\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e00\u5e74\u200b\u4ee5\u4e0a\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u6240\u200b\u5e2e\u52a9\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u4e13\u95e8\u200b\u8bbe\u8ba1\u200b\u4e3a\u200b\u9002\u5408\u200b\u521d\u5b66\u8005\u200b\u3002</p>"},{"location":"#_5","title":"\u8bfe\u7a0b\u200b\u5148\u51b3\u6761\u4ef6\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f","text":"<ol> <li>3-6 \u200b\u4e2a\u200b\u6708\u200b\u7684\u200b Python \u200b\u7f16\u7a0b\u200b\u7ecf\u9a8c\u200b\u3002</li> <li>\u200b\u81f3\u5c11\u200b\u5b8c\u6210\u200b\u4e00\u95e8\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5165\u95e8\u200b\u8bfe\u7a0b\u200b\uff08\u200b\u4e0d\u8fc7\u200b\u8fd9\u200b\u4e00\u9879\u200b\u53ef\u80fd\u200b\u53ef\u4ee5\u200b\u8df3\u8fc7\u200b\uff0c\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u4e3b\u9898\u200b\u7684\u200b\u8d44\u6e90\u200b\u5df2\u200b\u94fe\u63a5\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b Jupyter Notebooks \u200b\u6216\u200b Google Colab \u200b\u7684\u200b\u7ecf\u9a8c\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8fb9\u5b66\u8fb9\u200b\u638c\u63e1\u200b\uff09\u3002</li> <li>\u200b\u5b66\u4e60\u200b\u610f\u613f\u200b\uff08\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\uff09\u3002</li> </ol> <p>\u200b\u5bf9\u4e8e\u200b 1 \u200b\u548c\u200b 2\uff0c\u200b\u6211\u200b\u63a8\u8350\u200b Zero to Mastery \u200b\u6570\u636e\u200b\u79d1\u5b66\u200b\u4e0e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bad\u7ec3\u8425\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u6559\u200b\u4f60\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b Python \u200b\u7684\u200b\u57fa\u7840\u77e5\u8bc6\u200b\uff08\u200b\u6211\u200b\u6709\u70b9\u200b\u504f\u89c1\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u200b\u4e5f\u200b\u5728\u200b\u6559\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\uff09\u3002</p>"},{"location":"#_6","title":"\u8bfe\u7a0b\u200b\u662f\u200b\u5982\u4f55\u200b\u6559\u6388\u200b\u7684\u200b\uff1f","text":"<p>\u200b\u6240\u6709\u200b\u8bfe\u7a0b\u200b\u6750\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b learnpytorch.io \u200b\u4e0a\u200b\u7684\u200b\u5728\u7ebf\u200b\u4e66\u7c4d\u200b\u4e2d\u200b\u514d\u8d39\u200b\u83b7\u53d6\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u559c\u6b22\u200b\u9605\u8bfb\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u4f60\u200b\u901a\u8fc7\u200b\u90a3\u91cc\u200b\u7684\u200b\u8d44\u6e90\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u66f4\u200b\u559c\u6b22\u200b\u901a\u8fc7\u200b\u89c6\u9891\u200b\u5b66\u4e60\u200b\uff0c\u200b\u8bfe\u7a0b\u200b\u4e5f\u200b\u91c7\u7528\u200b\u5b66\u5f92\u200b\u5f0f\u200b\u6559\u5b66\u65b9\u5f0f\u200b\uff0c\u200b\u5373\u200b\u6211\u200b\u7f16\u5199\u200b PyTorch \u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u7f16\u5199\u200b PyTorch \u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\u5305\u62ec\u200b \u200b\u5982\u679c\u200b\u6709\u200b\u7591\u95ee\u200b\uff0c\u200b\u8fd0\u884c\u200b\u4ee3\u7801\u200b \u200b\u548c\u200b \u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff01 \u200b\u662f\u200b\u6709\u200b\u539f\u56e0\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u200b\u7684\u200b\u5168\u90e8\u200b\u76ee\u6807\u200b\u662f\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u505a\u200b\u4e00\u4ef6\u200b\u4e8b\u200b\uff1a\u200b\u901a\u8fc7\u200b\u7f16\u5199\u200b PyTorch \u200b\u4ee3\u7801\u200b\u6765\u200b\u5b66\u4e60\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u4ee3\u7801\u200b\u90fd\u200b\u662f\u200b\u901a\u8fc7\u200b Google Colab Notebooks\uff08\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b Jupyter Notebooks\uff09\u200b\u7f16\u5199\u200b\u7684\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5b9e\u9a8c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u7edd\u4f73\u200b\u514d\u8d39\u8d44\u6e90\u200b\u3002</p>"},{"location":"#_7","title":"\u5b8c\u6210\u200b\u8bfe\u7a0b\u200b\u540e\u200b\uff0c\u200b\u6211\u80fd\u200b\u83b7\u5f97\u200b\u4ec0\u4e48\u200b\uff1f","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u89c2\u770b\u200b\u5b8c\u200b\u6240\u6709\u200b\u89c6\u9891\u200b\uff0c\u200b\u4f1a\u200b\u6709\u200b\u8bc1\u4e66\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u4e1c\u897f\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8bc1\u4e66\u200b\u4e5f\u200b\u5c31\u200b\u90a3\u6837\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u628a\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\u770b\u4f5c\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u52a8\u529b\u200b\u52a9\u63a8\u5668\u200b\u3002</p> <p>\u200b\u5230\u200b\u8bfe\u7a0b\u200b\u7ed3\u675f\u200b\u65f6\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u7f16\u5199\u200b\u6570\u767e\u200b\u884c\u200b PyTorch \u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u5c06\u200b\u63a5\u89e6\u200b\u5230\u200b\u8bb8\u591a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6700\u200b\u5173\u952e\u200b\u7684\u200b\u6982\u5ff5\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u53bb\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\u6216\u200b\u68c0\u67e5\u200b\u7528\u200b PyTorch \u200b\u5236\u4f5c\u200b\u7684\u200b\u516c\u5171\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\u65f6\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u611f\u89c9\u200b\u5f88\u200b\u719f\u6089\u200b\uff1b\u200b\u5982\u679c\u200b\u611f\u89c9\u200b\u4e0d\u200b\u719f\u6089\u200b\uff0c\u200b\u81f3\u5c11\u200b\u4f60\u200b\u77e5\u9053\u200b\u8be5\u200b\u4ece\u200b\u54ea\u91cc\u627e\u200b\u8d77\u200b\u3002</p>"},{"location":"#_8","title":"\u5728\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u200b\u5c06\u200b\u6784\u5efa\u200b\u4ec0\u4e48\u200b\uff1f","text":"<p>\u200b\u6211\u4eec\u200b\u4ece\u200b PyTorch \u200b\u548c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u7840\u77e5\u8bc6\u200b\u5f00\u59cb\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5373\u4f7f\u200b\u4f60\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u65b0\u624b\u200b\uff0c\u200b\u4e5f\u200b\u80fd\u200b\u8ddf\u4e0a\u200b\u8fdb\u5ea6\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u63a2\u7d22\u200b\u66f4\u200b\u9ad8\u7ea7\u200b\u7684\u200b\u9886\u57df\u200b\uff0c\u200b\u5305\u62ec\u200b PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b\u200b\u3001PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3001\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u3001\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u3001\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u3001\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6211\u200b\u4e2a\u4eba\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\uff1a\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u79cd\u200b\u5f3a\u5927\u200b\u7684\u200b\u6280\u672f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5728\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u4e0a\u5b66\u200b\u5230\u200b\u7684\u200b\u77e5\u8bc6\u200b\u5e94\u7528\u200b\u5230\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u4e0a\u200b\uff01</p> <p>\u200b\u5728\u200b\u6b64\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u56f4\u7ed5\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b FoodVision \u200b\u7684\u200b\u603b\u4f53\u200b\u9879\u76ee\u200b\u6784\u5efa\u200b\u4e09\u4e2a\u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b\uff0cFoodVision \u200b\u662f\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5206\u7c7b\u200b\u98df\u7269\u200b\u56fe\u50cf\u200b\u7684\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u7ec3\u4e60\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6db5\u76d6\u200b\u91cd\u8981\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6982\u5ff5\u200b\uff0c\u200b\u5e76\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5411\u200b\u96c7\u4e3b\u200b\u5c55\u793a\u200b\u7684\u200b\u4f5c\u54c1\u96c6\u200b\uff0c\u200b\u8bf4\u200b\uff1a\u201c\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b\u505a\u200b\u8fc7\u200b\u7684\u200b\u9879\u76ee\u200b\u201d\u3002</p>"},{"location":"#_9","title":"\u6211\u8be5\u200b\u5982\u4f55\u200b\u5b66\u4e60\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\uff1f","text":"<p>\u200b\u5982\u524d\u6240\u8ff0\u200b\uff0c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u89c6\u9891\u200b\u7248\u672c\u200b\u91c7\u7528\u200b\u5b66\u5f92\u200b\u5f0f\u200b\u6559\u5b66\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u200b\u5199\u200b PyTorch \u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u5199\u200b PyTorch \u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u200b\u5efa\u8bae\u200b\u4f60\u200b\uff1a</p> <ol> <li>\u200b\u8ddf\u7740\u200b\u4ee3\u7801\u200b\u5199\u200b\uff08\u200b\u5982\u679c\u200b\u6709\u200b\u7591\u95ee\u200b\uff0c\u200b\u8fd0\u884c\u200b\u4ee3\u7801\u200b\uff09 - \u200b\u8ddf\u7740\u200b\u4ee3\u7801\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5c3d\u53ef\u80fd\u200b\u81ea\u5df1\u200b\u591a\u5199\u200b\u4e00\u4e9b\u200b\uff0c\u200b\u6301\u7eed\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u76f4\u5230\u200b\u4f60\u200b\u53d1\u73b0\u81ea\u5df1\u200b\u4e0d\u200b\u81ea\u89c9\u200b\u5730\u5199\u200b PyTorch \u200b\u4ee3\u7801\u200b\uff0c\u200b\u90a3\u65f6\u200b\u4f60\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u505c\u6b62\u200b\u4e00\u904d\u200b\u53c8\u200b\u4e00\u904d\u200b\u5730\u5199\u200b\u76f8\u540c\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e86\u200b\u3002</li> <li>\u200b\u63a2\u7d22\u200b\u548c\u200b\u5b9e\u9a8c\u200b\uff08\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff01\uff09 - \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff08\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff09\u200b\u975e\u5e38\u200b\u6ce8\u91cd\u200b\u5b9e\u9a8c\u200b\u3002\u200b\u6240\u4ee5\u200b\u5982\u679c\u200b\u4f60\u200b\u53d1\u73b0\u81ea\u5df1\u200b\u60f3\u200b\u5c1d\u8bd5\u200b\u4e00\u4e9b\u200b\u81ea\u5df1\u200b\u7684\u200b\u60f3\u6cd5\u200b\u5e76\u200b\u5ffd\u7565\u200b\u6750\u6599\u200b\uff0c\u200b\u90a3\u200b\u5c31\u200b\u53bb\u200b\u505a\u200b\u5427\u200b\u3002</li> <li>\u200b\u53ef\u89c6\u5316\u200b\u4f60\u200b\u4e0d\u200b\u7406\u89e3\u200b\u7684\u200b\u5185\u5bb9\u200b\uff08\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01\uff09 - \u200b\u7eb8\u4e0a\u200b\u7684\u200b\u6570\u5b57\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba9\u200b\u4eba\u200b\u56f0\u60d1\u200b\u3002\u200b\u6240\u4ee5\u200b\u8ba9\u200b\u4e8b\u60c5\u200b\u53d8\u5f97\u200b\u591a\u5f69\u200b\uff0c\u200b\u770b\u770b\u200b\u4f60\u200b\u7684\u200b\u4ee3\u7801\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u3002</li> <li>\u200b\u63d0\u95ee\u200b - \u200b\u5982\u679c\u200b\u4f60\u200b\u5361\u4f4f\u200b\u4e86\u200b\uff0c\u200b\u5c31\u200b\u95ee\u4e2a\u95ee\u9898\u200b\uff0c\u200b\u8bd5\u7740\u200b\u641c\u7d22\u200b\u5b83\u200b\uff0c\u200b\u5982\u679c\u200b\u627e\u200b\u4e0d\u5230\u200b\u7b54\u6848\u200b\uff0c\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub Discussions \u200b\u9875\u9762\u200b\u4f1a\u200b\u662f\u200b\u4f60\u200b\u53bb\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</li> <li>\u200b\u5b8c\u6210\u200b\u7ec3\u4e60\u200b - \u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u6a21\u5757\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u4e13\u95e8\u200b\u7684\u200b\u7ec3\u4e60\u200b\u90e8\u5206\u200b\u3002\u200b\u5c1d\u8bd5\u200b\u81ea\u5df1\u200b\u5b8c\u6210\u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u5f88\u200b\u91cd\u8981\u200b\u3002\u200b\u4f60\u200b\u4f1a\u200b\u5361\u4f4f\u200b\u3002\u200b\u4f46\u200b\u8fd9\u200b\u5c31\u662f\u200b\u5b66\u4e60\u200b\u65b0\u200b\u4e8b\u7269\u200b\u7684\u200b\u672c\u8d28\u200b\uff1a\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u90fd\u200b\u4f1a\u200b\u5361\u4f4f\u200b\u3002</li> <li>\u200b\u5206\u4eab\u200b\u4f60\u200b\u7684\u200b\u4f5c\u54c1\u200b - \u200b\u5982\u679c\u200b\u4f60\u200b\u5b66\u5230\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u9177\u200b\u7684\u200b\u4e1c\u897f\u200b\uff0c\u200b\u751a\u81f3\u200b\u66f4\u597d\u200b\uff0c\u200b\u521b\u9020\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u9177\u200b\u7684\u200b\u4e1c\u897f\u200b\uff0c\u200b\u5206\u4eab\u200b\u5b83\u200b\u3002\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b Discord \u200b\u7fa4\u7ec4\u200b\u6216\u200b GitHub \u200b\u9875\u9762\u200b\u4e0a\u200b\uff0c\u200b\u6216\u8005\u200b\u5728\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u3002\u200b\u5206\u4eab\u200b\u4f60\u200b\u7684\u200b\u4f5c\u54c1\u200b\u7684\u200b\u597d\u5904\u200b\u662f\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7ec3\u4e60\u200b\u6c9f\u901a\u200b\uff0c\u200b\u540c\u65f6\u200b\u5982\u679c\u200b\u522b\u4eba\u200b\u4e0d\u200b\u786e\u5b9a\u200b\u67d0\u4e9b\u200b\u4e8b\u60c5\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u3002</li> </ol>"},{"location":"#_10","title":"\u6211\u200b\u9700\u8981\u200b\u6309\u200b\u987a\u5e8f\u200b\u5b66\u4e60\u200b\u5417\u200b\uff1f","text":"<p>\u200b\u7b14\u8bb0\u672c\u200b/\u200b\u7ae0\u8282\u200b\u662f\u200b\u6309\u200b\u987a\u5e8f\u200b\u6784\u5efa\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u968f\u610f\u200b\u8df3\u8dc3\u200b\u3002</p>"},{"location":"#_11","title":"\u6211\u8be5\u200b\u5982\u4f55\u200b\u5f00\u59cb\u200b\uff1f","text":"<p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4efb\u4f55\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u9605\u8bfb\u200b\u6750\u6599\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e2a\u200b\u8bfe\u7a0b\u200b\u6700\u597d\u200b\u5728\u200b\u684c\u9762\u200b\u6d4f\u89c8\u5668\u200b\u4e0a\u200b\u67e5\u770b\u200b\u548c\u200b\u8ddf\u7740\u200b\u4ee3\u7801\u200b\u5199\u200b\u3002</p> <p>\u200b\u8bfe\u7a0b\u200b\u4f7f\u7528\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u514d\u8d39\u200b\u5de5\u5177\u200b\u53eb\u200b Google Colab\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u6ca1\u6709\u200b\u4f7f\u7528\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u5148\u200b\u901a\u8fc7\u200b\u514d\u8d39\u200b\u7684\u200b Google Colab \u200b\u5165\u95e8\u6559\u7a0b\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u56de\u6765\u200b\u3002</p> <p>\u200b\u5f00\u59cb\u200b\u6b65\u9aa4\u200b\uff1a</p> <ol> <li>\u200b\u70b9\u51fb\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u6216\u200b\u7ae0\u8282\u200b\u94fe\u63a5\u200b\uff0c\u200b\u6bd4\u5982\u200b \"00. PyTorch \u200b\u57fa\u7840\u200b\"\u3002</li> <li>\u200b\u70b9\u51fb\u200b\u9876\u90e8\u200b\u7684\u200b \"Open in Colab\" \u200b\u6309\u94ae\u200b\u3002</li> <li>\u200b\u6309\u200b\u51e0\u6b21\u200b SHIFT+Enter\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</li> </ol> <p>\u200b\u795d\u200b\u4f60\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6109\u5feb\u200b\uff01</p>"},{"location":"00_pytorch_fundamentals/","title":"00. PyTorch \u200b\u57fa\u7840","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b | \u200b\u89c2\u770b\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b</p> In\u00a0[1]: Copied! <pre>import torch\ntorch.__version__\n</pre> import torch torch.__version__ Out[1]: <pre>'1.13.1+cu116'</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u4e86\u200b PyTorch 1.10.0 \u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u5b66\u4e60\u200b\u8fd9\u4e9b\u200b\u8d44\u6599\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u5b83\u4eec\u200b\u4e0e\u200b PyTorch 1.10.0 \u200b\u53ca\u200b\u4ee5\u4e0a\u200b\u7248\u672c\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u5185\u5bb9\u200b\u662f\u200b\u517c\u5bb9\u200b\u7684\u200b\u3002\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u7248\u672c\u53f7\u200b\u8fdc\u9ad8\u4e8e\u200b\u6b64\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u4e00\u4e9b\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u4e4b\u200b\u5904\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u4efb\u4f55\u200b\u95ee\u9898\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub \u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b \u200b\u4e0a\u200b\u53d1\u5e16\u200b\u3002</p> In\u00a0[2]: Copied! <pre># Scalar\nscalar = torch.tensor(7)\nscalar\n</pre> # Scalar scalar = torch.tensor(7) scalar Out[2]: <pre>tensor(7)</pre> <p>\u200b\u770b\u770b\u200b\u4e0a\u9762\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u662f\u200b<code>tensor(7)</code>\uff1f</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5c3d\u7ba1\u200b<code>scalar</code>\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u6570\u503c\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u7684\u200b\u7c7b\u578b\u200b\u662f\u200b<code>torch.Tensor</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>ndim</code>\u200b\u5c5e\u6027\u200b\u6765\u200b\u68c0\u67e5\u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002</p> In\u00a0[3]: Copied! <pre>scalar.ndim\n</pre> scalar.ndim Out[3]: <pre>0</pre> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u4ece\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u63d0\u53d6\u200b\u6570\u503c\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5c06\u200b <code>torch.Tensor</code> \u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u6574\u6570\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>item()</code> \u200b\u65b9\u6cd5\u200b\u3002</p> In\u00a0[4]: Copied! <pre># Get the Python number within a tensor (only works with one-element tensors)\nscalar.item()\n</pre> # Get the Python number within a tensor (only works with one-element tensors) scalar.item() Out[4]: <pre>7</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u770b\u200b\u4e00\u4e2a\u200b\u5411\u91cf\u200b\u3002</p> <p>\u200b\u5411\u91cf\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u7ef4\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u6570\u5b57\u200b\u3002</p> <p>\u200b\u6bd4\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7528\u200b\u5411\u91cf\u200b <code>[3, 2]</code> \u200b\u6765\u200b\u63cf\u8ff0\u200b\u4f60\u5bb6\u200b\u7684\u200b <code>[\u200b\u5367\u5ba4\u200b, \u200b\u6d74\u5ba4\u200b]</code>\u3002\u200b\u6216\u8005\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7528\u200b <code>[3, 2, 2]</code> \u200b\u6765\u200b\u63cf\u8ff0\u200b\u4f60\u5bb6\u200b\u7684\u200b <code>[\u200b\u5367\u5ba4\u200b, \u200b\u6d74\u5ba4\u200b, \u200b\u8f66\u4f4d\u200b]</code>\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u91cd\u8981\u200b\u7684\u200b\u8d8b\u52bf\u200b\u662f\u200b\uff0c\u200b\u5411\u91cf\u200b\u5728\u200b\u5b83\u200b\u80fd\u200b\u8868\u793a\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e0a\u200b\u5177\u6709\u200b\u7075\u6d3b\u6027\u200b\uff08\u200b\u5f20\u91cf\u200b\u4e5f\u200b\u662f\u200b\u5982\u6b64\u200b\uff09\u3002</p> In\u00a0[5]: Copied! <pre># Vector\nvector = torch.tensor([7, 7])\nvector\n</pre> # Vector vector = torch.tensor([7, 7]) vector Out[5]: <pre>tensor([7, 7])</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c<code>vector</code> \u200b\u73b0\u5728\u200b\u5305\u542b\u200b\u4e86\u200b\u4e24\u4e2a\u200b 7\uff0c\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\u6570\u5b57\u200b\u3002</p> <p>\u200b\u4f60\u200b\u89c9\u5f97\u200b\u5b83\u4f1a\u200b\u6709\u200b\u591a\u5c11\u200b\u7ef4\u200b\u5462\u200b\uff1f</p> In\u00a0[6]: Copied! <pre># Check the number of dimensions of vector\nvector.ndim\n</pre> # Check the number of dimensions of vector vector.ndim Out[6]: <pre>1</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u8fd9\u200b\u6709\u70b9\u200b\u5947\u602a\u200b\uff0c<code>vector</code> \u200b\u5305\u542b\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u6570\u5b57\u200b\uff0c\u200b\u5374\u200b\u53ea\u6709\u200b\u4e00\u7ef4\u200b\u3002</p> <p>\u200b\u6211\u6765\u200b\u544a\u8bc9\u200b\u4f60\u200b\u4e00\u4e2a\u200b\u5c0f\u7a8d\u95e8\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5916\u4fa7\u200b\u65b9\u62ec\u53f7\u200b\uff08<code>[</code>\uff09\u200b\u7684\u200b\u6570\u91cf\u200b\u6765\u200b\u5224\u65ad\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u5f20\u91cf\u200b\u6709\u200b\u591a\u5c11\u200b\u7ef4\u200b\uff0c\u200b\u4f60\u200b\u53ea\u200b\u9700\u8981\u200b\u6570\u200b\u4e00\u8fb9\u200b\u7684\u200b\u65b9\u62ec\u53f7\u200b\u5373\u53ef\u200b\u3002</p> <p><code>vector</code> \u200b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u200b\u65b9\u62ec\u53f7\u200b\u5462\u200b\uff1f</p> <p>\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b\u5f20\u91cf\u200b\u7684\u200b\u91cd\u8981\u200b\u6982\u5ff5\u200b\u662f\u200b\u5b83\u4eec\u200b\u7684\u200b <code>shape</code> \u200b\u5c5e\u6027\u200b\u3002<code>shape</code> \u200b\u544a\u8bc9\u200b\u4f60\u200b\u8fd9\u4e9b\u200b\u5143\u7d20\u200b\u662f\u200b\u5982\u4f55\u200b\u6392\u5217\u200b\u7684\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b <code>vector</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p> In\u00a0[7]: Copied! <pre># Check shape of vector\nvector.shape\n</pre> # Check shape of vector vector.shape Out[7]: <pre>torch.Size([2])</pre> <p>\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u8fd4\u56de\u200b <code>torch.Size([2])</code>\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u7684\u200b\u5411\u91cf\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[2]</code>\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5728\u200b\u65b9\u62ec\u53f7\u200b\u5185\u200b\u653e\u7f6e\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u5143\u7d20\u200b\uff08<code>[7, 7]</code>\uff09\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u770b\u200b\u4e00\u4e2a\u200b\u77e9\u9635\u200b\u3002</p> In\u00a0[8]: Copied! <pre># Matrix\nMATRIX = torch.tensor([[7, 8], \n                       [9, 10]])\nMATRIX\n</pre> # Matrix MATRIX = torch.tensor([[7, 8],                         [9, 10]]) MATRIX Out[8]: <pre>tensor([[ 7,  8],\n        [ 9, 10]])</pre> <p>\u200b\u54c7\u200b\uff01\u200b\u66f4\u200b\u591a\u200b\u6570\u5b57\u200b\uff01\u200b\u77e9\u9635\u200b\u548c\u200b\u5411\u91cf\u200b\u4e00\u6837\u200b\u7075\u6d3b\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u5b83\u4eec\u200b\u591a\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u3002</p> In\u00a0[9]: Copied! <pre># Check number of dimensions\nMATRIX.ndim\n</pre> # Check number of dimensions MATRIX.ndim Out[9]: <pre>2</pre> <p><code>MATRIX</code> \u200b\u6709\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\uff08\u200b\u4f60\u200b\u662f\u5426\u200b\u6570\u8fc7\u200b\u4e00\u4fa7\u200b\u5916\u9762\u200b\u7684\u200b\u65b9\u62ec\u53f7\u200b\u6570\u91cf\u200b\uff1f\uff09\u3002</p> <p>\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u5b83\u4f1a\u200b\u6709\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b <code>shape</code>\uff1f</p> In\u00a0[10]: Copied! <pre>MATRIX.shape\n</pre> MATRIX.shape Out[10]: <pre>torch.Size([2, 2])</pre> <p>\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u8f93\u51fa\u200b <code>torch.Size([2, 2])</code>\uff0c\u200b\u56e0\u4e3a\u200b <code>MATRIX</code> \u200b\u6709\u200b\u4e24\u5c42\u200b\u5143\u7d20\u200b\u4e14\u200b\u6bcf\u5c42\u200b\u6709\u200b\u4e24\u884c\u200b\u3002</p> <p>\u200b\u90a3\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u5462\u200b\uff1f</p> In\u00a0[11]: Copied! <pre># Tensor\nTENSOR = torch.tensor([[[1, 2, 3],\n                        [3, 6, 9],\n                        [2, 4, 5]]])\nTENSOR\n</pre> # Tensor TENSOR = torch.tensor([[[1, 2, 3],                         [3, 6, 9],                         [2, 4, 5]]]) TENSOR Out[11]: <pre>tensor([[[1, 2, 3],\n         [3, 6, 9],\n         [2, 4, 5]]])</pre> <p>\u200b\u54c7\u200b\uff01\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u6211\u8981\u200b\u5f3a\u8c03\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u5f20\u91cf\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u521b\u5efa\u200b\u7684\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u662f\u200b\u725b\u6392\u200b\u548c\u200b\u674f\u4ec1\u200b\u9ec4\u6cb9\u200b\u5e97\u200b\u7684\u200b\u9500\u552e\u200b\u6570\u636e\u200b\uff08\u200b\u8fd9\u200b\u4e24\u6837\u200b\u662f\u200b\u6211\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\u98df\u7269\u200b\uff09\u3002</p> <p></p> <p>\u200b\u4f60\u200b\u89c9\u5f97\u200b\u5b83\u200b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u200b\u7ef4\u5ea6\u200b\uff1f\uff08\u200b\u63d0\u793a\u200b\uff1a\u200b\u4f7f\u7528\u200b\u65b9\u62ec\u53f7\u200b\u8ba1\u6570\u6cd5\u200b\uff09</p> In\u00a0[12]: Copied! <pre># Check number of dimensions for TENSOR\nTENSOR.ndim\n</pre> # Check number of dimensions for TENSOR TENSOR.ndim Out[12]: <pre>3</pre> <p>\u200b\u81f3\u4e8e\u200b\u5b83\u200b\u7684\u200b\u5f62\u72b6\u200b\u5462\u200b\uff1f</p> In\u00a0[13]: Copied! <pre># Check shape of TENSOR\nTENSOR.shape\n</pre> # Check shape of TENSOR TENSOR.shape Out[13]: <pre>torch.Size([1, 3, 3])</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u5b83\u200b\u8f93\u51fa\u200b\u4e86\u200b <code>torch.Size([1, 3, 3])</code>\u3002</p> <p>\u200b\u7ef4\u5ea6\u200b\u662f\u4ece\u200b\u5916\u5230\u200b\u5185\u200b\u7684\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6709\u200b\u4e00\u4e2a\u200b 3x3 \u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002</p> <p></p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u80fd\u200b\u6ce8\u610f\u200b\u5230\u200b\u6211\u200b\u7528\u200b\u5c0f\u5199\u5b57\u6bcd\u200b\u8868\u793a\u200b <code>scalar</code> \u200b\u548c\u200b <code>vector</code>\uff0c\u200b\u7528\u200b\u5927\u5199\u5b57\u6bcd\u200b\u8868\u793a\u200b <code>MATRIX</code> \u200b\u548c\u200b <code>TENSOR</code>\u3002\u200b\u8fd9\u662f\u200b\u6709\u610f\u200b\u4e3a\u200b\u4e4b\u200b\u7684\u200b\u3002\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u4f1a\u200b\u770b\u5230\u200b\u6807\u91cf\u200b\u548c\u200b\u5411\u91cf\u200b\u7528\u200b\u5c0f\u5199\u5b57\u6bcd\u200b\u8868\u793a\u200b\uff0c\u200b\u5982\u200b <code>y</code> \u200b\u6216\u200b <code>a</code>\u3002\u200b\u800c\u200b\u77e9\u9635\u200b\u548c\u200b\u5f20\u91cf\u200b\u7528\u200b\u5927\u5199\u5b57\u6bcd\u200b\u8868\u793a\u200b\uff0c\u200b\u5982\u200b <code>X</code> \u200b\u6216\u200b <code>W</code>\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u8fd8\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u77e9\u9635\u200b\u548c\u200b\u5f20\u91cf\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u8bcd\u200b\u53ef\u4ee5\u200b\u4e92\u6362\u200b\u4f7f\u7528\u200b\u3002\u200b\u8fd9\u662f\u200b\u5e38\u89c1\u200b\u7684\u200b\u505a\u6cd5\u200b\u3002\u200b\u56e0\u4e3a\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b <code>torch.Tensor</code>\uff08\u200b\u56e0\u6b64\u200b\u5f97\u540d\u200b\u5f20\u91cf\u200b\uff09\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u5176\u200b\u5185\u90e8\u200b\u7684\u200b\u5f62\u72b6\u200b\u548c\u200b\u7ef4\u5ea6\u200b\u5c06\u200b\u51b3\u5b9a\u200b\u5b83\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\u3002</p> \u200b\u540d\u79f0\u200b \u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f \u200b\u7ef4\u5ea6\u200b\u6570\u91cf\u200b \u200b\u901a\u5e38\u200b/\u200b\u793a\u4f8b\u200b\uff08\u200b\u5c0f\u5199\u200b\u6216\u200b\u5927\u5199\u200b\uff09 \u200b\u6807\u91cf\u200b \u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b 0 \u200b\u5c0f\u5199\u200b (<code>a</code>) \u200b\u5411\u91cf\u200b \u200b\u5e26\u6709\u200b\u65b9\u5411\u200b\u7684\u200b\u6570\u5b57\u200b\uff08\u200b\u4f8b\u5982\u200b\u98ce\u901f\u200b\u548c\u200b\u65b9\u5411\u200b\uff09\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u6570\u5b57\u200b 1 \u200b\u5c0f\u5199\u200b (<code>y</code>) \u200b\u77e9\u9635\u200b \u200b\u4e8c\u7ef4\u200b\u6570\u5b57\u200b\u6570\u7ec4\u200b 2 \u200b\u5927\u5199\u200b (<code>Q</code>) \u200b\u5f20\u91cf\u200b \u200b\u591a\u7ef4\u200b\u6570\u5b57\u200b\u6570\u7ec4\u200b \u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u610f\u200b\u6570\u91cf\u200b\uff0c0 \u200b\u7ef4\u200b\u5f20\u91cf\u200b\u662f\u200b\u6807\u91cf\u200b\uff0c1 \u200b\u7ef4\u200b\u5f20\u91cf\u200b\u662f\u200b\u5411\u91cf\u200b \u200b\u5927\u5199\u200b (<code>X</code>) <p></p> In\u00a0[14]: Copied! <pre># Create a random tensor of size (3, 4)\nrandom_tensor = torch.rand(size=(3, 4))\nrandom_tensor, random_tensor.dtype\n</pre> # Create a random tensor of size (3, 4) random_tensor = torch.rand(size=(3, 4)) random_tensor, random_tensor.dtype Out[14]: <pre>(tensor([[0.6541, 0.4807, 0.2162, 0.6168],\n         [0.4428, 0.6608, 0.6194, 0.8620],\n         [0.2795, 0.6055, 0.4958, 0.5483]]),\n torch.float32)</pre> <p><code>torch.rand()</code> \u200b\u7684\u200b\u7075\u6d3b\u6027\u200b\u5728\u4e8e\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b <code>size</code> \u200b\u8c03\u6574\u200b\u4e3a\u200b\u4efb\u4f55\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u4f60\u200b\u60f3\u8981\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b <code>[224, 224, 3]</code>\uff08\u200b\u5373\u200b <code>[\u200b\u9ad8\u5ea6\u200b, \u200b\u5bbd\u5ea6\u200b, \u200b\u989c\u8272\u200b\u901a\u9053\u200b]</code>\uff09\u3002</p> In\u00a0[15]: Copied! <pre># Create a random tensor of size (224, 224, 3)\nrandom_image_size_tensor = torch.rand(size=(224, 224, 3))\nrandom_image_size_tensor.shape, random_image_size_tensor.ndim\n</pre> # Create a random tensor of size (224, 224, 3) random_image_size_tensor = torch.rand(size=(224, 224, 3)) random_image_size_tensor.shape, random_image_size_tensor.ndim Out[15]: <pre>(torch.Size([224, 224, 3]), 3)</pre> In\u00a0[16]: Copied! <pre># Create a tensor of all zeros\nzeros = torch.zeros(size=(3, 4))\nzeros, zeros.dtype\n</pre> # Create a tensor of all zeros zeros = torch.zeros(size=(3, 4)) zeros, zeros.dtype Out[16]: <pre>(tensor([[0., 0., 0., 0.],\n         [0., 0., 0., 0.],\n         [0., 0., 0., 0.]]),\n torch.float32)</pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>torch.ones()</code>\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5168\u4e3a\u200b1\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> In\u00a0[17]: Copied! <pre># Create a tensor of all ones\nones = torch.ones(size=(3, 4))\nones, ones.dtype\n</pre> # Create a tensor of all ones ones = torch.ones(size=(3, 4)) ones, ones.dtype Out[17]: <pre>(tensor([[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]),\n torch.float32)</pre> In\u00a0[18]: Copied! <pre># Use torch.arange(), torch.range() is deprecated \nzero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n\n# Create a range of values 0 to 10\nzero_to_ten = torch.arange(start=0, end=10, step=1)\nzero_to_ten\n</pre> # Use torch.arange(), torch.range() is deprecated  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future  # Create a range of values 0 to 10 zero_to_ten = torch.arange(start=0, end=10, step=1) zero_to_ten <pre>/tmp/ipykernel_3695928/193451495.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n</pre> Out[18]: <pre>tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])</pre> <p>\u200b\u6709\u65f6\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\u7c7b\u578b\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5176\u200b\u5f62\u72b6\u200b\u4e0e\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u5148\u524d\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\u7684\u200b\u3001\u200b\u6240\u6709\u200b\u5143\u7d20\u200b\u5747\u200b\u4e3a\u200b\u96f6\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.zeros_like(input)</code> \u200b\u6216\u200b <code>torch.ones_like(input)</code>\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u51fd\u6570\u200b\u5206\u522b\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e0e\u200b <code>input</code> \u200b\u76f8\u540c\u200b\u3001\u200b\u5143\u7d20\u200b\u5168\u4e3a\u200b\u96f6\u200b\u6216\u200b\u5168\u4e3a\u200b1\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> In\u00a0[19]: Copied! <pre># Can also create a tensor of zeros similar to another tensor\nten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\nten_zeros\n</pre> # Can also create a tensor of zeros similar to another tensor ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape ten_zeros Out[19]: <pre>tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])</pre> In\u00a0[20]: Copied! <pre># Default datatype for tensors is float32\nfloat_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n                               device=None, # defaults to None, which uses the default tensor type\n                               requires_grad=False) # if True, operations performed on the tensor are recorded \n\nfloat_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device\n</pre> # Default datatype for tensors is float32 float_32_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed                                device=None, # defaults to None, which uses the default tensor type                                requires_grad=False) # if True, operations performed on the tensor are recorded   float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device Out[20]: <pre>(torch.Size([3]), torch.float32, device(type='cpu'))</pre> <p>\u200b\u9664\u4e86\u200b\u5f62\u72b6\u200b\u95ee\u9898\u200b\uff08\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\uff09\uff0c\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u4f60\u200b\u8fd8\u200b\u4f1a\u200b\u7ecf\u5e38\u200b\u9047\u5230\u200b\u53e6\u5916\u200b\u4e24\u79cd\u200b\u5e38\u89c1\u95ee\u9898\u200b\uff1a\u200b\u6570\u636e\u7c7b\u578b\u200b\u548c\u200b\u8bbe\u5907\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u662f\u200b <code>torch.float32</code> \u200b\u800c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u662f\u200b <code>torch.float16</code>\uff08PyTorch\u200b\u901a\u5e38\u200b\u5e0c\u671b\u200b\u5f20\u91cf\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u683c\u5f0f\u200b\uff09\u3002</p> <p>\u200b\u6216\u8005\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\uff0c\u200b\u800c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\uff08PyTorch\u200b\u5e0c\u671b\u200b\u5f20\u91cf\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5728\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fdb\u884c\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u66f4\u200b\u591a\u200b\u5730\u200b\u8ba8\u8bba\u200b\u8bbe\u5907\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>dtype=torch.float16</code> \u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> In\u00a0[21]: Copied! <pre>float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n                               dtype=torch.float16) # torch.half would also work\n\nfloat_16_tensor.dtype\n</pre> float_16_tensor = torch.tensor([3.0, 6.0, 9.0],                                dtype=torch.float16) # torch.half would also work  float_16_tensor.dtype Out[21]: <pre>torch.float16</pre> In\u00a0[22]: Copied! <pre># Create a tensor\nsome_tensor = torch.rand(3, 4)\n\n# Find out details about it\nprint(some_tensor)\nprint(f\"Shape of tensor: {some_tensor.shape}\")\nprint(f\"Datatype of tensor: {some_tensor.dtype}\")\nprint(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU\n</pre> # Create a tensor some_tensor = torch.rand(3, 4)  # Find out details about it print(some_tensor) print(f\"Shape of tensor: {some_tensor.shape}\") print(f\"Datatype of tensor: {some_tensor.dtype}\") print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU <pre>tensor([[0.4688, 0.0055, 0.8551, 0.0646],\n        [0.6538, 0.5157, 0.4071, 0.2109],\n        [0.9960, 0.3061, 0.9369, 0.7008]])\nShape of tensor: torch.Size([3, 4])\nDatatype of tensor: torch.float32\nDevice tensor is stored on: cpu\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\u65f6\u200b\uff0c\u200b\u5f88\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e0e\u200b\u4e0a\u8ff0\u200b\u4e09\u4e2a\u200b\u5c5e\u6027\u200b\u4e4b\u4e00\u200b\u6709\u5173\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5f53\u200b\u9519\u8bef\u4fe1\u606f\u200b\u51fa\u73b0\u200b\u65f6\u200b\uff0c\u200b\u7ed9\u200b\u81ea\u5df1\u200b\u5531\u200b\u4e00\u9996\u200b\u5c0f\u6b4c\u200b\uff0c\u200b\u53eb\u505a\u200b\u201c\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u54ea\u91cc\u200b\u201d\uff1a</p> <ul> <li>\u201c\u200b\u6211\u200b\u7684\u200b\u5f20\u91cf\u200b\u662f\u200b\u4ec0\u4e48\u200b\u5f62\u72b6\u200b\uff1f\u200b\u5b83\u4eec\u200b\u7684\u200b\u7c7b\u578b\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u5b58\u50a8\u200b\u5728\u200b\u54ea\u91cc\u200b\uff1f\u200b\u4ec0\u4e48\u200b\u5f62\u72b6\u200b\uff0c\u200b\u4ec0\u4e48\u200b\u7c7b\u578b\u200b\uff0c\u200b\u54ea\u91cc\u200b\u54ea\u91cc\u200b\u54ea\u91cc\u200b\u201d</li> </ul> In\u00a0[23]: Copied! <pre># Create a tensor of values and add a number to it\ntensor = torch.tensor([1, 2, 3])\ntensor + 10\n</pre> # Create a tensor of values and add a number to it tensor = torch.tensor([1, 2, 3]) tensor + 10 Out[23]: <pre>tensor([11, 12, 13])</pre> In\u00a0[24]: Copied! <pre># Multiply it by 10\ntensor * 10\n</pre> # Multiply it by 10 tensor * 10 Out[24]: <pre>tensor([10, 20, 30])</pre> <p>\u200b\u6ce8\u610f\u200b\u4e0a\u9762\u200b\u5f20\u91cf\u200b\u7684\u200b\u503c\u200b\u5e76\u200b\u6ca1\u6709\u200b\u53d8\u6210\u200b <code>tensor([110, 120, 130])</code>\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u5f20\u91cf\u200b\u5185\u90e8\u200b\u7684\u200b\u503c\u200b\u4e0d\u4f1a\u200b\u6539\u53d8\u200b\uff0c\u200b\u9664\u975e\u200b\u5b83\u4eec\u200b\u88ab\u200b\u91cd\u65b0\u200b\u8d4b\u503c\u200b\u3002</p> In\u00a0[25]: Copied! <pre># Tensors don't change unless reassigned\ntensor\n</pre> # Tensors don't change unless reassigned tensor Out[25]: <pre>tensor([1, 2, 3])</pre> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u51cf\u53bb\u200b\u4e00\u4e2a\u200b\u6570\u200b\uff0c\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u65b0\u200b\u8d4b\u503c\u200b\u7ed9\u200b <code>tensor</code> \u200b\u53d8\u91cf\u200b\u3002</p> In\u00a0[26]: Copied! <pre># Subtract and reassign\ntensor = tensor - 10\ntensor\n</pre> # Subtract and reassign tensor = tensor - 10 tensor Out[26]: <pre>tensor([-9, -8, -7])</pre> In\u00a0[27]: Copied! <pre># Add and reassign\ntensor = tensor + 10\ntensor\n</pre> # Add and reassign tensor = tensor + 10 tensor Out[27]: <pre>tensor([1, 2, 3])</pre> <p>PyTorch \u200b\u4e5f\u200b\u5185\u7f6e\u200b\u4e86\u200b\u8bb8\u591a\u200b\u51fd\u6570\u200b\uff0c\u200b\u5982\u200b <code>torch.mul()</code>\uff08\u200b\u4e58\u6cd5\u200b\u7b80\u5199\u200b\uff09\u200b\u548c\u200b <code>torch.add()</code>\uff0c\u200b\u7528\u4e8e\u200b\u6267\u884c\u200b\u57fa\u672c\u200b\u8fd0\u7b97\u200b\u3002</p> In\u00a0[28]: Copied! <pre># Can also use torch functions\ntorch.multiply(tensor, 10)\n</pre> # Can also use torch functions torch.multiply(tensor, 10) Out[28]: <pre>tensor([10, 20, 30])</pre> In\u00a0[29]: Copied! <pre># Original tensor is still unchanged \ntensor\n</pre> # Original tensor is still unchanged  tensor Out[29]: <pre>tensor([1, 2, 3])</pre> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u66f4\u200b\u5e38\u89c1\u200b\u7684\u200b\u662f\u200b\u4f7f\u7528\u200b\u8fd0\u7b97\u7b26\u200b\u7b26\u53f7\u200b\uff0c\u200b\u5982\u200b <code>*</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>torch.mul()</code>\u3002</p> In\u00a0[30]: Copied! <pre># Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2)\nprint(tensor, \"*\", tensor)\nprint(\"Equals:\", tensor * tensor)\n</pre> # Element-wise multiplication (each element multiplies its equivalent, index 0-&gt;0, 1-&gt;1, 2-&gt;2) print(tensor, \"*\", tensor) print(\"Equals:\", tensor * tensor) <pre>tensor([1, 2, 3]) * tensor([1, 2, 3])\nEquals: tensor([1, 4, 9])\n</pre> In\u00a0[31]: Copied! <pre>import torch\ntensor = torch.tensor([1, 2, 3])\ntensor.shape\n</pre> import torch tensor = torch.tensor([1, 2, 3]) tensor.shape Out[31]: <pre>torch.Size([3])</pre> <p>\u200b\u5143\u7d20\u200b\u95f4\u200b\u4e58\u6cd5\u200b\u4e0e\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b\u503c\u200b\u7684\u200b\u76f8\u52a0\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u503c\u4e3a\u200b <code>[1, 2, 3]</code> \u200b\u7684\u200b <code>tensor</code> \u200b\u53d8\u91cf\u200b\uff1a</p> \u200b\u64cd\u4f5c\u200b \u200b\u8ba1\u7b97\u200b \u200b\u4ee3\u7801\u200b \u200b\u5143\u7d20\u200b\u95f4\u200b\u4e58\u6cd5\u200b <code>[1*1, 2*2, 3*3]</code> = <code>[1, 4, 9]</code> <code>tensor * tensor</code> \u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b <code>[1*1 + 2*2 + 3*3]</code> = <code>[14]</code> <code>tensor.matmul(tensor)</code> In\u00a0[32]: Copied! <pre># Element-wise matrix multiplication\ntensor * tensor\n</pre> # Element-wise matrix multiplication tensor * tensor Out[32]: <pre>tensor([1, 4, 9])</pre> In\u00a0[33]: Copied! <pre># Matrix multiplication\ntorch.matmul(tensor, tensor)\n</pre> # Matrix multiplication torch.matmul(tensor, tensor) Out[33]: <pre>tensor(14)</pre> In\u00a0[34]: Copied! <pre># Can also use the \"@\" symbol for matrix multiplication, though not recommended\ntensor @ tensor\n</pre> # Can also use the \"@\" symbol for matrix multiplication, though not recommended tensor @ tensor Out[34]: <pre>tensor(14)</pre> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u624b\u52a8\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u5e76\u200b\u4e0d\u200b\u63a8\u8350\u200b\u8fd9\u6837\u200b\u505a\u200b\u3002</p> <p>\u200b\u5185\u7f6e\u200b\u7684\u200b <code>torch.matmul()</code> \u200b\u65b9\u6cd5\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\u3002</p> In\u00a0[35]: Copied! <pre>%%time\n# Matrix multiplication by hand \n# (avoid doing operations with for loops at all cost, they are computationally expensive)\nvalue = 0\nfor i in range(len(tensor)):\n  value += tensor[i] * tensor[i]\nvalue\n</pre> %%time # Matrix multiplication by hand  # (avoid doing operations with for loops at all cost, they are computationally expensive) value = 0 for i in range(len(tensor)):   value += tensor[i] * tensor[i] value <pre>CPU times: user 773 \u00b5s, sys: 0 ns, total: 773 \u00b5s\nWall time: 499 \u00b5s\n</pre> Out[35]: <pre>tensor(14)</pre> In\u00a0[36]: Copied! <pre>%%time\ntorch.matmul(tensor, tensor)\n</pre> %%time torch.matmul(tensor, tensor) <pre>CPU times: user 146 \u00b5s, sys: 83 \u00b5s, total: 229 \u00b5s\nWall time: 171 \u00b5s\n</pre> Out[36]: <pre>tensor(14)</pre> In\u00a0[37]: Copied! <pre># Shapes need to be in the right way  \ntensor_A = torch.tensor([[1, 2],\n                         [3, 4],\n                         [5, 6]], dtype=torch.float32)\n\ntensor_B = torch.tensor([[7, 10],\n                         [8, 11], \n                         [9, 12]], dtype=torch.float32)\n\ntorch.matmul(tensor_A, tensor_B) # (this will error)\n</pre> # Shapes need to be in the right way   tensor_A = torch.tensor([[1, 2],                          [3, 4],                          [5, 6]], dtype=torch.float32)  tensor_B = torch.tensor([[7, 10],                          [8, 11],                           [9, 12]], dtype=torch.float32)  torch.matmul(tensor_A, tensor_B) # (this will error) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 75 in &lt;cell line: 10&gt;()\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'&gt;2&lt;/a&gt; tensor_A = torch.tensor([[1, 2],\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'&gt;3&lt;/a&gt;                          [3, 4],\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'&gt;4&lt;/a&gt;                          [5, 6]], dtype=torch.float32)\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'&gt;6&lt;/a&gt; tensor_B = torch.tensor([[7, 10],\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'&gt;7&lt;/a&gt;                          [8, 11], \n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'&gt;8&lt;/a&gt;                          [9, 12]], dtype=torch.float32)\n---&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'&gt;10&lt;/a&gt; torch.matmul(tensor_A, tensor_B)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)</pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u200b <code>tensor_A</code> \u200b\u548c\u200b <code>tensor_B</code> \u200b\u7684\u200b\u5185\u7ef4\u200b\u5339\u914d\u200b\u6765\u200b\u5b9e\u73b0\u200b\u5b83\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4e4b\u4e00\u200b\u662f\u200b\u4f7f\u7528\u200b\u8f6c\u7f6e\u200b\uff08\u200b\u4ea4\u6362\u200b\u7ed9\u5b9a\u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff09\u3002</p> <p>\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u4efb\u4e00\u200b\u65b9\u6cd5\u200b\u8fdb\u884c\u200b\u8f6c\u7f6e\u200b\uff1a</p> <ul> <li><code>torch.transpose(input, dim0, dim1)</code> - \u200b\u5176\u4e2d\u200b <code>input</code> \u200b\u662f\u200b\u9700\u8981\u200b\u8f6c\u7f6e\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c<code>dim0</code> \u200b\u548c\u200b <code>dim1</code> \u200b\u662f\u200b\u8981\u200b\u4ea4\u6362\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002</li> <li><code>tensor.T</code> - \u200b\u5176\u4e2d\u200b <code>tensor</code> \u200b\u662f\u200b\u9700\u8981\u200b\u8f6c\u7f6e\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</li> </ul> <p>\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u540e\u8005\u200b\u3002</p> In\u00a0[38]: Copied! <pre># View tensor_A and tensor_B\nprint(tensor_A)\nprint(tensor_B)\n</pre> # View tensor_A and tensor_B print(tensor_A) print(tensor_B) <pre>tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7., 10.],\n        [ 8., 11.],\n        [ 9., 12.]])\n</pre> In\u00a0[39]: Copied! <pre># View tensor_A and tensor_B.T\nprint(tensor_A)\nprint(tensor_B.T)\n</pre> # View tensor_A and tensor_B.T print(tensor_A) print(tensor_B.T) <pre>tensor([[1., 2.],\n        [3., 4.],\n        [5., 6.]])\ntensor([[ 7.,  8.,  9.],\n        [10., 11., 12.]])\n</pre> In\u00a0[40]: Copied! <pre># The operation works when tensor_B is transposed\nprint(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\nprint(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\nprint(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\")\nprint(\"Output:\\n\")\noutput = torch.matmul(tensor_A, tensor_B.T)\nprint(output) \nprint(f\"\\nOutput shape: {output.shape}\")\n</pre> # The operation works when tensor_B is transposed print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\") print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\") print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} &lt;- inner dimensions match\\n\") print(\"Output:\\n\") output = torch.matmul(tensor_A, tensor_B.T) print(output)  print(f\"\\nOutput shape: {output.shape}\") <pre>Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n\nNew shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n\nMultiplying: torch.Size([3, 2]) * torch.Size([2, 3]) &lt;- inner dimensions match\n\nOutput:\n\ntensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])\n\nOutput shape: torch.Size([3, 3])\n</pre> <p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.mm()</code>\uff0c\u200b\u5b83\u200b\u662f\u200b <code>torch.matmul()</code> \u200b\u7684\u200b\u7b80\u5199\u200b\u5f62\u5f0f\u200b\u3002</p> In\u00a0[41]: Copied! <pre># torch.mm is a shortcut for matmul\ntorch.mm(tensor_A, tensor_B.T)\n</pre> # torch.mm is a shortcut for matmul torch.mm(tensor_A, tensor_B.T) Out[41]: <pre>tensor([[ 27.,  30.,  33.],\n        [ 61.,  68.,  75.],\n        [ 95., 106., 117.]])</pre> <p>\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\u8f6c\u7f6e\u200b\uff0c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u89c4\u5219\u200b\u5c31\u200b\u65e0\u6cd5\u200b\u6ee1\u8db3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5f97\u5230\u200b\u5982\u200b\u4e0a\u200b\u6240\u793a\u200b\u7684\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u6765\u4e2a\u200b\u89c6\u89c9\u200b\u6f14\u793a\u200b\u600e\u4e48\u6837\u200b\uff1f</p> <p></p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u521b\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u89c6\u89c9\u200b\u6f14\u793a\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u79cd\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u4e24\u4e2a\u200b\u77e9\u9635\u200b\u7684\u200b\u70b9\u79ef\u200b\u3002</p> <p>\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u5145\u6ee1\u200b\u4e86\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u548c\u200b\u70b9\u79ef\u200b\u8fd0\u7b97\u200b\u3002</p> <p><code>torch.nn.Linear()</code> \u200b\u6a21\u5757\u200b\uff08\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u770b\u5230\u200b\u5b83\u200b\u7684\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\uff09\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u524d\u9988\u200b\u5c42\u200b\u6216\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u8f93\u5165\u200b <code>x</code> \u200b\u4e0e\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b <code>A</code> \u200b\u4e4b\u95f4\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>$$ y = x \\cdot A^T + b $$</p> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>x</code> \u200b\u662f\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\uff08\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u662f\u200b\u7531\u200b\u591a\u4e2a\u200b\u5c42\u200b\uff08\u200b\u5982\u200b <code>torch.nn.Linear()</code> \u200b\u548c\u200b\u5176\u4ed6\u200b\u5c42\u200b\uff09\u200b\u5806\u53e0\u200b\u800c\u6210\u200b\u7684\u200b\uff09\u3002</li> <li><code>A</code> \u200b\u662f\u200b\u7531\u5c42\u200b\u521b\u5efa\u200b\u7684\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\uff0c\u200b\u521d\u59cb\u200b\u65f6\u4e3a\u200b\u968f\u673a\u6570\u200b\uff0c\u200b\u968f\u7740\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5b66\u4e60\u200b\u66f4\u597d\u200b\u5730\u200b\u8868\u793a\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\u800c\u200b\u8c03\u6574\u200b\uff08\u200b\u6ce8\u610f\u200b\u201c<code>T</code>\u201d\uff0c\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u88ab\u200b\u8f6c\u7f6e\u200b\u4e86\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u80fd\u200b\u7ecf\u5e38\u200b\u770b\u5230\u200b\u7528\u200b <code>W</code> \u200b\u6216\u200b\u5176\u4ed6\u200b\u5b57\u6bcd\u200b\u5982\u200b <code>X</code> \u200b\u6765\u200b\u8868\u793a\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u3002</li> </ul> </li> <li><code>b</code> \u200b\u662f\u200b\u7528\u4e8e\u200b\u7a0d\u5fae\u200b\u504f\u79fb\u200b\u6743\u91cd\u200b\u548c\u200b\u8f93\u5165\u200b\u7684\u200b\u504f\u7f6e\u200b\u9879\u200b\u3002</li> <li><code>y</code> \u200b\u662f\u200b\u8f93\u51fa\u200b\uff08\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u8f93\u5165\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\u4ee5\u671f\u200b\u53d1\u73b0\u200b\u5176\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff09\u3002</li> </ul> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff08\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5728\u200b\u9ad8\u4e2d\u200b\u6216\u200b\u5176\u4ed6\u200b\u5730\u65b9\u200b\u89c1\u8fc7\u200b\u7c7b\u4f3c\u200b $y = mx + b$ \u200b\u7684\u200b\u5f62\u5f0f\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u7ed8\u5236\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u73a9\u200b\u4e00\u73a9\u200b\u7ebf\u6027\u200b\u5c42\u200b\u3002</p> <p>\u200b\u5c1d\u8bd5\u200b\u66f4\u6539\u200b\u4e0b\u9762\u200b <code>in_features</code> \u200b\u548c\u200b <code>out_features</code> \u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u4f60\u200b\u6ce8\u610f\u200b\u5230\u200b\u5f62\u72b6\u200b\u65b9\u9762\u200b\u6709\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\u5417\u200b\uff1f</p> In\u00a0[42]: Copied! <pre># Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\ntorch.manual_seed(42)\n# This uses matrix multiplication\nlinear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n                         out_features=6) # out_features = describes outer value \nx = tensor_A\noutput = linear(x)\nprint(f\"Input shape: {x.shape}\\n\")\nprint(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")\n</pre> # Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later) torch.manual_seed(42) # This uses matrix multiplication linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input                           out_features=6) # out_features = describes outer value  x = tensor_A output = linear(x) print(f\"Input shape: {x.shape}\\n\") print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\") <pre>Input shape: torch.Size([3, 2])\n\nOutput:\ntensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n       grad_fn=&lt;AddmmBackward0&gt;)\n\nOutput shape: torch.Size([3, 6])\n</pre> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u5982\u679c\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u4e2d\u5c06\u200b <code>in_features</code> \u200b\u4ece\u200b 2 \u200b\u6539\u4e3a\u200b 3\uff0c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u60c5\u51b5\u200b\uff1f\u200b\u662f\u5426\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u9519\u8bef\u200b\uff1f\u200b\u5982\u4f55\u200b\u66f4\u6539\u200b\u8f93\u5165\u200b (<code>x</code>) \u200b\u7684\u200b\u5f62\u72b6\u200b\u4ee5\u200b\u9002\u5e94\u200b\u8fd9\u79cd\u200b\u9519\u8bef\u200b\uff1f\u200b\u63d0\u793a\u200b\uff1a\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b <code>tensor_B</code> \u200b\u4e0a\u200b\u505a\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u5982\u679c\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u4e2d\u5c06\u200b <code>in_features</code> \u200b\u4ece\u200b 2 \u200b\u6539\u4e3a\u200b 3\uff0c\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u9519\u8bef\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b <code>x</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0e\u200b\u65b0\u200b\u7684\u200b <code>in_features</code> \u200b\u4e0d\u200b\u5339\u914d\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b <code>x</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b <code>(batch_size, 2)</code>\uff0c\u200b\u800c\u200b <code>in_features</code> \u200b\u6539\u4e3a\u200b 3\uff0c\u200b\u90a3\u4e48\u200b <code>x</code> \u200b\u7684\u200b\u5217\u6570\u200b\uff08\u200b\u5373\u200b\u7279\u5f81\u200b\u6570\u200b\uff09\u200b\u4e0e\u200b <code>in_features</code> \u200b\u4e0d\u200b\u4e00\u81f4\u200b\uff0c\u200b\u4ece\u800c\u200b\u5f15\u53d1\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u8c03\u6574\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b <code>x</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u4e0e\u200b\u65b0\u200b\u7684\u200b <code>in_features</code> \u200b\u5339\u914d\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u5c06\u200b <code>x</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u4ece\u200b <code>(batch_size, 2)</code> \u200b\u6539\u4e3a\u200b <code>(batch_size, 3)</code>\u3002\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5bf9\u200b <code>x</code> \u200b\u8fdb\u884c\u200b\u9002\u5f53\u200b\u7684\u200b\u53d8\u6362\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b <code>tensor_B</code> \u200b\u4e0a\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(batch_size, 2)</code> \u200b\u7684\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b <code>x</code>\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u5217\u6765\u200b\u5c06\u200b\u5176\u200b\u5f62\u72b6\u200b\u6539\u4e3a\u200b <code>(batch_size, 3)</code>\u3002\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u5b9e\u73b0\u200b\uff1a</p> <pre>import torch\n\n# \u200b\u5047\u8bbe\u200b x \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b (batch_size, 2) \u200b\u7684\u200b\u5f20\u91cf\u200b\nx = torch.randn(3, 2)  # \u200b\u793a\u4f8b\u200b\u8f93\u5165\u200b\n\n# \u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u5217\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u5f62\u72b6\u200b\u53d8\u4e3a\u200b (batch_size, 3)\nx_expanded = torch.cat((x, torch.zeros(3, 1)), dim=1)\n\n# \u200b\u73b0\u5728\u200b x_expanded \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b (3, 3)\uff0c\u200b\u53ef\u4ee5\u200b\u4e0e\u200b in_features=3 \u200b\u7684\u200b\u7ebf\u6027\u200b\u5c42\u200b\u5339\u914d\u200b\n</pre> <p>\u200b\u901a\u8fc7\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u8f93\u5165\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4ee5\u200b\u9002\u5e94\u200b\u65b0\u200b\u7684\u200b <code>in_features</code> \u200b\u503c\u200b\uff0c\u200b\u4ece\u800c\u200b\u907f\u514d\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u4ee5\u524d\u200b\u4ece\u672a\u200b\u63a5\u89e6\u200b\u8fc7\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u4e00\u200b\u5f00\u59cb\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u89c9\u5f97\u200b\u8fd9\u4e2a\u200b\u8bdd\u9898\u200b\u6709\u4e9b\u200b\u4ee4\u4eba\u56f0\u60d1\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5f53\u200b\u4f60\u200b\u591a\u6b21\u200b\u5c1d\u8bd5\u200b\u5e76\u200b\u6df1\u5165\u7814\u7a76\u200b\u4e00\u4e9b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u5b83\u200b\u65e0\u5904\u4e0d\u5728\u200b\u3002</p> <p>\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u5c31\u662f\u200b\u4f60\u200b\u6240\u200b\u9700\u8981\u200b\u7684\u200b\u4e00\u5207\u200b\u3002</p> <p></p> <p>\u200b\u5f53\u200b\u4f60\u200b\u5f00\u59cb\u200b\u6df1\u5165\u7814\u7a76\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\u5e76\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u65e0\u5904\u4e0d\u5728\u200b\u3002\u200b\u6765\u6e90\u200b\uff1a https://marksaroufim.substack.com/p/working-class-deep-learner</p> In\u00a0[43]: Copied! <pre># Create a tensor\nx = torch.arange(0, 100, 10)\nx\n</pre> # Create a tensor x = torch.arange(0, 100, 10) x Out[43]: <pre>tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u805a\u5408\u200b\u64cd\u4f5c\u200b\u3002</p> In\u00a0[44]: Copied! <pre>print(f\"Minimum: {x.min()}\")\nprint(f\"Maximum: {x.max()}\")\n# print(f\"Mean: {x.mean()}\") # this will error\nprint(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\nprint(f\"Sum: {x.sum()}\")\n</pre> print(f\"Minimum: {x.min()}\") print(f\"Maximum: {x.max()}\") # print(f\"Mean: {x.mean()}\") # this will error print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype print(f\"Sum: {x.sum()}\") <pre>Minimum: 0\nMaximum: 90\nMean: 45.0\nSum: 450\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u4e00\u4e9b\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b <code>torch.mean()</code>\uff0c\u200b\u9700\u8981\u200b\u5f20\u91cf\u200b\u5904\u4e8e\u200b <code>torch.float32</code>\uff08\u200b\u6700\u200b\u5e38\u89c1\u200b\uff09\u200b\u6216\u200b\u5176\u4ed6\u200b\u7279\u5b9a\u200b\u6570\u636e\u7c7b\u578b\u200b\uff0c\u200b\u5426\u5219\u200b\u64cd\u4f5c\u200b\u5c06\u200b\u5931\u8d25\u200b\u3002</p> <p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u5b9e\u73b0\u200b\u4e0a\u8ff0\u200b\u64cd\u4f5c\u200b\u3002</p> In\u00a0[45]: Copied! <pre>torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)\n</pre> torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x) Out[45]: <pre>(tensor(90), tensor(0), tensor(45.), tensor(450))</pre> In\u00a0[46]: Copied! <pre># Create a tensor\ntensor = torch.arange(10, 100, 10)\nprint(f\"Tensor: {tensor}\")\n\n# Returns index of max and min values\nprint(f\"Index where max value occurs: {tensor.argmax()}\")\nprint(f\"Index where min value occurs: {tensor.argmin()}\")\n</pre> # Create a tensor tensor = torch.arange(10, 100, 10) print(f\"Tensor: {tensor}\")  # Returns index of max and min values print(f\"Index where max value occurs: {tensor.argmax()}\") print(f\"Index where min value occurs: {tensor.argmin()}\") <pre>Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\nIndex where max value occurs: 8\nIndex where min value occurs: 0\n</pre> In\u00a0[47]: Copied! <pre># Create a tensor and check its datatype\ntensor = torch.arange(10., 100., 10.)\ntensor.dtype\n</pre> # Create a tensor and check its datatype tensor = torch.arange(10., 100., 10.) tensor.dtype Out[47]: <pre>torch.float32</pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u4f46\u200b\u5c06\u200b\u5176\u200b\u6570\u636e\u7c7b\u578b\u200b\u66f4\u200b\u6539\u4e3a\u200b <code>torch.float16</code>\u3002</p> In\u00a0[48]: Copied! <pre># Create a float16 tensor\ntensor_float16 = tensor.type(torch.float16)\ntensor_float16\n</pre> # Create a float16 tensor tensor_float16 = tensor.type(torch.float16) tensor_float16 Out[48]: <pre>tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)</pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>torch.int8</code> \u200b\u5f20\u91cf\u200b\u3002</p> In\u00a0[49]: Copied! <pre># Create a int8 tensor\ntensor_int8 = tensor.type(torch.int8)\ntensor_int8\n</pre> # Create a int8 tensor tensor_int8 = tensor.type(torch.int8) tensor_int8 Out[49]: <pre>tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e00\u200b\u5f00\u59cb\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba9\u200b\u4eba\u200b\u611f\u5230\u200b\u56f0\u60d1\u200b\u3002\u200b\u4f46\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u60f3\u200b\uff0c\u200b\u6570\u5b57\u200b\u8d8a\u5c0f\u200b\uff08\u200b\u4f8b\u5982\u200b 32\u300116\u30018\uff09\uff0c\u200b\u8ba1\u7b97\u673a\u200b\u5b58\u50a8\u200b\u7684\u200b\u503c\u200b\u5c31\u200b\u8d8a\u200b\u4e0d\u200b\u7cbe\u786e\u200b\u3002\u200b\u800c\u200b\u5b58\u50a8\u91cf\u200b\u8d8a\u200b\u5c11\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u6a21\u578b\u200b\u6574\u4f53\u200b\u66f4\u200b\u5c0f\u200b\u3002\u200b\u57fa\u4e8e\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b 8 \u200b\u4f4d\u200b\u6574\u6570\u200b\u8fdb\u884c\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7f51\u7edc\u200b\u66f4\u200b\u5c0f\u200b\u3001\u200b\u8fd0\u884c\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u4f46\u200b\u7cbe\u786e\u5ea6\u200b\u4e0d\u5982\u200b\u4f7f\u7528\u200b float32 \u200b\u7684\u200b\u7f51\u7edc\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5efa\u8bae\u200b\u9605\u8bfb\u200b\u5173\u4e8e\u200b\u8ba1\u7b97\u7cbe\u5ea6\u200b)\u200b\u7684\u200b\u8d44\u6599\u200b\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u5f20\u91cf\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b <code>torch.Tensor</code> \u200b\u6587\u6863\u200b\u4e2d\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u5185\u5bb9\u200b\u3002\u200b\u6211\u200b\u5efa\u8bae\u200b\u82b1\u200b 10 \u200b\u5206\u949f\u200b\u65f6\u95f4\u200b\u6d4f\u89c8\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u770b\u770b\u200b\u6709\u6ca1\u6709\u200b\u4ec0\u4e48\u200b\u5438\u5f15\u200b\u4f60\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u70b9\u51fb\u200b\u5b83\u4eec\u200b\uff0c\u200b\u7136\u540e\u200b\u81ea\u5df1\u200b\u52a8\u624b\u200b\u5199\u200b\u4ee3\u7801\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> In\u00a0[50]: Copied! <pre># Create a tensor\nimport torch\nx = torch.arange(1., 8.)\nx, x.shape\n</pre> # Create a tensor import torch x = torch.arange(1., 8.) x, x.shape Out[50]: <pre>(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b <code>torch.reshape()</code> \u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002</p> In\u00a0[51]: Copied! <pre># Add an extra dimension\nx_reshaped = x.reshape(1, 7)\nx_reshaped, x_reshaped.shape\n</pre> # Add an extra dimension x_reshaped = x.reshape(1, 7) x_reshaped, x_reshaped.shape Out[51]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.view()</code> \u200b\u6765\u200b\u6539\u53d8\u200b\u89c6\u56fe\u200b\u3002</p> In\u00a0[52]: Copied! <pre># Change view (keeps same data as original but changes view)\n# See more: https://stackoverflow.com/a/54507446/7900723\nz = x.view(1, 7)\nz, z.shape\n</pre> # Change view (keeps same data as original but changes view) # See more: https://stackoverflow.com/a/54507446/7900723 z = x.view(1, 7) z, z.shape Out[52]: <pre>(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))</pre> <p>\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torch.view()</code> \u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\u7684\u200b\u89c6\u56fe\u200b\u5b9e\u9645\u4e0a\u200b\u53ea\u662f\u200b\u521b\u5efa\u200b\u4e86\u200b\u540c\u4e00\u200b\u5f20\u91cf\u200b\u7684\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u89c6\u56fe\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6539\u53d8\u200b\u89c6\u56fe\u200b\u4e5f\u200b\u4f1a\u200b\u6539\u53d8\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u3002</p> In\u00a0[53]: Copied! <pre># Changing z changes x\nz[:, 0] = 5\nz, x\n</pre> # Changing z changes x z[:, 0] = 5 z, x Out[53]: <pre>(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))</pre> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u200b\u5c06\u200b\u65b0\u521b\u5efa\u200b\u7684\u200b\u5f20\u91cf\u200b\u5728\u200b\u81ea\u8eab\u200b\u4e0a\u200b\u5806\u53e0\u200b\u4e94\u6b21\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.stack()</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</p> In\u00a0[54]: Copied! <pre># Stack tensors on top of each other\nx_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\nx_stacked\n</pre> # Stack tensors on top of each other x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens x_stacked Out[54]: <pre>tensor([[5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.],\n        [5., 2., 3., 4., 5., 6., 7.]])</pre> <p>\u200b\u5982\u4f55\u200b\u4ece\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u79fb\u9664\u200b\u6240\u6709\u200b\u5355\u4e00\u200b\u7ef4\u5ea6\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.squeeze()</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff08\u200b\u6211\u200b\u8bb0\u5f97\u200b\u8fd9\u200b\u5c31\u200b\u50cf\u662f\u200b\u201c\u200b\u6324\u538b\u200b\u201d\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u53ea\u200b\u4fdd\u7559\u200b\u5927\u4e8e\u200b1\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff09\u3002</p> In\u00a0[55]: Copied! <pre>print(f\"Previous tensor: {x_reshaped}\")\nprint(f\"Previous shape: {x_reshaped.shape}\")\n\n# Remove extra dimension from x_reshaped\nx_squeezed = x_reshaped.squeeze()\nprint(f\"\\nNew tensor: {x_squeezed}\")\nprint(f\"New shape: {x_squeezed.shape}\")\n</pre> print(f\"Previous tensor: {x_reshaped}\") print(f\"Previous shape: {x_reshaped.shape}\")  # Remove extra dimension from x_reshaped x_squeezed = x_reshaped.squeeze() print(f\"\\nNew tensor: {x_squeezed}\") print(f\"New shape: {x_squeezed.shape}\") <pre>Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nPrevious shape: torch.Size([1, 7])\n\nNew tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nNew shape: torch.Size([7])\n</pre> <p>\u200b\u8981\u200b\u5b9e\u73b0\u200b\u4e0e\u200b <code>torch.squeeze()</code> \u200b\u76f8\u53cd\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.unsqueeze()</code> \u200b\u5728\u200b\u7279\u5b9a\u200b\u7d22\u5f15\u200b\u5904\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u503c\u4e3a\u200b 1 \u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002</p> In\u00a0[56]: Copied! <pre>print(f\"Previous tensor: {x_squeezed}\")\nprint(f\"Previous shape: {x_squeezed.shape}\")\n\n## Add an extra dimension with unsqueeze\nx_unsqueezed = x_squeezed.unsqueeze(dim=0)\nprint(f\"\\nNew tensor: {x_unsqueezed}\")\nprint(f\"New shape: {x_unsqueezed.shape}\")\n</pre> print(f\"Previous tensor: {x_squeezed}\") print(f\"Previous shape: {x_squeezed.shape}\")  ## Add an extra dimension with unsqueeze x_unsqueezed = x_squeezed.unsqueeze(dim=0) print(f\"\\nNew tensor: {x_unsqueezed}\") print(f\"New shape: {x_unsqueezed.shape}\") <pre>Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\nPrevious shape: torch.Size([7])\n\nNew tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\nNew shape: torch.Size([1, 7])\n</pre> <p>\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.permute(input, dims)</code> \u200b\u6765\u200b\u91cd\u65b0\u6392\u5217\u200b\u8f74\u200b\u7684\u200b\u987a\u5e8f\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>input</code> \u200b\u4f1a\u200b\u88ab\u200b\u8f6c\u6362\u6210\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u65b0\u200b <code>dims</code> \u200b\u7684\u200b\u89c6\u56fe\u200b\u3002</p> In\u00a0[57]: Copied! <pre># Create tensor with specific shape\nx_original = torch.rand(size=(224, 224, 3))\n\n# Permute the original tensor to rearrange the axis order\nx_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0\n\nprint(f\"Previous shape: {x_original.shape}\")\nprint(f\"New shape: {x_permuted.shape}\")\n</pre> # Create tensor with specific shape x_original = torch.rand(size=(224, 224, 3))  # Permute the original tensor to rearrange the axis order x_permuted = x_original.permute(2, 0, 1) # shifts axis 0-&gt;1, 1-&gt;2, 2-&gt;0  print(f\"Previous shape: {x_original.shape}\") print(f\"New shape: {x_permuted.shape}\") <pre>Previous shape: torch.Size([224, 224, 3])\nNew shape: torch.Size([3, 224, 224])\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u7531\u4e8e\u200b\u7f6e\u6362\u200b\u8fd4\u56de\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u200b\u89c6\u56fe\u200b\uff08\u200b\u4e0e\u200b\u539f\u59cb\u200b\u6570\u636e\u5171\u4eab\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u200b\uff09\uff0c\u200b\u56e0\u6b64\u200b\u7f6e\u6362\u200b\u540e\u200b\u7684\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\u5c06\u200b\u4e0e\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\u76f8\u540c\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u66f4\u6539\u200b\u4e86\u200b\u89c6\u56fe\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5b83\u200b\u4e5f\u200b\u4f1a\u200b\u6539\u53d8\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\u3002</p> In\u00a0[58]: Copied! <pre># Create a tensor \nimport torch\nx = torch.arange(1, 10).reshape(1, 3, 3)\nx, x.shape\n</pre> # Create a tensor  import torch x = torch.arange(1, 10).reshape(1, 3, 3) x, x.shape Out[58]: <pre>(tensor([[[1, 2, 3],\n          [4, 5, 6],\n          [7, 8, 9]]]),\n torch.Size([1, 3, 3]))</pre> <p>\u200b\u7d22\u5f15\u200b\u503c\u200b\u7684\u200b\u987a\u5e8f\u200b\u662f\u4ece\u200b\u5916\u200b\u7ef4\u5ea6\u200b\u5230\u200b\u5185\u200b\u7ef4\u5ea6\u200b\uff08\u200b\u8bf7\u200b\u53c2\u8003\u200b\u65b9\u62ec\u53f7\u200b\u7684\u200b\u4f7f\u7528\u200b\uff09\u3002</p> In\u00a0[59]: Copied! <pre># Let's index bracket by bracket\nprint(f\"First square bracket:\\n{x[0]}\") \nprint(f\"Second square bracket: {x[0][0]}\") \nprint(f\"Third square bracket: {x[0][0][0]}\")\n</pre> # Let's index bracket by bracket print(f\"First square bracket:\\n{x[0]}\")  print(f\"Second square bracket: {x[0][0]}\")  print(f\"Third square bracket: {x[0][0][0]}\") <pre>First square bracket:\ntensor([[1, 2, 3],\n        [4, 5, 6],\n        [7, 8, 9]])\nSecond square bracket: tensor([1, 2, 3])\nThird square bracket: 1\n</pre> <p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>:</code> \u200b\u6765\u200b\u6307\u5b9a\u200b\u201c\u200b\u8be5\u200b\u7ef4\u5ea6\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u503c\u200b\u201d\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u9017\u53f7\u200b\uff08<code>,</code>\uff09\u200b\u6765\u200b\u6dfb\u52a0\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u3002</p> In\u00a0[60]: Copied! <pre># Get all values of 0th dimension and the 0 index of 1st dimension\nx[:, 0]\n</pre> # Get all values of 0th dimension and the 0 index of 1st dimension x[:, 0] Out[60]: <pre>tensor([[1, 2, 3]])</pre> In\u00a0[61]: Copied! <pre># Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension\nx[:, :, 1]\n</pre> # Get all values of 0th &amp; 1st dimensions but only index 1 of 2nd dimension x[:, :, 1] Out[61]: <pre>tensor([[2, 5, 8]])</pre> In\u00a0[62]: Copied! <pre># Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\nx[:, 1, 1]\n</pre> # Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension x[:, 1, 1] Out[62]: <pre>tensor([5])</pre> In\u00a0[63]: Copied! <pre># Get index 0 of 0th and 1st dimension and all values of 2nd dimension \nx[0, 0, :] # same as x[0][0]\n</pre> # Get index 0 of 0th and 1st dimension and all values of 2nd dimension  x[0, 0, :] # same as x[0][0] Out[63]: <pre>tensor([1, 2, 3])</pre> <p>\u200b\u7d22\u5f15\u200b\u4e00\u200b\u5f00\u59cb\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba9\u200b\u4eba\u200b\u611f\u5230\u200b\u76f8\u5f53\u200b\u56f0\u60d1\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\u5bf9\u4e8e\u200b\u8f83\u5927\u200b\u7684\u200b\u5f20\u91cf\u200b\uff08\u200b\u6211\u200b\u4ecd\u7136\u200b\u9700\u8981\u200b\u5c1d\u8bd5\u200b\u591a\u6b21\u200b\u7d22\u5f15\u200b\u624d\u80fd\u200b\u6b63\u786e\u200b\u64cd\u4f5c\u200b\uff09\u3002\u200b\u4f46\u200b\u53ea\u8981\u200b\u7a0d\u52a0\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u5e76\u200b\u9075\u5faa\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff08\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u518d\u200b\u53ef\u89c6\u5316\u200b\uff09\uff0c\u200b\u4f60\u200b\u5c31\u200b\u4f1a\u200b\u5f00\u59cb\u200b\u638c\u63e1\u200b\u5176\u4e2d\u200b\u7684\u200b\u7a8d\u95e8\u200b\u3002</p> In\u00a0[64]: Copied! <pre># NumPy array to tensor\nimport torch\nimport numpy as np\narray = np.arange(1.0, 8.0)\ntensor = torch.from_numpy(array)\narray, tensor\n</pre> # NumPy array to tensor import torch import numpy as np array = np.arange(1.0, 8.0) tensor = torch.from_numpy(array) array, tensor Out[64]: <pre>(array([1., 2., 3., 4., 5., 6., 7.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0cNumPy \u200b\u6570\u7ec4\u200b\u4f1a\u4ee5\u200b <code>float64</code> \u200b\u6570\u636e\u7c7b\u578b\u200b\u521b\u5efa\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b PyTorch \u200b\u5f20\u91cf\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u4fdd\u6301\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08\u200b\u5982\u4e0a\u6240\u8ff0\u200b\uff09\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8bb8\u591a\u200b PyTorch \u200b\u8ba1\u7b97\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200b <code>float32</code>\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b NumPy \u200b\u6570\u7ec4\u200b\uff08float64\uff09\u200b\u8f6c\u6362\u200b\u4e3a\u200b PyTorch \u200b\u5f20\u91cf\u200b\uff08float64\uff09\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u8f6c\u6362\u200b\u4e3a\u200b PyTorch \u200b\u5f20\u91cf\u200b\uff08float32\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>tensor = torch.from_numpy(array).type(torch.float32)</code>\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u91cd\u65b0\u200b\u8d4b\u503c\u200b\u4e86\u200b <code>tensor</code>\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\uff0c\u200b\u6570\u7ec4\u200b\u5c06\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> In\u00a0[65]: Copied! <pre># Change the array, keep the tensor\narray = array + 1\narray, tensor\n</pre> # Change the array, keep the tensor array = array + 1 array, tensor Out[65]: <pre>(array([2., 3., 4., 5., 6., 7., 8.]),\n tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))</pre> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u4ece\u200b PyTorch \u200b\u5f20\u91cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b NumPy \u200b\u6570\u7ec4\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b <code>tensor.numpy()</code>\u3002</p> In\u00a0[66]: Copied! <pre># Tensor to NumPy array\ntensor = torch.ones(7) # create a tensor of ones with dtype=float32\nnumpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\ntensor, numpy_tensor\n</pre> # Tensor to NumPy array tensor = torch.ones(7) # create a tensor of ones with dtype=float32 numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed tensor, numpy_tensor Out[66]: <pre>(tensor([1., 1., 1., 1., 1., 1., 1.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</pre> <p>\u200b\u540c\u6837\u200b\u5730\u200b\uff0c\u200b\u9075\u5faa\u200b\u4e0a\u8ff0\u200b\u89c4\u5219\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u4fee\u6539\u200b\u4e86\u200b\u539f\u59cb\u200b\u7684\u200b <code>tensor</code>\uff0c\u200b\u65b0\u200b\u7684\u200b <code>numpy_tensor</code> \u200b\u5c06\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> In\u00a0[67]: Copied! <pre># Change the tensor, keep the array the same\ntensor = tensor + 1\ntensor, numpy_tensor\n</pre> # Change the tensor, keep the array the same tensor = tensor + 1 tensor, numpy_tensor Out[67]: <pre>(tensor([2., 2., 2., 2., 2., 2., 2.]),\n array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))</pre> In\u00a0[68]: Copied! <pre>import torch\n\n# Create two random tensors\nrandom_tensor_A = torch.rand(3, 4)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"Tensor A:\\n{random_tensor_A}\\n\")\nprint(f\"Tensor B:\\n{random_tensor_B}\\n\")\nprint(f\"Does Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n</pre> import torch  # Create two random tensors random_tensor_A = torch.rand(3, 4) random_tensor_B = torch.rand(3, 4)  print(f\"Tensor A:\\n{random_tensor_A}\\n\") print(f\"Tensor B:\\n{random_tensor_B}\\n\") print(f\"Does Tensor A equal Tensor B? (anywhere)\") random_tensor_A == random_tensor_B <pre>Tensor A:\ntensor([[0.8016, 0.3649, 0.6286, 0.9663],\n        [0.7687, 0.4566, 0.5745, 0.9200],\n        [0.3230, 0.8613, 0.0919, 0.3102]])\n\nTensor B:\ntensor([[0.9536, 0.6002, 0.0351, 0.6826],\n        [0.3743, 0.5220, 0.1336, 0.9666],\n        [0.9754, 0.8474, 0.8988, 0.1105]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n</pre> Out[68]: <pre>tensor([[False, False, False, False],\n        [False, False, False, False],\n        [False, False, False, False]])</pre> <p>\u200b\u6b63\u5982\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9884\u6599\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5f20\u91cf\u200b\u7684\u200b\u503c\u200b\u5404\u4e0d\u76f8\u540c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u503c\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5f20\u91cf\u200b\u4ecd\u7136\u200b\u5305\u542b\u200b\u968f\u673a\u200b\u503c\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u7684\u200b\u201c\u200b\u98ce\u5473\u200b\u201d\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u8fd9\u65f6\u200b\uff0c<code>torch.manual_seed(seed)</code> \u200b\u5c31\u200b\u6d3e\u4e0a\u7528\u573a\u200b\u4e86\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>seed</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u6574\u6570\u200b\uff08\u200b\u6bd4\u5982\u200b <code>42</code>\uff0c\u200b\u4f46\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u4f55\u200b\u503c\u200b\uff09\uff0c\u200b\u7528\u4e8e\u200b\u7ed9\u200b\u968f\u673a\u6027\u200b\u201c\u200b\u8c03\u5473\u200b\u201d\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u66f4\u5177\u200b\u201c\u200b\u98ce\u5473\u200b\u201d\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\u3002</p> In\u00a0[69]: Copied! <pre>import torch\nimport random\n\n# # Set the random seed\nRANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\ntorch.manual_seed(seed=RANDOM_SEED) \nrandom_tensor_C = torch.rand(3, 4)\n\n# Have to reset the seed every time a new rand() is called \n# Without this, tensor_D would be different to tensor_C \ntorch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\nrandom_tensor_D = torch.rand(3, 4)\n\nprint(f\"Tensor C:\\n{random_tensor_C}\\n\")\nprint(f\"Tensor D:\\n{random_tensor_D}\\n\")\nprint(f\"Does Tensor C equal Tensor D? (anywhere)\")\nrandom_tensor_C == random_tensor_D\n</pre> import torch import random  # # Set the random seed RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below torch.manual_seed(seed=RANDOM_SEED)  random_tensor_C = torch.rand(3, 4)  # Have to reset the seed every time a new rand() is called  # Without this, tensor_D would be different to tensor_C  torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens random_tensor_D = torch.rand(3, 4)  print(f\"Tensor C:\\n{random_tensor_C}\\n\") print(f\"Tensor D:\\n{random_tensor_D}\\n\") print(f\"Does Tensor C equal Tensor D? (anywhere)\") random_tensor_C == random_tensor_D <pre>Tensor C:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor D:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor C equal Tensor D? (anywhere)\n</pre> Out[69]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u8bbe\u7f6e\u200b\u79cd\u5b50\u200b\u8d77\u200b\u4f5c\u7528\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u6d89\u53ca\u200b\u7684\u200b\u5185\u5bb9\u200b\u53ea\u662f\u200b PyTorch \u200b\u4e2d\u200b\u53ef\u91cd\u590d\u6027\u200b\u7684\u200b\u51b0\u5c71\u4e00\u89d2\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u53ef\u91cd\u590d\u6027\u200b\u4ee5\u53ca\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u7684\u200b\u77e5\u8bc6\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u67e5\u770b\u200b\u4ee5\u4e0b\u200b\u8d44\u6599\u200b\uff1a</p> <ul> <li>PyTorch \u200b\u53ef\u91cd\u590d\u6027\u200b\u6587\u6863\u200b\uff08\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u7ec3\u4e60\u200b\u662f\u200b\u82b1\u200b 10 \u200b\u5206\u949f\u200b\u9605\u8bfb\u200b\u8fd9\u4e2a\u200b\u6587\u6863\u200b\uff0c\u200b\u5373\u4f7f\u200b\u4f60\u200b\u73b0\u5728\u200b\u4e0d\u200b\u7406\u89e3\u200b\u5b83\u200b\uff0c\u200b\u4f46\u200b\u610f\u8bc6\u200b\u5230\u200b\u5b83\u200b\u7684\u200b\u5b58\u5728\u200b\u662f\u200b\u5f88\u200b\u91cd\u8981\u200b\u7684\u200b\uff09\u3002</li> <li>\u200b\u7ef4\u57fa\u767e\u79d1\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u9875\u9762\u200b\uff08\u200b\u8fd9\u200b\u5c06\u200b\u4e3a\u200b\u4f60\u200b\u63d0\u4f9b\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u548c\u200b\u4f2a\u200b\u968f\u673a\u6027\u200b\u7684\u200b\u4e00\u822c\u200b\u6982\u8ff0\u200b\uff09\u3002</li> </ul> In\u00a0[70]: Copied! <pre>!nvidia-smi\n</pre> !nvidia-smi <pre>Sat Jan 21 08:34:23 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA TITAN RTX    On   | 00000000:01:00.0 Off |                  N/A |\n| 40%   30C    P8     7W / 280W |    177MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1061      G   /usr/lib/xorg/Xorg                 53MiB |\n|    0   N/A  N/A   2671131      G   /usr/lib/xorg/Xorg                 97MiB |\n|    0   N/A  N/A   2671256      G   /usr/bin/gnome-shell                9MiB |\n+-----------------------------------------------------------------------------+\n</pre> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u6ca1\u6709\u200b\u53ef\u7528\u200b\u7684\u200b Nvidia GPU\uff0c\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u5c06\u200b\u8f93\u51fa\u200b\u7c7b\u4f3c\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> <pre><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n</code></pre> <p>\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8bf7\u200b\u8fd4\u56de\u200b\u5e76\u200b\u6309\u7167\u200b\u5b89\u88c5\u200b\u6b65\u9aa4\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u786e\u5b9e\u200b\u6709\u200b GPU\uff0c\u200b\u4e0a\u8ff0\u200b\u547d\u4ee4\u200b\u5c06\u200b\u8f93\u51fa\u200b\u7c7b\u4f3c\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> <pre><code>Wed Jan 19 22:09:08 2022       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n</code></pre> In\u00a0[71]: Copied! <pre># Check for GPU\nimport torch\ntorch.cuda.is_available()\n</pre> # Check for GPU import torch torch.cuda.is_available() Out[71]: <pre>True</pre> <p>\u200b\u5982\u679c\u200b\u4e0a\u8ff0\u200b\u8f93\u51fa\u200b\u4e3a\u200b <code>True</code>\uff0c\u200b\u5219\u200b\u8868\u793a\u200b PyTorch \u200b\u53ef\u4ee5\u200b\u8bc6\u522b\u200b\u5e76\u200b\u4f7f\u7528\u200b GPU\uff1b\u200b\u5982\u679c\u200b\u8f93\u51fa\u200b\u4e3a\u200b <code>False</code>\uff0c\u200b\u5219\u200b\u8868\u793a\u200b PyTorch \u200b\u65e0\u6cd5\u200b\u8bc6\u522b\u200b GPU\uff0c\u200b\u6b64\u65f6\u200b\u4f60\u200b\u9700\u8981\u200b\u91cd\u65b0\u200b\u68c0\u67e5\u200b\u5b89\u88c5\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u5047\u8bbe\u200b\u4f60\u200b\u5e0c\u671b\u200b\u8bbe\u7f6e\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u80fd\u591f\u200b\u5728\u200b CPU \u200b\u6216\u200b\u53ef\u7528\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u4e00\u6765\u200b\uff0c\u200b\u65e0\u8bba\u200b\u4f60\u200b\u6216\u200b\u5176\u4ed6\u4eba\u200b\u4f7f\u7528\u200b\u4f55\u79cd\u200b\u8ba1\u7b97\u200b\u8bbe\u5907\u200b\u8fd0\u884c\u200b\u4f60\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5b83\u200b\u90fd\u200b\u80fd\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>device</code> \u200b\u53d8\u91cf\u200b\u6765\u200b\u5b58\u50a8\u200b\u53ef\u7528\u200b\u8bbe\u5907\u200b\u7684\u200b\u7c7b\u578b\u200b\u3002</p> In\u00a0[72]: Copied! <pre># Set device type\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Set device type device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[72]: <pre>'cuda'</pre> <p>\u200b\u5982\u679c\u200b\u4e0a\u8ff0\u200b\u8f93\u51fa\u200b\u4e3a\u200b <code>\"cuda\"</code>\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6240\u6709\u200b PyTorch \u200b\u4ee3\u7801\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u4f7f\u7528\u200b\u53ef\u7528\u200b\u7684\u200b CUDA \u200b\u8bbe\u5907\u200b\uff08\u200b\u5373\u200b GPU\uff09\uff0c\u200b\u800c\u200b\u5982\u679c\u200b\u8f93\u51fa\u200b\u4e3a\u200b <code>\"cpu\"</code>\uff0c\u200b\u5219\u200b\u6211\u4eec\u200b\u7684\u200b PyTorch \u200b\u4ee3\u7801\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u4f7f\u7528\u200b CPU\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u7f16\u5199\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\u662f\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4ee3\u7801\u200b\u5c06\u200b\u5728\u200b CPU\uff08\u200b\u59cb\u7ec8\u200b\u53ef\u7528\u200b\uff09\u200b\u6216\u200b GPU\uff08\u200b\u5982\u679c\u200b\u53ef\u7528\u200b\uff09\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u8fdb\u884c\u200b\u66f4\u5feb\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b GPU\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u8fdb\u884c\u200b\u66f4\u5feb\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u591a\u4e2a\u200b GPU\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.cuda.device_count()</code> \u200b\u6765\u200b\u8ba1\u7b97\u200b PyTorch \u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b\u7684\u200b GPU \u200b\u6570\u91cf\u200b\u3002</p> In\u00a0[73]: Copied! <pre># Count number of devices\ntorch.cuda.device_count()\n</pre> # Count number of devices torch.cuda.device_count() Out[73]: <pre>1</pre> <p>\u200b\u4e86\u89e3\u200bPyTorch\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b\u7684\u200bGPU\u200b\u6570\u91cf\u200b\u662f\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\uff0c\u200b\u4ee5\u9632\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5728\u200b\u4e00\u4e2a\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u67d0\u4e2a\u200b\u8fdb\u7a0b\u200b\uff0c\u200b\u800c\u200b\u5728\u200b\u53e6\u200b\u4e00\u4e2a\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u8fdb\u7a0b\u200b\uff08PyTorch\u200b\u8fd8\u200b\u5177\u5907\u200b\u8ba9\u4f60\u5728\u200b\u6240\u6709\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u8fdb\u7a0b\u200b\u7684\u200b\u529f\u80fd\u200b\uff09\u3002</p> In\u00a0[4]: Copied! <pre># Check for Apple Silicon GPU\nimport torch\ntorch.backends.mps.is_available() # Note this will print false if you're not running on a Mac\n</pre> # Check for Apple Silicon GPU import torch torch.backends.mps.is_available() # Note this will print false if you're not running on a Mac Out[4]: <pre>True</pre> In\u00a0[7]: Copied! <pre># Set device type\ndevice = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\ndevice\n</pre> # Set device type device = \"mps\" if torch.backends.mps.is_available() else \"cpu\" device Out[7]: <pre>'mps'</pre> <p>\u200b\u5982\u524d\u6240\u8ff0\u200b\uff0c\u200b\u5982\u679c\u200b\u4e0a\u8ff0\u200b\u8f93\u51fa\u200b\u4e3a\u200b <code>\"mps\"</code>\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6240\u6709\u200b PyTorch \u200b\u4ee3\u7801\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u4f7f\u7528\u200b\u53ef\u7528\u200b\u7684\u200b Apple Silicon GPU\u3002</p> In\u00a0[8]: Copied! <pre>if torch.cuda.is_available():\n    device = \"cuda\" # Use NVIDIA GPU (if available)\nelif torch.backends.mps.is_available():\n    device = \"mps\" # Use Apple Silicon GPU (if available)\nelse:\n    device = \"cpu\" # Default to CPU if no GPU is available\n</pre> if torch.cuda.is_available():     device = \"cuda\" # Use NVIDIA GPU (if available) elif torch.backends.mps.is_available():     device = \"mps\" # Use Apple Silicon GPU (if available) else:     device = \"cpu\" # Default to CPU if no GPU is available In\u00a0[9]: Copied! <pre># Create tensor (default on CPU)\ntensor = torch.tensor([1, 2, 3])\n\n# Tensor not on GPU\nprint(tensor, tensor.device)\n\n# Move tensor to GPU (if available)\ntensor_on_gpu = tensor.to(device)\ntensor_on_gpu\n</pre> # Create tensor (default on CPU) tensor = torch.tensor([1, 2, 3])  # Tensor not on GPU print(tensor, tensor.device)  # Move tensor to GPU (if available) tensor_on_gpu = tensor.to(device) tensor_on_gpu <pre>tensor([1, 2, 3]) cpu\n</pre> Out[9]: <pre>tensor([1, 2, 3], device='mps:0')</pre> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u53ef\u7528\u200b\u7684\u200bGPU\uff0c\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u5c06\u200b\u8f93\u51fa\u200b\u7c7b\u4f3c\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> <pre><code>tensor([1, 2, 3]) cpu\ntensor([1, 2, 3], device='cuda:0')\n</code></pre> <p>\u200b\u6ce8\u610f\u200b\u7b2c\u4e8c\u4e2a\u200b\u5f20\u91cf\u200b\u5e26\u6709\u200b <code>device='cuda:0'</code>\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u5b58\u50a8\u200b\u5728\u200b\u53ef\u7528\u200b\u7684\u200b\u7b2c\u200b0\u200b\u53f7\u200bGPU\u200b\u4e0a\u200b\uff08GPU\u200b\u7684\u200b\u7d22\u5f15\u200b\u4ece\u200b0\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u4e24\u4e2a\u200bGPU\u200b\u53ef\u7528\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5c06\u200b\u5206\u522b\u200b\u662f\u200b <code>'cuda:0'</code> \u200b\u548c\u200b <code>'cuda:1'</code>\uff0c\u200b\u4ee5\u6b64\u7c7b\u63a8\u200b\uff0c\u200b\u6700\u591a\u5230\u200b <code>'cuda:n'</code>\uff09\u3002</p> In\u00a0[75]: Copied! <pre># If tensor is on GPU, can't transform it to NumPy (this will error)\ntensor_on_gpu.numpy()\n</pre> # If tensor is on GPU, can't transform it to NumPy (this will error) tensor_on_gpu.numpy() <pre>\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 157 in &lt;cell line: 2&gt;()\n      &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'&gt;1&lt;/a&gt; # If tensor is on GPU, can't transform it to NumPy (this will error)\n----&gt; &lt;a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'&gt;2&lt;/a&gt; tensor_on_gpu.numpy()\n\nTypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.</pre> <p>\u200b\u76f8\u53cd\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u5c06\u200b\u5f20\u91cf\u200b\u8fd4\u56de\u200b\u5230\u200b CPU \u200b\u5e76\u200b\u4f7f\u200b\u5176\u200b\u53ef\u200b\u4e0e\u200b NumPy \u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>Tensor.cpu()</code>\u3002</p> <p>\u200b\u8fd9\u4f1a\u200b\u5c06\u200b\u5f20\u91cf\u200b\u590d\u5236\u5230\u200b CPU \u200b\u5185\u5b58\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b CPU \u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p> In\u00a0[76]: Copied! <pre># Instead, copy the tensor back to cpu\ntensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\ntensor_back_on_cpu\n</pre> # Instead, copy the tensor back to cpu tensor_back_on_cpu = tensor_on_gpu.cpu().numpy() tensor_back_on_cpu Out[76]: <pre>array([1, 2, 3])</pre> <p>\u200b\u4e0a\u8ff0\u200b\u64cd\u4f5c\u200b\u8fd4\u56de\u200b\u7684\u200b\u662f\u200b\u4f4d\u4e8e\u200bCPU\u200b\u5185\u5b58\u200b\u4e2d\u200b\u7684\u200bGPU\u200b\u5f20\u91cf\u200b\u526f\u672c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u4ecd\u7136\u200b\u4fdd\u7559\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u3002</p> In\u00a0[77]: Copied! <pre>tensor_on_gpu\n</pre> tensor_on_gpu Out[77]: <pre>tensor([1, 2, 3], device='cuda:0')</pre>"},{"location":"00_pytorch_fundamentals/#00-pytorch","title":"00. PyTorch \u200b\u57fa\u7840\u200b\u00b6","text":""},{"location":"00_pytorch_fundamentals/#pytorch","title":"\u4ec0\u4e48\u200b\u662f\u200b PyTorch\uff1f\u00b6","text":"<p>PyTorch \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f00\u6e90\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/#pytorch","title":"PyTorch \u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u505a\u200b\u4ec0\u4e48\u200b\uff1f\u00b6","text":"<p>PyTorch \u200b\u5141\u8bb8\u200b\u4f60\u200b\u4f7f\u7528\u200b Python \u200b\u4ee3\u7801\u200b\u64cd\u4f5c\u200b\u548c\u200b\u5904\u7406\u200b\u6570\u636e\u200b\uff0c\u200b\u5e76\u200b\u7f16\u5199\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/#pytorch","title":"\u8c01\u200b\u5728\u200b\u4f7f\u7528\u200b PyTorch\uff1f\u00b6","text":"<p>\u200b\u8bb8\u591a\u200b\u4e16\u754c\u200b\u6700\u5927\u200b\u7684\u200b\u79d1\u6280\u200b\u516c\u53f8\u200b\uff0c\u200b\u5982\u200b Meta (Facebook)\u3001\u200b\u7279\u65af\u62c9\u200b\u548c\u200b\u5fae\u8f6f\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7814\u7a76\u200b\u516c\u53f8\u200b\u5982\u200b OpenAI\uff0c\u200b\u90fd\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6765\u200b\u652f\u6301\u200b\u7814\u7a76\u200b\u548c\u200b\u5c06\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5f15\u5165\u200b\u4ed6\u4eec\u200b\u7684\u200b\u4ea7\u54c1\u200b\u3002</p> <p></p> <p>\u200b\u4f8b\u5982\u200b\uff0cAndrej Karpathy\uff08\u200b\u7279\u65af\u62c9\u200b AI \u200b\u8d1f\u8d23\u4eba\u200b\uff09\u200b\u5728\u200b\u591a\u4e2a\u200b\u6f14\u8bb2\u200b\u4e2d\u200b\uff08PyTorch DevCon 2019\uff0cTesla AI Day 2021\uff09\u200b\u8bb2\u8ff0\u200b\u4e86\u200b\u7279\u65af\u62c9\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6765\u200b\u652f\u6301\u200b\u4ed6\u4eec\u200b\u7684\u200b\u81ea\u52a8\u200b\u9a7e\u9a76\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</p> <p>PyTorch \u200b\u8fd8\u200b\u5728\u200b\u5176\u4ed6\u200b\u884c\u4e1a\u200b\u4e2d\u200b\u4f7f\u7528\u200b\uff0c\u200b\u5982\u200b\u519c\u4e1a\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4e3a\u200b\u62d6\u62c9\u673a\u200b\u63d0\u4f9b\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/#pytorch","title":"\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u4f7f\u7528\u200b PyTorch\uff1f\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u975e\u5e38\u200b\u559c\u6b22\u200b\u4f7f\u7528\u200b PyTorch\u3002\u200b\u622a\u81f3\u200b 2022 \u200b\u5e74\u200b 2 \u200b\u6708\u200b\uff0cPyTorch \u200b\u662f\u200b Papers With Code \u200b\u4e0a\u200b\u4f7f\u7528\u200b\u6700\u591a\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u8ffd\u8e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u53ca\u5176\u200b\u76f8\u5173\u200b\u4ee3\u7801\u200b\u5e93\u200b\u7684\u200b\u7f51\u7ad9\u200b\u3002</p> <p>PyTorch \u200b\u8fd8\u200b\u80fd\u200b\u5728\u200b\u540e\u53f0\u200b\u5904\u7406\u200b\u8bb8\u591a\u200b\u4e8b\u60c5\u200b\uff0c\u200b\u5982\u200b GPU \u200b\u52a0\u901f\u200b\uff08\u200b\u4f7f\u200b\u4f60\u200b\u7684\u200b\u4ee3\u7801\u8fd0\u884c\u200b\u66f4\u200b\u5feb\u200b\uff09\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u64cd\u4f5c\u200b\u6570\u636e\u200b\u548c\u200b\u7f16\u5199\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u800c\u200b PyTorch \u200b\u4f1a\u200b\u786e\u4fdd\u200b\u5176\u200b\u8fd0\u884c\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u50cf\u200b\u7279\u65af\u62c9\u200b\u548c\u200b Meta (Facebook) \u200b\u8fd9\u6837\u200b\u7684\u200b\u516c\u53f8\u200b\u4f7f\u7528\u200b\u5b83\u200b\u6765\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u90e8\u7f72\u200b\u5230\u200b\u6570\u767e\u4e2a\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u3001\u200b\u6570\u5343\u8f86\u200b\u6c7d\u8f66\u200b\u548c\u200b\u6570\u5341\u4ebf\u200b\u7528\u6237\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e2d\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u200b\u5728\u200b\u5f00\u53d1\u200b\u65b9\u9762\u200b\u663e\u7136\u200b\u4e5f\u200b\u662f\u200b\u5f3a\u5927\u200b\u7684\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u672c\u200b\u6a21\u5757\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u5206\u4e3a\u200b\u4e0d\u540c\u200b\u7684\u200b\u90e8\u5206\u200b\uff08\u200b\u7b14\u8bb0\u672c\u200b\uff09\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u6db5\u76d6\u200b\u4e86\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b\u91cd\u8981\u200b\u6982\u5ff5\u200b\u548c\u200b\u601d\u60f3\u200b\u3002</p> <p>\u200b\u540e\u7eed\u200b\u7b14\u8bb0\u672c\u200b\u5efa\u7acb\u200b\u5728\u200b\u524d\u9762\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u77e5\u8bc6\u200b\u57fa\u7840\u200b\u4e0a\u200b\uff08\u200b\u7f16\u53f7\u200b\u4ece\u200b 00\u300101\u300102 \u200b\u5f00\u59cb\u200b\uff0c\u200b\u4e00\u76f4\u200b\u5ef6\u7eed\u200b\u5230\u200b\u7ed3\u675f\u200b\uff09\u3002</p> <p>\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u6d89\u53ca\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u672c\u200b\u6784\u5efa\u200b\u5757\u200b\u2014\u2014\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\uff1a</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b \u200b\u5f20\u91cf\u200b\u4ecb\u7ecd\u200b \u200b\u5f20\u91cf\u200b\u662f\u200b\u6240\u6709\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u672c\u200b\u6784\u5efa\u200b\u5757\u200b\u3002 \u200b\u521b\u5efa\u200b\u5f20\u91cf\u200b \u200b\u5f20\u91cf\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\uff08\u200b\u56fe\u50cf\u200b\u3001\u200b\u6587\u5b57\u200b\u3001\u200b\u6570\u5b57\u200b\u8868\u683c\u200b\uff09\u3002 \u200b\u4ece\u200b\u5f20\u91cf\u200b\u83b7\u53d6\u4fe1\u606f\u200b \u200b\u5982\u679c\u200b\u4f60\u200b\u80fd\u200b\u628a\u200b\u4fe1\u606f\u200b\u653e\u5165\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u4f1a\u200b\u60f3\u8981\u200b\u53d6\u51fa\u200b\u5b83\u200b\u3002 \u200b\u64cd\u4f5c\u200b\u5f20\u91cf\u200b \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\uff08\u200b\u5982\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u6d89\u53ca\u200b\u4ee5\u200b\u591a\u79cd\u200b\u65b9\u5f0f\u200b\u64cd\u4f5c\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5982\u200b\u52a0\u6cd5\u200b\u3001\u200b\u4e58\u6cd5\u200b\u3001\u200b\u7ec4\u5408\u200b\u3002 \u200b\u5904\u7406\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b\u662f\u200b\u5904\u7406\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\uff08\u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u9519\u8bef\u200b\u5f62\u72b6\u200b\u7684\u200b\u5f20\u91cf\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u5f20\u91cf\u200b\u6df7\u5408\u200b\uff09\u3002 \u200b\u5f20\u91cf\u200b\u7d22\u5f15\u200b \u200b\u5982\u679c\u200b\u4f60\u200b\u5bf9\u200b Python \u200b\u5217\u8868\u200b\u6216\u200b NumPy \u200b\u6570\u7ec4\u200b\u8fdb\u884c\u200b\u8fc7\u200b\u7d22\u5f15\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u53ea\u662f\u200b\u5f20\u91cf\u200b\u53ef\u4ee5\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002 \u200b\u6df7\u5408\u200b PyTorch \u200b\u5f20\u91cf\u200b\u548c\u200b NumPy PyTorch \u200b\u4f7f\u7528\u200b\u5f20\u91cf\u200b\uff08<code>torch.Tensor</code>\uff09\uff0cNumPy \u200b\u559c\u6b22\u200b\u6570\u7ec4\u200b\uff08<code>np.ndarray</code>\uff09\uff0c\u200b\u6709\u65f6\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u6df7\u5408\u200b\u4f7f\u7528\u200b\u5b83\u4eec\u200b\u3002 \u200b\u53ef\u91cd\u590d\u6027\u200b \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u975e\u5e38\u200b\u5b9e\u9a8c\u6027\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u4f7f\u7528\u200b\u5927\u91cf\u200b\u7684\u200b\u968f\u673a\u6027\u200b\u6765\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u6709\u65f6\u200b\u4f60\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u8fd9\u79cd\u200b\u968f\u673a\u6027\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u968f\u673a\u200b\u3002 \u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u5f20\u91cf\u200b GPU\uff08\u200b\u56fe\u5f62\u200b\u5904\u7406\u5355\u5143\u200b\uff09\u200b\u4f7f\u200b\u4f60\u200b\u7684\u200b\u4ee3\u7801\u8fd0\u884c\u200b\u66f4\u200b\u5feb\u200b\uff0cPyTorch \u200b\u4f7f\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u4ee3\u7801\u200b\u53d8\u5f97\u200b\u5bb9\u6613\u200b\u3002"},{"location":"00_pytorch_fundamentals/","title":"\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u54ea\u91cc\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u6750\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b Discussions \u200b\u9875\u9762\u200b \u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u8fd8\u6709\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\u7684\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u4ea4\u6d41\u5e73\u53f0\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/#pytorch","title":"\u5bfc\u5165\u200b PyTorch\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b\u8fd0\u884c\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u4ee3\u7801\u200b\u4e4b\u524d\u200b\uff0c\u200b\u60a8\u200b\u5e94\u8be5\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b PyTorch \u200b\u5b89\u88c5\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u5728\u200b Google Colab \u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4e00\u5207\u200b\u5e94\u8be5\u200b\u90fd\u200b\u80fd\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\uff08Google Colab \u200b\u81ea\u5e26\u200b PyTorch \u200b\u548c\u200b\u5176\u4ed6\u200b\u5e93\u200b\u7684\u200b\u5b89\u88c5\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u5bfc\u5165\u200b PyTorch \u200b\u5e76\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u7248\u672c\u200b\u5f00\u59cb\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u5f20\u91cf\u200b\u7b80\u4ecb\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5bfc\u5165\u200b\u4e86\u200b PyTorch\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u5b66\u4e60\u200b\u5f20\u91cf\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5f20\u91cf\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u57fa\u672c\u200b\u6784\u5efa\u200b\u5757\u200b\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u7684\u200b\u4f5c\u7528\u200b\u662f\u200b\u4ee5\u200b\u6570\u503c\u200b\u65b9\u5f0f\u200b\u8868\u793a\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u8868\u793a\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[3, 224, 224]</code> \u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b <code>[\u200b\u989c\u8272\u200b\u901a\u9053\u200b, \u200b\u9ad8\u5ea6\u200b, \u200b\u5bbd\u5ea6\u200b]</code>\uff0c\u200b\u5373\u200b\u56fe\u50cf\u200b\u6709\u200b <code>3</code> \u200b\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff08\u200b\u7ea2\u200b\u3001\u200b\u7eff\u200b\u3001\u200b\u84dd\u200b\uff09\uff0c\u200b\u9ad8\u5ea6\u200b\u4e3a\u200b <code>224</code> \u200b\u50cf\u7d20\u200b\uff0c\u200b\u5bbd\u5ea6\u200b\u4e3a\u200b <code>224</code> \u200b\u50cf\u7d20\u200b\u3002</p> <p></p> <p>\u200b\u5728\u200b\u5f20\u91cf\u200b\u672f\u8bed\u200b\uff08\u200b\u7528\u4e8e\u200b\u63cf\u8ff0\u200b\u5f20\u91cf\u200b\u7684\u200b\u8bed\u8a00\u200b\uff09\u200b\u4e2d\u200b\uff0c\u200b\u8be5\u200b\u5f20\u91cf\u200b\u5177\u6709\u200b\u4e09\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5206\u522b\u200b\u5bf9\u5e94\u200b <code>\u200b\u989c\u8272\u200b\u901a\u9053\u200b</code>\u3001<code>\u200b\u9ad8\u5ea6\u200b</code> \u200b\u548c\u200b <code>\u200b\u5bbd\u5ea6\u200b</code>\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u6709\u70b9\u200b\u8d85\u524d\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u7f16\u7801\u200b\u6765\u200b\u8fdb\u4e00\u6b65\u200b\u4e86\u89e3\u200b\u5f20\u91cf\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u521b\u5efa\u200b\u5f20\u91cf\u200b\u00b6","text":"<p>PyTorch \u200b\u70ed\u7231\u200b\u5f20\u91cf\u200b\u3002\u200b\u4ee5\u81f3\u4e8e\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u7684\u200b\u6587\u6863\u200b\u9875\u9762\u200b\u4e13\u95e8\u200b\u4ecb\u7ecd\u200b <code>torch.Tensor</code> \u200b\u7c7b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u7684\u200b\u7b2c\u4e00\u9879\u200b\u5bb6\u5ead\u4f5c\u4e1a\u200b\u662f\u200b \u200b\u9605\u8bfb\u200b <code>torch.Tensor</code> \u200b\u7684\u200b\u6587\u6863\u200b 10 \u200b\u5206\u949f\u200b\u3002\u200b\u4f46\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7a0d\u540e\u200b\u518d\u200b\u505a\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8981\u200b\u521b\u5efa\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u4e1c\u897f\u200b\u662f\u200b \u200b\u6807\u91cf\u200b\u3002</p> <p>\u200b\u6807\u91cf\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u6570\u5b57\u200b\uff0c\u200b\u5728\u200b\u5f20\u91cf\u200b\u672f\u8bed\u200b\u4e2d\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u4e00\u4e2a\u96f6\u200b\u7ef4\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u200b\u662f\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u4e00\u4e2a\u200b\u8d8b\u52bf\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u7f16\u5199\u200b\u7279\u5b9a\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002\u200b\u4f46\u200b\u901a\u5e38\u200b\u6211\u4f1a\u200b\u8bbe\u7f6e\u200b\u4e00\u4e9b\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u6d89\u53ca\u200b\u9605\u8bfb\u200b\u548c\u200b\u719f\u6089\u200b PyTorch \u200b\u6587\u6863\u200b\u3002\u200b\u6bd5\u7adf\u200b\uff0c\u200b\u4e00\u65e6\u200b\u4f60\u200b\u5b8c\u6210\u200b\u4e86\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u4f60\u200b\u65e0\u7591\u200b\u4f1a\u60f3\u200b\u5b66\u4e60\u200b\u66f4\u200b\u591a\u200b\u3002\u200b\u800c\u200b\u6587\u6863\u200b\u662f\u200b\u4f60\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u53bb\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u968f\u673a\u200b\u5f20\u91cf\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u786e\u7acb\u200b\u4e86\u200b\u5f20\u91cf\u200b\u4ee3\u8868\u200b\u67d0\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u800c\u200b\u8bf8\u5982\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e4b\u7c7b\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5219\u200b\u4f1a\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u64cd\u4f5c\u200b\u5e76\u200b\u4ece\u4e2d\u200b\u5bfb\u627e\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5728\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6784\u5efa\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u5f20\u91cf\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff09\u200b\u7684\u200b\u60c5\u51b5\u200b\u975e\u5e38\u200b\u7f55\u89c1\u200b\u3002</p> <p>\u200b\u76f8\u53cd\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u901a\u5e38\u200b\u4ece\u200b\u5305\u542b\u200b\u5927\u91cf\u200b\u968f\u673a\u6570\u200b\u7684\u200b\u5927\u200b\u5f20\u91cf\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8c03\u6574\u200b\u8fd9\u4e9b\u200b\u968f\u673a\u6570\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u66f4\u597d\u200b\u5730\u200b\u8868\u793a\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff1a</p> <p><code>\u200b\u4ece\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b -&gt; \u200b\u89c2\u5bdf\u200b\u6570\u636e\u200b -&gt; \u200b\u66f4\u65b0\u200b\u968f\u673a\u6570\u200b -&gt; \u200b\u89c2\u5bdf\u200b\u6570\u636e\u200b -&gt; \u200b\u66f4\u65b0\u200b\u968f\u673a\u6570\u200b...</code></p> <p>\u200b\u4f5c\u4e3a\u200b\u6570\u636e\u200b\u79d1\u5b66\u5bb6\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5b9a\u4e49\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5982\u4f55\u200b\u5f00\u59cb\u200b\uff08\u200b\u521d\u59cb\u5316\u200b\uff09\u3001\u200b\u5982\u4f55\u200b\u89c2\u5bdf\u200b\u6570\u636e\u200b\uff08\u200b\u8868\u793a\u200b\uff09\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u66f4\u65b0\u200b\uff08\u200b\u4f18\u5316\u200b\uff09\u200b\u5176\u200b\u968f\u673a\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u5b9e\u8df5\u200b\u4e2d\u200b\u4eb2\u81ea\u200b\u4f53\u9a8c\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u968f\u673a\u6570\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.rand()</code> \u200b\u5e76\u200b\u4f20\u5165\u200b <code>size</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u96f6\u200b\u548c\u200b\u4e00\u200b\u00b6","text":"<p>\u200b\u6709\u65f6\u200b\uff0c\u200b\u4f60\u200b\u53ea\u200b\u60f3\u200b\u7528\u200b\u96f6\u200b\u6216\u200b\u4e00\u200b\u586b\u5145\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5728\u200b\u63a9\u7801\u200b\u64cd\u4f5c\u200b\u4e2d\u200b\u5f88\u200b\u5e38\u89c1\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7528\u200b\u96f6\u200b\u63a9\u7801\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u7684\u200b\u67d0\u4e9b\u200b\u503c\u200b\uff0c\u200b\u8ba9\u200b\u6a21\u578b\u200b\u77e5\u9053\u200b\u4e0d\u8981\u200b\u5b66\u4e60\u200b\u8fd9\u4e9b\u200b\u503c\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torch.zeros()</code> \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5145\u6ee1\u200b\u96f6\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u540c\u6837\u200b\uff0c<code>size</code> \u200b\u53c2\u6570\u200b\u5728\u200b\u8fd9\u91cc\u200b\u8d77\u200b\u4f5c\u7528\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8303\u56f4\u200b\u548c\u200b\u7c7b\u4f3c\u200b\u5f20\u91cf\u200b\u00b6","text":"<p>\u200b\u6709\u65f6\u5019\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u8303\u56f4\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4ece\u200b1\u200b\u5230\u200b10\u200b\u6216\u200b\u4ece\u200b0\u200b\u5230\u200b100\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.arange(start, end, step)</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>start</code> = \u200b\u8303\u56f4\u200b\u7684\u200b\u8d77\u59cb\u503c\u200b\uff08\u200b\u4f8b\u5982\u200b 0\uff09</li> <li><code>end</code> = \u200b\u8303\u56f4\u200b\u7684\u200b\u7ed3\u675f\u200b\u503c\u200b\uff08\u200b\u4f8b\u5982\u200b 10\uff09</li> <li><code>step</code> = \u200b\u6bcf\u200b\u4e24\u4e2a\u200b\u503c\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6b65\u957f\u200b\uff08\u200b\u4f8b\u5982\u200b 1\uff09</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b Python \u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>range()</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8303\u56f4\u200b\u3002\u200b\u7136\u800c\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c<code>torch.range()</code> \u200b\u5df2\u200b\u88ab\u200b\u5f03\u7528\u200b\uff0c\u200b\u672a\u6765\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u663e\u793a\u200b\u9519\u8bef\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u5f20\u91cf\u200b\u6570\u636e\u7c7b\u578b\u200b\u00b6","text":"<p>\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b \u200b\u5f20\u91cf\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u6709\u4e9b\u200b\u4e13\u7528\u200b\u4e8e\u200b CPU\uff0c\u200b\u6709\u4e9b\u200b\u5219\u200b\u66f4\u200b\u9002\u5408\u200b GPU\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u5b83\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200b\u533a\u522b\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u901a\u5e38\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u770b\u5230\u200b <code>torch.cuda</code> \u200b\u5b57\u6837\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8fd9\u4e2a\u200b\u5f20\u91cf\u200b\u662f\u200b\u7528\u4e8e\u200b GPU \u200b\u7684\u200b\uff08\u200b\u56e0\u4e3a\u200b Nvidia GPU \u200b\u4f7f\u7528\u200b\u540d\u4e3a\u200b CUDA \u200b\u7684\u200b\u8ba1\u7b97\u200b\u5de5\u5177\u5305\u200b\uff09\u3002</p> <p>\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u7c7b\u578b\u200b\uff08\u200b\u901a\u5e38\u200b\u4e5f\u200b\u662f\u200b\u9ed8\u8ba4\u200b\u7c7b\u578b\u200b\uff09\u200b\u662f\u200b <code>torch.float32</code> \u200b\u6216\u200b <code>torch.float</code>\u3002</p> <p>\u200b\u8fd9\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c32 \u200b\u4f4d\u200b\u6d6e\u70b9\u6570\u200b\u201d\u3002</p> <p>\u200b\u4f46\u200b\u4e5f\u200b\u6709\u200b 16 \u200b\u4f4d\u200b\u6d6e\u70b9\u6570\u200b\uff08<code>torch.float16</code> \u200b\u6216\u200b <code>torch.half</code>\uff09\u200b\u548c\u200b 64 \u200b\u4f4d\u200b\u6d6e\u70b9\u6570\u200b\uff08<code>torch.float64</code> \u200b\u6216\u200b <code>torch.double</code>\uff09\u3002</p> <p>\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8fd8\u6709\u200b 8 \u200b\u4f4d\u200b\u300116 \u200b\u4f4d\u200b\u300132 \u200b\u4f4d\u200b\u548c\u200b 64 \u200b\u4f4d\u200b\u6574\u6570\u200b\u3002</p> <p>\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6574\u6570\u200b\u662f\u200b\u50cf\u200b <code>7</code> \u200b\u8fd9\u6837\u200b\u7684\u200b\u5e73\u6ed1\u200b\u5706\u200b\u6574\u6570\u200b\uff0c\u200b\u800c\u200b\u6d6e\u70b9\u6570\u200b\u6709\u200b\u5c0f\u6570\u70b9\u200b\uff0c\u200b\u5982\u200b <code>7.0</code>\u3002</p> <p>\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u5b58\u5728\u200b\u90fd\u200b\u4e0e\u200b \u200b\u8ba1\u7b97\u7cbe\u5ea6\u200b \u200b\u6709\u5173\u200b\u3002</p> <p>\u200b\u7cbe\u5ea6\u200b\u662f\u200b\u6307\u200b\u63cf\u8ff0\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200b\u7ec6\u8282\u200b\u91cf\u200b\u3002</p> <p>\u200b\u7cbe\u5ea6\u200b\u503c\u8d8a\u200b\u9ad8\u200b\uff088\u300116\u300132\uff09\uff0c\u200b\u8868\u793a\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u7ec6\u8282\u200b\u548c\u200b\u6570\u636e\u200b\u5c31\u200b\u8d8a\u200b\u591a\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b\u6570\u503c\u200b\u8ba1\u7b97\u200b\u4e2d\u200b\u5f88\u200b\u91cd\u8981\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u5982\u6b64\u200b\u591a\u200b\u7684\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u8ba1\u7b97\u6240\u200b\u9700\u200b\u7684\u200b\u7ec6\u8282\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7684\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u5c31\u200b\u8d8a\u200b\u591a\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8f83\u200b\u4f4e\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u901a\u5e38\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff08\u200b\u5982\u200b\u51c6\u786e\u6027\u200b\uff09\u200b\u4e0a\u200b\u4f1a\u200b\u727a\u7272\u200b\u4e00\u4e9b\u200b\u6027\u80fd\u200b\uff08\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u5feb\u200b\u4f46\u200b\u7cbe\u5ea6\u200b\u8f83\u200b\u4f4e\u200b\uff09\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u67e5\u770b\u200b PyTorch \u200b\u6587\u6863\u200b\u4e2d\u200b\u6240\u6709\u200b\u53ef\u7528\u200b\u5f20\u91cf\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u5217\u8868\u200b\u3002</li> <li>\u200b\u9605\u8bfb\u200b Wikipedia \u200b\u9875\u9762\u200b\u4e86\u89e3\u200b\u8ba1\u7b97\u200b\u4e2d\u200b\u7684\u200b\u7cbe\u5ea6\u200b\u6982\u8ff0\u200b)\u3002</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u5177\u6709\u200b\u7279\u5b9a\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>dtype</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u83b7\u53d6\u200b\u5f20\u91cf\u200b\u7684\u200b\u4fe1\u606f\u200b\u00b6","text":"<p>\u200b\u4e00\u65e6\u200b\u4f60\u200b\u521b\u5efa\u200b\u4e86\u200b\u5f20\u91cf\u200b\uff08\u200b\u6216\u8005\u200b\u7531\u200b\u4ed6\u4eba\u200b\u6216\u200b PyTorch \u200b\u6a21\u5757\u200b\u4e3a\u200b\u4f60\u200b\u521b\u5efa\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u4ece\u4e2d\u200b\u83b7\u53d6\u200b\u4e00\u4e9b\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u5df2\u7ecf\u200b\u89c1\u8fc7\u200b\u8fd9\u4e9b\u200b\u5c5e\u6027\u200b\uff0c\u200b\u4f46\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e09\u4e2a\u200b\u5f20\u91cf\u200b\u5c5e\u6027\u200b\u662f\u200b\uff1a</p> <ul> <li><code>shape</code> - \u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\uff08\u200b\u67d0\u4e9b\u200b\u64cd\u4f5c\u200b\u9700\u8981\u200b\u7279\u5b9a\u200b\u7684\u200b\u5f62\u72b6\u200b\u89c4\u5219\u200b\uff09</li> <li><code>dtype</code> - \u200b\u5f20\u91cf\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\u5b58\u50a8\u200b\u4e3a\u4ec0\u4e48\u200b\u6570\u636e\u7c7b\u578b\u200b\uff1f</li> <li><code>device</code> - \u200b\u5f20\u91cf\u200b\u5b58\u50a8\u200b\u5728\u200b\u54ea\u4e2a\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff1f\uff08\u200b\u901a\u5e38\u200b\u662f\u200b GPU \u200b\u6216\u200b CPU\uff09</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5e76\u200b\u627e\u51fa\u200b\u5b83\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u64cd\u4f5c\u200b\u5f20\u91cf\u200b\uff08\u200b\u5f20\u91cf\u200b\u8fd0\u7b97\u200b\uff09\u00b6","text":"<p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u6570\u636e\u200b\uff08\u200b\u56fe\u50cf\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u89c6\u9891\u200b\u3001\u200b\u97f3\u9891\u200b\u3001\u200b\u86cb\u767d\u8d28\u200b\u7ed3\u6784\u200b\u7b49\u200b\uff09\u200b\u88ab\u200b\u8868\u793a\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u6a21\u578b\u200b\u901a\u8fc7\u200b\u7814\u7a76\u200b\u8fd9\u4e9b\u200b\u5f20\u91cf\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u6267\u884c\u200b\u4e00\u7cfb\u5217\u200b\u64cd\u4f5c\u200b\uff08\u200b\u53ef\u80fd\u200b\u662f\u200b\u6570\u767e\u4e07\u200b\u6b21\u200b\uff09\u200b\u6765\u200b\u5b66\u4e60\u200b\uff0c\u200b\u4ece\u800c\u200b\u521b\u5efa\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u4e2d\u200b\u6a21\u5f0f\u200b\u7684\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u901a\u5e38\u200b\u662f\u200b\u4ee5\u4e0b\u200b\u51e0\u79cd\u200b\u57fa\u672c\u200b\u8fd0\u7b97\u200b\u7684\u200b\u7cbe\u5f69\u200b\u7ec4\u5408\u200b\uff1a</p> <ul> <li>\u200b\u52a0\u6cd5\u200b</li> <li>\u200b\u51cf\u6cd5\u200b</li> <li>\u200b\u4e58\u6cd5\u200b\uff08\u200b\u9010\u200b\u5143\u7d20\u200b\uff09</li> <li>\u200b\u9664\u6cd5\u200b</li> <li>\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b</li> </ul> <p>\u200b\u4ec5\u6b64\u800c\u5df2\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u57fa\u672c\u200b\u6784\u5efa\u200b\u5757\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b\u4ee5\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5806\u53e0\u200b\u8fd9\u4e9b\u200b\u6784\u5efa\u200b\u5757\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u6700\u200b\u590d\u6742\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4e50\u9ad8\u200b\u79ef\u6728\u200b\u4e00\u6837\u200b\uff01\uff09\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u57fa\u672c\u64cd\u4f5c\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u4e9b\u200b\u57fa\u672c\u200b\u7684\u200b\u64cd\u4f5c\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5305\u62ec\u200b\u52a0\u6cd5\u200b\uff08<code>+</code>\uff09\u3001\u200b\u51cf\u6cd5\u200b\uff08<code>-</code>\uff09\u200b\u548c\u200b\u4e58\u6cd5\u200b\uff08<code>*</code>\uff09\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u7684\u200b\u5de5\u4f5c\u200b\u65b9\u5f0f\u200b\u6b63\u5982\u200b\u4f60\u200b\u6240\u200b\u60f3\u200b\u7684\u200b\u90a3\u6837\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u4e00\u5207\u200b\u5c3d\u200b\u5728\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u4e2d\u200b\uff09\u00b6","text":"<p>\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\uff08\u200b\u5982\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u4e2d\u200b\uff0c\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u64cd\u4f5c\u200b\u4e4b\u4e00\u200b\u5c31\u662f\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>PyTorch \u200b\u5728\u200b <code>torch.matmul()</code> \u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u9700\u8981\u200b\u8bb0\u4f4f\u200b\u7684\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u89c4\u5219\u200b\u662f\u200b\uff1a</p> <ol> <li>\u200b\u5185\u200b\u7ef4\u5ea6\u200b\u5fc5\u987b\u200b\u5339\u914d\u200b\uff1a<ul> <li><code>(3, 2) @ (3, 2)</code> \u200b\u65e0\u6cd5\u200b\u8fdb\u884c\u200b</li> <li><code>(2, 3) @ (3, 2)</code> \u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b</li> <li><code>(3, 2) @ (2, 3)</code> \u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b</li> </ul> </li> <li>\u200b\u7ed3\u679c\u200b\u77e9\u9635\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\u5916\u200b\u7ef4\u5ea6\u200b\uff1a<ul> <li><code>(2, 3) @ (3, 2)</code> -&gt; <code>(2, 2)</code></li> <li><code>(3, 2) @ (2, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> </li> </ol> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b Python \u200b\u4e2d\u200b\uff0c\"<code>@</code>\" \u200b\u7b26\u53f7\u200b\u8868\u793a\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b PyTorch \u200b\u6587\u6863\u200b\u4e2d\u200b\u67e5\u770b\u200b\u6240\u6709\u200b\u5173\u4e8e\u200b <code>torch.matmul()</code> \u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u89c4\u5219\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u5b9e\u73b0\u200b\u9010\u200b\u5143\u7d20\u200b\u4e58\u6cd5\u200b\u548c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u9519\u8bef\u200b\u4e4b\u4e00\u200b\uff08\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff09\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5f88\u5927\u200b\u4e00\u90e8\u5206\u200b\u6d89\u53ca\u200b\u77e9\u9635\u200b\u7684\u200b\u4e58\u6cd5\u200b\u548c\u200b\u8fd0\u7b97\u200b\uff0c\u200b\u800c\u200b\u77e9\u9635\u200b\u5bf9\u4e8e\u200b\u5f62\u72b6\u200b\u548c\u200b\u5927\u5c0f\u200b\u7684\u200b\u7ec4\u5408\u200b\u6709\u200b\u4e25\u683c\u200b\u7684\u200b\u89c4\u5219\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f60\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u4f1a\u200b\u9047\u5230\u200b\u7684\u200b\u6700\u200b\u5e38\u89c1\u200b\u9519\u8bef\u200b\u4e4b\u4e00\u200b\u5c31\u662f\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u67e5\u627e\u200b\u6700\u5c0f\u503c\u200b\u3001\u200b\u6700\u5927\u503c\u200b\u3001\u200b\u5747\u503c\u200b\u3001\u200b\u603b\u548c\u200b\u7b49\u200b\uff08\u200b\u805a\u5408\u200b\u64cd\u4f5c\u200b\uff09\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u51e0\u79cd\u200b\u64cd\u4f5c\u200b\u5f20\u91cf\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u63a2\u8ba8\u200b\u51e0\u79cd\u200b\u805a\u5408\u200b\u5f20\u91cf\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff08\u200b\u4ece\u200b\u591a\u4e2a\u200b\u503c\u200b\u53d8\u4e3a\u200b\u8f83\u5c11\u503c\u200b\uff09\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u7136\u540e\u200b\u627e\u51fa\u200b\u5b83\u200b\u7684\u200b\u6700\u5927\u503c\u200b\u3001\u200b\u6700\u5c0f\u503c\u200b\u3001\u200b\u5747\u503c\u200b\u548c\u200b\u603b\u548c\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u4f4d\u7f6e\u200b\u6700\u5c0f\u503c\u200b/\u200b\u6700\u5927\u503c\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5206\u522b\u200b\u4f7f\u7528\u200b <code>torch.argmax()</code> \u200b\u548c\u200b <code>torch.argmin()</code> \u200b\u627e\u5230\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u6700\u5927\u503c\u200b\u6216\u200b\u6700\u5c0f\u503c\u200b\u51fa\u73b0\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u7d22\u5f15\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5728\u200b\u53ea\u200b\u9700\u8981\u200b\u77e5\u9053\u200b\u6700\u9ad8\u200b\uff08\u200b\u6216\u200b\u6700\u4f4e\u200b\uff09\u200b\u503c\u200b\u6240\u5728\u4f4d\u7f6e\u200b\u800c\u200b\u4e0d\u200b\u9700\u8981\u200b\u5b9e\u9645\u200b\u503c\u200b\u672c\u8eab\u200b\u65f6\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\u770b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4f8b\u5982\u200b\u5728\u200b\u4f7f\u7528\u200b softmax \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u65f6\u200b\uff09\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u6539\u53d8\u200b\u5f20\u91cf\u200b\u6570\u636e\u7c7b\u578b\u200b\u00b6","text":"<p>\u200b\u5982\u524d\u6240\u8ff0\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u64cd\u4f5c\u200b\u4e2d\u200b\u5e38\u89c1\u200b\u7684\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b\u662f\u200b\u5f20\u91cf\u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u662f\u200b <code>torch.float64</code> \u200b\u7c7b\u578b\u200b\uff0c\u200b\u800c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u662f\u200b <code>torch.float32</code> \u200b\u7c7b\u578b\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u4e00\u4e9b\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6709\u200b\u4e00\u4e2a\u200b\u89e3\u51b3\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.Tensor.type(dtype=None)</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>dtype</code> \u200b\u53c2\u6570\u200b\u662f\u200b\u4f60\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u5e76\u200b\u68c0\u67e5\u200b\u5176\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u662f\u200b <code>torch.float32</code>\uff09\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u91cd\u5851\u200b\u3001\u200b\u5806\u53e0\u200b\u3001\u200b\u538b\u7f29\u200b\u548c\u200b\u89e3\u538b\u7f29\u200b\u00b6","text":"<p>\u200b\u5f88\u591a\u200b\u65f6\u5019\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u5728\u200b\u4e0d\u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\u5185\u90e8\u200b\u503c\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u91cd\u5851\u200b\u6216\u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u4e00\u4e9b\u200b\u5e38\u7528\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5305\u62ec\u200b\uff1a</p> \u200b\u65b9\u6cd5\u200b \u200b\u4e00\u884c\u200b\u63cf\u8ff0\u200b <code>torch.reshape(input, shape)</code> \u200b\u5c06\u200b <code>input</code> \u200b\u91cd\u5851\u200b\u4e3a\u200b <code>shape</code>\uff08\u200b\u5982\u679c\u200b\u517c\u5bb9\u200b\uff09\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.Tensor.reshape()</code>\u3002 <code>Tensor.view(shape)</code> \u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b <code>shape</code> \u200b\u7684\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u89c6\u56fe\u200b\uff0c\u200b\u4f46\u200b\u4e0e\u200b\u539f\u59cb\u200b\u5f20\u91cf\u200b\u5171\u4eab\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u3002 <code>torch.stack(tensors, dim=0)</code> \u200b\u6cbf\u65b0\u200b\u7ef4\u5ea6\u200b (<code>dim</code>) \u200b\u8fde\u63a5\u200b\u4e00\u7cfb\u5217\u200b <code>tensors</code>\uff0c\u200b\u6240\u6709\u200b <code>tensors</code> \u200b\u5fc5\u987b\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002 <code>torch.squeeze(input)</code> \u200b\u538b\u7f29\u200b <code>input</code> \u200b\u4ee5\u200b\u79fb\u9664\u200b\u6240\u6709\u200b\u503c\u4e3a\u200b <code>1</code> \u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002 <code>torch.unsqueeze(input, dim)</code> \u200b\u5728\u200b <code>dim</code> \u200b\u5904\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u503c\u4e3a\u200b <code>1</code> \u200b\u7684\u200b\u7ef4\u5ea6\u200b\u8fd4\u56de\u200b <code>input</code>\u3002 <code>torch.permute(input, dims)</code> \u200b\u8fd4\u56de\u200b\u539f\u59cb\u200b <code>input</code> \u200b\u7684\u200b\u4e00\u4e2a\u200b\u89c6\u56fe\u200b\uff0c\u200b\u5176\u200b\u7ef4\u5ea6\u200b\u6309\u200b <code>dims</code> \u200b\u91cd\u65b0\u6392\u5217\u200b\u3002 <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff08\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u90fd\u200b\u662f\u200b\u4ee5\u200b\u67d0\u79cd\u200b\u65b9\u5f0f\u200b\u64cd\u7eb5\u200b\u5f20\u91cf\u200b\u7684\u200b\u3002\u200b\u7531\u4e8e\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u89c4\u5219\u200b\uff0c\u200b\u5982\u679c\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u9047\u5230\u200b\u9519\u8bef\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u786e\u4fdd\u200b\u4f60\u200b\u7684\u200b\u5f20\u91cf\u200b\u7684\u200b\u6b63\u786e\u200b\u5143\u7d20\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u5f20\u91cf\u200b\u7684\u200b\u6b63\u786e\u200b\u5143\u7d20\u200b\u6df7\u5408\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u7d22\u5f15\u200b\uff08\u200b\u4ece\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u9009\u62e9\u200b\u6570\u636e\u200b\uff09\u00b6","text":"<p>\u200b\u6709\u65f6\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u4ece\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u9009\u62e9\u200b\u7279\u5b9a\u200b\u6570\u636e\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4ec5\u200b\u9009\u62e9\u200b\u7b2c\u4e00\u5217\u200b\u6216\u200b\u7b2c\u4e8c\u884c\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u7d22\u5f15\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u60a8\u200b\u66fe\u7ecf\u200b\u5728\u200b Python \u200b\u5217\u8868\u200b\u6216\u200b NumPy \u200b\u6570\u7ec4\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8fc7\u200b\u7d22\u5f15\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\u64cd\u4f5c\u200b\u4e0e\u200b\u6b64\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/#pytorch-numpy","title":"PyTorch \u200b\u5f20\u91cf\u200b\u4e0e\u200b NumPy\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b NumPy \u200b\u662f\u200b Python \u200b\u4e2d\u200b\u6d41\u884c\u200b\u7684\u200b\u6570\u503c\u200b\u8ba1\u7b97\u200b\u5e93\u200b\uff0cPyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e0e\u200b NumPy \u200b\u826f\u597d\u200b\u4ea4\u4e92\u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u4ece\u200b NumPy \u200b\u5230\u200b PyTorch\uff08\u200b\u4ee5\u53ca\u200b\u53cd\u8fc7\u6765\u200b\uff09\u200b\u7684\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u65b9\u6cd5\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li><code>torch.from_numpy(ndarray)</code> - NumPy \u200b\u6570\u7ec4\u200b -&gt; PyTorch \u200b\u5f20\u91cf\u200b\u3002</li> <li><code>torch.Tensor.numpy()</code> - PyTorch \u200b\u5f20\u91cf\u200b -&gt; NumPy \u200b\u6570\u7ec4\u200b\u3002</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u53ef\u91cd\u590d\u6027\u200b\uff08\u200b\u8bd5\u56fe\u200b\u4ece\u200b\u968f\u673a\u200b\u4e2d\u200b\u53bb\u9664\u200b\u968f\u673a\u6027\u200b\uff09\u00b6","text":"<p>\u200b\u968f\u7740\u200b\u4f60\u200b\u5bf9\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u548c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e86\u89e3\u200b\u52a0\u6df1\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u968f\u673a\u6027\u200b\u5728\u200b\u5176\u4e2d\u200b\u626e\u6f14\u200b\u4e86\u200b\u591a\u4e48\u200b\u91cd\u8981\u200b\u7684\u200b\u89d2\u8272\u200b\u3002</p> <p>\u200b\u55ef\u200b\uff0c\u200b\u786e\u5207\u200b\u5730\u200b\u8bf4\u200b\u662f\u200b\u4f2a\u200b\u968f\u673a\u6027\u200b\u3002\u200b\u6bd5\u7adf\u200b\uff0c\u200b\u4ece\u200b\u8bbe\u8ba1\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0c\u200b\u8ba1\u7b97\u673a\u200b\u672c\u8d28\u200b\u4e0a\u200b\u662f\u975e\u200b\u968f\u673a\u200b\u7684\u200b\uff08\u200b\u6bcf\u200b\u4e00\u6b65\u200b\u90fd\u200b\u662f\u200b\u53ef\u200b\u9884\u6d4b\u200b\u7684\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u5b83\u4eec\u200b\u4ea7\u751f\u200b\u7684\u200b\u968f\u673a\u6027\u200b\u662f\u200b\u6a21\u62df\u200b\u51fa\u6765\u200b\u7684\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u5bf9\u6b64\u200b\u4e5f\u200b\u6709\u200b\u4e89\u8bae\u200b\uff0c\u200b\u4f46\u200b\u65e2\u7136\u200b\u6211\u200b\u4e0d\u662f\u200b\u8ba1\u7b97\u673a\u200b\u79d1\u5b66\u5bb6\u200b\uff0c\u200b\u5c31\u8ba9\u200b\u4f60\u200b\u81ea\u5df1\u200b\u53bb\u200b\u63a2\u7d22\u200b\u66f4\u200b\u591a\u200b\u5427\u200b\uff09\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u8fd9\u200b\u4e0e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6709\u200b\u4ec0\u4e48\u200b\u5173\u7cfb\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u8fc7\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4ece\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b\u6765\u200b\u63cf\u8ff0\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u968f\u673a\u6570\u200b\u662f\u200b\u7cdf\u7cd5\u200b\u7684\u200b\u63cf\u8ff0\u200b\uff09\uff0c\u200b\u5e76\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b\u5f20\u91cf\u200b\u64cd\u4f5c\u200b\uff08\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u5c1a\u672a\u200b\u8ba8\u8bba\u200b\u7684\u200b\u5176\u4ed6\u200b\u4e00\u4e9b\u200b\u65b9\u6cd5\u200b\uff09\u200b\u6765\u200b\u6539\u8fdb\u200b\u8fd9\u4e9b\u200b\u968f\u673a\u6570\u200b\uff0c\u200b\u4ee5\u200b\u66f4\u597d\u200b\u5730\u200b\u63cf\u8ff0\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u7b80\u800c\u8a00\u4e4b\u200b\uff1a</p> <p><code>\u200b\u4ece\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b -&gt; \u200b\u5f20\u91cf\u200b\u64cd\u4f5c\u200b -&gt; \u200b\u5c1d\u8bd5\u200b\u53d8\u5f97\u200b\u66f4\u597d\u200b\uff08\u200b\u4e00\u6b21\u200b\u53c8\u200b\u4e00\u6b21\u200b\uff09</code></p> <p>\u200b\u867d\u7136\u200b\u968f\u673a\u6027\u200b\u65e2\u200b\u7f8e\u597d\u200b\u53c8\u200b\u5f3a\u5927\u200b\uff0c\u200b\u4f46\u200b\u6709\u65f6\u200b\u4f60\u200b\u5e0c\u671b\u200b\u968f\u673a\u6027\u200b\u5c11\u200b\u4e00\u4e9b\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u8fd9\u6837\u200b\u4f60\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u53ef\u200b\u91cd\u590d\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u8fbe\u5230\u200bX\u200b\u6027\u80fd\u200b\u7684\u200b\u7b97\u6cd5\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\u4f60\u200b\u7684\u200b\u670b\u53cb\u200b\u5c1d\u8bd5\u200b\u9a8c\u8bc1\u200b\u4f60\u200b\u5e76\u200b\u6ca1\u6709\u200b\u53d1\u75af\u200b\u3002</p> <p>\u200b\u4ed6\u4eec\u200b\u5982\u4f55\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u53ef\u91cd\u590d\u6027\u200b\u7684\u200b\u4f5c\u7528\u200b\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u4f60\u200b\u80fd\u200b\u5728\u200b\u4f60\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u76f8\u540c\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5f97\u5230\u200b\u4e0e\u200b\u6211\u200b\u4e00\u6837\u200b\u7684\u200b\uff08\u200b\u6216\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\u7684\u200b\uff09\u200b\u7ed3\u679c\u200b\u5417\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u200b\u4e00\u4e2a\u200bPyTorch\u200b\u4e2d\u200b\u53ef\u91cd\u590d\u6027\u200b\u7684\u200b\u7b80\u77ed\u200b\u793a\u4f8b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\u5f00\u59cb\u200b\uff0c\u200b\u65e2\u7136\u200b\u5b83\u4eec\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u671f\u671b\u200b\u5b83\u4eec\u200b\u662f\u200b\u4e0d\u540c\u200b\u7684\u200b\uff0c\u200b\u5bf9\u200b\u5427\u200b\uff1f</p>"},{"location":"00_pytorch_fundamentals/#gpu","title":"\u5728\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u5f20\u91cf\u200b\uff08\u200b\u5e76\u200b\u52a0\u5feb\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\uff09\u00b6","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u9700\u8981\u200b\u5927\u91cf\u200b\u7684\u200b\u6570\u503c\u200b\u8fd0\u7b97\u200b\u3002</p> <p>\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u8fd0\u7b97\u200b\u901a\u5e38\u200b\u5728\u200bCPU\uff08\u200b\u4e2d\u592e\u200b\u5904\u7406\u5355\u5143\u200b\uff09\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd8\u6709\u200b\u53e6\u200b\u4e00\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u786c\u4ef6\u200b\u2014\u2014GPU\uff08\u200b\u56fe\u5f62\u200b\u5904\u7406\u5355\u5143\u200b\uff09\uff0c\u200b\u5b83\u200b\u901a\u5e38\u200b\u5728\u200b\u6267\u884c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6240\u200b\u9700\u200b\u7279\u5b9a\u200b\u7c7b\u578b\u200b\u7684\u200b\u8fd0\u7b97\u200b\uff08\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff09\u200b\u65b9\u9762\u200b\u6bd4\u200bCPU\u200b\u5feb\u5f97\u591a\u200b\u3002</p> <p>\u200b\u4f60\u200b\u7684\u200b\u7535\u8111\u200b\u53ef\u80fd\u200b\u5c31\u200b\u6709\u200b\u4e00\u5757\u200bGPU\u3002</p> <p>\u200b\u5982\u679c\u200b\u662f\u200b\u8fd9\u6837\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u5c3d\u53ef\u80fd\u200b\u5730\u200b\u5229\u7528\u200b\u5b83\u200b\u6765\u200b\u8bad\u7ec3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5927\u5e45\u200b\u7f29\u77ed\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6709\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u9996\u5148\u200b\u83b7\u53d6\u200bGPU\u200b\u7684\u200b\u8bbf\u95ee\u200b\u6743\u9650\u200b\uff0c\u200b\u7136\u540e\u200b\u8ba9\u200bPyTorch\u200b\u4f7f\u7528\u200bGPU\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u5f53\u200b\u6211\u200b\u63d0\u5230\u200b\u201cGPU\u201d\u200b\u65f6\u200b\uff0c\u200b\u9664\u975e\u200b\u53e6\u6709\u200b\u8bf4\u660e\u200b\uff0c\u200b\u6211\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u542f\u7528\u200b\u4e86\u200bNvidia GPU with CUDA\uff08CUDA\u200b\u662f\u200b\u4e00\u79cd\u200b\u8ba1\u7b97\u200b\u5e73\u53f0\u200b\u548c\u200bAPI\uff0c\u200b\u6709\u52a9\u4e8e\u200b\u4f7f\u200bGPU\u200b\u80fd\u591f\u200b\u7528\u4e8e\u200b\u901a\u7528\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b\u4e0d\u4ec5\u4ec5\u200b\u662f\u200b\u56fe\u5f62\u5904\u7406\u200b\uff09\u3002</p>"},{"location":"00_pytorch_fundamentals/#1-gpu","title":"1. \u200b\u83b7\u53d6\u200bGPU\u00b6","text":"<p>\u200b\u5f53\u200b\u4f60\u200b\u542c\u5230\u200bGPU\u200b\u8fd9\u4e2a\u200b\u8bcd\u65f6\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u77e5\u9053\u200b\u662f\u200b\u600e\u4e48\u56de\u4e8b\u200b\u4e86\u200b\u3002\u200b\u4f46\u200b\u5982\u679c\u200b\u4f60\u200b\u8fd8\u200b\u4e0d\u200b\u6e05\u695a\u200b\uff0c\u200b\u6709\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u83b7\u53d6\u200bGPU\u200b\u7684\u200b\u4f7f\u7528\u200b\u6743\u9650\u200b\u3002</p> \u200b\u65b9\u6cd5\u200b \u200b\u8bbe\u7f6e\u200b\u96be\u5ea6\u200b \u200b\u4f18\u70b9\u200b \u200b\u7f3a\u70b9\u200b \u200b\u8bbe\u7f6e\u200b\u65b9\u6cd5\u200b Google Colab \u200b\u7b80\u5355\u200b \u200b\u514d\u8d39\u200b\u4f7f\u7528\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u65e0\u9700\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u901a\u8fc7\u200b\u94fe\u63a5\u200b\u5206\u4eab\u200b\u5de5\u4f5c\u200b \u200b\u4e0d\u200b\u4fdd\u5b58\u200b\u6570\u636e\u200b\u8f93\u51fa\u200b\uff0c\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u6709\u9650\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8d85\u65f6\u200b \u200b\u9075\u5faa\u200bGoogle Colab\u200b\u6307\u5357\u200b \u200b\u4f7f\u7528\u200b\u81ea\u5df1\u200b\u7684\u200b \u200b\u4e2d\u7b49\u200b \u200b\u5728\u200b\u672c\u5730\u200b\u673a\u5668\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b GPU\u200b\u4e0d\u662f\u200b\u514d\u8d39\u200b\u7684\u200b\uff0c\u200b\u9700\u8981\u200b\u524d\u671f\u200b\u6210\u672c\u200b \u200b\u9075\u5faa\u200bPyTorch\u200b\u5b89\u88c5\u200b\u6307\u5357\u200b \u200b\u4e91\u200b\u8ba1\u7b97\u200b\uff08AWS\u3001GCP\u3001Azure\uff09 \u200b\u4e2d\u7b49\u200b\u81f3\u200b\u56f0\u96be\u200b \u200b\u524d\u671f\u200b\u6210\u672c\u200b\u5c0f\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u65e0\u9650\u200b\u4f7f\u7528\u200b\u8ba1\u7b97\u8d44\u6e90\u200b \u200b\u5982\u679c\u200b\u6301\u7eed\u200b\u8fd0\u884c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f88\u200b\u6602\u8d35\u200b\uff0c\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u65f6\u95f4\u200b\u6765\u200b\u6b63\u786e\u200b\u8bbe\u7f6e\u200b \u200b\u9075\u5faa\u200bPyTorch\u200b\u5b89\u88c5\u200b\u6307\u5357\u200b <p>\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u4f7f\u7528\u200bGPU\u200b\u7684\u200b\u9009\u9879\u200b\uff0c\u200b\u4f46\u200b\u4e0a\u8ff0\u200b\u4e09\u79cd\u200b\u65b9\u6cd5\u200b\u76ee\u524d\u200b\u8db3\u591f\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u5c31\u200b\u6211\u200b\u4e2a\u4eba\u200b\u800c\u8a00\u200b\uff0c\u200b\u6211\u200b\u7ed3\u5408\u200b\u4f7f\u7528\u200bGoogle Colab\u200b\u548c\u200b\u81ea\u5df1\u200b\u7684\u200b\u4e2a\u4eba\u7535\u8111\u200b\u8fdb\u884c\u200b\u5c0f\u89c4\u6a21\u200b\u5b9e\u9a8c\u200b\uff08\u200b\u4ee5\u53ca\u200b\u521b\u5efa\u200b\u8fd9\u4e2a\u200b\u8bfe\u7a0b\u200b\uff09\uff0c\u200b\u5e76\u200b\u5728\u200b\u9700\u8981\u200b\u66f4\u200b\u591a\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u65f6\u200b\u8f6c\u5411\u200b\u4e91\u200b\u8d44\u6e90\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u8d2d\u4e70\u200b\u81ea\u5df1\u200b\u7684\u200bGPU\u200b\u4f46\u200b\u4e0d\u200b\u786e\u5b9a\u200b\u9009\u62e9\u200b\u54ea\u200b\u79cd\u200b\uff0cTim Dettmers\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5f88\u68d2\u200b\u7684\u200b\u6307\u5357\u200b\u3002</p> <p>\u200b\u8981\u200b\u68c0\u67e5\u200b\u4f60\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200bNvidia GPU\uff0c\u200b\u53ef\u4ee5\u200b\u8fd0\u884c\u200b<code>!nvidia-smi</code>\uff0c\u200b\u5176\u4e2d\u200b<code>!</code>\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200bbang\uff09\u200b\u8868\u793a\u200b\u201c\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u6b64\u200b\u547d\u4ee4\u200b\u201d\u3002</p>"},{"location":"00_pytorch_fundamentals/#2-pytorch-gpu","title":"2. \u200b\u8ba9\u200b PyTorch \u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u00b6","text":"<p>\u200b\u4e00\u65e6\u200b\u4f60\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u8bbf\u95ee\u200b\u7684\u200b GPU\uff0c\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u5c31\u662f\u200b\u8ba9\u200b PyTorch \u200b\u4f7f\u7528\u200b\u5b83\u200b\u6765\u200b\u5b58\u50a8\u200b\u6570\u636e\u200b\uff08\u200b\u5f20\u91cf\u200b\uff09\u200b\u548c\u200b\u5904\u7406\u200b\u6570\u636e\u200b\uff08\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.cuda</code> \u200b\u5305\u200b\u3002</p> <p>\u200b\u4e0e\u5176\u200b\u8c08\u8bba\u200b\u5b83\u200b\uff0c\u200b\u4e0d\u5982\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.cuda.is_available()</code> \u200b\u6765\u200b\u6d4b\u8bd5\u200b PyTorch \u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b GPU\u3002</p>"},{"location":"00_pytorch_fundamentals/#21-m-pytorch","title":"2.1 \u200b\u5728\u200b\u82f9\u679c\u200b M\u200b\u7cfb\u200b\u82af\u7247\u200b\u4e0a\u200b\u8fd0\u884c\u200b PyTorch\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u5728\u200b\u82f9\u679c\u200b\u7684\u200b M1/M2/M3 GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b PyTorch\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.backends.mps</code> \u200b\u6a21\u5757\u200b\u3002</p> <p>\u200b\u8bf7\u200b\u786e\u4fdd\u200b macOS \u200b\u548c\u200b PyTorch \u200b\u7684\u200b\u7248\u672c\u200b\u5df2\u200b\u66f4\u65b0\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.backends.mps.is_available()</code> \u200b\u6765\u200b\u6d4b\u8bd5\u200b PyTorch \u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b GPU\u3002</p>"},{"location":"00_pytorch_fundamentals/#3-gpu","title":"3. \u200b\u5c06\u200b\u5f20\u91cf\u200b\uff08\u200b\u548c\u200b\u6a21\u578b\u200b\uff09\u200b\u7f6e\u4e8e\u200bGPU\u200b\u4e0a\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8c03\u7528\u200b <code>to(device)</code> \u200b\u65b9\u6cd5\u200b\u5c06\u200b\u5f20\u91cf\u200b\uff08\u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u200b\u7f6e\u4e8e\u200b\u7279\u5b9a\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002\u200b\u5176\u4e2d\u200b\uff0c<code>device</code> \u200b\u662f\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5f20\u91cf\u200b\uff08\u200b\u6216\u200b\u6a21\u578b\u200b\uff09\u200b\u6240\u200b\u8981\u200b\u653e\u7f6e\u200b\u7684\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1f</p> <p>GPU \u200b\u63d0\u4f9b\u200b\u7684\u200b\u6570\u503c\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u8fdc\u5feb\u200b\u4e8e\u200b CPU\uff0c\u200b\u5e76\u4e14\u200b\u5982\u679c\u200b GPU \u200b\u4e0d\u53ef\u200b\u7528\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b \u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\uff08\u200b\u89c1\u200b\u4e0a\u6587\u200b\uff09\uff0c\u200b\u5b83\u200b\u5c06\u200b\u8fd0\u884c\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f7f\u7528\u200b <code>to(device)</code> \u200b\u65b9\u6cd5\u200b\u5c06\u200b\u5f20\u91cf\u200b\u7f6e\u4e8e\u200b GPU \u200b\u4e0a\u200b\uff08\u200b\u4f8b\u5982\u200b <code>some_tensor.to(device)</code>\uff09\u200b\u4f1a\u200b\u8fd4\u56de\u200b\u8be5\u200b\u5f20\u91cf\u200b\u7684\u200b\u526f\u672c\u200b\uff0c\u200b\u5373\u200b\u76f8\u540c\u200b\u7684\u200b\u5f20\u91cf\u200b\u5c06\u200b\u540c\u65f6\u200b\u5b58\u5728\u200b\u4e8e\u200b CPU \u200b\u548c\u200b GPU \u200b\u4e0a\u200b\u3002\u200b\u8981\u200b\u8986\u76d6\u200b\u5f20\u91cf\u200b\uff0c\u200b\u9700\u8981\u200b\u91cd\u65b0\u200b\u8d4b\u503c\u200b\uff1a</p> <p><code>some_tensor = some_tensor.to(device)</code></p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u7f6e\u4e8e\u200b GPU \u200b\u4e0a\u200b\uff08\u200b\u5982\u679c\u200b GPU \u200b\u53ef\u7528\u200b\uff09\u3002</p>"},{"location":"00_pytorch_fundamentals/#4-cpu","title":"4. \u200b\u5c06\u200b\u5f20\u91cf\u200b\u79fb\u56de\u200bCPU\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u200b\u5c06\u200b\u5f20\u91cf\u200b\u79fb\u56de\u200bCPU\uff0c\u200b\u5e94\u8be5\u200b\u600e\u4e48\u200b\u505a\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u4f7f\u7528\u200bNumPy\u200b\u4e0e\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u4ea4\u4e92\u200b\uff08NumPy\u200b\u4e0d\u200b\u5229\u7528\u200bGPU\uff09\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u6211\u4eec\u200b\u7684\u200b<code>tensor_on_gpu</code>\u200b\u4f7f\u7528\u200b<code>torch.Tensor.numpy()</code>\u200b\u65b9\u6cd5\u200b\u3002</p>"},{"location":"00_pytorch_fundamentals/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u65e8\u5728\u200b\u7ec3\u4e60\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u6216\u200b\u9075\u5faa\u200b\u6240\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>00\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b\u3002</li> <li>00\u200b\u7ec3\u4e60\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5728\u200b\u67e5\u770b\u200b\u6b64\u200b\u5185\u5bb9\u200b\u4e4b\u524d\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\uff09\u3002</li> </ul> <ol> <li>\u200b\u6587\u6863\u200b\u9605\u8bfb\u200b - \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff08\u200b\u4ee5\u53ca\u200b\u4e00\u822c\u200b\u7f16\u7a0b\u200b\u5b66\u4e60\u200b\uff09\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u5927\u90e8\u5206\u200b\u662f\u200b\u719f\u6089\u200b\u4f60\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u67d0\u4e2a\u200b\u6846\u67b6\u200b\u7684\u200b\u6587\u6863\u200b\u3002\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u5176\u4f59\u90e8\u5206\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5927\u91cf\u200b\u4f7f\u7528\u200bPyTorch\u200b\u6587\u6863\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u4f60\u200b\u82b1\u200b10\u200b\u5206\u949f\u200b\u9605\u8bfb\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff08\u200b\u5982\u679c\u200b\u4f60\u200b\u73b0\u5728\u200b\u4e0d\u200b\u660e\u767d\u200b\u67d0\u4e9b\u200b\u5185\u5bb9\u200b\u4e5f\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u91cd\u70b9\u200b\u8fd8\u200b\u4e0d\u662f\u200b\u5b8c\u5168\u200b\u7406\u89e3\u200b\uff0c\u200b\u800c\u662f\u200b\u610f\u8bc6\u200b\uff09\u3002\u200b\u8bf7\u53c2\u9605\u200b<code>torch.Tensor</code>\u200b\u548c\u200b<code>torch.cuda</code>\u200b\u7684\u200b\u6587\u6863\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(7, 7)</code>\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\u3002</li> <li>\u200b\u5bf9\u200b\u7b2c\u200b2\u200b\u9898\u4e2d\u200b\u7684\u200b\u5f20\u91cf\u200b\u4e0e\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(1, 7)</code>\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u63d0\u793a\u200b\uff1a\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u8f6c\u7f6e\u200b\u7b2c\u4e8c\u4e2a\u200b\u5f20\u91cf\u200b\uff09\u3002</li> <li>\u200b\u5c06\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b<code>0</code>\uff0c\u200b\u7136\u540e\u200b\u91cd\u65b0\u200b\u505a\u200b\u7b2c\u200b2\u200b\u9898\u200b\u548c\u200b\u7b2c\u200b3\u200b\u9898\u200b\u3002</li> <li>\u200b\u8bf4\u200b\u5230\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4e86\u200b\u5982\u4f55\u200b\u7528\u200b<code>torch.manual_seed()</code>\u200b\u8bbe\u7f6e\u200b\u5b83\u200b\uff0c\u200b\u4f46\u200b\u6709\u6ca1\u6709\u200bGPU\u200b\u7684\u200b\u7b49\u6548\u200b\u65b9\u6cd5\u200b\uff1f\uff08\u200b\u63d0\u793a\u200b\uff1a\u200b\u4f60\u200b\u9700\u8981\u200b\u67e5\u9605\u200b<code>torch.cuda</code>\u200b\u7684\u200b\u6587\u6863\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u6709\u200b\uff0c\u200b\u5c06\u200bGPU\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b<code>1234</code>\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(2, 3)</code>\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u90fd\u200b\u53d1\u9001\u5230\u200bGPU\uff08\u200b\u4f60\u200b\u9700\u8981\u200b\u6709\u200bGPU\u200b\u8bbf\u95ee\u200b\u6743\u9650\u200b\uff09\u3002\u200b\u5728\u200b\u521b\u5efa\u200b\u5f20\u91cf\u200b\u65f6\u200b\u8bbe\u7f6e\u200b<code>torch.manual_seed(1234)</code>\uff08\u200b\u8fd9\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u662f\u200bGPU\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff09\u3002</li> <li>\u200b\u5bf9\u200b\u7b2c\u200b6\u200b\u9898\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u518d\u6b21\u200b\u63d0\u9192\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u8c03\u6574\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\uff09\u3002</li> <li>\u200b\u627e\u51fa\u200b\u7b2c\u200b7\u200b\u9898\u200b\u8f93\u51fa\u200b\u4e2d\u200b\u7684\u200b\u6700\u5927\u503c\u200b\u548c\u200b\u6700\u5c0f\u503c\u200b\u3002</li> <li>\u200b\u627e\u51fa\u200b\u7b2c\u200b7\u200b\u9898\u200b\u8f93\u51fa\u200b\u4e2d\u200b\u7684\u200b\u6700\u5927\u503c\u200b\u548c\u200b\u6700\u5c0f\u503c\u200b\u7684\u200b\u7d22\u5f15\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(1, 1, 1, 10)</code>\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u7136\u540e\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u79fb\u9664\u200b\u6240\u6709\u200b<code>1</code>\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u7559\u4e0b\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>(10)</code>\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002\u200b\u8bbe\u7f6e\u200b\u79cd\u5b50\u200b\u4e3a\u200b<code>7</code>\uff0c\u200b\u5e76\u6253\u5370\u200b\u51fa\u200b\u7b2c\u4e00\u4e2a\u200b\u5f20\u91cf\u200b\u53ca\u5176\u200b\u5f62\u72b6\u200b\u4ee5\u53ca\u200b\u7b2c\u4e8c\u4e2a\u200b\u5f20\u91cf\u200b\u53ca\u5176\u200b\u5f62\u72b6\u200b\u3002</li> </ol>"},{"location":"00_pytorch_fundamentals/","title":"\u8bfe\u5916\u200b\u5b66\u4e60\u200b\u00b6","text":"<ul> <li>\u200b\u82b1\u200b1\u200b\u5c0f\u65f6\u200b\u6d4f\u89c8\u200bPyTorch\u200b\u57fa\u7840\u6559\u7a0b\u200b\uff08\u200b\u63a8\u8350\u200b\u9605\u8bfb\u200b\u5feb\u901f\u200b\u5165\u95e8\u200b\u548c\u200b\u5f20\u91cf\u200b\u90e8\u5206\u200b\uff09\u3002</li> <li>\u200b\u60f3\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u5f20\u91cf\u200b\u5982\u4f55\u200b\u8868\u793a\u200b\u6570\u636e\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u200b\u89c2\u770b\u200b\u6b64\u200b\u89c6\u9891\u200b\uff1a\u200b\u4ec0\u4e48\u200b\u662f\u200b\u5f20\u91cf\u200b\uff1f</li> </ul>"},{"location":"01_pytorch_workflow/","title":"01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b | \u200b\u89c2\u770b\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b</p> In\u00a0[1]: Copied! <pre>what_were_covering = {1: \"data (prepare and load)\",\n    2: \"build model\",\n    3: \"fitting the model to data (training)\",\n    4: \"making predictions and evaluating a model (inference)\",\n    5: \"saving and loading a model\",\n    6: \"putting it all together\"\n}\n</pre> what_were_covering = {1: \"data (prepare and load)\",     2: \"build model\",     3: \"fitting the model to data (training)\",     4: \"making predictions and evaluating a model (inference)\",     5: \"saving and loading a model\",     6: \"putting it all together\" } <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bfc\u5165\u200b\u8fd9\u4e2a\u200b\u6a21\u5757\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5e93\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f15\u5165\u200b <code>torch</code>\u3001<code>torch.nn</code>\uff08<code>nn</code> \u200b\u4ee3\u8868\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5305\u200b\u5305\u542b\u200b\u4e86\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u521b\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6784\u5efa\u200b\u5757\u200b\uff09\u200b\u548c\u200b <code>matplotlib</code>\u3002</p> In\u00a0[2]: Copied! <pre>import torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[2]: <pre>'1.12.1+cu113'</pre> In\u00a0[3]: Copied! <pre># Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1)\ny = weight * X + bias\n\nX[:10], y[:10]\n</pre> # Create *known* parameters weight = 0.7 bias = 0.3  # Create data start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) y = weight * X + bias  X[:10], y[:10] Out[3]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c06\u200b\u671d\u7740\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u5b66\u4e60\u200b <code>X</code>\uff08\u200b\u7279\u5f81\u200b\uff09\u200b\u548c\u200b <code>y</code>\uff08\u200b\u6807\u7b7e\u200b\uff09\u200b\u4e4b\u95f4\u200b\u5173\u7cfb\u200b\u7684\u200b\u6a21\u578b\u200b\u8fc8\u8fdb\u200b\u3002</p> In\u00a0[4]: Copied! <pre># Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Create train/test split train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[4]: <pre>(40, 40, 10, 10)</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b40\u200b\u4e2a\u200b\u6837\u672c\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\uff08<code>X_train</code> \u200b\u548c\u200b <code>y_train</code>\uff09\uff0c\u200b\u4ee5\u53ca\u200b10\u200b\u4e2a\u200b\u6837\u672c\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\uff08<code>X_test</code> \u200b\u548c\u200b <code>y_test</code>\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u5c06\u200b\u5c1d\u8bd5\u200b\u5b66\u4e60\u200b <code>X_train</code> \u200b\u548c\u200b <code>y_train</code> \u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b <code>X_test</code> \u200b\u548c\u200b <code>y_test</code> \u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u5b83\u200b\u7684\u200b\u5b66\u4e60\u200b\u6210\u679c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u8fd8\u200b\u53ea\u662f\u200b\u7eb8\u4e0a\u200b\u7684\u200b\u6570\u5b57\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u3002</p> In\u00a0[5]: Copied! <pre>def plot_predictions(train_data=X_train, \n                     train_labels=y_train, \n                     test_data=X_test, \n                     test_labels=y_test, \n                     predictions=None):\n  \"\"\"\n  Plots training data, test data and compares predictions.\n  \"\"\"\n  plt.figure(figsize=(10, 7))\n\n  # Plot training data in blue\n  plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n  \n  # Plot test data in green\n  plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n  if predictions is not None:\n    # Plot the predictions in red (predictions were made on the test data)\n    plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n  # Show the legend\n  plt.legend(prop={\"size\": 14});\n</pre> def plot_predictions(train_data=X_train,                       train_labels=y_train,                       test_data=X_test,                       test_labels=y_test,                       predictions=None):   \"\"\"   Plots training data, test data and compares predictions.   \"\"\"   plt.figure(figsize=(10, 7))    # Plot training data in blue   plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")      # Plot test data in green   plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")    if predictions is not None:     # Plot the predictions in red (predictions were made on the test data)     plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")    # Show the legend   plt.legend(prop={\"size\": 14}); In\u00a0[6]: Copied! <pre>plot_predictions();\n</pre> plot_predictions(); <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u4e0d\u518d\u200b\u53ea\u662f\u200b\u7eb8\u4e0a\u200b\u7684\u200b\u6570\u5b57\u200b\uff0c\u200b\u800c\u662f\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u5411\u200b\u4f60\u200b\u4ecb\u7ecd\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\u4e86\u200b...\u201c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01\u201d</p> <p>\u200b\u6bcf\u5f53\u200b\u4f60\u200b\u5728\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u6570\u5b57\u200b\u65f6\u200b\uff0c\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u80fd\u200b\u5c06\u200b\u6570\u636e\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u6781\u5927\u200b\u5730\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u7406\u89e3\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u559c\u6b22\u200b\u6570\u5b57\u200b\uff0c\u200b\u6211\u4eec\u200b\u4eba\u7c7b\u200b\u4e5f\u200b\u559c\u6b22\u200b\u6570\u5b57\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u8fd8\u200b\u559c\u6b22\u200b\u770b\u200b\u4e1c\u897f\u200b\u3002</p> In\u00a0[7]: Copied! <pre># Create a Linear Regression model class\nclass LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)\n    def __init__(self):\n        super().__init__() \n        self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)\n                                                dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                   requires_grad=True) # &lt;- can we update this value with gradient descent?)\n\n        self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)\n                                            dtype=torch.float), # &lt;- PyTorch loves float32 by default\n                                requires_grad=True) # &lt;- can we update this value with gradient descent?))\n\n    # Forward defines the computation in the model\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)\n        return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b)\n</pre> # Create a Linear Regression model class class LinearRegressionModel(nn.Module): # &lt;- almost everything in PyTorch is a nn.Module (think of this as neural network lego blocks)     def __init__(self):         super().__init__()          self.weights = nn.Parameter(torch.randn(1, # &lt;- start with random weights (this will get adjusted as the model learns)                                                 dtype=torch.float), # &lt;- PyTorch loves float32 by default                                    requires_grad=True) # &lt;- can we update this value with gradient descent?)          self.bias = nn.Parameter(torch.randn(1, # &lt;- start with random bias (this will get adjusted as the model learns)                                             dtype=torch.float), # &lt;- PyTorch loves float32 by default                                 requires_grad=True) # &lt;- can we update this value with gradient descent?))      # Forward defines the computation in the model     def forward(self, x: torch.Tensor) -&gt; torch.Tensor: # &lt;- \"x\" is the input data (e.g. training/testing features)         return self.weights * x + self.bias # &lt;- this is the linear regression formula (y = m*x + b) <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\u6709\u70b9\u200b\u591a\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u70b9\u4e00\u70b9\u200b\u5730\u6765\u200b\u89e3\u6790\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Python \u200b\u7c7b\u6765\u200b\u521b\u5efa\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5404\u79cd\u200b\u7ec4\u4ef6\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u4e0d\u200b\u719f\u6089\u200b Python \u200b\u7c7b\u200b\u7684\u200b\u8868\u793a\u6cd5\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u4f60\u200b\u591a\u6b21\u200b\u9605\u8bfb\u200b Real Python \u200b\u7684\u200b Python 3 \u200b\u9762\u5411\u5bf9\u8c61\u7f16\u7a0b\u200b\u6307\u5357\u200b\u3002</p> In\u00a0[8]: Copied! <pre># Set manual seed since nn.Parameter are randomly initialzied\ntorch.manual_seed(42)\n\n# Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s))\nmodel_0 = LinearRegressionModel()\n\n# Check the nn.Parameter(s) within the nn.Module subclass we created\nlist(model_0.parameters())\n</pre> # Set manual seed since nn.Parameter are randomly initialzied torch.manual_seed(42)  # Create an instance of the model (this is a subclass of nn.Module that contains nn.Parameter(s)) model_0 = LinearRegressionModel()  # Check the nn.Parameter(s) within the nn.Module subclass we created list(model_0.parameters()) Out[8]: <pre>[Parameter containing:\n tensor([0.3367], requires_grad=True),\n Parameter containing:\n tensor([0.1288], requires_grad=True)]</pre> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>.state_dict()</code> \u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u7684\u200b\u72b6\u6001\u200b\uff08\u200b\u5373\u200b\u6a21\u578b\u200b\u5305\u542b\u200b\u7684\u200b\u5185\u5bb9\u200b\uff09\u3002</p> In\u00a0[9]: Copied! <pre># List named parameters \nmodel_0.state_dict()\n</pre> # List named parameters  model_0.state_dict() Out[9]: <pre>OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])</pre> <p>\u200b\u6ce8\u610f\u200b\u5230\u200b\u4ece\u200b <code>model_0.state_dict()</code> \u200b\u4e2d\u200b\u5f97\u5230\u200b\u7684\u200b <code>weights</code> \u200b\u548c\u200b <code>bias</code> \u200b\u503c\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\u6d6e\u70b9\u6570\u200b\u5f20\u91cf\u200b\u5417\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u4f7f\u7528\u200b <code>torch.randn()</code> \u200b\u521d\u59cb\u5316\u200b\u4e86\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4ece\u200b\u968f\u673a\u200b\u7684\u200b\u53c2\u6570\u200b\u5f00\u59cb\u200b\uff0c\u200b\u8ba9\u200b\u6a21\u578b\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u4e3a\u200b\u6700\u200b\u9002\u5408\u200b\u6211\u4eec\u200b\u6570\u636e\u200b\u7684\u200b\u53c2\u6570\u200b\uff08\u200b\u5373\u200b\u6211\u4eec\u200b\u5728\u200b\u521b\u5efa\u200b\u76f4\u7ebf\u200b\u6570\u636e\u200b\u65f6\u786c\u200b\u7f16\u7801\u200b\u7684\u200b <code>weight</code> \u200b\u548c\u200b <code>bias</code> \u200b\u503c\u200b\uff09\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u5c1d\u8bd5\u200b\u66f4\u6539\u200b\u4e0a\u9762\u200b\u4e24\u4e2a\u200b\u5355\u5143\u683c\u200b\u4e2d\u200b\u7684\u200b <code>torch.manual_seed()</code> \u200b\u503c\u200b\uff0c\u200b\u770b\u770b\u200b <code>weights</code> \u200b\u548c\u200b <code>bias</code> \u200b\u503c\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4ece\u200b\u968f\u673a\u200b\u503c\u200b\u5f00\u59cb\u200b\uff0c\u200b\u76ee\u524d\u200b\u5b83\u200b\u7684\u200b\u9884\u6d4b\u200b\u80fd\u529b\u200b\u4f1a\u200b\u5f88\u200b\u5dee\u200b\u3002</p> In\u00a0[10]: Copied! <pre># Make predictions with model\nwith torch.inference_mode(): \n    y_preds = model_0(X_test)\n\n# Note: in older PyTorch code you might also see torch.no_grad()\n# with torch.no_grad():\n#   y_preds = model_0(X_test)\n</pre> # Make predictions with model with torch.inference_mode():      y_preds = model_0(X_test)  # Note: in older PyTorch code you might also see torch.no_grad() # with torch.no_grad(): #   y_preds = model_0(X_test) <p>\u200b\u55ef\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u6ce8\u610f\u200b\u5230\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>torch.inference_mode()</code> \u200b\u4f5c\u4e3a\u200b \u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b\uff08\u200b\u8fd9\u200b\u5c31\u662f\u200b <code>with torch.inference_mode():</code> \u200b\u7684\u200b\u4f5c\u7528\u200b\uff09\u200b\u6765\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u987e\u540d\u601d\u4e49\u200b\uff0c<code>torch.inference_mode()</code> \u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u63a8\u7406\u200b\uff08\u200b\u5373\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff09\u200b\u65f6\u200b\u3002</p> <p><code>torch.inference_mode()</code> \u200b\u5173\u95ed\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u529f\u80fd\u200b\uff08\u200b\u5982\u200b\u68af\u5ea6\u200b\u8ddf\u8e2a\u200b\uff0c\u200b\u8fd9\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e2d\u662f\u200b\u5fc5\u8981\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u63a8\u7406\u200b\u4e2d\u200b\u4e0d\u200b\u9700\u8981\u200b\uff09\uff0c\u200b\u4ee5\u200b\u4f7f\u200b \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\uff08\u200b\u6570\u636e\u200b\u901a\u8fc7\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\uff09\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b\u8f83\u200b\u65e7\u200b\u7684\u200b PyTorch \u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u770b\u5230\u200b <code>torch.no_grad()</code> \u200b\u7528\u4e8e\u200b\u63a8\u7406\u200b\u3002\u200b\u867d\u7136\u200b <code>torch.inference_mode()</code> \u200b\u548c\u200b <code>torch.no_grad()</code> \u200b\u4f5c\u7528\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4f46\u200b <code>torch.inference_mode()</code> \u200b\u662f\u200b\u8f83\u200b\u65b0\u200b\u7684\u200b\uff0c\u200b\u53ef\u80fd\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u56e0\u6b64\u200b\u66f4\u53d7\u200b\u63a8\u8350\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b PyTorch \u200b\u7684\u200b\u8fd9\u6761\u200b\u63a8\u6587\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p> In\u00a0[11]: Copied! <pre># Check the predictions\nprint(f\"Number of testing samples: {len(X_test)}\") \nprint(f\"Number of predictions made: {len(y_preds)}\")\nprint(f\"Predicted values:\\n{y_preds}\")\n</pre> # Check the predictions print(f\"Number of testing samples: {len(X_test)}\")  print(f\"Number of predictions made: {len(y_preds)}\") print(f\"Predicted values:\\n{y_preds}\") <pre>Number of testing samples: 10\nNumber of predictions made: 10\nPredicted values:\ntensor([[0.3982],\n        [0.4049],\n        [0.4116],\n        [0.4184],\n        [0.4251],\n        [0.4318],\n        [0.4386],\n        [0.4453],\n        [0.4520],\n        [0.4588]])\n</pre> <p>\u200b\u6ce8\u610f\u200b\u5230\u200b\u6bcf\u4e2a\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u9884\u6d4b\u503c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u4e00\u4e2a\u200b <code>X</code> \u200b\u503c\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b <code>y</code> \u200b\u503c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u975e\u5e38\u7075\u6d3b\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6709\u200b 100 \u200b\u4e2a\u200b <code>X</code> \u200b\u503c\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u3001\u200b\u4e24\u4e2a\u200b\u3001\u200b\u4e09\u4e2a\u200b\u6216\u200b\u5341\u4e2a\u200b <code>y</code> \u200b\u503c\u200b\u3002\u200b\u8fd9\u200b\u4e00\u5207\u200b\u90fd\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u5728\u200b\u505a\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u9884\u6d4b\u200b\u4ecd\u7136\u200b\u662f\u200b\u7eb8\u200b\u4e0a\u200b\u7684\u200b\u6570\u5b57\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b\u4e0a\u9762\u200b\u521b\u5efa\u200b\u7684\u200b <code>plot_predictions()</code> \u200b\u51fd\u6570\u200b\u5c06\u200b\u5b83\u4eec\u200b\u53ef\u89c6\u5316\u200b\u3002</p> In\u00a0[12]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) In\u00a0[13]: Copied! <pre>y_test - y_preds\n</pre> y_test - y_preds Out[13]: <pre>tensor([[0.4618],\n        [0.4691],\n        [0.4764],\n        [0.4836],\n        [0.4909],\n        [0.4982],\n        [0.5054],\n        [0.5127],\n        [0.5200],\n        [0.5272]])</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u770b\u8d77\u6765\u200b\u76f8\u5f53\u200b\u7cdf\u7cd5\u200b...</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u8bb0\u5f97\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53ea\u662f\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u53c2\u6570\u503c\u200b\u6765\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u200b\u4e0d\u96be\u7406\u89e3\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5b83\u200b\u751a\u81f3\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u770b\u8fc7\u200b\u84dd\u70b9\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u9884\u6d4b\u200b\u7eff\u70b9\u200b\u3002</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u6539\u53d8\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u4e86\u200b\u3002</p> In\u00a0[14]: Copied! <pre># Create the loss function\nloss_fn = nn.L1Loss() # MAE loss is same as L1Loss\n\n# Create the optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize\n                            lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time))\n</pre> # Create the loss function loss_fn = nn.L1Loss() # MAE loss is same as L1Loss  # Create the optimizer optimizer = torch.optim.SGD(params=model_0.parameters(), # parameters of target model to optimize                             lr=0.01) # learning rate (how much the optimizer should change parameters at each step, higher=more (less stable), lower=less (might take a long time)) In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs (how many times the model will pass over the training data)\nepochs = 100\n\n# Create empty loss lists to track values\ntrain_loss_values = []\ntest_loss_values = []\nepoch_count = []\n\nfor epoch in range(epochs):\n    ### Training\n\n    # Put model in training mode (this is the default state of a model)\n    model_0.train()\n\n    # 1. Forward pass on train data using the forward() method inside \n    y_pred = model_0(X_train)\n    # print(y_pred)\n\n    # 2. Calculate the loss (how different are our models predictions to the ground truth)\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad of the optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Progress the optimizer\n    optimizer.step()\n\n    ### Testing\n\n    # Put the model in evaluation mode\n    model_0.eval()\n\n    with torch.inference_mode():\n      # 1. Forward pass on test data\n      test_pred = model_0(X_test)\n\n      # 2. Caculate loss on test data\n      test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type\n\n      # Print out what's happening\n      if epoch % 10 == 0:\n            epoch_count.append(epoch)\n            train_loss_values.append(loss.detach().numpy())\n            test_loss_values.append(test_loss.detach().numpy())\n            print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \")\n</pre> torch.manual_seed(42)  # Set the number of epochs (how many times the model will pass over the training data) epochs = 100  # Create empty loss lists to track values train_loss_values = [] test_loss_values = [] epoch_count = []  for epoch in range(epochs):     ### Training      # Put model in training mode (this is the default state of a model)     model_0.train()      # 1. Forward pass on train data using the forward() method inside      y_pred = model_0(X_train)     # print(y_pred)      # 2. Calculate the loss (how different are our models predictions to the ground truth)     loss = loss_fn(y_pred, y_train)      # 3. Zero grad of the optimizer     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Progress the optimizer     optimizer.step()      ### Testing      # Put the model in evaluation mode     model_0.eval()      with torch.inference_mode():       # 1. Forward pass on test data       test_pred = model_0(X_test)        # 2. Caculate loss on test data       test_loss = loss_fn(test_pred, y_test.type(torch.float)) # predictions come in torch.float datatype, so comparisons need to be done with tensors of the same type        # Print out what's happening       if epoch % 10 == 0:             epoch_count.append(epoch)             train_loss_values.append(loss.detach().numpy())             test_loss_values.append(test_loss.detach().numpy())             print(f\"Epoch: {epoch} | MAE Train Loss: {loss} | MAE Test Loss: {test_loss} \") <pre>Epoch: 0 | MAE Train Loss: 0.31288138031959534 | MAE Test Loss: 0.48106518387794495 \nEpoch: 10 | MAE Train Loss: 0.1976713240146637 | MAE Test Loss: 0.3463551998138428 \nEpoch: 20 | MAE Train Loss: 0.08908725529909134 | MAE Test Loss: 0.21729660034179688 \nEpoch: 30 | MAE Train Loss: 0.053148526698350906 | MAE Test Loss: 0.14464017748832703 \nEpoch: 40 | MAE Train Loss: 0.04543796554207802 | MAE Test Loss: 0.11360953003168106 \nEpoch: 50 | MAE Train Loss: 0.04167863354086876 | MAE Test Loss: 0.09919948130846024 \nEpoch: 60 | MAE Train Loss: 0.03818932920694351 | MAE Test Loss: 0.08886633068323135 \nEpoch: 70 | MAE Train Loss: 0.03476089984178543 | MAE Test Loss: 0.0805937647819519 \nEpoch: 80 | MAE Train Loss: 0.03132382780313492 | MAE Test Loss: 0.07232122868299484 \nEpoch: 90 | MAE Train Loss: 0.02788739837706089 | MAE Test Loss: 0.06473556160926819 \n</pre> <p>\u200b\u54e6\u200b\uff0c\u200b\u4f60\u200b\u77a7\u77a7\u200b\u90a3\u4e2a\u200b\uff01\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u968f\u7740\u200b\u6bcf\u4e2a\u200b\u5468\u671f\u200b\u5728\u200b\u4e0b\u964d\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u7ed8\u5236\u200b\u4e00\u4e0b\u200b\u56fe\u8868\u200b\u770b\u770b\u200b\u3002</p> In\u00a0[16]: Copied! <pre># Plot the loss curves\nplt.plot(epoch_count, train_loss_values, label=\"Train loss\")\nplt.plot(epoch_count, test_loss_values, label=\"Test loss\")\nplt.title(\"Training and test loss curves\")\nplt.ylabel(\"Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Plot the loss curves plt.plot(epoch_count, train_loss_values, label=\"Train loss\") plt.plot(epoch_count, test_loss_values, label=\"Test loss\") plt.title(\"Training and test loss curves\") plt.ylabel(\"Loss\") plt.xlabel(\"Epochs\") plt.legend(); <p>\u200b\u4e0d\u9519\u200b\uff01\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u663e\u793a\u200b\u635f\u5931\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u4e0b\u964d\u200b\u3002\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u635f\u5931\u200b\u662f\u200b\u8861\u91cf\u200b\u6a21\u578b\u200b\u9519\u8bef\u200b\u7a0b\u5ea6\u200b\u7684\u200b\u6307\u6807\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8d8a\u4f4e\u200b\u8d8a\u200b\u597d\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e3a\u4ec0\u4e48\u200b\u635f\u5931\u200b\u4f1a\u200b\u4e0b\u964d\u200b\u5462\u200b\uff1f</p> <p>\u200b\u55ef\u200b\uff0c\u200b\u591a\u4e8f\u200b\u4e86\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u5185\u90e8\u200b\u53c2\u6570\u200b\uff08<code>\u200b\u6743\u91cd\u200b</code>\u200b\u548c\u200b<code>\u200b\u504f\u5dee\u200b</code>\uff09\u200b\u88ab\u200b\u66f4\u65b0\u200b\uff0c\u200b\u4ee5\u200b\u66f4\u597d\u200b\u5730\u200b\u53cd\u6620\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6f5c\u5728\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u7684\u200b<code>.state_dict()</code>\uff0c\u200b\u770b\u770b\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6211\u4eec\u200b\u5728\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u5dee\u200b\u4e0a\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u539f\u59cb\u200b\u503c\u200b\u7684\u200b\u63a5\u8fd1\u200b\u7a0b\u5ea6\u200b\u3002</p> In\u00a0[17]: Copied! <pre># Find our model's learned parameters\nprint(\"The model learned the following values for weights and bias:\")\nprint(model_0.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters print(\"The model learned the following values for weights and bias:\") print(model_0.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('weights', tensor([0.5784])), ('bias', tensor([0.3513]))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>\u200b\u54c7\u200b\uff01\u200b\u8fd9\u6709\u200b\u591a\u9177\u200b\u554a\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\u4e8e\u200b\u8ba1\u7b97\u200b\u51fa\u200b<code>weight</code>\u200b\u548c\u200b<code>bias</code>\u200b\u7684\u200b\u7cbe\u786e\u200b\u539f\u59cb\u200b\u503c\u200b\uff08\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u5f97\u200b\u66f4\u200b\u4e45\u200b\uff0c\u200b\u5b83\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u66f4\u200b\u63a5\u8fd1\u200b\uff09\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u4e0a\u9762\u200b\u7684\u200b<code>epochs</code>\u200b\u503c\u200b\u6539\u4e3a\u200b200\uff0c\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u7f6e\u200b\u53c2\u6570\u503c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\uff1f</p> <p>\u200b\u5b83\u200b\u5f88\u200b\u53ef\u80fd\u200b\u6c38\u8fdc\u200b\u65e0\u6cd5\u200b\u5b8c\u7f8e\u200b\u731c\u200b\u51fa\u200b\u8fd9\u4e9b\u200b\u503c\u200b\uff08\u200b\u5c24\u5176\u200b\u662f\u200b\u5728\u200b\u4f7f\u7528\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u65f6\u200b\uff09\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e00\u4e2a\u200b\u63a5\u8fd1\u200b\u7684\u200b\u8fd1\u4f3c\u503c\u200b\u505a\u200b\u5f88\u591a\u200b\u5f88\u9177\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u5168\u90e8\u200b\u7406\u5ff5\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e9b\u200b\u7406\u60f3\u200b\u503c\u200b\u53ef\u4ee5\u200b\u63cf\u8ff0\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u4e0d\u662f\u200b\u624b\u52a8\u200b\u53bb\u200b\u8ba1\u7b97\u200b\u8fd9\u4e9b\u200b\u503c\u200b\uff0c\u200b\u800c\u662f\u200b\u53ef\u4ee5\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u4ee5\u200b\u7f16\u7a0b\u200b\u65b9\u5f0f\u200b\u627e\u51fa\u200b\u5b83\u4eec\u200b\u3002</p> In\u00a0[18]: Copied! <pre># 1. Set the model in evaluation mode\nmodel_0.eval()\n\n# 2. Setup the inference mode context manager\nwith torch.inference_mode():\n  # 3. Make sure the calculations are done with the model and data on the same device\n  # in our case, we haven't setup device-agnostic code yet so our data and model are\n  # on the CPU by default.\n  # model_0.to(device)\n  # X_test = X_test.to(device)\n  y_preds = model_0(X_test)\ny_preds\n</pre> # 1. Set the model in evaluation mode model_0.eval()  # 2. Setup the inference mode context manager with torch.inference_mode():   # 3. Make sure the calculations are done with the model and data on the same device   # in our case, we haven't setup device-agnostic code yet so our data and model are   # on the CPU by default.   # model_0.to(device)   # X_test = X_test.to(device)   y_preds = model_0(X_test) y_preds Out[18]: <pre>tensor([[0.8141],\n        [0.8256],\n        [0.8372],\n        [0.8488],\n        [0.8603],\n        [0.8719],\n        [0.8835],\n        [0.8950],\n        [0.9066],\n        [0.9182]])</pre> <p>\u200b\u4e0d\u9519\u200b\uff01\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\uff0c\u200b\u90a3\u4e48\u200b\u7ed3\u679c\u200b\u770b\u8d77\u6765\u200b\u5982\u4f55\u200b\u5462\u200b\uff1f</p> In\u00a0[19]: Copied! <pre>plot_predictions(predictions=y_preds)\n</pre> plot_predictions(predictions=y_preds) <p>\u200b\u54c7\u200b\u54e6\u200b\uff01\u200b\u90a3\u4e9b\u200b\u7ea2\u200b\u70b9\u200b\u770b\u8d77\u6765\u200b\u6bd4\u200b\u4e4b\u524d\u200b\u8fd1\u591a\u200b\u4e86\u200b\uff01</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5b66\u4e60\u200b\u5982\u4f55\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u3002</p> <pre># \u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\ntorch.save(model.state_dict(), 'model.pth')\n\n# \u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\nmodel = TheModelClass(*args, **kwargs)\nmodel.load_state_dict(torch.load('model.pth'))\nmodel.eval()\n</pre> In\u00a0[20]: Copied! <pre>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_0.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # 1. Create models directory  MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Create model save path  MODEL_NAME = \"01_pytorch_workflow_model_0.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Save the model state dict  print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_0.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <pre>Saving model to: models/01_pytorch_workflow_model_0.pth\n</pre> In\u00a0[21]: Copied! <pre># Check the saved file path\n!ls -l models/01_pytorch_workflow_model_0.pth\n</pre> # Check the saved file path !ls -l models/01_pytorch_workflow_model_0.pth <pre>-rw-rw-r-- 1 daniel daniel 1063 Nov 10 16:07 models/01_pytorch_workflow_model_0.pth\n</pre> In\u00a0[22]: Copied! <pre># Instantiate a new instance of our model (this will be instantiated with random weights)\nloaded_model_0 = LinearRegressionModel()\n\n# Load the state_dict of our saved model (this will update the new instance of our model with trained weights)\nloaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n</pre> # Instantiate a new instance of our model (this will be instantiated with random weights) loaded_model_0 = LinearRegressionModel()  # Load the state_dict of our saved model (this will update the new instance of our model with trained weights) loaded_model_0.load_state_dict(torch.load(f=MODEL_SAVE_PATH)) Out[22]: <pre>&lt;All keys matched successfully&gt;</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u4e00\u5207\u200b\u90fd\u200b\u5f88\u200b\u543b\u5408\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u6d4b\u8bd5\u200b\u6211\u4eec\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u4e0a\u7528\u200b\u5b83\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff08\u200b\u5373\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff09\u3002</p> <p>\u200b\u8fd8\u200b\u8bb0\u5f97\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u7684\u200b\u89c4\u5219\u200b\u5417\u200b\uff1f</p> <p>\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u662f\u200b\u4e00\u4e2a\u200b\u590d\u4e60\u200b\uff1a</p> PyTorch \u200b\u63a8\u7406\u200b\u89c4\u5219\u200b <ol> <li> \u200b\u5c06\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u8bc4\u4f30\u200b\u6a21\u5f0f\u200b\uff08<code>model.eval()</code>\uff09\u3002 </li> <li> \u200b\u4f7f\u7528\u200b\u63a8\u7406\u200b\u6a21\u5f0f\u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08<code>with torch.inference_mode(): ...</code>\uff09\u3002 </li> <li> \u200b\u6240\u6709\u200b\u9884\u6d4b\u200b\u90fd\u200b\u5e94\u8be5\u200b\u5728\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fdb\u884c\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u90fd\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff0c\u200b\u6216\u8005\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u90fd\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\uff09\u3002</li> </ol> In\u00a0[23]: Copied! <pre># 1. Put the loaded model into evaluation mode\nloaded_model_0.eval()\n\n# 2. Use the inference mode context manager to make predictions\nwith torch.inference_mode():\n    loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model\n</pre> # 1. Put the loaded model into evaluation mode loaded_model_0.eval()  # 2. Use the inference mode context manager to make predictions with torch.inference_mode():     loaded_model_preds = loaded_model_0(X_test) # perform a forward pass on the test data with the loaded model <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u7528\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u662f\u5426\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u7684\u200b\u9884\u6d4b\u200b\u76f8\u540c\u200b\u3002</p> In\u00a0[24]: Copied! <pre># Compare previous model predictions with loaded model predictions (these should be the same)\ny_preds == loaded_model_preds\n</pre> # Compare previous model predictions with loaded model predictions (these should be the same) y_preds == loaded_model_preds Out[24]: <pre>tensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]])</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u4fdd\u5b58\u200b\u524d\u200b\u7684\u200b\u9884\u6d4b\u200b\uff09\u200b\u4e00\u81f4\u200b\u3002\u200b\u8fd9\u200b\u8868\u660e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u529f\u80fd\u200b\u6b63\u5e38\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u6211\u200b\u5c06\u200b\u7559\u7ed9\u200b\u989d\u5916\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u548c\u200b\u6df1\u5165\u200b\u9605\u8bfb\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u53c2\u9605\u200b PyTorch \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u7684\u200b\u6307\u5357\u200b\u3002</p> In\u00a0[25]: Copied! <pre># Import PyTorch and matplotlib\nimport torch\nfrom torch import nn # nn contains all of PyTorch's building blocks for neural networks\nimport matplotlib.pyplot as plt\n\n# Check PyTorch version\ntorch.__version__\n</pre> # Import PyTorch and matplotlib import torch from torch import nn # nn contains all of PyTorch's building blocks for neural networks import matplotlib.pyplot as plt  # Check PyTorch version torch.__version__ Out[25]: <pre>'1.12.1+cu113'</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u8ba9\u200b\u4ee3\u7801\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b <code>device=\"cuda\"</code> \u200b\u5982\u679c\u200b\u5b83\u200b\u53ef\u7528\u200b\uff0c\u200b\u5426\u5219\u200b\u5b83\u200b\u5c06\u200b\u9ed8\u8ba4\u200b\u5230\u200b <code>device=\"cpu\"</code>\u3002</p> In\u00a0[26]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Using device: {device}\") <pre>Using device: cuda\n</pre> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u80fd\u591f\u200b\u8bbf\u95ee\u200bGPU\uff0c\u200b\u4e0a\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u8f93\u51fa\u200b\uff1a</p> <pre><code>Using device: cuda\n</code></pre> <p>\u200b\u5426\u5219\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u4f7f\u7528\u200bCPU\u200b\u8fdb\u884c\u200b\u540e\u7eed\u200b\u7684\u200b\u8ba1\u7b97\u200b\u3002\u200b\u8fd9\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u5c0f\u200b\u6570\u636e\u200b\u96c6\u200b\u6765\u8bf4\u200b\u662f\u200b\u53ef\u4ee5\u200b\u63a5\u53d7\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u66f4\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u65f6\u95f4\u200b\u4f1a\u200b\u66f4\u957f\u200b\u3002</p> In\u00a0[27]: Copied! <pre># Create weight and bias\nweight = 0.7\nbias = 0.3\n\n# Create range values\nstart = 0\nend = 1\nstep = 0.02\n\n# Create X and y (features and labels)\nX = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers)\ny = weight * X + bias \nX[:10], y[:10]\n</pre> # Create weight and bias weight = 0.7 bias = 0.3  # Create range values start = 0 end = 1 step = 0.02  # Create X and y (features and labels) X = torch.arange(start, end, step).unsqueeze(dim=1) # without unsqueeze, errors will happen later on (shapes within linear layers) y = weight * X + bias  X[:10], y[:10] Out[27]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u62c6\u200b\u5206\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u91c7\u7528\u200b 80/20 \u200b\u7684\u200b\u62c6\u5206\u200b\u6bd4\u4f8b\u200b\uff0c\u200b\u5176\u4e2d\u200b 80% \u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c20% \u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u3002</p> In\u00a0[28]: Copied! <pre># Split data\ntrain_split = int(0.8 * len(X))\nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Split data train_split = int(0.8 * len(X)) X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[28]: <pre>(40, 40, 10, 10)</pre> <p>\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u786e\u4fdd\u200b\u5b83\u4eec\u200b\u770b\u8d77\u6765\u200b\u6ca1\u200b\u95ee\u9898\u200b\u3002</p> In\u00a0[29]: Copied! <pre># Note: If you've reset your runtime, this function won't work, \n# you'll have to rerun the cell above where it's instantiated.\nplot_predictions(X_train, y_train, X_test, y_test)\n</pre> # Note: If you've reset your runtime, this function won't work,  # you'll have to rerun the cell above where it's instantiated. plot_predictions(X_train, y_train, X_test, y_test) In\u00a0[30]: Copied! <pre># Subclass nn.Module to make our model\nclass LinearRegressionModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\n# Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens)\ntorch.manual_seed(42)\nmodel_1 = LinearRegressionModelV2()\nmodel_1, model_1.state_dict()\n</pre> # Subclass nn.Module to make our model class LinearRegressionModelV2(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  # Set the manual seed when creating the model (this isn't always need but is used for demonstrative purposes, try commenting it out and seeing what happens) torch.manual_seed(42) model_1 = LinearRegressionModelV2() model_1, model_1.state_dict() Out[30]: <pre>(LinearRegressionModelV2(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.7645]])),\n              ('linear_layer.bias', tensor([0.8300]))]))</pre> <p>\u200b\u6ce8\u610f\u200b\u89c2\u5bdf\u200b <code>model_1.state_dict()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c<code>nn.Linear()</code> \u200b\u5c42\u4e3a\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e86\u200b\u968f\u673a\u200b\u7684\u200b <code>weight</code> \u200b\u548c\u200b <code>bias</code> \u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6a21\u578b\u200b\u653e\u5230\u200b GPU \u200b\u4e0a\u200b\uff08\u200b\u5982\u679c\u200b\u53ef\u7528\u200b\u7684\u8bdd\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>.to(device)</code> \u200b\u6765\u200b\u6539\u53d8\u200b PyTorch \u200b\u5bf9\u8c61\u200b\u6240\u5728\u200b\u7684\u200b\u8bbe\u5907\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u5f53\u524d\u200b\u6240\u5728\u200b\u7684\u200b\u8bbe\u5907\u200b\u3002</p> In\u00a0[31]: Copied! <pre># Check model device\nnext(model_1.parameters()).device\n</pre> # Check model device next(model_1.parameters()).device Out[31]: <pre>device(type='cpu')</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6a21\u578b\u200b\u9ed8\u8ba4\u200b\u662f\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u6539\u4e3a\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff08\u200b\u5982\u679c\u200b GPU \u200b\u53ef\u7528\u200b\u7684\u8bdd\u200b\uff09\u3002</p> In\u00a0[32]: Copied! <pre># Set model to GPU if it's availalble, otherwise it'll default to CPU\nmodel_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not\nnext(model_1.parameters()).device\n</pre> # Set model to GPU if it's availalble, otherwise it'll default to CPU model_1.to(device) # the device variable was set above to be \"cuda\" if available or \"cpu\" if not next(model_1.parameters()).device Out[32]: <pre>device(type='cuda', index=0)</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e0e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\uff0c\u200b\u65e0\u8bba\u200b\u662f\u5426\u200b\u5177\u5907\u200bGPU\uff0c\u200b\u4e0a\u8ff0\u200b\u5355\u5143\u683c\u200b\u90fd\u200b\u80fd\u200b\u6b63\u5e38\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u786e\u5b9e\u200b\u80fd\u591f\u200b\u8bbf\u95ee\u200b\u652f\u6301\u200bCUDA\u200b\u7684\u200bGPU\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u770b\u5230\u200b\u7c7b\u4f3c\u200b\u5982\u4e0b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff1a</p> <pre><code>device(type='cuda', index=0)\n</code></pre> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u4e86\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u7528\u8fc7\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c<code>nn.L1Loss()</code> \u200b\u548c\u200b <code>torch.optim.SGD()</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u65b0\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\uff08<code>model.parameters()</code>\uff09\u200b\u4f20\u9012\u200b\u7ed9\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8c03\u6574\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u4e4b\u524d\u200b\u5b66\u4e60\u200b\u7387\u200b <code>0.01</code> \u200b\u6548\u679c\u200b\u4e0d\u9519\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u503c\u200b\u3002</p> In\u00a0[33]: Copied! <pre># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Create loss function loss_fn = nn.L1Loss()  # Create optimizer optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) <p>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u6765\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u6211\u4eec\u200b\u552f\u4e00\u200b\u4e0d\u540c\u200b\u7684\u200b\u64cd\u4f5c\u200b\u662f\u200b\u5c06\u200b\u6570\u636e\u200b\u653e\u5230\u200b\u76ee\u6807\u200b <code>device</code> \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u901a\u8fc7\u200b <code>model_1.to(device)</code> \u200b\u5c06\u200b\u6a21\u578b\u200b\u653e\u5230\u200b\u4e86\u200b\u76ee\u6807\u200b <code>device</code> \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u540c\u6837\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\uff0c\u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff0c\u200b\u6570\u636e\u200b\u4e5f\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff08\u200b\u53cd\u4e4b\u4ea6\u7136\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u96be\u5ea6\u200b\uff0c\u200b\u8bbe\u7f6e\u200b <code>epochs=1000</code>\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9700\u8981\u200b\u56de\u987e\u200b PyTorch \u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u8bf7\u200b\u770b\u200b\u4e0b\u9762\u200b\u7684\u200b\u8be6\u7ec6\u200b\u5185\u5bb9\u200b\u3002</p> PyTorch \u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u6b65\u9aa4\u200b <ol> <li>\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b - \u200b\u6a21\u578b\u200b\u904d\u5386\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e00\u6b21\u200b\uff0c\u200b\u6267\u884c\u200b\u5176\u200b             <code>forward()</code> \u200b\u51fd\u6570\u200b             \u200b\u8ba1\u7b97\u200b\uff08<code>model(x_train)</code>\uff09\u3002         </li> <li>\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b - \u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u9884\u6d4b\u503c\u200b\uff09\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u503c\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u8bc4\u4f30\u200b\u5b83\u4eec\u200b\u7684\u200b\u8bef\u5dee\u200b             \uff08<code>loss = loss_fn(y_pred, y_train)</code>\uff09\u3002</li> <li>\u200b\u6e05\u96f6\u200b\u68af\u5ea6\u200b - \u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u68af\u5ea6\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u96f6\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5b83\u4eec\u200b\u4f1a\u200b\u88ab\u200b\u7d2f\u79ef\u200b\uff09\uff0c\u200b\u4ee5\u4fbf\u200b\u4e3a\u200b\u5f53\u524d\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u91cd\u65b0\u200b\u8ba1\u7b97\u200b             \uff08<code>optimizer.zero_grad()</code>\uff09\u3002</li> <li>\u200b\u5bf9\u200b\u635f\u5931\u200b\u8fdb\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b - \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u6bcf\u4e2a\u200b\u9700\u8981\u200b\u66f4\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b             \uff08\u200b\u6bcf\u4e2a\u200b\u53c2\u6570\u200b             \u200b\u5e26\u6709\u200b <code>requires_grad=True</code>\uff09\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b \u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u79f0\u4e3a\u200b\u201c\u200b\u53cd\u5411\u200b\u201d             \uff08<code>loss.backward()</code>\uff09\u3002</li> <li>\u200b\u66f4\u65b0\u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09 - \u200b\u6839\u636e\u200b\u635f\u5931\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u5e26\u6709\u200b <code>requires_grad=True</code>             \u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u200b\u6539\u8fdb\u200b\u5b83\u4eec\u200b             \uff08<code>optimizer.step()</code>\uff09\u3002</li> </ol> In\u00a0[34]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, error will happen (not all model/data on device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Set the number of epochs  epochs = 1000   # Put data on the available device # Without this, error will happen (not all model/data on device) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <pre>Epoch: 0 | Train loss: 0.5551779866218567 | Test loss: 0.5739762187004089\nEpoch: 100 | Train loss: 0.006215683650225401 | Test loss: 0.014086711220443249\nEpoch: 200 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 300 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 400 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 500 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 600 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 700 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 800 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\nEpoch: 900 | Train loss: 0.0012645035749301314 | Test loss: 0.013801801018416882\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7531\u4e8e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u968f\u673a\u6027\u200b\uff0c\u200b\u6839\u636e\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b CPU \u200b\u8fd8\u662f\u200b GPU \u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f97\u5230\u200b\u7565\u6709\u4e0d\u540c\u200b\u7684\u200b\u7ed3\u679c\u200b\uff08\u200b\u4e0d\u540c\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u548c\u200b\u9884\u6d4b\u503c\u200b\uff09\u3002\u200b\u5373\u4f7f\u200b\u5728\u200b\u4efb\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e5f\u200b\u5b58\u5728\u200b\u3002\u200b\u5982\u679c\u200b\u5dee\u5f02\u200b\u8f83\u5927\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u68c0\u67e5\u200b\u9519\u8bef\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u5dee\u5f02\u200b\u5f88\u5c0f\u200b\uff08\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5e94\u8be5\u200b\u662f\u200b\u8fd9\u6837\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5ffd\u7565\u200b\u5b83\u200b\u3002</p> <p>\u200b\u5f88\u200b\u597d\u200b\uff01\u200b\u90a3\u4e2a\u200b\u635f\u5931\u200b\u503c\u200b\u770b\u8d77\u6765\u200b\u76f8\u5f53\u200b\u4f4e\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u6a21\u578b\u200b\u5b66\u5230\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5e76\u200b\u4e0e\u200b\u6211\u4eec\u200b\u786c\u200b\u7f16\u7801\u200b\u7684\u200b\u539f\u59cb\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p> In\u00a0[35]: Copied! <pre># Find our model's learned parameters\nfrom pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html \nprint(\"The model learned the following values for weights and bias:\")\npprint(model_1.state_dict())\nprint(\"\\nAnd the original values for weights and bias are:\")\nprint(f\"weights: {weight}, bias: {bias}\")\n</pre> # Find our model's learned parameters from pprint import pprint # pprint = pretty print, see: https://docs.python.org/3/library/pprint.html  print(\"The model learned the following values for weights and bias:\") pprint(model_1.state_dict()) print(\"\\nAnd the original values for weights and bias are:\") print(f\"weights: {weight}, bias: {bias}\") <pre>The model learned the following values for weights and bias:\nOrderedDict([('linear_layer.weight', tensor([[0.6968]], device='cuda:0')),\n             ('linear_layer.bias', tensor([0.3025], device='cuda:0'))])\n\nAnd the original values for weights and bias are:\nweights: 0.7, bias: 0.3\n</pre> <p>\u200b\u5475\u5475\u200b\uff01\u200b\u73b0\u5728\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\u5b8c\u7f8e\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\u8981\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f88\u5c11\u200b\u80fd\u200b\u4e8b\u5148\u200b\u77e5\u9053\u200b\u5b8c\u7f8e\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4e8b\u5148\u200b\u5c31\u200b\u77e5\u9053\u200b\u6a21\u578b\u200b\u9700\u8981\u200b\u5b66\u4e60\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u90a3\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e50\u8da3\u200b\u53c8\u200b\u5728\u200b\u54ea\u91cc\u200b\u5462\u200b\uff1f</p> <p>\u200b\u800c\u4e14\u200b\uff0c\u200b\u5728\u200b\u8bb8\u591a\u200b\u73b0\u5b9e\u200b\u4e16\u754c\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\u4e2d\u200b\uff0c\u200b\u53c2\u6570\u200b\u7684\u200b\u6570\u91cf\u200b\u53ef\u80fd\u200b\u8fdc\u8fdc\u200b\u8d85\u8fc7\u200b\u6570\u5343\u4e07\u200b\u3002</p> <p>\u200b\u6211\u200b\u4e0d\u200b\u77e5\u9053\u200b\u4f60\u200b\u600e\u4e48\u200b\u60f3\u200b\uff0c\u200b\u4f46\u200b\u6211\u200b\u5b81\u613f\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u8ba9\u200b\u8ba1\u7b97\u673a\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u624b\u52a8\u200b\u53bb\u200b\u505a\u200b\u3002</p> In\u00a0[36]: Copied! <pre># Turn model into evaluation mode\nmodel_1.eval()\n\n# Make predictions on the test data\nwith torch.inference_mode():\n    y_preds = model_1(X_test)\ny_preds\n</pre> # Turn model into evaluation mode model_1.eval()  # Make predictions on the test data with torch.inference_mode():     y_preds = model_1(X_test) y_preds Out[36]: <pre>tensor([[0.8600],\n        [0.8739],\n        [0.8878],\n        [0.9018],\n        [0.9157],\n        [0.9296],\n        [0.9436],\n        [0.9575],\n        [0.9714],\n        [0.9854]], device='cuda:0')</pre> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200bGPU\u200b\u4e0a\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u4e0a\u8ff0\u200b\u8f93\u51fa\u200b\u7684\u200b\u672b\u5c3e\u200b\u6709\u200b<code>device='cuda:0'</code>\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200bCUDA\u200b\u8bbe\u5907\u200b0\uff08\u200b\u7531\u4e8e\u200b\u96f6\u200b\u7d22\u5f15\u200b\uff0c\u200b\u8fd9\u200b\u662f\u200b\u4f60\u200b\u7684\u200b\u7cfb\u7edf\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b\u7684\u200b\u7b2c\u4e00\u5757\u200bGPU\uff09\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5c06\u6765\u200b\u4f7f\u7528\u200b\u591a\u5757\u200bGPU\uff0c\u200b\u8fd9\u4e2a\u200b\u6570\u5b57\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u66f4\u200b\u9ad8\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ed8\u5236\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bb8\u591a\u200b\u6570\u636e\u200b\u79d1\u5b66\u200b\u5e93\u200b\uff0c\u200b\u5982\u200bpandas\u3001matplotlib\u200b\u548c\u200bNumPy\uff0c\u200b\u65e0\u6cd5\u200b\u4f7f\u7528\u200b\u5b58\u50a8\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u5e93\u4e2d\u200b\u7684\u200b\u51fd\u6570\u200b\u5904\u7406\u200b\u672a\u200b\u5b58\u50a8\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u7684\u200b\u5f20\u91cf\u200b\u6570\u636e\u200b\u65f6\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u4e00\u4e9b\u200b\u95ee\u9898\u200b\u3002\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b<code>.cpu()</code>\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5c06\u200b\u76ee\u6807\u200b\u5f20\u91cf\u200b\u590d\u5236\u5230\u200bCPU\u200b\u4e0a\u200b\u3002</p> In\u00a0[37]: Copied! <pre># plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU\n\n# Put data on the CPU and plot it\nplot_predictions(predictions=y_preds.cpu())\n</pre> # plot_predictions(predictions=y_preds) # -&gt; won't work... data not on CPU  # Put data on the CPU and plot it plot_predictions(predictions=y_preds.cpu()) <p>\u200b\u54c7\u200b\uff01\u200b\u770b\u200b\u90a3\u4e9b\u200b\u7ea2\u200b\u70b9\u200b\uff0c\u200b\u5b83\u4eec\u200b\u51e0\u4e4e\u200b\u5b8c\u7f8e\u200b\u5730\u200b\u4e0e\u200b\u7eff\u70b9\u200b\u5bf9\u9f50\u200b\u4e86\u200b\u3002\u200b\u770b\u6765\u200b\u589e\u52a0\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5468\u671f\u200b\u786e\u5b9e\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002</p> In\u00a0[38]: Copied! <pre>from pathlib import Path\n\n# 1. Create models directory \nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, exist_ok=True)\n\n# 2. Create model save path \nMODEL_NAME = \"01_pytorch_workflow_model_1.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# 3. Save the model state dict \nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # 1. Create models directory  MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, exist_ok=True)  # 2. Create model save path  MODEL_NAME = \"01_pytorch_workflow_model_1.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # 3. Save the model state dict  print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_1.state_dict(), # only saving the state_dict() only saves the models learned parameters            f=MODEL_SAVE_PATH)  <pre>Saving model to: models/01_pytorch_workflow_model_1.pth\n</pre> <p>\u200b\u4e3a\u4e86\u200b\u786e\u4fdd\u200b\u4e00\u5207\u200b\u8fd0\u884c\u200b\u6b63\u5e38\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u91cd\u65b0\u200b\u52a0\u8f7d\u200b\u8fdb\u6765\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ul> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b <code>LinearRegressionModelV2()</code> \u200b\u7c7b\u200b\u5b9e\u4f8b\u200b</li> <li>\u200b\u4f7f\u7528\u200b <code>torch.nn.Module.load_state_dict()</code> \u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u72b6\u6001\u200b\u5b57\u5178\u200b</li> <li>\u200b\u5c06\u200b\u65b0\u200b\u5b9e\u4f8b\u200b\u7684\u200b\u6a21\u578b\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\uff08\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u4ee3\u7801\u200b\u662f\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\uff09</li> </ul> In\u00a0[39]: Copied! <pre># Instantiate a fresh instance of LinearRegressionModelV2\nloaded_model_1 = LinearRegressionModelV2()\n\n# Load model state dict \nloaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))\n\n# Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions)\nloaded_model_1.to(device)\n\nprint(f\"Loaded model:\\n{loaded_model_1}\")\nprint(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\")\n</pre> # Instantiate a fresh instance of LinearRegressionModelV2 loaded_model_1 = LinearRegressionModelV2()  # Load model state dict  loaded_model_1.load_state_dict(torch.load(MODEL_SAVE_PATH))  # Put model to target device (if your data is on GPU, model will have to be on GPU to make predictions) loaded_model_1.to(device)  print(f\"Loaded model:\\n{loaded_model_1}\") print(f\"Model on device:\\n{next(loaded_model_1.parameters()).device}\") <pre>Loaded model:\nLinearRegressionModelV2(\n  (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n)\nModel on device:\ncuda:0\n</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8bc4\u4f30\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u7684\u200b\u9884\u6d4b\u200b\u662f\u5426\u200b\u4e0e\u200b\u4fdd\u5b58\u200b\u4e4b\u524d\u200b\u7684\u200b\u9884\u6d4b\u200b\u4e00\u81f4\u200b\u3002</p> In\u00a0[40]: Copied! <pre># Evaluate loaded model\nloaded_model_1.eval()\nwith torch.inference_mode():\n    loaded_model_1_preds = loaded_model_1(X_test)\ny_preds == loaded_model_1_preds\n</pre> # Evaluate loaded model loaded_model_1.eval() with torch.inference_mode():     loaded_model_1_preds = loaded_model_1(X_test) y_preds == loaded_model_1_preds Out[40]: <pre>tensor([[True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True],\n        [True]], device='cuda:0')</pre> <p>\u200b\u4e00\u5207\u200b\u90fd\u200b\u6709\u200b\u610f\u4e49\u200b\uff01\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8d70\u200b\u4e86\u200b\u5f88\u957f\u200b\u7684\u200b\u8def\u200b\u3002\u200b\u4f60\u200b\u73b0\u5728\u200b\u5df2\u7ecf\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u6784\u5efa\u200b\u5e76\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4f60\u200b\u7684\u200b\u524d\u200b\u4e24\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u7ec3\u4e60\u200b\u4f60\u200b\u7684\u200b\u6280\u80fd\u200b\u4e86\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#01-pytorch","title":"01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u7cbe\u9ad3\u200b\u5728\u4e8e\u200b\u5229\u7528\u200b\u5386\u53f2\u6570\u636e\u200b\uff0c\u200b\u6784\u5efa\u200b\u7b97\u6cd5\u200b\uff08\u200b\u5982\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff09\u200b\u6765\u200b\u53d1\u73b0\u200b\u5176\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u5229\u7528\u200b\u8fd9\u4e9b\u200b\u53d1\u73b0\u200b\u7684\u200b\u6a21\u5f0f\u200b\u6765\u200b\u9884\u6d4b\u200b\u672a\u6765\u200b\u3002</p> <p>\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6709\u200b\u5f88\u591a\u200b\uff0c\u200b\u800c\u4e14\u200b\u4e0d\u65ad\u200b\u6709\u200b\u65b0\u200b\u7684\u200b\u65b9\u6cd5\u200b\u88ab\u200b\u53d1\u73b0\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u7b80\u5355\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u4e0d\u5982\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u5f00\u59cb\u200b\uff1f</p> <p>\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u80fd\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u5b83\u200b\u5b66\u4e60\u200b\u8fd9\u200b\u6761\u200b\u76f4\u7ebf\u200b\u7684\u200b\u6a21\u5f0f\u200b\u5e76\u200b\u4e0e\u200b\u4e4b\u200b\u5339\u914d\u200b\u3002</p>"},{"location":"01_pytorch_workflow/","title":"\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u672c\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u4e00\u4e2a\u200b\u6807\u51c6\u200b\u7684\u200b PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff08\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u6db5\u76d6\u200b\u4e86\u200b\u4e3b\u8981\u200b\u6b65\u9aa4\u200b\u7684\u200b\u6982\u8981\u200b\uff09\u3002</p> <p></p> <p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u6765\u200b\u9884\u6d4b\u200b\u4e00\u6761\u200b\u7b80\u5355\u200b\u7684\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e2a\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u7684\u200b\u6b65\u9aa4\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u8fdb\u884c\u200b\u91cd\u590d\u200b\u548c\u200b\u66f4\u6539\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 1. \u200b\u51c6\u5907\u200b\u6570\u636e\u200b \u200b\u6570\u636e\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u5165\u95e8\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u6761\u200b\u7b80\u5355\u200b\u7684\u200b\u76f4\u7ebf\u200b 2. \u200b\u6784\u5efa\u200b\u6a21\u578b\u200b \u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u5e76\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u3002 3. \u200b\u5c06\u200b\u6a21\u578b\u200b\u62df\u5408\u200b\u5230\u200b\u6570\u636e\u200b\uff08\u200b\u8bad\u7ec3\u200b\uff09 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u6570\u636e\u200b\u548c\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8ba9\u200b\u6a21\u578b\u200b\uff08\u200b\u5c1d\u8bd5\u200b\uff09\u200b\u5728\u200b\uff08\u200b\u8bad\u7ec3\u200b\uff09\u200b\u6570\u636e\u200b\u4e2d\u200b\u627e\u5230\u200b\u6a21\u5f0f\u200b\u3002 4. \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\uff08\u200b\u63a8\u7406\u200b\uff09 \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u5728\u200b\u6570\u636e\u200b\u4e2d\u200b\u627e\u5230\u200b\u4e86\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u200b\u7684\u200b\u53d1\u73b0\u200b\u4e0e\u200b\u5b9e\u9645\u200b\u7684\u200b\uff08\u200b\u6d4b\u8bd5\u200b\uff09\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002 5. \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b \u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u5728\u200b\u5176\u4ed6\u200b\u5730\u65b9\u200b\u4f7f\u7528\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6216\u8005\u200b\u7a0d\u540e\u200b\u518d\u200b\u56de\u6765\u200b\u4f7f\u7528\u200b\u5b83\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002 6. \u200b\u7efc\u5408\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e0a\u8ff0\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\u3002"},{"location":"01_pytorch_workflow/","title":"\u5982\u4f55\u200b\u83b7\u53d6\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b \u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b \u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u6709\u52a9\u4e8e\u200b\u89e3\u51b3\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u5e73\u53f0\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u5c06\u8981\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u65e5\u540e\u200b\u53c2\u8003\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#1","title":"1. \u200b\u6570\u636e\u200b\uff08\u200b\u51c6\u5907\u200b\u4e0e\u200b\u52a0\u8f7d\u200b\uff09\u00b6","text":"<p>\u200b\u6211\u200b\u5fc5\u987b\u200b\u5f3a\u8c03\u200b\uff0c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u201c\u200b\u6570\u636e\u200b\u201d\u200b\u53ef\u4ee5\u200b\u662f\u200b\u51e0\u4e4e\u200b\u4f60\u200b\u80fd\u200b\u60f3\u8c61\u200b\u5230\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\u3002\u200b\u6bd4\u5982\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\u8868\u683c\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200bExcel\u200b\u7535\u5b50\u8868\u683c\u200b\uff09\u3001\u200b\u5404\u79cd\u200b\u56fe\u50cf\u200b\u3001\u200b\u89c6\u9891\u200b\uff08YouTube\u200b\u4e0a\u200b\u6709\u200b\u5f88\u591a\u200b\u6570\u636e\u200b\uff01\uff09\u3001\u200b\u97f3\u9891\u6587\u4ef6\u200b\u5982\u200b\u6b4c\u66f2\u200b\u6216\u64ad\u5ba2\u200b\u3001\u200b\u86cb\u767d\u8d28\u200b\u7ed3\u6784\u200b\u3001\u200b\u6587\u672c\u200b\u7b49\u7b49\u200b\u3002</p> <p></p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u53ef\u4ee5\u200b\u5206\u4e3a\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ol> <li>\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u65e0\u8bba\u662f\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u6570\u5b57\u200b\uff08\u200b\u4e00\u79cd\u200b\u8868\u793a\u200b\uff09\u3002</li> <li>\u200b\u9009\u62e9\u200b\u6216\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u4ee5\u200b\u5c3d\u53ef\u80fd\u200b\u597d\u200b\u5730\u200b\u5b66\u4e60\u200b\u8fd9\u79cd\u200b\u8868\u793a\u200b\u3002</li> </ol> <p>\u200b\u6709\u65f6\u5019\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6b65\u9aa4\u200b\u53ef\u4ee5\u200b\u540c\u65f6\u200b\u8fdb\u884c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u4f60\u200b\u6ca1\u6709\u200b\u6570\u636e\u200b\u600e\u4e48\u529e\u200b\uff1f</p> <p>\u200b\u55ef\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u9762\u4e34\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6ca1\u6709\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u521b\u9020\u200b\u4e00\u4e9b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u4f5c\u4e3a\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u6765\u200b\u751f\u6210\u200b\u5177\u6709\u200b\u5df2\u77e5\u200b\u53c2\u6570\u200b\uff08\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e1c\u897f\u200b\uff09\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bPyTorch\u200b\u6765\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u6765\u200b\u4f30\u8ba1\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4e0a\u9762\u200b\u7684\u200b\u672f\u8bed\u200b\u73b0\u5728\u200b\u5bf9\u200b\u4f60\u200b\u6765\u8bf4\u200b\u610f\u4e49\u200b\u4e0d\u200b\u5927\u200b\uff0c\u200b\u4e0d\u7528\u200b\u62c5\u5fc3\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\u770b\u5230\u200b\u5b83\u4eec\u200b\uff0c\u200b\u6211\u200b\u4e5f\u200b\u4f1a\u200b\u5728\u200b\u4e0b\u9762\u200b\u63d0\u4f9b\u200b\u989d\u5916\u200b\u7684\u200b\u8d44\u6e90\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u5b66\u4e60\u200b\u3002</p>"},{"location":"01_pytorch_workflow/","title":"\u5c06\u200b\u6570\u636e\u200b\u5206\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5728\u200b\u5efa\u7acb\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u5176\u200b\u62c6\u5206\u200b\u5f00\u6765\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u6b65\u9aa4\u200b\u4e4b\u4e00\u200b\u662f\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff08\u200b\u4ee5\u53ca\u200b\u5728\u200b\u9700\u8981\u200b\u65f6\u200b\u521b\u5efa\u200b\u9a8c\u8bc1\u200b\u96c6\u200b\uff09\u3002</p> <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u62c6\u5206\u200b\u90fd\u200b\u670d\u52a1\u200b\u4e8e\u200b\u7279\u5b9a\u200b\u7684\u200b\u76ee\u7684\u200b\uff1a</p> \u200b\u62c6\u5206\u200b \u200b\u76ee\u7684\u200b \u200b\u5360\u200b\u603b\u4f53\u200b\u6570\u636e\u200b\u7684\u200b\u6bd4\u4f8b\u200b \u200b\u4f7f\u7528\u200b\u9891\u7387\u200b \u200b\u8bad\u7ec3\u200b\u96c6\u200b \u200b\u6a21\u578b\u200b\u4ece\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u4e2d\u200b\u5b66\u4e60\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4f60\u200b\u5728\u200b\u5b66\u671f\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u6750\u6599\u200b\uff09\u3002 \u200b\u7ea6\u200b60-80% \u200b\u603b\u662f\u200b \u200b\u9a8c\u8bc1\u200b\u96c6\u200b \u200b\u6a21\u578b\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8c03\u4f18\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4f60\u200b\u5728\u200b\u671f\u672b\u8003\u8bd5\u200b\u524d\u200b\u53c2\u52a0\u200b\u7684\u200b\u6a21\u62df\u8003\u8bd5\u200b\uff09\u3002 \u200b\u7ea6\u200b10-20% \u200b\u7ecf\u5e38\u200b\u4f46\u200b\u4e0d\u200b\u603b\u662f\u200b \u200b\u6d4b\u8bd5\u200b\u96c6\u200b \u200b\u6a21\u578b\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u4ee5\u200b\u6d4b\u8bd5\u200b\u5176\u200b\u5b66\u4e60\u200b\u6210\u679c\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4f60\u200b\u5728\u200b\u5b66\u671f\u672b\u200b\u53c2\u52a0\u200b\u7684\u200b\u671f\u672b\u8003\u8bd5\u200b\uff09\u3002 \u200b\u7ea6\u200b10-20% \u200b\u603b\u662f\u200b <p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u53ea\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u5c06\u200b\u6709\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4f9b\u200b\u6a21\u578b\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u62c6\u5206\u200b\u6211\u4eec\u200b\u7684\u200b <code>X</code> \u200b\u548c\u200b <code>y</code> \u200b\u5f20\u91cf\u200b\u6765\u200b\u521b\u5efa\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b\u5904\u7406\u200b\u771f\u5b9e\u4e16\u754c\u200b\u7684\u200b\u6570\u636e\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u4e00\u200b\u6b65\u9aa4\u200b\u901a\u5e38\u200b\u5728\u200b\u9879\u76ee\u200b\u5f00\u59cb\u200b\u65f6\u200b\u5c31\u200b\u5b8c\u6210\u200b\uff08\u200b\u6d4b\u8bd5\u200b\u96c6\u5e94\u200b\u59cb\u7ec8\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u6570\u636e\u200b\u5206\u5f00\u200b\uff09\u3002\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u5b66\u4e60\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u5176\u200b\u5bf9\u200b\u672a\u200b\u89c1\u200b\u793a\u4f8b\u200b\u7684\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#2","title":"2. \u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u5229\u7528\u200b\u84dd\u70b9\u200b\u6765\u200b\u9884\u6d4b\u200b\u7eff\u70b9\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u76f4\u63a5\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5148\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u89e3\u91ca\u200b\u4e00\u5207\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7eaf\u200b PyTorch \u200b\u6765\u200b\u590d\u73b0\u200b\u4e00\u4e2a\u200b\u6807\u51c6\u200b\u7684\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch","title":"PyTorch \u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u8981\u70b9\u200b\u00b6","text":"<p>PyTorch \u200b\u6709\u200b\u56db\u4e2a\u200b\uff08\u200b\u6216\u591a\u6216\u5c11\u200b\uff09\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u5b83\u4eec\u200b\u521b\u5efa\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u4f60\u200b\u80fd\u200b\u60f3\u8c61\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u662f\u200b <code>torch.nn</code>\u3001<code>torch.optim</code>\u3001<code>torch.utils.data.Dataset</code> \u200b\u548c\u200b <code>torch.utils.data.DataLoader</code>\u3002\u200b\u76ee\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u70b9\u200b\u653e\u5728\u200b\u524d\u200b\u4e24\u4e2a\u200b\u6a21\u5757\u200b\u4e0a\u200b\uff0c\u200b\u7a0d\u540e\u200b\u518d\u200b\u4ecb\u7ecd\u200b\u53e6\u5916\u200b\u4e24\u4e2a\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5df2\u7ecf\u200b\u731c\u200b\u5230\u200b\u5b83\u4eec\u200b\u7684\u200b\u4f5c\u7528\u200b\uff09\u3002</p> PyTorch \u200b\u6a21\u5757\u200b \u200b\u529f\u80fd\u200b\u63cf\u8ff0\u200b <code>torch.nn</code> \u200b\u5305\u542b\u200b\u6240\u6709\u200b\u8ba1\u7b97\u200b\u56fe\u200b\u7684\u200b\u6784\u5efa\u200b\u5757\u200b\uff08\u200b\u672c\u8d28\u200b\u4e0a\u200b\u662f\u200b\u4e00\u7cfb\u5217\u200b\u4ee5\u200b\u7279\u5b9a\u200b\u65b9\u5f0f\u200b\u6267\u884c\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff09\u3002 <code>torch.nn.Parameter</code> \u200b\u5b58\u50a8\u200b\u53ef\u4ee5\u200b\u4e0e\u200b <code>nn.Module</code> \u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002\u200b\u5982\u679c\u200b <code>requires_grad=True</code>\uff0c\u200b\u5219\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u8ba1\u7b97\u200b\u68af\u5ea6\u200b\uff08\u200b\u7528\u4e8e\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff09\uff0c\u200b\u8fd9\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u81ea\u52a8\u200b\u68af\u5ea6\u200b\u201d\u3002 <code>torch.nn.Module</code> \u200b\u6240\u6709\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u5757\u200b\u7684\u200b\u57fa\u7c7b\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6240\u6709\u200b\u6784\u5efa\u200b\u5757\u200b\u90fd\u200b\u662f\u200b\u5176\u5b50\u200b\u7c7b\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u5e94\u8be5\u200b\u7ee7\u627f\u200b <code>nn.Module</code>\u3002\u200b\u9700\u8981\u200b\u5b9e\u73b0\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u3002 <code>torch.optim</code> \u200b\u5305\u542b\u200b\u5404\u79cd\u200b\u4f18\u5316\u200b\u7b97\u6cd5\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u7b97\u6cd5\u200b\u544a\u8bc9\u200b\u5b58\u50a8\u200b\u5728\u200b <code>nn.Parameter</code> \u200b\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u5982\u4f55\u200b\u6700\u597d\u200b\u5730\u200b\u6539\u53d8\u200b\u4ee5\u200b\u6539\u5584\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff0c\u200b\u8fdb\u800c\u200b\u51cf\u5c11\u200b\u635f\u5931\u200b\uff09\u3002 <code>def forward()</code> \u200b\u6240\u6709\u200b <code>nn.Module</code> \u200b\u5b50\u7c7b\u200b\u90fd\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fd9\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u5728\u200b\u4f20\u9012\u200b\u7ed9\u200b\u7279\u5b9a\u200b <code>nn.Module</code> \u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u5c06\u200b\u8fdb\u884c\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff08\u200b\u4f8b\u5982\u200b\u4e0a\u9762\u200b\u7684\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\uff09\u3002 <p>\u200b\u5982\u679c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\u542c\u200b\u8d77\u6765\u200b\u590d\u6742\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u7406\u89e3\u200b\uff1aPyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u51e0\u4e4e\u200b\u6240\u6709\u200b\u4e1c\u897f\u200b\u90fd\u200b\u6765\u81ea\u200b <code>torch.nn</code>\uff0c</p> <ul> <li><code>nn.Module</code> \u200b\u5305\u542b\u200b\u8f83\u5927\u200b\u7684\u200b\u6784\u5efa\u200b\u5757\u200b\uff08\u200b\u5c42\u200b\uff09</li> <li><code>nn.Parameter</code> \u200b\u5305\u542b\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u5dee\u200b\uff08\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\u4ee5\u200b\u6784\u5efa\u200b <code>nn.Module</code>\uff09</li> <li><code>forward()</code> \u200b\u544a\u8bc9\u200b\u8f83\u5927\u200b\u7684\u200b\u5757\u200b\u5982\u4f55\u200b\u5728\u200b <code>nn.Module</code> \u200b\u4e2d\u200b\u5bf9\u200b\u8f93\u5165\u200b\uff08\u200b\u5145\u6ee1\u200b\u6570\u636e\u200b\u7684\u200b\u5f20\u91cf\u200b\uff09\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b</li> <li><code>torch.optim</code> \u200b\u5305\u542b\u200b\u4f18\u5316\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u7528\u4e8e\u200b\u6539\u5584\u200b <code>nn.Parameter</code> \u200b\u4e2d\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u200b\u66f4\u597d\u200b\u5730\u200b\u8868\u793a\u200b\u8f93\u5165\u200b\u6570\u636e\u200b</li> </ul> <p> \u200b\u901a\u8fc7\u200b\u7ee7\u627f\u200b <code>nn.Module</code> \u200b\u521b\u5efa\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u672c\u200b\u6784\u5efa\u200b\u5757\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u7ee7\u627f\u200b <code>nn.Module</code> \u200b\u7684\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u5fc5\u987b\u200b\u5b9a\u4e49\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u5728\u200b PyTorch \u200b\u901f\u67e5\u8868\u200b \u200b\u4e2d\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u57fa\u672c\u200b\u6a21\u5757\u200b\u53ca\u5176\u200b\u7528\u4f8b\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch","title":"\u68c0\u67e5\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u89e3\u51b3\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u7c7b\u6765\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>.parameters()</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u68c0\u67e5\u200b\u5176\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#torchinference_mode","title":"\u4f7f\u7528\u200b <code>torch.inference_mode()</code> \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u9a8c\u8bc1\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6d4b\u8bd5\u6570\u636e\u200b <code>X_test</code> \u200b\u4f20\u9012\u200b\u7ed9\u200b\u5b83\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u5bf9\u200b <code>y_test</code> \u200b\u7684\u200b\u9884\u6d4b\u200b\u6709\u200b\u591a\u200b\u63a5\u8fd1\u200b\u3002</p> <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u5411\u200b\u6a21\u578b\u200b\u4f20\u9012\u6570\u636e\u200b\u65f6\u200b\uff0c\u200b\u6570\u636e\u200b\u4f1a\u200b\u901a\u8fc7\u200b\u6a21\u578b\u200b\u7684\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5e76\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u7684\u200b\u8ba1\u7b97\u200b\u4ea7\u751f\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#3","title":"3. \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u53c2\u6570\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u8fd9\u200b\u57fa\u672c\u4e0a\u200b\u662f\u200b\u5728\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u66f4\u65b0\u200b\u5176\u200b\u5185\u90e8\u200b\u53c2\u6570\u200b\uff08\u200b\u6211\u200b\u4e5f\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u79f0\u4e3a\u200b\u6a21\u5f0f\u200b\uff09\uff0c\u200b\u5373\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>nn.Parameter()</code> \u200b\u548c\u200b <code>torch.randn()</code> \u200b\u968f\u673a\u200b\u8bbe\u7f6e\u200b\u7684\u200b <code>\u200b\u6743\u91cd\u200b</code> \u200b\u548c\u200b <code>\u200b\u504f\u5dee\u200b</code> \u200b\u503c\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u66f4\u597d\u200b\u5730\u200b\u53cd\u6620\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u786c\u200b\u7f16\u7801\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u9ed8\u8ba4\u503c\u200b <code>weight=0.7</code> \u200b\u548c\u200b <code>bias=0.3</code>\uff09\uff0c\u200b\u4f46\u200b\u90a3\u6837\u200b\u505a\u200b\u6709\u200b\u4ec0\u4e48\u200b\u4e50\u8da3\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5f88\u591a\u200b\u65f6\u5019\u200b\uff0c\u200b\u4f60\u200b\u5e76\u4e0d\u77e5\u9053\u200b\u6a21\u578b\u200b\u7684\u200b\u7406\u60f3\u200b\u53c2\u6570\u200b\u662f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u76f8\u53cd\u200b\uff0c\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u8ba9\u200b\u6a21\u578b\u200b\u5c1d\u8bd5\u200b\u81ea\u5df1\u200b\u627e\u51fa\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u4f1a\u200b\u66f4\u200b\u6709\u8da3\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch","title":"\u5728\u200b PyTorch \u200b\u4e2d\u200b\u521b\u5efa\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u81ea\u884c\u200b\u66f4\u65b0\u200b\u5176\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u4e00\u4e9b\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b \u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u548c\u200b \u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u7684\u200b\u4f5c\u7528\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u51fd\u6570\u200b \u200b\u4f5c\u7528\u200b \u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b\u4f4d\u7f6e\u200b \u200b\u5e38\u89c1\u200b\u503c\u200b \u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u8861\u91cf\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\uff08\u200b\u4f8b\u5982\u200b <code>y_preds</code>\uff09\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\uff08\u200b\u4f8b\u5982\u200b <code>y_test</code>\uff09\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002\u200b\u6570\u503c\u200b\u8d8a\u4f4e\u200b\u8d8a\u200b\u597d\u200b\u3002 PyTorch \u200b\u5728\u200b <code>torch.nn</code> \u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5185\u7f6e\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002 \u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\u7684\u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\uff08MAE\uff0c<code>torch.nn.L1Loss()</code>\uff09\u3002\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u7684\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\uff08<code>torch.nn.BCELoss()</code>\uff09\u3002 \u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u544a\u8bc9\u200b\u6a21\u578b\u200b\u5982\u4f55\u200b\u66f4\u65b0\u200b\u5176\u200b\u5185\u90e8\u200b\u53c2\u6570\u200b\u4ee5\u200b\u6700\u597d\u200b\u5730\u200b\u964d\u4f4e\u200b\u635f\u5931\u200b\u3002 \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>torch.optim</code> \u200b\u4e2d\u200b\u627e\u5230\u200b\u5404\u79cd\u200b\u4f18\u5316\u200b\u51fd\u6570\u200b\u7684\u200b\u5b9e\u73b0\u200b\u3002 \u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff08<code>torch.optim.SGD()</code>\uff09\u3002Adam \u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>torch.optim.Adam()</code>\uff09\u3002 <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5e2e\u52a9\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u7c7b\u578b\u200b\u5c06\u200b\u51b3\u5b9a\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e9b\u200b\u5e38\u89c1\u200b\u7684\u200b\u503c\u200b\u88ab\u200b\u8bc1\u660e\u200b\u662f\u200b\u6709\u6548\u200b\u7684\u200b\uff0c\u200b\u4f8b\u5982\u200b SGD\uff08\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09\u200b\u6216\u200b Adam \u200b\u4f18\u5316\u200b\u5668\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\uff08\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u6570\u503c\u200b\uff09\uff0c\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b MAE\uff08\u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\uff09\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff1b\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff08\u200b\u9884\u6d4b\u200b\u662f\u200b\u6216\u5426\u200b\uff09\uff0c\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u662f\u200b\u5728\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u6570\u503c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b MAE\uff08\u200b\u5373\u200b <code>torch.nn.L1Loss()</code>\uff09\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p> \u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\uff08MAE\uff0c\u200b\u5728\u200b PyTorch \u200b\u4e2d\u4e3a\u200b <code>torch.nn.L1Loss</code>\uff09\u200b\u6d4b\u91cf\u200b\u4e24\u4e2a\u200b\u70b9\u200b\uff08\u200b\u9884\u6d4b\u503c\u200b\u548c\u200b\u6807\u7b7e\u200b\uff09\u200b\u4e4b\u95f4\u200b\u7684\u200b\u7edd\u5bf9\u200b\u5dee\u5f02\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u6240\u6709\u200b\u6837\u672c\u200b\u53d6\u200b\u5e73\u5747\u503c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b SGD\uff0c\u200b\u5373\u200b <code>torch.optim.SGD(params, lr)</code>\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>params</code> \u200b\u662f\u200b\u4f60\u200b\u5e0c\u671b\u200b\u4f18\u5316\u200b\u7684\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\uff08\u200b\u4f8b\u5982\u200b\u4e4b\u524d\u200b\u968f\u673a\u200b\u8bbe\u7f6e\u200b\u7684\u200b <code>weights</code> \u200b\u548c\u200b <code>bias</code> \u200b\u503c\u200b\uff09\u3002</li> <li><code>lr</code> \u200b\u662f\u200b\u4f60\u200b\u5e0c\u671b\u200b\u4f18\u5316\u200b\u5668\u200b\u66f4\u65b0\u200b\u53c2\u6570\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\uff0c\u200b\u8f83\u200b\u9ad8\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u610f\u5473\u7740\u200b\u4f18\u5316\u200b\u5668\u4f1a\u200b\u5c1d\u8bd5\u200b\u66f4\u5927\u200b\u7684\u200b\u66f4\u65b0\u200b\uff08\u200b\u6709\u65f6\u200b\u8fd9\u4e9b\u200b\u66f4\u65b0\u200b\u53ef\u80fd\u200b\u592a\u200b\u5927\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u4f18\u5316\u200b\u5668\u200b\u65e0\u6cd5\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\uff09\uff0c\u200b\u8f83\u200b\u4f4e\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u610f\u5473\u7740\u200b\u4f18\u5316\u200b\u5668\u4f1a\u200b\u5c1d\u8bd5\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u66f4\u65b0\u200b\uff08\u200b\u6709\u65f6\u200b\u8fd9\u4e9b\u200b\u66f4\u65b0\u200b\u53ef\u80fd\u200b\u592a\u5c0f\u200b\uff0c\u200b\u5bfc\u81f4\u200b\u4f18\u5316\u200b\u5668\u200b\u82b1\u8d39\u200b\u592a\u200b\u957f\u65f6\u95f4\u200b\u627e\u5230\u200b\u7406\u60f3\u200b\u503c\u200b\uff09\u3002\u200b\u5b66\u4e60\u200b\u7387\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u4e00\u4e2a\u200b \u200b\u8d85\u200b\u53c2\u6570\u200b\uff08\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u7531\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u5e08\u200b\u8bbe\u7f6e\u200b\uff09\u3002\u200b\u5e38\u89c1\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u521d\u59cb\u503c\u200b\u4e3a\u200b <code>0.01</code>\u3001<code>0.001</code>\u3001<code>0.0001</code>\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u8c03\u6574\u200b\uff08\u200b\u8fd9\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u5ea6\u200b\uff09\u3002</li> </ul> <p>\u200b\u54c7\u200b\uff0c\u200b\u8fd9\u200b\u5185\u5bb9\u200b\u771f\u200b\u591a\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch","title":"\u5728\u200b PyTorch \u200b\u4e2d\u200b\u521b\u5efa\u200b\u4f18\u5316\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u54c7\u200b\u54e6\u200b\uff01\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff08\u200b\u4ee5\u53ca\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\uff09\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u6d89\u53ca\u200b\u6a21\u578b\u200b\u904d\u5386\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u5e76\u200b\u5b66\u4e60\u200b <code>features</code> \u200b\u548c\u200b <code>labels</code> \u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\u3002</p> <p>\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u6d89\u53ca\u200b\u904d\u5386\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u5e76\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u5b66\u200b\u5230\u200b\u7684\u200b\u6a21\u5f0f\u200b\u7684\u200b\u4f18\u52a3\u200b\uff08\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4ece\u672a\u89c1\u8fc7\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u5faa\u73af\u200b\u201d\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6a21\u578b\u200b\u67e5\u770b\u200b\uff08\u200b\u904d\u5386\u200b\uff09\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u521b\u5efa\u200b\u8fd9\u4e9b\u200b\u5faa\u73af\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b Python \u200b\u7684\u200b <code>for</code> \u200b\u5faa\u73af\u200b\uff0c\u200b\u4e3b\u9898\u200b\u6765\u81ea\u200b\u975e\u5b98\u65b9\u200b PyTorch \u200b\u4f18\u5316\u200b\u5faa\u73af\u200b\u6b4c\u66f2\u200b\uff08\u200b\u8fd8\u6709\u200b\u89c6\u9891\u200b\u7248\u672c\u200b\uff09\u3002</p> <p> \u200b\u975e\u5b98\u65b9\u200b PyTorch \u200b\u4f18\u5316\u200b\u5faa\u73af\u200b\u6b4c\u66f2\u200b\uff0c\u200b\u4e00\u79cd\u200b\u6709\u8da3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6765\u200b\u8bb0\u4f4f\u200b PyTorch \u200b\u8bad\u7ec3\u200b\uff08\u200b\u548c\u200b\u6d4b\u8bd5\u200b\uff09\u200b\u5faa\u73af\u200b\u7684\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u4f1a\u200b\u6709\u200b\u76f8\u5f53\u200b\u591a\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5b8c\u5168\u200b\u53ef\u4ee5\u200b\u5e94\u5bf9\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch","title":"PyTorch \u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u5bf9\u4e8e\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\uff1a</p> \u200b\u7f16\u53f7\u200b \u200b\u6b65\u9aa4\u200b\u540d\u79f0\u200b \u200b\u4f5c\u7528\u200b \u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b 1 \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b \u200b\u6a21\u578b\u200b\u904d\u5386\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e00\u6b21\u200b\uff0c\u200b\u6267\u884c\u200b\u5176\u200b <code>forward()</code> \u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u3002 <code>model(x_train)</code> 2 \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b \u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u9884\u6d4b\u200b\uff09\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u503c\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u8bc4\u4f30\u200b\u5176\u200b\u9519\u8bef\u200b\u7a0b\u5ea6\u200b\u3002 <code>loss = loss_fn(y_pred, y_train)</code> 3 \u200b\u6e05\u96f6\u200b\u68af\u5ea6\u200b \u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u68af\u5ea6\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u96f6\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5b83\u4eec\u200b\u662f\u200b\u7d2f\u79ef\u200b\u7684\u200b\uff09\uff0c\u200b\u4ee5\u4fbf\u200b\u4e3a\u200b\u7279\u5b9a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u91cd\u65b0\u200b\u8ba1\u7b97\u200b\u3002 <code>optimizer.zero_grad()</code> 4 \u200b\u5bf9\u200b\u635f\u5931\u200b\u8fdb\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u6bcf\u4e2a\u200b\u5f85\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u6bcf\u4e2a\u200b <code>requires_grad=True</code> \u200b\u7684\u200b\u53c2\u6570\u200b\uff09\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u79f0\u4e3a\u200b\u201c\u200b\u53cd\u5411\u200b\u201d\u3002 <code>loss.backward()</code> 5 \u200b\u66f4\u65b0\u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09 \u200b\u6839\u636e\u200b\u635f\u5931\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b <code>requires_grad=True</code> \u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u200b\u6539\u8fdb\u200b\u5b83\u4eec\u200b\u3002 <code>optimizer.step()</code> <p></p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0a\u8ff0\u200b\u6b65\u9aa4\u200b\u7684\u200b\u987a\u5e8f\u200b\u6216\u200b\u63cf\u8ff0\u200b\u53ea\u662f\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u3002\u200b\u968f\u7740\u200b\u7ecf\u9a8c\u200b\u7684\u200b\u79ef\u7d2f\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u6784\u5efa\u200b PyTorch \u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u53ef\u4ee5\u200b\u975e\u5e38\u7075\u6d3b\u200b\u3002</p> <p>\u200b\u81f3\u4e8e\u200b\u6b65\u9aa4\u200b\u7684\u200b\u987a\u5e8f\u200b\uff0c\u200b\u4e0a\u8ff0\u200b\u662f\u200b\u4e00\u4e2a\u200b\u826f\u597d\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u987a\u5e8f\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u770b\u5230\u200b\u7a0d\u5fae\u200b\u4e0d\u540c\u200b\u7684\u200b\u987a\u5e8f\u200b\u3002\u200b\u4e00\u4e9b\u200b\u7ecf\u9a8c\u200b\u6cd5\u5219\u200b\uff1a</p> <ul> <li>\u200b\u5728\u200b\u6267\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff08<code>loss.backward()</code>\uff09\u200b\u4e4b\u524d\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\uff08<code>loss = ...</code>\uff09\u3002</li> <li>\u200b\u5728\u200b\u66f4\u65b0\u200b\u68af\u5ea6\u200b\uff08<code>optimizer.step()</code>\uff09\u200b\u4e4b\u524d\u200b\u6e05\u96f6\u200b\u68af\u5ea6\u200b\uff08<code>optimizer.zero_grad()</code>\uff09\u3002</li> <li>\u200b\u5728\u200b\u6267\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff08<code>loss.backward()</code>\uff09\u200b\u4e4b\u540e\u200b\u66f4\u65b0\u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>optimizer.step()</code>\uff09\u3002</li> </ul> <p>\u200b\u8981\u200b\u4e86\u89e3\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u548c\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u80cc\u540e\u200b\u7684\u200b\u539f\u7406\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b\u90e8\u5206\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch","title":"PyTorch \u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u5bf9\u4e8e\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\uff08\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff09\uff0c\u200b\u5178\u578b\u200b\u7684\u200b\u6b65\u9aa4\u200b\u5305\u62ec\u200b\uff1a</p> \u200b\u7f16\u53f7\u200b \u200b\u6b65\u9aa4\u200b\u540d\u79f0\u200b \u200b\u4f5c\u7528\u200b \u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b 1 \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b \u200b\u6a21\u578b\u200b\u904d\u5386\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e00\u6b21\u200b\uff0c\u200b\u6267\u884c\u200b\u5176\u200b <code>forward()</code> \u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u3002 <code>model(x_test)</code> 2 \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b \u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u9884\u6d4b\u200b\uff09\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u503c\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u8bc4\u4f30\u200b\u5176\u200b\u9519\u8bef\u200b\u7a0b\u5ea6\u200b\u3002 <code>loss = loss_fn(y_pred, y_test)</code> 3 \u200b\u8ba1\u7b97\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff08\u200b\u53ef\u200b\u9009\u200b\uff09 \u200b\u9664\u4e86\u200b\u635f\u5931\u200b\u503c\u5916\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u8fd8\u200b\u60f3\u200b\u8ba1\u7b97\u200b\u5176\u4ed6\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff0c\u200b\u5982\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002 \u200b\u81ea\u5b9a\u4e49\u200b\u51fd\u6570\u200b <p>\u200b\u6ce8\u610f\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u4e0d\u200b\u5305\u542b\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff08<code>loss.backward()</code>\uff09\u200b\u6216\u200b\u4f18\u5316\u200b\u5668\u200b\u6b65\u8fdb\u200b\uff08<code>optimizer.step()</code>\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u4e0d\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5df2\u7ecf\u200b\u8ba1\u7b97\u200b\u597d\u200b\u4e86\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u5173\u5fc3\u200b\u6a21\u578b\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p></p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u4e0a\u8ff0\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b 100 \u200b\u4e2a\u200b epoch\uff08\u200b\u6570\u636e\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u6bcf\u200b 10 \u200b\u4e2a\u200b epoch \u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u8bc4\u4f30\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#4-pytorch","title":"4. \u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200bPyTorch\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08\u200b\u63a8\u7406\u200b\uff09\u00b6","text":"<p>\u200b\u4e00\u65e6\u200b\u4f60\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u60f3\u8981\u200b\u7528\u200b\u5b83\u200b\u6765\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u4ee3\u7801\u200b\u4e2d\u200b\u770b\u5230\u200b\u4e86\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u4e4b\u5916\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u7684\u200b\u6b65\u9aa4\u200b\u4e0e\u200b\u4e4b\u200b\u7c7b\u4f3c\u200b\u3002</p> <p>\u200b\u5728\u200b\u4f7f\u7528\u200bPyTorch\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u6267\u884c\u200b\u63a8\u7406\u200b\uff09\u200b\u65f6\u200b\uff0c\u200b\u6709\u200b\u4e09\u70b9\u200b\u9700\u8981\u200b\u8bb0\u4f4f\u200b\uff1a</p> <ol> <li>\u200b\u5c06\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u8bc4\u4f30\u200b\u6a21\u5f0f\u200b\uff08<code>model.eval()</code>\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u63a8\u7406\u200b\u6a21\u5f0f\u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08<code>with torch.inference_mode(): ...</code>\uff09\u3002</li> <li>\u200b\u6240\u6709\u200b\u9884\u6d4b\u200b\u5e94\u200b\u4f7f\u7528\u200b\u4f4d\u4e8e\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u7684\u200b\u5bf9\u8c61\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u4ec5\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u6216\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u4ec5\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\uff09\u3002</li> </ol> <p>\u200b\u524d\u200b\u4e24\u70b9\u200b\u786e\u4fdd\u200b\u4e86\u200bPyTorch\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u6240\u6709\u200b\u6709\u52a9\u4e8e\u200b\u8ba1\u7b97\u200b\u548c\u200b\u8bbe\u7f6e\u200b\u4f46\u200b\u5728\u200b\u63a8\u7406\u200b\u65f6\u200b\u4e0d\u200b\u9700\u8981\u200b\u7684\u200b\u529f\u80fd\u200b\u90fd\u200b\u88ab\u200b\u5173\u95ed\u200b\uff08\u200b\u8fd9\u4f1a\u200b\u52a0\u5feb\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\uff09\u3002\u200b\u7b2c\u4e09\u70b9\u200b\u5219\u200b\u786e\u4fdd\u200b\u4f60\u200b\u4e0d\u4f1a\u200b\u9047\u5230\u200b\u8de8\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#5-pytorch","title":"5. \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b PyTorch \u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\uff0c\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u4fdd\u5b58\u200b\u5b83\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5bfc\u51fa\u200b\u5230\u200b\u67d0\u4e2a\u200b\u5730\u65b9\u200b\u3002</p> <p>\u200b\u6bd4\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b Google Colab \u200b\u6216\u200b\u5e26\u6709\u200b GPU \u200b\u7684\u200b\u672c\u5730\u200b\u673a\u5668\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u73b0\u5728\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5c06\u200b\u5176\u200b\u5bfc\u51fa\u200b\u5230\u200b\u67d0\u79cd\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u4e2d\u200b\uff0c\u200b\u8ba9\u200b\u5176\u4ed6\u4eba\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u8fdb\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u4ee5\u540e\u200b\u56de\u8fc7\u200b\u6765\u200b\u91cd\u65b0\u200b\u52a0\u8f7d\u200b\u5b83\u200b\u3002</p> <p>\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u6709\u200b\u4e09\u79cd\u200b\u4e3b\u8981\u200b\u65b9\u6cd5\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4e86\u89e3\u200b\uff08\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\u5747\u200b\u6765\u81ea\u200b PyTorch \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u6307\u5357\u200b\uff09\uff1a</p> PyTorch \u200b\u65b9\u6cd5\u200b \u200b\u4f5c\u7528\u200b <code>torch.save</code> \u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>pickle</code> \u200b\u5de5\u5177\u200b\u5c06\u200b\u5e8f\u5217\u5316\u200b\u5bf9\u8c61\u200b\u4fdd\u5b58\u200b\u5230\u200b\u78c1\u76d8\u200b\u3002\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.save</code> \u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3001\u200b\u5f20\u91cf\u200b\u548c\u200b\u5404\u79cd\u200b\u5176\u4ed6\u200b Python \u200b\u5bf9\u8c61\u200b\uff0c\u200b\u5982\u200b\u5b57\u5178\u200b\u3002 <code>torch.load</code> \u200b\u4f7f\u7528\u200b <code>pickle</code> \u200b\u7684\u200b\u89e3\u200b\u5e8f\u5217\u5316\u200b\u529f\u80fd\u200b\u5c06\u200b\u5e8f\u5217\u5316\u200b\u7684\u200b Python \u200b\u5bf9\u8c61\u200b\u6587\u4ef6\u200b\uff08\u200b\u5982\u200b\u6a21\u578b\u200b\u3001\u200b\u5f20\u91cf\u200b\u6216\u200b\u5b57\u5178\u200b\uff09\u200b\u53cd\u200b\u5e8f\u5217\u5316\u200b\u5e76\u200b\u52a0\u8f7d\u200b\u5230\u200b\u5185\u5b58\u200b\u4e2d\u200b\u3002\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u52a0\u8f7d\u200b\u5bf9\u8c61\u200b\u7684\u200b\u8bbe\u5907\u200b\uff08CPU\u3001GPU \u200b\u7b49\u200b\uff09\u3002 <code>torch.nn.Module.load_state_dict</code> \u200b\u4f7f\u7528\u200b\u4fdd\u5b58\u200b\u7684\u200b <code>state_dict()</code> \u200b\u5bf9\u8c61\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u5b57\u5178\u200b\uff08<code>model.state_dict()</code>\uff09\u3002 <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u200b Python \u200b\u7684\u200b <code>pickle</code> \u200b\u6587\u6863\u200b \u200b\u6240\u8ff0\u200b\uff0c<code>pickle</code> \u200b\u6a21\u5757\u200b\u4e0d\u200b\u5b89\u5168\u200b\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4f60\u200b\u53ea\u80fd\u200b\u89e3\u200b\u5e8f\u5217\u5316\u200b\uff08\u200b\u52a0\u8f7d\u200b\uff09\u200b\u4f60\u200b\u4fe1\u4efb\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u9002\u7528\u200b\u4e8e\u200b\u52a0\u8f7d\u200b PyTorch \u200b\u6a21\u578b\u200b\u3002\u200b\u53ea\u5e94\u200b\u4f7f\u7528\u200b\u6765\u81ea\u200b\u53ef\u4fe1\u200b\u6765\u6e90\u200b\u7684\u200b\u4fdd\u5b58\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch-state_dict","title":"\u4fdd\u5b58\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code>\u00b6","text":"<p>\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff08\u200b\u9884\u6d4b\u200b\uff09\u200b\u7684\u200b\u63a8\u8350\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u51e0\u4e2a\u200b\u6b65\u9aa4\u200b\u6765\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>pathlib</code> \u200b\u6a21\u5757\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>models</code> \u200b\u7684\u200b\u76ee\u5f55\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u6765\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8c03\u7528\u200b <code>torch.save(obj, f)</code>\uff0c\u200b\u5176\u4e2d\u200b <code>obj</code> \u200b\u662f\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code>\uff0c<code>f</code> \u200b\u662f\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u6587\u4ef6\u540d\u200b\u3002</li> </ol> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u6216\u200b\u5bf9\u8c61\u200b\u901a\u5e38\u200b\u4ee5\u200b <code>.pt</code> \u200b\u6216\u200b <code>.pth</code> \u200b\u7ed3\u5c3e\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>saved_model_01.pth</code>\u3002</p>"},{"location":"01_pytorch_workflow/#pytorch-state_dict","title":"\u52a0\u8f7d\u200b\u4fdd\u5b58\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code>\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5df2\u7ecf\u200b\u5728\u200b <code>models/01_pytorch_workflow_model_0.pth</code> \u200b\u8def\u5f84\u200b\u4e0b\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code>\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.nn.Module.load_state_dict(torch.load(f))</code> \u200b\u6765\u200b\u52a0\u8f7d\u200b\u5b83\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>f</code> \u200b\u662f\u200b\u6211\u4eec\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b <code>state_dict()</code> \u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u5728\u200b <code>torch.nn.Module.load_state_dict()</code> \u200b\u5185\u90e8\u200b\u8c03\u7528\u200b <code>torch.load()</code>\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u53ea\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code>\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\u7684\u200b\u5b57\u5178\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u3002\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u4f7f\u7528\u200b <code>torch.load()</code> \u200b\u52a0\u8f7d\u200b <code>state_dict()</code>\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u8fd9\u4e2a\u200b <code>state_dict()</code> \u200b\u4f20\u9012\u200b\u7ed9\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u65b0\u200b\u5b9e\u4f8b\u200b\uff08\u200b\u5b83\u200b\u662f\u200b <code>nn.Module</code> \u200b\u7684\u200b\u5b50\u7c7b\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u4e0d\u200b\u4fdd\u5b58\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff1f</p> <p>\u200b\u4fdd\u5b58\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b \u200b\u800c\u200b\u4e0d\u662f\u200b\u4ec5\u4ec5\u200b\u4fdd\u5b58\u200b <code>state_dict()</code> \u200b\u53ef\u80fd\u200b\u66f4\u200b\u76f4\u89c2\u200b\uff0c\u200b\u4f46\u662f\u200b\uff0c\u200b\u5f15\u7528\u200b PyTorch \u200b\u6587\u6863\u200b\uff08\u200b\u659c\u4f53\u200b\u90e8\u5206\u200b\u662f\u200b\u6211\u52a0\u200b\u7684\u200b\uff09\uff1a</p> <p>\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\uff08\u200b\u4fdd\u5b58\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\uff09\u200b\u7684\u200b\u7f3a\u70b9\u200b\u662f\u200b\u5e8f\u5217\u5316\u200b\u6570\u636e\u200b\u7ed1\u5b9a\u200b\u5230\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200b\u7279\u5b9a\u200b\u7c7b\u200b\u548c\u200b\u786e\u5207\u200b\u76ee\u5f55\u200b\u7ed3\u6784\u200b...</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5f53\u200b\u5728\u200b\u5176\u4ed6\u200b\u9879\u76ee\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u6216\u200b\u5728\u200b\u91cd\u6784\u200b\u540e\u200b\uff0c\u200b\u60a8\u200b\u7684\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u4f1a\u4ee5\u200b\u5404\u79cd\u200b\u65b9\u5f0f\u200b\u5d29\u6e83\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b\u4f7f\u7528\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b <code>state_dict()</code> \u200b\u7684\u200b\u7075\u6d3b\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8fd9\u200b\u57fa\u672c\u4e0a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u7684\u200b\u5b57\u5178\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b <code>LinearRegressionModel()</code> \u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5b9e\u4f8b\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b <code>torch.nn.Module</code> \u200b\u7684\u200b\u5b50\u7c7b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5c06\u200b\u5177\u6709\u200b\u5185\u7f6e\u200b\u65b9\u6cd5\u200b <code>load_state_dict()</code>\u3002</p>"},{"location":"01_pytorch_workflow/#6","title":"6. \u200b\u7efc\u5408\u200b\u8fd0\u7528\u200b\u00b6","text":"<p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6db5\u76d6\u200b\u4e86\u200b\u5f88\u591a\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e00\u65e6\u200b\u4f60\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u4f60\u200b\u5c31\u200b\u4f1a\u200b\u50cf\u200b\u5728\u200b\u8857\u4e0a\u200b\u8df3\u821e\u200b\u4e00\u6837\u200b\u8f7b\u677e\u200b\u6267\u884c\u200b\u4e0a\u8ff0\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u8bf4\u200b\u5230\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u4e4b\u524d\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e00\u5207\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e00\u6b21\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u200b\u4ee3\u7801\u200b\u4e0e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\uff08\u200b\u5982\u679c\u200b\u6709\u200bGPU\u200b\u53ef\u7528\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u4f7f\u7528\u200bGPU\uff1b\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200bCPU\uff09\u3002</p> <p>\u200b\u4e0e\u200b\u524d\u9762\u200b\u7684\u200b\u90e8\u5206\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u8282\u200b\u7684\u200b\u6ce8\u91ca\u200b\u4f1a\u5c11\u200b\u5f97\u200b\u591a\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5373\u5c06\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u5185\u5bb9\u200b\u4e4b\u524d\u200b\u5df2\u7ecf\u200b\u8bb2\u89e3\u200b\u8fc7\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u5bfc\u5165\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6807\u51c6\u200b\u5e93\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bGoogle Colab\uff0c\u200b\u8981\u200b\u8bbe\u7f6e\u200bGPU\uff0c\u200b\u8bf7\u200b\u8f6c\u5230\u200b\u201c\u200b\u8fd0\u884c\u200b\u65f6\u200b\u201d -&gt; \u201c\u200b\u66f4\u6539\u200b\u8fd0\u884c\u200b\u65f6\u200b\u7c7b\u578b\u200b\u201d -&gt; \u201c\u200b\u786c\u4ef6\u52a0\u901f\u200b\u201d -&gt; \u201cGPU\u201d\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u91cd\u7f6e\u200bColab\u200b\u8fd0\u884c\u200b\u65f6\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f60\u200b\u4f1a\u200b\u4e22\u5931\u200b\u4fdd\u5b58\u200b\u7684\u200b\u53d8\u91cf\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#61","title":"6.1 \u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u50cf\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u786c\u200b\u7f16\u7801\u200b\u4e00\u4e9b\u200b <code>weight</code> \u200b\u548c\u200b <code>bias</code> \u200b\u503c\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u751f\u6210\u200b\u4e00\u4e2a\u200b\u4ecb\u4e8e\u200b 0 \u200b\u548c\u200b 1 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u6570\u5b57\u200b\u8303\u56f4\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5c06\u200b\u662f\u200b\u6211\u4eec\u200b\u7684\u200b <code>X</code> \u200b\u503c\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>X</code> \u200b\u503c\u200b\u4ee5\u53ca\u200b <code>weight</code> \u200b\u548c\u200b <code>bias</code> \u200b\u503c\u200b\uff0c\u200b\u901a\u8fc7\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\uff08<code>y = weight * X + bias</code>\uff09\u200b\u6765\u200b\u521b\u5efa\u200b <code>y</code>\u3002</p>"},{"location":"01_pytorch_workflow/#62-pytorch","title":"6.2 \u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u7ebf\u6027\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u76f8\u540c\u200b\u98ce\u683c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u518d\u200b\u4f7f\u7528\u200b <code>nn.Parameter()</code> \u200b\u624b\u52a8\u200b\u5b9a\u4e49\u200b\u6a21\u578b\u200b\u7684\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u7f6e\u200b\u53c2\u6570\u200b\uff0c\u200b\u800c\u662f\u200b\u4f7f\u7528\u200b <code>nn.Linear(in_features, out_features)</code> \u200b\u6765\u200b\u4e3a\u200b\u6211\u4eec\u200b\u5b8c\u6210\u200b\u8fd9\u9879\u200b\u5de5\u4f5c\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b <code>in_features</code> \u200b\u662f\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u91cf\u200b\uff0c<code>out_features</code> \u200b\u662f\u200b\u4f60\u200b\u5e0c\u671b\u200b\u8f93\u51fa\u200b\u6570\u636e\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u91cf\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u503c\u200b\u90fd\u200b\u662f\u200b <code>1</code>\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u6bcf\u4e2a\u200b\u6807\u7b7e\u200b\uff08<code>y</code>\uff09\u200b\u6709\u200b <code>1</code> \u200b\u4e2a\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\uff08<code>X</code>\uff09\u3002</p> <p> \u200b\u4f7f\u7528\u200b <code>nn.Parameter</code> \u200b\u4e0e\u200b <code>nn.Linear</code> \u200b\u521b\u5efa\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u6a21\u578b\u200b\u7684\u200b\u6bd4\u8f83\u200b\u3002<code>torch.nn</code> \u200b\u6a21\u5757\u200b\u4e2d\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5305\u62ec\u200b\u8bb8\u591a\u200b\u6d41\u884c\u200b\u4e14\u200b\u6709\u7528\u200b\u7684\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#63","title":"6.3 \u200b\u8bad\u7ec3\u200b\u00b6","text":""},{"location":"01_pytorch_workflow/#64","title":"6.4 \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u5207\u6362\u200b\u5230\u200b\u8bc4\u4f30\u200b\u6a21\u5f0f\u200b\u5e76\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\u3002</p>"},{"location":"01_pytorch_workflow/#65","title":"6.5 \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5bf9\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u611f\u5230\u200b\u6ee1\u610f\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5c06\u200b\u5176\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u65e5\u540e\u200b\u4f7f\u7528\u200b\u3002</p>"},{"location":"01_pytorch_workflow/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u5747\u200b\u53d7\u5230\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u4ee3\u7801\u200b\u7684\u200b\u542f\u53d1\u200b\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u4e3b\u8981\u200b\u90e8\u5206\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u60a8\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u5176\u200b\u7279\u5b9a\u200b\u90e8\u5206\u200b\u6765\u200b\u5b8c\u6210\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u60a8\u200b\u7684\u200b\u4ee3\u7801\u200b\u5e94\u8be5\u200b\u662f\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\uff08\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5728\u200b CPU \u200b\u6216\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u5982\u679c\u200b\u53ef\u7528\u200b\u7684\u8bdd\u200b\uff09\u3002</p> <ol> <li>\u200b\u4f7f\u7528\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u516c\u5f0f\u200b\uff08<code>weight * X + bias</code>\uff09\u200b\u521b\u5efa\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u6570\u636e\u200b\u96c6\u200b\u3002<ul> <li>\u200b\u8bbe\u7f6e\u200b <code>weight=0.3</code> \u200b\u548c\u200b <code>bias=0.9</code>\uff0c\u200b\u603b\u6570\u200b\u636e\u70b9\u200b\u81f3\u5c11\u200b\u4e3a\u200b 100 \u200b\u4e2a\u200b\u3002</li> <li>\u200b\u5c06\u200b\u6570\u636e\u200b\u5206\u4e3a\u200b 80% \u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b 20% \u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</li> <li>\u200b\u7ed8\u5236\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u53ef\u89c6\u5316\u200b\u3002</li> </ul> </li> <li>\u200b\u901a\u8fc7\u200b\u5b50\u200b\u7c7b\u5316\u200b <code>nn.Module</code> \u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\u3002<ul> <li>\u200b\u5185\u90e8\u200b\u5e94\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u521d\u59cb\u5316\u200b\u7684\u200b <code>nn.Parameter()</code>\uff0c\u200b\u8bbe\u7f6e\u200b <code>requires_grad=True</code>\uff0c\u200b\u5206\u522b\u200b\u7528\u4e8e\u200b <code>weights</code> \u200b\u548c\u200b <code>bias</code>\u3002</li> <li>\u200b\u5b9e\u73b0\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u8ba1\u7b97\u200b\u60a8\u200b\u5728\u200b\u7b2c\u200b 1 \u200b\u90e8\u5206\u200b\u4e2d\u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u7ebf\u6027\u200b\u56de\u5f52\u200b\u51fd\u6570\u200b\u3002</li> <li>\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u521b\u5efa\u200b\u5176\u5b9e\u200b\u4f8b\u200b\u5e76\u200b\u68c0\u67e5\u200b\u5176\u200b <code>state_dict()</code>\u3002</li> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u613f\u610f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>nn.Linear()</code> \u200b\u4ee3\u66ff\u200b <code>nn.Parameter()</code>\u3002</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200b <code>nn.L1Loss()</code> \u200b\u548c\u200b <code>torch.optim.SGD(params, lr)</code> \u200b\u5206\u522b\u200b\u521b\u5efa\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u3002<ul> <li>\u200b\u5c06\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b 0.01\uff0c\u200b\u8981\u200b\u4f18\u5316\u200b\u7684\u200b\u53c2\u6570\u200b\u5e94\u4e3a\u200b\u60a8\u200b\u5728\u200b\u7b2c\u200b 2 \u200b\u90e8\u5206\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u3002</li> <li>\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff0c\u200b\u6267\u884c\u200b 300 \u200b\u4e2a\u200b epoch \u200b\u7684\u200b\u9002\u5f53\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u3002</li> <li>\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u5e94\u200b\u6bcf\u200b 20 \u200b\u4e2a\u200b epoch \u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\u3002</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002<ul> <li>\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u539f\u59cb\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u8fdb\u884c\u200b\u53ef\u89c6\u5316\u200b\uff08\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u60f3\u200b\u4f7f\u7528\u200b\u975e\u200b CUDA \u200b\u652f\u6301\u200b\u7684\u200b\u5e93\u200b\uff08\u200b\u5982\u200b matplotlib\uff09\u200b\u8fdb\u884c\u200b\u7ed8\u56fe\u200b\uff0c\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u9884\u6d4b\u200b\u4e0d\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff09\u3002</li> </ul> </li> <li>\u200b\u5c06\u200b\u60a8\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b <code>state_dict()</code> \u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u3002<ul> <li>\u200b\u521b\u5efa\u200b\u60a8\u200b\u5728\u200b\u7b2c\u200b 2 \u200b\u90e8\u5206\u200b\u4e2d\u200b\u5236\u4f5c\u200b\u7684\u200b\u6a21\u578b\u200b\u7c7b\u200b\u7684\u200b\u65b0\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u52a0\u8f7d\u200b\u521a\u521a\u200b\u4fdd\u5b58\u200b\u7684\u200b <code>state_dict()</code>\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5e76\u200b\u786e\u8ba4\u200b\u5b83\u4eec\u200b\u4e0e\u200b\u7b2c\u200b 4 \u200b\u90e8\u5206\u200b\u4e2d\u200b\u539f\u59cb\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u5339\u914d\u200b\u3002</li> </ul> </li> </ol> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u53c2\u89c1\u200b\u8bfe\u7a0b\u200b GitHub \u200b\u4e0a\u200b\u7684\u200b \u200b\u7ec3\u4e60\u200b\u7b14\u8bb0\u672c\u200b\u6a21\u677f\u200b \u200b\u548c\u200b \u200b\u89e3\u51b3\u65b9\u6848\u200b\u3002</p>"},{"location":"01_pytorch_workflow/","title":"\u989d\u5916\u200b\u5b66\u4e60\u200b\u00b6","text":"<ul> <li>\u200b\u542c\u542c\u200b The Unofficial PyTorch Optimization Loop Song\uff08\u200b\u5e2e\u52a9\u200b\u8bb0\u5fc6\u200b PyTorch \u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff09\u3002</li> <li>\u200b\u9605\u8bfb\u200b Jeremy Howard \u200b\u7684\u200b What is <code>torch.nn</code>, really?\uff0c\u200b\u6df1\u5165\u200b\u4e86\u89e3\u200b PyTorch \u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u6a21\u5757\u200b\u4e4b\u4e00\u200b\u7684\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\u3002</li> <li>\u200b\u82b1\u200b 10 \u200b\u5206\u949f\u200b\u6d4f\u89c8\u200b\u5e76\u200b\u67e5\u770b\u200b PyTorch \u200b\u6587\u6863\u200b\u901f\u67e5\u8868\u200b\uff0c\u200b\u4e86\u89e3\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u7684\u200b\u5404\u79cd\u200b PyTorch \u200b\u6a21\u5757\u200b\u3002</li> <li>\u200b\u82b1\u200b 10 \u200b\u5206\u949f\u200b\u9605\u8bfb\u200b PyTorch \u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u7684\u200b\u52a0\u8f7d\u200b\u548c\u200b\u4fdd\u5b58\u6587\u6863\u200b\uff0c\u200b\u719f\u6089\u200b PyTorch \u200b\u4e2d\u200b\u4e0d\u540c\u200b\u7684\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u9009\u9879\u200b\u3002</li> <li>\u200b\u82b1\u200b 1-2 \u200b\u5c0f\u65f6\u200b\u9605\u8bfb\u200b/\u200b\u89c2\u770b\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff0c\u200b\u6982\u8ff0\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u548c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u7684\u200b\u5185\u90e8\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\uff0c\u200b\u8fd9\u662f\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e3b\u8981\u200b\u7b97\u6cd5\u200b\u3002<ul> <li>Wikipedia \u200b\u4e0a\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u9875\u9762\u200b</li> <li>Robert Kwiatkowski \u200b\u7684\u200b Gradient Descent Algorithm \u2014 a deep dive</li> <li>3Blue1Brown \u200b\u7684\u200b Gradient descent, how neural networks learn \u200b\u89c6\u9891\u200b</li> <li>3Blue1Brown \u200b\u7684\u200b What is backpropagation really doing? \u200b\u89c6\u9891\u200b</li> <li>Wikipedia \u200b\u4e0a\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u9875\u9762\u200b</li> </ul> </li> </ul>"},{"location":"02_pytorch_classification/","title":"02. PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b | \u200b\u89c2\u770b\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b</p> In\u00a0[1]: Copied! <pre>from sklearn.datasets import make_circles\n\n\n# Make 1000 samples \nn_samples = 1000\n\n# Create circles\nX, y = make_circles(n_samples,\n                    noise=0.03, # a little bit of noise to the dots\n                    random_state=42) # keep random state so we get the same values\n</pre> from sklearn.datasets import make_circles   # Make 1000 samples  n_samples = 1000  # Create circles X, y = make_circles(n_samples,                     noise=0.03, # a little bit of noise to the dots                     random_state=42) # keep random state so we get the same values <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u67e5\u770b\u200b\u524d\u200b5\u200b\u4e2a\u200b<code>X</code>\u200b\u548c\u200b<code>y</code>\u200b\u7684\u200b\u503c\u200b\u3002</p> In\u00a0[2]: Copied! <pre>print(f\"First 5 X features:\\n{X[:5]}\")\nprint(f\"\\nFirst 5 y labels:\\n{y[:5]}\")\n</pre> print(f\"First 5 X features:\\n{X[:5]}\") print(f\"\\nFirst 5 y labels:\\n{y[:5]}\") <pre>First 5 X features:\n[[ 0.75424625  0.23148074]\n [-0.75615888  0.15325888]\n [-0.81539193  0.17328203]\n [-0.39373073  0.69288277]\n [ 0.44220765 -0.89672343]]\n\nFirst 5 y labels:\n[1 1 1 1 0]\n</pre> <p>\u200b\u770b\u8d77\u6765\u200b\u6bcf\u4e2a\u200b <code>y</code> \u200b\u503c\u200b\u5bf9\u5e94\u200b\u4e24\u4e2a\u200b <code>X</code> \u200b\u503c\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u9075\u5faa\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff1a\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b pandas DataFrame \u200b\u4e2d\u200b\u3002</p> In\u00a0[3]: Copied! <pre># Make DataFrame of circle data\nimport pandas as pd\ncircles = pd.DataFrame({\"X1\": X[:, 0],\n    \"X2\": X[:, 1],\n    \"label\": y\n})\ncircles.head(10)\n</pre> # Make DataFrame of circle data import pandas as pd circles = pd.DataFrame({\"X1\": X[:, 0],     \"X2\": X[:, 1],     \"label\": y }) circles.head(10) Out[3]: X1 X2 label 0 0.754246 0.231481 1 1 -0.756159 0.153259 1 2 -0.815392 0.173282 1 3 -0.393731 0.692883 1 4 0.442208 -0.896723 0 5 -0.479646 0.676435 1 6 -0.013648 0.803349 1 7 0.771513 0.147760 1 8 -0.169322 -0.793456 1 9 -0.121486 1.021509 0 <p>\u200b\u770b\u8d77\u6765\u200b\u6bcf\u200b\u5bf9\u200b <code>X</code> \u200b\u7279\u5f81\u200b\uff08<code>X1</code> \u200b\u548c\u200b <code>X2</code>\uff09\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\uff08<code>y</code>\uff09\u200b\u503c\u200b\uff0c\u200b\u8981\u4e48\u200b\u662f\u200b 0\uff0c\u200b\u8981\u4e48\u200b\u662f\u200b 1\u3002</p> <p>\u200b\u8fd9\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\u662f\u200b\u4e8c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u53ea\u6709\u200b\u4e24\u4e2a\u200b\u9009\u9879\u200b\uff080 \u200b\u6216\u200b 1\uff09\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u7c7b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u503c\u200b\u5462\u200b\uff1f</p> In\u00a0[4]: Copied! <pre># Check different labels\ncircles.label.value_counts()\n</pre> # Check different labels circles.label.value_counts() Out[4]: <pre>1    500\n0    500\nName: label, dtype: int64</pre> <p>\u200b\u6bcf\u7ec4\u200b500\u200b\u4e2a\u200b\uff0c\u200b\u6570\u91cf\u200b\u5747\u8861\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u5b83\u4eec\u200b\u7ed8\u5236\u200b\u51fa\u6765\u200b\u3002</p> In\u00a0[5]: Copied! <pre># Visualize with a plot\nimport matplotlib.pyplot as plt\nplt.scatter(x=X[:, 0], \n            y=X[:, 1], \n            c=y, \n            cmap=plt.cm.RdYlBu);\n</pre> # Visualize with a plot import matplotlib.pyplot as plt plt.scatter(x=X[:, 0],              y=X[:, 1],              c=y,              cmap=plt.cm.RdYlBu); <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u9700\u8981\u200b\u89e3\u51b3\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u63a2\u8ba8\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5c06\u70b9\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u7ea2\u8272\u200b\uff080\uff09\u200b\u6216\u200b\u84dd\u8272\u200b\uff081\uff09\u200b\u7684\u200bPyTorch\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u901a\u5e38\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u73a9\u5177\u200b\u95ee\u9898\u200b\uff08\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5c1d\u8bd5\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u95ee\u9898\u200b\uff09\u3002</p> <p>\u200b\u4f46\u200b\u5b83\u200b\u4ee3\u8868\u200b\u4e86\u200b\u5206\u7c7b\u200b\u7684\u200b\u5173\u952e\u200b\u8981\u70b9\u200b\uff0c\u200b\u4f60\u200b\u6709\u200b\u4e00\u4e9b\u200b\u4ee5\u200b\u6570\u503c\u200b\u5f62\u5f0f\u200b\u8868\u793a\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u5c06\u200b\u5176\u200b\u5206\u4e3a\u200b\u7ea2\u8272\u200b\u6216\u200b\u84dd\u8272\u200b\u7684\u200b\u70b9\u200b\u3002</p> In\u00a0[6]: Copied! <pre># Check the shapes of our features and labels\nX.shape, y.shape\n</pre> # Check the shapes of our features and labels X.shape, y.shape Out[6]: <pre>((1000, 2), (1000,))</pre> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u7b2c\u4e00\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u5339\u914d\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u6709\u200b 1000 \u200b\u4e2a\u200b <code>X</code> \u200b\u548c\u200b 1000 \u200b\u4e2a\u200b <code>y</code>\u3002</p> <p>\u200b\u4f46\u200b <code>X</code> \u200b\u7684\u200b\u7b2c\u4e8c\u200b\u7ef4\u5ea6\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u67e5\u770b\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\uff08\u200b\u7279\u5f81\u200b\u548c\u200b\u6807\u7b7e\u200b\uff09\u200b\u7684\u200b\u503c\u200b\u548c\u200b\u5f62\u72b6\u200b\u901a\u5e38\u200b\u4f1a\u200b\u6709\u6240\u200b\u5e2e\u52a9\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u505a\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u4e86\u89e3\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u671f\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p> In\u00a0[7]: Copied! <pre># View the first example of features and labels\nX_sample = X[0]\ny_sample = y[0]\nprint(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\nprint(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\")\n</pre> # View the first example of features and labels X_sample = X[0] y_sample = y[0] print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\") print(f\"Shapes for one sample of X: {X_sample.shape} and the same for y: {y_sample.shape}\") <pre>Values for one sample of X: [0.75424625 0.23148074] and the same for y: 1\nShapes for one sample of X: (2,) and the same for y: ()\n</pre> <p>\u200b\u8fd9\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\uff0c<code>X</code>\u200b\u7684\u200b\u7b2c\u4e8c\u4e2a\u200b\u7ef4\u5ea6\u200b\u8868\u793a\u200b\u5b83\u200b\u6709\u200b\u4e24\u4e2a\u200b\u7279\u5f81\u200b\uff08\u200b\u5411\u91cf\u200b\uff09\uff0c\u200b\u800c\u200b<code>y</code>\u200b\u6709\u200b\u4e00\u4e2a\u200b\u7279\u5f81\u200b\uff08\u200b\u6807\u91cf\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6709\u200b\u4e24\u4e2a\u200b\u8f93\u5165\u200b\uff0c\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u3002</p> In\u00a0[8]: Copied! <pre># Turn data into tensors\n# Otherwise this causes issues with computations later on\nimport torch\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# View the first five samples\nX[:5], y[:5]\n</pre> # Turn data into tensors # Otherwise this causes issues with computations later on import torch X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # View the first five samples X[:5], y[:5] Out[8]: <pre>(tensor([[ 0.7542,  0.2315],\n         [-0.7562,  0.1533],\n         [-0.8154,  0.1733],\n         [-0.3937,  0.6929],\n         [ 0.4422, -0.8967]]),\n tensor([1., 1., 1., 1., 0.]))</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u5f20\u91cf\u200b\u683c\u5f0f\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u62c6\u200b\u5206\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Scikit-Learn \u200b\u63d0\u4f9b\u200b\u7684\u200b\u4fbf\u6377\u200b\u51fd\u6570\u200b <code>train_test_split()</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>test_size=0.2</code>\uff0880% \u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\uff0c20% \u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u7531\u4e8e\u200b\u62c6\u5206\u200b\u662f\u200b\u968f\u673a\u200b\u8fdb\u884c\u200b\u7684\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>random_state=42</code> \u200b\u4ee5\u200b\u786e\u4fdd\u200b\u62c6\u5206\u200b\u7ed3\u679c\u200b\u53ef\u200b\u590d\u73b0\u200b\u3002</p> In\u00a0[9]: Copied! <pre># Split data into train and test sets\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2, # 20% test, 80% train\n                                                    random_state=42) # make the random split reproducible\n\nlen(X_train), len(X_test), len(y_train), len(y_test)\n</pre> # Split data into train and test sets from sklearn.model_selection import train_test_split  X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2, # 20% test, 80% train                                                     random_state=42) # make the random split reproducible  len(X_train), len(X_test), len(y_train), len(y_test) Out[9]: <pre>(800, 200, 800, 200)</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u6709\u200b800\u200b\u4e2a\u200b\u8bad\u7ec3\u6837\u672c\u200b\u548c\u200b200\u200b\u4e2a\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u3002</p> In\u00a0[10]: Copied! <pre># Standard PyTorch imports\nimport torch\nfrom torch import nn\n\n# Make device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Standard PyTorch imports import torch from torch import nn  # Make device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[10]: <pre>'cuda'</pre> <p>\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u73b0\u5728\u200b <code>device</code> \u200b\u5df2\u7ecf\u200b\u8bbe\u7f6e\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5b83\u200b\u7528\u4e8e\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u4efb\u4f55\u200b\u6570\u636e\u200b\u6216\u200b\u6a21\u578b\u200b\uff0cPyTorch \u200b\u4f1a\u200b\u81ea\u52a8\u200b\u5728\u200b CPU\uff08\u200b\u9ed8\u8ba4\u200b\uff09\u200b\u6216\u200b GPU\uff08\u200b\u5982\u679c\u200b\u6709\u200b\u7684\u8bdd\u200b\uff09\u200b\u4e0a\u200b\u5904\u7406\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5427\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u6211\u4eec\u200b\u7684\u200b <code>X</code> \u200b\u6570\u636e\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\uff0c\u200b\u5e76\u200b\u751f\u6210\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b <code>y</code> \u200b\u6570\u636e\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u7ed9\u5b9a\u200b <code>X</code>\uff08\u200b\u7279\u5f81\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b <code>y</code>\uff08\u200b\u6807\u7b7e\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u79cd\u200b\u5177\u6709\u7279\u5f81\u200b\u548c\u200b\u6807\u7b7e\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u76d1\u7763\u200b\u5b66\u4e60\u200b\u3002\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u4f1a\u200b\u544a\u8bc9\u200b\u6a21\u578b\u200b\u5728\u200b\u7ed9\u5b9a\u200b\u67d0\u4e2a\u200b\u8f93\u5165\u200b\u65f6\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u8981\u200b\u521b\u5efa\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u200b\u9700\u8981\u200b\u5904\u7406\u200b <code>X</code> \u200b\u548c\u200b <code>y</code> \u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u8fd8\u200b\u8bb0\u5f97\u200b\u6211\u200b\u8bf4\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u5f88\u200b\u91cd\u8981\u200b\u5417\u200b\uff1f\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u5c06\u200b\u770b\u5230\u200b\u539f\u56e0\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u7c7b\u200b\uff0c\u200b\u5b83\u200b\uff1a</p> <ol> <li>\u200b\u7ee7\u627f\u200b\u81ea\u200b <code>nn.Module</code>\uff08\u200b\u51e0\u4e4e\u200b\u6240\u6709\u200b PyTorch \u200b\u6a21\u578b\u200b\u90fd\u200b\u662f\u200b <code>nn.Module</code> \u200b\u7684\u200b\u5b50\u7c7b\u200b\uff09\u3002</li> <li>\u200b\u5728\u200b\u6784\u9020\u51fd\u6570\u200b\u4e2d\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u80fd\u591f\u200b\u5904\u7406\u200b <code>X</code> \u200b\u548c\u200b <code>y</code> \u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7684\u200b <code>nn.Linear</code> \u200b\u5c42\u200b\u3002</li> <li>\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6a21\u578b\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u8ba1\u7b97\u200b\u7684\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u6a21\u578b\u200b\u7c7b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b <code>device</code>\u3002</li> </ol> In\u00a0[11]: Copied! <pre># 1. Construct a model class that subclasses nn.Module\nclass CircleModelV0(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes\n        self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features\n        self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)\n    \n    # 3. Define a forward method containing the forward pass computation\n    def forward(self, x):\n        # Return the output of layer_2, a single feature, the same shape as y\n        return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2\n\n# 4. Create an instance of the model and send it to target device\nmodel_0 = CircleModelV0().to(device)\nmodel_0\n</pre> # 1. Construct a model class that subclasses nn.Module class CircleModelV0(nn.Module):     def __init__(self):         super().__init__()         # 2. Create 2 nn.Linear layers capable of handling X and y input and output shapes         self.layer_1 = nn.Linear(in_features=2, out_features=5) # takes in 2 features (X), produces 5 features         self.layer_2 = nn.Linear(in_features=5, out_features=1) # takes in 5 features, produces 1 feature (y)          # 3. Define a forward method containing the forward pass computation     def forward(self, x):         # Return the output of layer_2, a single feature, the same shape as y         return self.layer_2(self.layer_1(x)) # computation goes through layer_1 first then the output of layer_1 goes through layer_2  # 4. Create an instance of the model and send it to target device model_0 = CircleModelV0().to(device) model_0 Out[11]: <pre>CircleModelV0(\n  (layer_1): Linear(in_features=2, out_features=5, bias=True)\n  (layer_2): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>\u200b\u8fd9\u662f\u200b\u600e\u4e48\u56de\u4e8b\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u5df2\u7ecf\u200b\u89c1\u8fc7\u200b\u5176\u4e2d\u200b\u7684\u200b\u4e00\u4e9b\u200b\u6b65\u9aa4\u200b\u4e86\u200b\u3002</p> <p>\u200b\u552f\u4e00\u200b\u7684\u200b\u4e3b\u8981\u200b\u53d8\u5316\u200b\u5728\u4e8e\u200b <code>self.layer_1</code> \u200b\u548c\u200b <code>self.layer_2</code> \u200b\u4e4b\u95f4\u200b\u53d1\u751f\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <p><code>self.layer_1</code> \u200b\u63a5\u53d7\u200b 2 \u200b\u4e2a\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b <code>in_features=2</code> \u200b\u5e76\u200b\u4ea7\u751f\u200b 5 \u200b\u4e2a\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b <code>out_features=5</code>\u3002</p> <p>\u200b\u8fd9\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u6709\u200b 5 \u200b\u4e2a\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6216\u200b\u795e\u7ecf\u5143\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u5c06\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u4ece\u200b\u5177\u6709\u200b 2 \u200b\u4e2a\u200b\u7279\u5f81\u200b\u8f6c\u53d8\u200b\u4e3a\u200b 5 \u200b\u4e2a\u200b\u7279\u5f81\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u4f7f\u5f97\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u4ece\u200b 5 \u200b\u4e2a\u200b\u6570\u5b57\u200b\u800c\u200b\u4e0d\u662f\u200b\u4ec5\u4ec5\u200b 2 \u200b\u4e2a\u200b\u6570\u5b57\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6f5c\u5728\u5730\u200b \u200b\u5bfc\u81f4\u200b\u66f4\u597d\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u6211\u200b\u8bf4\u200b\u201c\u200b\u6f5c\u5728\u5730\u200b\u201d\u200b\u662f\u56e0\u4e3a\u200b\u6709\u65f6\u5019\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u200b\u594f\u6548\u200b\u3002</p> <p>\u200b\u5728\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u91cf\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff08\u200b\u4e00\u4e2a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u81ea\u5df1\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u503c\u200b\uff09\uff0c\u200b\u5e76\u200b\u6ca1\u6709\u200b\u56fa\u5b9a\u200b\u4e0d\u53d8\u200b\u7684\u200b\u503c\u200b\u4f60\u200b\u5fc5\u987b\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u8d8a\u591a\u8d8a\u597d\u200b\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u6709\u200b\u8fc7\u72b9\u4e0d\u53ca\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002\u200b\u4f60\u200b\u9009\u62e9\u200b\u7684\u200b\u6570\u91cf\u200b\u5c06\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u7c7b\u578b\u200b\u548c\u200b\u6240\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u5f88\u5c0f\u200b\u4e14\u200b\u7b80\u5355\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u4fdd\u6301\u200b\u8f83\u200b\u5c0f\u89c4\u6a21\u200b\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u7684\u200b\u552f\u4e00\u200b\u89c4\u5219\u200b\u662f\u200b\uff0c\u200b\u4e0b\u200b\u4e00\u5c42\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u662f\u200b <code>self.layer_2</code>\uff0c\u200b\u5fc5\u987b\u200b\u63a5\u53d7\u200b\u4e0e\u200b\u524d\u200b\u4e00\u5c42\u200b <code>out_features</code> \u200b\u76f8\u540c\u200b\u7684\u200b <code>in_features</code>\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u4e3a\u4ec0\u4e48\u200b <code>self.layer_2</code> \u200b\u6709\u200b <code>in_features=5</code>\uff0c\u200b\u5b83\u200b\u63a5\u53d7\u200b <code>self.layer_1</code> \u200b\u7684\u200b <code>out_features=5</code> \u200b\u5e76\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u7ebf\u6027\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>out_features=1</code>\uff08\u200b\u4e0e\u200b <code>y</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\uff09\u3002</p> <p> \u200b\u4e0e\u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u6784\u5efa\u200b\u7684\u200b\u7c7b\u4f3c\u200b\u5206\u7c7b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u793a\u4f8b\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b TensorFlow Playground \u200b\u7f51\u7ad9\u200b \u200b\u4e0a\u200b\u5c1d\u8bd5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u81ea\u5df1\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>nn.Sequential</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u4e0a\u8ff0\u200b\u76f8\u540c\u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p><code>nn.Sequential</code> \u200b\u6309\u7167\u200b\u5c42\u200b\u51fa\u73b0\u200b\u7684\u200b\u987a\u5e8f\u200b\u5bf9\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u8ba1\u7b97\u200b\u3002</p> In\u00a0[12]: Copied! <pre># Replicate CircleModelV0 with nn.Sequential\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=2, out_features=5),\n    nn.Linear(in_features=5, out_features=1)\n).to(device)\n\nmodel_0\n</pre> # Replicate CircleModelV0 with nn.Sequential model_0 = nn.Sequential(     nn.Linear(in_features=2, out_features=5),     nn.Linear(in_features=5, out_features=1) ).to(device)  model_0 Out[12]: <pre>Sequential(\n  (0): Linear(in_features=2, out_features=5, bias=True)\n  (1): Linear(in_features=5, out_features=1, bias=True)\n)</pre> <p>\u200b\u54c7\u200b\uff0c\u200b\u8fd9\u200b\u770b\u8d77\u6765\u200b\u6bd4\u200b\u7ee7\u627f\u200b <code>nn.Module</code> \u200b\u7b80\u5355\u200b\u591a\u200b\u4e86\u200b\uff0c\u200b\u4e3a\u4ec0\u4e48\u200b\u4e0d\u200b\u603b\u662f\u200b\u4f7f\u7528\u200b <code>nn.Sequential</code> \u200b\u5462\u200b\uff1f</p> <p><code>nn.Sequential</code> \u200b\u975e\u5e38\u9002\u5408\u200b\u76f4\u63a5\u200b\u7684\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u6b63\u5982\u200b\u547d\u540d\u200b\u7a7a\u95f4\u200b\u6240\u793a\u200b\uff0c\u200b\u5b83\u200b\u603b\u662f\u200b\u6309\u200b\u987a\u5e8f\u200b\u6267\u884c\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u9700\u8981\u200b\u6267\u884c\u200b\u5176\u4ed6\u200b\u64cd\u4f5c\u200b\uff08\u200b\u800c\u200b\u4e0d\u662f\u200b\u7b80\u5355\u200b\u7684\u200b\u987a\u5e8f\u200b\u8ba1\u7b97\u200b\uff09\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u5b9a\u4e49\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>nn.Module</code> \u200b\u5b50\u7c7b\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5f53\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u65f6\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> In\u00a0[13]: Copied! <pre># Make predictions with the model\nuntrained_preds = model_0(X_test.to(device))\nprint(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\")\nprint(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\")\nprint(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\")\nprint(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\")\n</pre> # Make predictions with the model untrained_preds = model_0(X_test.to(device)) print(f\"Length of predictions: {len(untrained_preds)}, Shape: {untrained_preds.shape}\") print(f\"Length of test samples: {len(y_test)}, Shape: {y_test.shape}\") print(f\"\\nFirst 10 predictions:\\n{untrained_preds[:10]}\") print(f\"\\nFirst 10 test labels:\\n{y_test[:10]}\") <pre>Length of predictions: 200, Shape: torch.Size([200, 1])\nLength of test samples: 200, Shape: torch.Size([200])\n\nFirst 10 predictions:\ntensor([[-0.4279],\n        [-0.3417],\n        [-0.5975],\n        [-0.3801],\n        [-0.5078],\n        [-0.4559],\n        [-0.2842],\n        [-0.3107],\n        [-0.6010],\n        [-0.3350]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)\n\nFirst 10 test labels:\ntensor([1., 0., 1., 0., 1., 1., 0., 0., 1., 0.])\n</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u9884\u6d4b\u200b\u7684\u200b\u6570\u91cf\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u6807\u7b7e\u200b\u7684\u200b\u6570\u91cf\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u9884\u6d4b\u200b\u7684\u200b\u683c\u5f0f\u200b\u6216\u200b\u5f62\u72b6\u200b\u4f3c\u4e4e\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u6807\u7b7e\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91c7\u53d6\u200b\u51e0\u4e2a\u200b\u6b65\u9aa4\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u7a0d\u540e\u200b\u6211\u4eec\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\u3002</p> In\u00a0[14]: Copied! <pre># Create a loss function\n# loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in\nloss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in\n\n# Create an optimizer\noptimizer = torch.optim.SGD(params=model_0.parameters(), \n                            lr=0.1)\n</pre> # Create a loss function # loss_fn = nn.BCELoss() # BCELoss = no sigmoid built-in loss_fn = nn.BCEWithLogitsLoss() # BCEWithLogitsLoss = sigmoid built-in  # Create an optimizer optimizer = torch.optim.SGD(params=model_0.parameters(),                              lr=0.1) <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e5f\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u3002</p> <p>\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u63d0\u4f9b\u200b\u5173\u4e8e\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u89c6\u89d2\u200b\u3002</p> <p>\u200b\u5982\u679c\u8bf4\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8861\u91cf\u200b\u7684\u200b\u662f\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u6709\u200b\u591a\u200b\u9519\u8bef\u200b\uff0c\u200b\u6211\u200b\u66f4\u200b\u613f\u610f\u200b\u5c06\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u89c6\u4e3a\u200b\u8861\u91cf\u200b\u6a21\u578b\u200b\u6709\u200b\u591a\u200b\u6b63\u786e\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba4\u4e3a\u200b\u8fd9\u200b\u4e24\u8005\u200b\u505a\u200b\u7684\u200b\u662f\u200b\u540c\u200b\u4e00\u4ef6\u200b\u4e8b\u200b\uff0c\u200b\u4f46\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u89c6\u89d2\u200b\u3002</p> <p>\u200b\u6bd5\u7adf\u200b\uff0c\u200b\u5728\u200b\u8bc4\u4f30\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u4ece\u200b\u591a\u4e2a\u200b\u89d2\u5ea6\u200b\u6765\u200b\u770b\u5f85\u200b\u4e8b\u7269\u200b\u662f\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u6709\u200b\u51e0\u79cd\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f46\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u51c6\u786e\u7387\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u51c6\u786e\u7387\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u7684\u200b\u603b\u6570\u200b\u9664\u4ee5\u200b\u9884\u6d4b\u200b\u7684\u200b\u603b\u6570\u200b\u6765\u200b\u8861\u91cf\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5728\u200b100\u200b\u6b21\u200b\u9884\u6d4b\u200b\u4e2d\u200b\u505a\u51fa\u200b99\u200b\u6b21\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5176\u200b\u51c6\u786e\u7387\u200b\u5c31\u662f\u200b99%\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5199\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> In\u00a0[15]: Copied! <pre># Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal\n    acc = (correct / len(y_pred)) * 100 \n    return acc\n</pre> # Calculate accuracy (a classification metric) def accuracy_fn(y_true, y_pred):     correct = torch.eq(y_true, y_pred).sum().item() # torch.eq() calculates where two tensors are equal     acc = (correct / len(y_pred)) * 100      return acc <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u65f6\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u8861\u91cf\u200b\u5176\u200b\u6027\u80fd\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u89c2\u5bdf\u200b\u635f\u5931\u200b\u503c\u200b\u3002</p> In\u00a0[16]: Copied! <pre># View the frist 5 outputs of the forward pass on the test data\ny_logits = model_0(X_test.to(device))[:5]\ny_logits\n</pre> # View the frist 5 outputs of the forward pass on the test data y_logits = model_0(X_test.to(device))[:5] y_logits Out[16]: <pre>tensor([[-0.4279],\n        [-0.3417],\n        [-0.5975],\n        [-0.3801],\n        [-0.5078]], device='cuda:0', grad_fn=&lt;SliceBackward0&gt;)</pre> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5c1a\u672a\u200b\u7ecf\u8fc7\u8bad\u7ec3\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u8f93\u51fa\u200b\u57fa\u672c\u4e0a\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5b83\u4eec\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u5b83\u4eec\u200b\u662f\u200b\u6211\u4eec\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4e24\u5c42\u200b <code>nn.Linear()</code>\uff0c\u200b\u5185\u90e8\u200b\u8c03\u7528\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u65b9\u7a0b\u200b\uff1a</p> <p>$$ \\mathbf{y} = x \\cdot \\mathbf{Weights}^T  + \\mathbf{bias} $$</p> <p>\u200b\u8fd9\u4e2a\u200b\u65b9\u7a0b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\uff08\u200b\u672a\u7ecf\u200b\u4fee\u6539\u200b\uff09($\\mathbf{y}$)\uff0c\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\uff0c\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200blogits\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u63a5\u6536\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\uff08\u200b\u65b9\u7a0b\u200b\u4e2d\u200b\u7684\u200b $x$ \u200b\u6216\u200b\u4ee3\u7801\u200b\u4e2d\u200b\u7684\u200b <code>X_test</code>\uff09\u200b\u65f6\u200b\u8f93\u51fa\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5373\u200b logits\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6570\u5b57\u200b\u5f88\u96be\u200b\u89e3\u91ca\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5f97\u5230\u200b\u4e00\u4e9b\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u53ef\u200b\u6bd4\u8f83\u200b\u7684\u200b\u6570\u5b57\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\uff08logits\uff09\u200b\u8f6c\u6362\u6210\u200b\u8fd9\u79cd\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bsigmoid \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u770b\u200b\u3002</p> In\u00a0[17]: Copied! <pre># Use sigmoid on model logits\ny_pred_probs = torch.sigmoid(y_logits)\ny_pred_probs\n</pre> # Use sigmoid on model logits y_pred_probs = torch.sigmoid(y_logits) y_pred_probs Out[17]: <pre>tensor([[0.3946],\n        [0.4154],\n        [0.3549],\n        [0.4061],\n        [0.3757]], device='cuda:0', grad_fn=&lt;SigmoidBackward0&gt;)</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u73b0\u5728\u200b\u770b\u6765\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u6709\u200b\u4e86\u200b\u4e00\u5b9a\u200b\u7684\u200b\u89c4\u5f8b\u6027\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u5b83\u4eec\u200b\u4ecd\u7136\u200b\u662f\u200b\u968f\u673a\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u73b0\u5728\u200b\u4ee5\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5448\u73b0\u200b\uff08\u200b\u6211\u200b\u901a\u5e38\u200b\u79f0\u4e4b\u4e3a\u200b <code>y_pred_probs</code>\uff09\uff0c\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6570\u503c\u200b\u8868\u793a\u200b\u6a21\u578b\u200b\u8ba4\u4e3a\u200b\u6570\u636e\u200b\u70b9\u200b\u5c5e\u4e8e\u200b\u67d0\u200b\u4e00\u7c7b\u200b\u522b\u7684\u200b\u7a0b\u5ea6\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u7531\u4e8e\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u7406\u60f3\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u662f\u200b 0 \u200b\u6216\u200b 1\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6570\u503c\u200b\u53ef\u4ee5\u200b\u770b\u4f5c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u51b3\u7b56\u200b\u8fb9\u754c\u200b\u3002</p> <p>\u200b\u8d8a\u200b\u63a5\u8fd1\u200b 0\uff0c\u200b\u6a21\u578b\u200b\u8d8a\u200b\u8ba4\u4e3a\u200b\u6837\u672c\u200b\u5c5e\u4e8e\u200b\u7c7b\u522b\u200b 0\uff1b\u200b\u8d8a\u200b\u63a5\u8fd1\u200b 1\uff0c\u200b\u6a21\u578b\u200b\u8d8a\u200b\u8ba4\u4e3a\u200b\u6837\u672c\u200b\u5c5e\u4e8e\u200b\u7c7b\u522b\u200b 1\u3002</p> <p>\u200b\u66f4\u200b\u5177\u4f53\u5730\u8bf4\u200b\uff1a</p> <ul> <li>\u200b\u5982\u679c\u200b <code>y_pred_probs</code> &gt;= 0.5\uff0c\u200b\u5219\u200b <code>y=1</code>\uff08\u200b\u7c7b\u522b\u200b 1\uff09</li> <li>\u200b\u5982\u679c\u200b <code>y_pred_probs</code> &lt; 0.5\uff0c\u200b\u5219\u200b <code>y=0</code>\uff08\u200b\u7c7b\u522b\u200b 0\uff09</li> </ul> <p>\u200b\u4e3a\u4e86\u200b\u5c06\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b sigmoid \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u8fdb\u884c\u200b\u56db\u820d\u4e94\u5165\u200b\u3002</p> In\u00a0[18]: Copied! <pre># Find the predicted labels (round the prediction probabilities)\ny_preds = torch.round(y_pred_probs)\n\n# In full\ny_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n\n# Check for equality\nprint(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))\n\n# Get rid of extra dimension\ny_preds.squeeze()\n</pre> # Find the predicted labels (round the prediction probabilities) y_preds = torch.round(y_pred_probs)  # In full y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))  # Check for equality print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))  # Get rid of extra dimension y_preds.squeeze() <pre>tensor([True, True, True, True, True], device='cuda:0')\n</pre> Out[18]: <pre>tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=&lt;SqueezeBackward0&gt;)</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u73b0\u5728\u200b\u770b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\uff08<code>y_test</code>\uff09\u200b\u7684\u200b\u5f62\u5f0f\u200b\u4e00\u81f4\u200b\u4e86\u200b\u3002</p> In\u00a0[19]: Copied! <pre>y_test[:5]\n</pre> y_test[:5] Out[19]: <pre>tensor([1., 0., 1., 0., 1.])</pre> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u5c06\u200b\u80fd\u591f\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u6807\u7b7e\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u5176\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u3002</p> <p>\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b sigmoid \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\uff08logits\uff09\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u56db\u820d\u4e94\u5165\u200b\u5c06\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a sigmoid \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u901a\u5e38\u200b\u4ec5\u200b\u7528\u4e8e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b logits\u3002\u200b\u5bf9\u4e8e\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u4f1a\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b softmax \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u200b\u8fd9\u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u5185\u5bb9\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\uff09\u3002</p> <p>\u200b\u5e76\u4e14\u200b\uff0c\u200b\u5f53\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>nn.BCEWithLogitsLoss</code> \u200b\u65f6\u200b\uff0c\u200b\u4e0d\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b sigmoid \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u201clogits\u201d\u200b\u635f\u5931\u200b\u4e2d\u200b\u7684\u200b\u201clogits\u201d\u200b\u662f\u56e0\u4e3a\u200b\u5b83\u200b\u4f5c\u7528\u200b\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b logits \u200b\u8f93\u51fa\u200b\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u8be5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5185\u7f6e\u200b\u4e86\u200b sigmoid \u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 100\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\n# Build training and evaluation loop\nfor epoch in range(epochs):\n    ### Training\n    model_0.train()\n\n    # 1. Forward pass (model outputs raw logits)\n    y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device \n    y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls\n  \n    # 2. Calculate loss/accuracy\n    # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()\n    #                y_train) \n    loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits\n                   y_train) \n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred) \n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_0.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_0(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  # Set the number of epochs epochs = 100  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  # Build training and evaluation loop for epoch in range(epochs):     ### Training     model_0.train()      # 1. Forward pass (model outputs raw logits)     y_logits = model_0(X_train).squeeze() # squeeze to remove extra `1` dimensions, this won't work unless model and data are on same device      y_pred = torch.round(torch.sigmoid(y_logits)) # turn logits -&gt; pred probs -&gt; pred labls        # 2. Calculate loss/accuracy     # loss = loss_fn(torch.sigmoid(y_logits), # Using nn.BCELoss you need torch.sigmoid()     #                y_train)      loss = loss_fn(y_logits, # Using nn.BCEWithLogitsLoss works with raw logits                    y_train)      acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)       # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_0.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_0(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.72090, Accuracy: 50.00% | Test loss: 0.72196, Test acc: 50.00%\nEpoch: 10 | Loss: 0.70291, Accuracy: 50.00% | Test loss: 0.70542, Test acc: 50.00%\nEpoch: 20 | Loss: 0.69659, Accuracy: 50.00% | Test loss: 0.69942, Test acc: 50.00%\nEpoch: 30 | Loss: 0.69432, Accuracy: 43.25% | Test loss: 0.69714, Test acc: 41.00%\nEpoch: 40 | Loss: 0.69349, Accuracy: 47.00% | Test loss: 0.69623, Test acc: 46.50%\nEpoch: 50 | Loss: 0.69319, Accuracy: 49.00% | Test loss: 0.69583, Test acc: 46.00%\nEpoch: 60 | Loss: 0.69308, Accuracy: 50.12% | Test loss: 0.69563, Test acc: 46.50%\nEpoch: 70 | Loss: 0.69303, Accuracy: 50.38% | Test loss: 0.69551, Test acc: 46.00%\nEpoch: 80 | Loss: 0.69302, Accuracy: 51.00% | Test loss: 0.69543, Test acc: 46.00%\nEpoch: 90 | Loss: 0.69301, Accuracy: 51.00% | Test loss: 0.69537, Test acc: 46.00%\n</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u5173\u4e8e\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u6027\u80fd\u200b\uff0c\u200b\u4f60\u200b\u6709\u200b\u4ec0\u4e48\u200b\u53d1\u73b0\u200b\u5417\u200b\uff1f</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6a21\u578b\u200b\u987a\u5229\u5b8c\u6210\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6b65\u9aa4\u200b\uff0c\u200b\u4f46\u200b\u7ed3\u679c\u200b\u4f3c\u4e4e\u200b\u6ca1\u6709\u200b\u592a\u5927\u200b\u7684\u200b\u53d8\u5316\u200b\u3002</p> <p>\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u5206\u5272\u200b\u4e0a\u200b\uff0c\u200b\u51c6\u786e\u7387\u200b\u52c9\u5f3a\u200b\u8d85\u8fc7\u200b50%\u3002</p> <p>\u200b\u800c\u4e14\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5e73\u8861\u200b\u7684\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u548c\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u5dee\u4e0d\u591a\u200b\uff08\u200b\u5728\u200b500\u200b\u4e2a\u200b\u7c7b\u522b\u200b0\u200b\u548c\u200b\u7c7b\u522b\u200b1\u200b\u7684\u200b\u6837\u672c\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6bcf\u6b21\u200b\u90fd\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b1\uff0c\u200b\u51c6\u786e\u7387\u200b\u4e5f\u200b\u53ea\u80fd\u200b\u8fbe\u5230\u200b50%\uff09\u3002</p> In\u00a0[21]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n\nfrom helper_functions import plot_predictions, plot_decision_boundary\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content)  from helper_functions import plot_predictions, plot_decision_boundary <pre>helper_functions.py already exists, skipping download\n</pre> <pre>/home/daniel/.local/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/daniel/.local/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN3c106detail19maybe_wrap_dim_slowIlEET_S2_S2_b\n  warn(f\"Failed to load image Python extension: {e}\")\n</pre> In\u00a0[22]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_0, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_0, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_0, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_0, X_test, y_test) <p>\u200b\u54e6\u200b\uff0c\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u627e\u5230\u200b\u4e86\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\u95ee\u9898\u200b\u7684\u200b\u6839\u6e90\u200b\u3002</p> <p>\u200b\u5b83\u200b\u76ee\u524d\u200b\u6b63\u8bd5\u56fe\u200b\u7528\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u6765\u200b\u533a\u5206\u200b\u7ea2\u8272\u200b\u548c\u200b\u84dd\u8272\u200b\u7684\u200b\u70b9\u200b...</p> <p>\u200b\u8fd9\u200b\u5c31\u200b\u89e3\u91ca\u200b\u4e86\u200b\u4e3a\u4ec0\u4e48\u200b\u51c6\u786e\u7387\u200b\u53ea\u6709\u200b50%\u3002\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u5706\u5f62\u200b\u7684\u200b\uff0c\u200b\u753b\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u6700\u200b\u591a\u200b\u53ea\u80fd\u200b\u5c06\u200b\u5176\u200b\u4e00\u5206\u4e3a\u4e8c\u200b\u3002</p> <p>\u200b\u7528\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u672f\u8bed\u200b\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6b20\u200b\u62df\u5408\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u6ca1\u6709\u200b\u4ece\u200b\u6570\u636e\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u5230\u200b\u9884\u6d4b\u6027\u200b\u7684\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u6539\u8fdb\u200b\u5462\u200b\uff1f</p> In\u00a0[23]: Copied! <pre>class CircleModelV1(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        \n    def forward(self, x): # note: always make sure forward is spelt correctly!\n        # Creating a model like this is the same as below, though below\n        # generally benefits from speedups where possible.\n        # z = self.layer_1(x)\n        # z = self.layer_2(z)\n        # z = self.layer_3(z)\n        # return z\n        return self.layer_3(self.layer_2(self.layer_1(x)))\n\nmodel_1 = CircleModelV1().to(device)\nmodel_1\n</pre> class CircleModelV1(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10) # extra layer         self.layer_3 = nn.Linear(in_features=10, out_features=1)              def forward(self, x): # note: always make sure forward is spelt correctly!         # Creating a model like this is the same as below, though below         # generally benefits from speedups where possible.         # z = self.layer_1(x)         # z = self.layer_2(z)         # z = self.layer_3(z)         # return z         return self.layer_3(self.layer_2(self.layer_1(x)))  model_1 = CircleModelV1().to(device) model_1 Out[23]: <pre>CircleModelV1(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u76f8\u540c\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u3002</p> In\u00a0[24]: Copied! <pre># loss_fn = nn.BCELoss() # Requires sigmoid on input\nloss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input\noptimizer = torch.optim.SGD(model_1.parameters(), lr=0.1)\n</pre> # loss_fn = nn.BCELoss() # Requires sigmoid on input loss_fn = nn.BCEWithLogitsLoss() # Does not require sigmoid on input optimizer = torch.optim.SGD(model_1.parameters(), lr=0.1) <p>\u200b\u7f8e\u89c2\u200b\u7684\u200b\u6a21\u578b\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u90fd\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e00\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff08<code>epochs=1000</code> \u200b\u5bf9\u6bd4\u200b <code>epochs=100</code>\uff09\uff0c\u200b\u770b\u770b\u200b\u662f\u5426\u200b\u80fd\u200b\u63d0\u5347\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\u3002</p> In\u00a0[25]: Copied! <pre>torch.manual_seed(42)\n\nepochs = 1000 # Train for longer\n\n# Put data to target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    # 1. Forward pass\n    y_logits = model_1(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels\n\n    # 2. Calculate loss/accuracy\n    loss = loss_fn(y_logits, y_train)\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_1.eval()\n    with torch.inference_mode():\n        # 1. Forward pass\n        test_logits = model_1(X_test).squeeze() \n        test_pred = torch.round(torch.sigmoid(test_logits))\n        # 2. Caculate loss/accuracy\n        test_loss = loss_fn(test_logits,\n                            y_test)\n        test_acc = accuracy_fn(y_true=y_test,\n                               y_pred=test_pred)\n\n    # Print out what's happening every 10 epochs\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")\n</pre> torch.manual_seed(42)  epochs = 1000 # Train for longer  # Put data to target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     ### Training     # 1. Forward pass     y_logits = model_1(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; predicition probabilities -&gt; prediction labels      # 2. Calculate loss/accuracy     loss = loss_fn(y_logits, y_train)     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_1.eval()     with torch.inference_mode():         # 1. Forward pass         test_logits = model_1(X_test).squeeze()          test_pred = torch.round(torch.sigmoid(test_logits))         # 2. Caculate loss/accuracy         test_loss = loss_fn(test_logits,                             y_test)         test_acc = accuracy_fn(y_true=y_test,                                y_pred=test_pred)      # Print out what's happening every 10 epochs     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 0.69396, Accuracy: 50.88% | Test loss: 0.69261, Test acc: 51.00%\nEpoch: 100 | Loss: 0.69305, Accuracy: 50.38% | Test loss: 0.69379, Test acc: 48.00%\nEpoch: 200 | Loss: 0.69299, Accuracy: 51.12% | Test loss: 0.69437, Test acc: 46.00%\nEpoch: 300 | Loss: 0.69298, Accuracy: 51.62% | Test loss: 0.69458, Test acc: 45.00%\nEpoch: 400 | Loss: 0.69298, Accuracy: 51.12% | Test loss: 0.69465, Test acc: 46.00%\nEpoch: 500 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69467, Test acc: 46.00%\nEpoch: 600 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 700 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 800 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\nEpoch: 900 | Loss: 0.69298, Accuracy: 51.00% | Test loss: 0.69468, Test acc: 46.00%\n</pre> <p>\u200b\u4ec0\u4e48\u200b\uff1f\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u66f4\u957f\u200b\uff0c\u200b\u8fd8\u200b\u989d\u5916\u200b\u52a0\u200b\u4e86\u200b\u4e00\u5c42\u200b\uff0c\u200b\u4f46\u200b\u770b\u8d77\u6765\u200b\u5b83\u200b\u5e76\u200b\u6ca1\u6709\u200b\u6bd4\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u66f4\u597d\u200b\u5730\u200b\u5b66\u4e60\u200b\u5230\u200b\u4efb\u4f55\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[26]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_1, X_test, y_test)\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_1, X_test, y_test) <p>\u200b\u55ef\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4ecd\u7136\u200b\u5728\u200b\u7ea2\u70b9\u200b\u548c\u200b\u84dd\u70b9\u200b\u4e4b\u95f4\u200b\u753b\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u753b\u200b\u7684\u200b\u662f\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u5b83\u200b\u80fd\u200b\u6a21\u62df\u200b\u7ebf\u6027\u200b\u6570\u636e\u200b\u5417\u200b\uff1f\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b01\u200b\u4e2d\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff1f</p> In\u00a0[27]: Copied! <pre># Create some data (same as notebook 01)\nweight = 0.7\nbias = 0.3\nstart = 0\nend = 1\nstep = 0.01\n\n# Create data\nX_regression = torch.arange(start, end, step).unsqueeze(dim=1)\ny_regression = weight * X_regression + bias # linear regression formula\n\n# Check the data\nprint(len(X_regression))\nX_regression[:5], y_regression[:5]\n</pre> # Create some data (same as notebook 01) weight = 0.7 bias = 0.3 start = 0 end = 1 step = 0.01  # Create data X_regression = torch.arange(start, end, step).unsqueeze(dim=1) y_regression = weight * X_regression + bias # linear regression formula  # Check the data print(len(X_regression)) X_regression[:5], y_regression[:5] <pre>100\n</pre> Out[27]: <pre>(tensor([[0.0000],\n         [0.0100],\n         [0.0200],\n         [0.0300],\n         [0.0400]]),\n tensor([[0.3000],\n         [0.3070],\n         [0.3140],\n         [0.3210],\n         [0.3280]]))</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6570\u636e\u200b\u5206\u6210\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p> In\u00a0[28]: Copied! <pre># Create train and test splits\ntrain_split = int(0.8 * len(X_regression)) # 80% of data used for training set\nX_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split]\nX_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]\n\n# Check the lengths of each split\nprint(len(X_train_regression), \n    len(y_train_regression), \n    len(X_test_regression), \n    len(y_test_regression))\n</pre> # Create train and test splits train_split = int(0.8 * len(X_regression)) # 80% of data used for training set X_train_regression, y_train_regression = X_regression[:train_split], y_regression[:train_split] X_test_regression, y_test_regression = X_regression[train_split:], y_regression[train_split:]  # Check the lengths of each split print(len(X_train_regression),      len(y_train_regression),      len(X_test_regression),      len(y_test_regression)) <pre>80 80 20 20\n</pre> <p>\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u6570\u636e\u200b\u7684\u200b\u6837\u5b50\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b01\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>plot_predictions()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5b83\u200b\u5305\u542b\u200b\u5728\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u4e0b\u8f7d\u200b\u7684\u200b Learn PyTorch for Deep Learning \u200b\u4ed3\u5e93\u200b\u4e2d\u200b\u7684\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b \u200b\u91cc\u200b\u3002</p> In\u00a0[29]: Copied! <pre>plot_predictions(train_data=X_train_regression,\n    train_labels=y_train_regression,\n    test_data=X_test_regression,\n    test_labels=y_test_regression\n);\n</pre> plot_predictions(train_data=X_train_regression,     train_labels=y_train_regression,     test_data=X_test_regression,     test_labels=y_test_regression ); In\u00a0[30]: Copied! <pre># Same architecture as model_1 (but using nn.Sequential)\nmodel_2 = nn.Sequential(\n    nn.Linear(in_features=1, out_features=10),\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=1)\n).to(device)\n\nmodel_2\n</pre> # Same architecture as model_1 (but using nn.Sequential) model_2 = nn.Sequential(     nn.Linear(in_features=1, out_features=10),     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=1) ).to(device)  model_2 Out[30]: <pre>Sequential(\n  (0): Linear(in_features=1, out_features=10, bias=True)\n  (1): Linear(in_features=10, out_features=10, bias=True)\n  (2): Linear(in_features=10, out_features=1, bias=True)\n)</pre> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e3a\u200b <code>nn.L1Loss()</code>\uff08\u200b\u4e0e\u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\u76f8\u540c\u200b\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u4f18\u5316\u200b\u5668\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>torch.optim.SGD()</code>\u3002</p> In\u00a0[31]: Copied! <pre># Loss and optimizer\nloss_fn = nn.L1Loss()\noptimizer = torch.optim.SGD(model_2.parameters(), lr=0.1)\n</pre> # Loss and optimizer loss_fn = nn.L1Loss() optimizer = torch.optim.SGD(model_2.parameters(), lr=0.1) <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u5e38\u89c4\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u6b65\u9aa4\u200b\u6765\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bbe\u7f6e\u200b <code>epochs=1000</code>\uff08\u200b\u5c31\u200b\u50cf\u200b <code>model_1</code> \u200b\u4e00\u6837\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u7f16\u5199\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u4ee3\u7801\u200b\u3002\u200b\u6211\u200b\u6545\u610f\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u662f\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u5927\u5bb6\u200b\u4e0d\u65ad\u200b\u7ec3\u4e60\u200b\u3002\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u4f60\u4eec\u200b\u6709\u6ca1\u6709\u200b\u60f3\u8fc7\u200b\u5982\u4f55\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u4ee3\u7801\u200b\u529f\u80fd\u5316\u200b\u5462\u200b\uff1f\u200b\u8fd9\u6837\u200b\u5c06\u6765\u200b\u53ef\u4ee5\u200b\u8282\u7701\u200b\u4e0d\u5c11\u200b\u7f16\u7801\u200b\u5de5\u4f5c\u200b\u3002\u200b\u53ef\u80fd\u200b\u53ef\u4ee5\u200b\u6709\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u7684\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[32]: Copied! <pre># Train the model\ntorch.manual_seed(42)\n\n# Set the number of epochs\nepochs = 1000\n\n# Put data to target device\nX_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device)\nX_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)\n\nfor epoch in range(epochs):\n    ### Training \n    # 1. Forward pass\n    y_pred = model_2(X_train_regression)\n    \n    # 2. Calculate loss (no accuracy since it's a regression problem, not classification)\n    loss = loss_fn(y_pred, y_train_regression)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_2.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_pred = model_2(X_test_regression)\n      # 2. Calculate the loss \n      test_loss = loss_fn(test_pred, y_test_regression)\n\n    # Print out what's happening\n    if epoch % 100 == 0: \n        print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\")\n</pre> # Train the model torch.manual_seed(42)  # Set the number of epochs epochs = 1000  # Put data to target device X_train_regression, y_train_regression = X_train_regression.to(device), y_train_regression.to(device) X_test_regression, y_test_regression = X_test_regression.to(device), y_test_regression.to(device)  for epoch in range(epochs):     ### Training      # 1. Forward pass     y_pred = model_2(X_train_regression)          # 2. Calculate loss (no accuracy since it's a regression problem, not classification)     loss = loss_fn(y_pred, y_train_regression)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_2.eval()     with torch.inference_mode():       # 1. Forward pass       test_pred = model_2(X_test_regression)       # 2. Calculate the loss        test_loss = loss_fn(test_pred, y_test_regression)      # Print out what's happening     if epoch % 100 == 0:          print(f\"Epoch: {epoch} | Train loss: {loss:.5f}, Test loss: {test_loss:.5f}\") <pre>Epoch: 0 | Train loss: 0.75986, Test loss: 0.54143\nEpoch: 100 | Train loss: 0.09309, Test loss: 0.02901\nEpoch: 200 | Train loss: 0.07376, Test loss: 0.02850\nEpoch: 300 | Train loss: 0.06745, Test loss: 0.00615\nEpoch: 400 | Train loss: 0.06107, Test loss: 0.02004\nEpoch: 500 | Train loss: 0.05698, Test loss: 0.01061\nEpoch: 600 | Train loss: 0.04857, Test loss: 0.01326\nEpoch: 700 | Train loss: 0.06109, Test loss: 0.02127\nEpoch: 800 | Train loss: 0.05599, Test loss: 0.01426\nEpoch: 900 | Train loss: 0.05571, Test loss: 0.00603\n</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u4e0e\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u4e0a\u200b\u7684\u200b<code>model_1</code>\u200b\u4e0d\u540c\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b<code>model_2</code>\u200b\u7684\u200b\u635f\u5931\u200b\u5b9e\u9645\u4e0a\u200b\u5728\u200b\u4e0b\u964d\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ed8\u5236\u200b\u5b83\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u770b\u770b\u200b\u662f\u5426\u200b\u786e\u5b9e\u200b\u5982\u6b64\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u76ee\u6807\u200b<code>device</code>\uff0c\u200b\u800c\u200b\u8fd9\u4e2a\u200b\u8bbe\u5907\u200b\u53ef\u80fd\u200b\u662f\u200bGPU\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u7ed8\u56fe\u200b\u51fd\u6570\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200bmatplotlib\uff0c\u200b\u800c\u200bmatplotlib\u200b\u65e0\u6cd5\u200b\u5904\u7406\u200b\u4f4d\u4e8e\u200bGPU\u200b\u4e0a\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5904\u7406\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u5c06\u200b\u6570\u636e\u200b\u4f20\u9012\u200b\u7ed9\u200b<code>plot_predictions()</code>\u200b\u65f6\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>.cpu()</code>\u200b\u5c06\u200b\u6240\u6709\u200b\u6570\u636e\u200b\u53d1\u9001\u5230\u200bCPU\u3002</p> In\u00a0[33]: Copied! <pre># Turn on evaluation mode\nmodel_2.eval()\n\n# Make predictions (inference)\nwith torch.inference_mode():\n    y_preds = model_2(X_test_regression)\n\n# Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU)\n# (try removing .cpu() from one of the below and see what happens)\nplot_predictions(train_data=X_train_regression.cpu(),\n                 train_labels=y_train_regression.cpu(),\n                 test_data=X_test_regression.cpu(),\n                 test_labels=y_test_regression.cpu(),\n                 predictions=y_preds.cpu());\n</pre> # Turn on evaluation mode model_2.eval()  # Make predictions (inference) with torch.inference_mode():     y_preds = model_2(X_test_regression)  # Plot data and predictions with data on the CPU (matplotlib can't handle data on the GPU) # (try removing .cpu() from one of the below and see what happens) plot_predictions(train_data=X_train_regression.cpu(),                  train_labels=y_train_regression.cpu(),                  test_data=X_test_regression.cpu(),                  test_labels=y_test_regression.cpu(),                  predictions=y_preds.cpu()); <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u76f4\u7ebf\u200b\u95ee\u9898\u200b\u4e0a\u200b\u80fd\u591f\u200b\u8fdc\u8d85\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u7684\u200b\u8868\u73b0\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u200b\u4e2a\u200b\u597d\u200b\u73b0\u8c61\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u81f3\u5c11\u200b\u5177\u5907\u200b\u4e00\u5b9a\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b\u6784\u5efa\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6709\u7528\u200b\u7684\u200b\u6545\u969c\u200b\u6392\u9664\u200b\u6b65\u9aa4\u200b\u662f\u4ece\u200b\u5c3d\u53ef\u80fd\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4ee5\u200b\u786e\u8ba4\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u6709\u6548\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u610f\u5473\u7740\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08\u200b\u5c42\u6570\u200b\u4e0d\u200b\u591a\u200b\uff0c\u200b\u9690\u85cf\u200b\u795e\u7ecf\u5143\u200b\u4e0d\u200b\u591a\u200b\uff09\u200b\u548c\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u8fd9\u4e2a\u200b\uff09\u200b\u5f00\u59cb\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u589e\u52a0\u200b\u6570\u636e\u91cf\u200b\u6216\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b/\u200b\u8bbe\u8ba1\u200b\u4e4b\u524d\u200b\uff0c\u200b\u5148\u8fc7\u200b\u62df\u5408\u200b\uff08\u200b\u4f7f\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u8fc7\u4e8e\u200b\u4f18\u79c0\u200b\uff09\u200b\u8fd9\u4e2a\u200b\u5c0f\u200b\u793a\u4f8b\u200b\uff0c\u200b\u4ee5\u200b\u51cf\u5c11\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u95ee\u9898\u200b\u53ef\u80fd\u200b\u51fa\u200b\u5728\u200b\u54ea\u91cc\u200b\u5462\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u63a2\u200b\u7a76\u7adf\u200b\u3002</p> In\u00a0[34]: Copied! <pre># Make and plot data\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_circles\n\nn_samples = 1000\n\nX, y = make_circles(n_samples=1000,\n    noise=0.03,\n    random_state=42,\n)\n\nplt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu);\n</pre> # Make and plot data import matplotlib.pyplot as plt from sklearn.datasets import make_circles  n_samples = 1000  X, y = make_circles(n_samples=1000,     noise=0.03,     random_state=42, )  plt.scatter(X[:, 0], X[:, 1], c=y, cmap=plt.cm.RdBu); <p>\u200b\u597d\u200b\u7684\u200b\uff01\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6570\u636e\u200b\u96c6\u200b\u6309\u7167\u200b80%\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b20%\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\u8fdb\u884c\u200b\u5206\u5272\u200b\u3002</p> <pre>from sklearn.model_selection import train_test_split\n\n# Assuming X contains the features and y contains the labels\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n</pre> In\u00a0[35]: Copied! <pre># Convert to tensors and split into train and test sets\nimport torch\nfrom sklearn.model_selection import train_test_split\n\n# Turn data into tensors\nX = torch.from_numpy(X).type(torch.float)\ny = torch.from_numpy(y).type(torch.float)\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, \n                                                    y, \n                                                    test_size=0.2,\n                                                    random_state=42\n)\n\nX_train[:5], y_train[:5]\n</pre> # Convert to tensors and split into train and test sets import torch from sklearn.model_selection import train_test_split  # Turn data into tensors X = torch.from_numpy(X).type(torch.float) y = torch.from_numpy(y).type(torch.float)  # Split into train and test sets X_train, X_test, y_train, y_test = train_test_split(X,                                                      y,                                                      test_size=0.2,                                                     random_state=42 )  X_train[:5], y_train[:5] Out[35]: <pre>(tensor([[ 0.6579, -0.4651],\n         [ 0.6319, -0.7347],\n         [-1.0086, -0.1240],\n         [-0.9666, -0.2256],\n         [-0.1666,  0.7994]]),\n tensor([1., 0., 0., 0., 1.]))</pre> In\u00a0[36]: Copied! <pre># Build model with non-linear activation function\nfrom torch import nn\nclass CircleModelV2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer_1 = nn.Linear(in_features=2, out_features=10)\n        self.layer_2 = nn.Linear(in_features=10, out_features=10)\n        self.layer_3 = nn.Linear(in_features=10, out_features=1)\n        self.relu = nn.ReLU() # &lt;- add in ReLU activation function\n        # Can also put sigmoid in the model \n        # This would mean you don't need to use it on the predictions\n        # self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n      # Intersperse the ReLU activation function between layers\n       return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))\n\nmodel_3 = CircleModelV2().to(device)\nprint(model_3)\n</pre> # Build model with non-linear activation function from torch import nn class CircleModelV2(nn.Module):     def __init__(self):         super().__init__()         self.layer_1 = nn.Linear(in_features=2, out_features=10)         self.layer_2 = nn.Linear(in_features=10, out_features=10)         self.layer_3 = nn.Linear(in_features=10, out_features=1)         self.relu = nn.ReLU() # &lt;- add in ReLU activation function         # Can also put sigmoid in the model          # This would mean you don't need to use it on the predictions         # self.sigmoid = nn.Sigmoid()      def forward(self, x):       # Intersperse the ReLU activation function between layers        return self.layer_3(self.relu(self.layer_2(self.relu(self.layer_1(x)))))  model_3 = CircleModelV2().to(device) print(model_3) <pre>CircleModelV2(\n  (layer_1): Linear(in_features=2, out_features=10, bias=True)\n  (layer_2): Linear(in_features=10, out_features=10, bias=True)\n  (layer_3): Linear(in_features=10, out_features=1, bias=True)\n  (relu): ReLU()\n)\n</pre> <p> \u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u6784\u5efa\u200b\u7684\u200b\u5206\u7c7b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08\u200b\u4f7f\u7528\u200bReLU\u200b\u6fc0\u6d3b\u200b\uff09\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u793a\u4f8b\u200b\u3002\u200b\u5c1d\u8bd5\u200b\u5728\u200bTensorFlow Playground\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u5728\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65f6\u200b\uff0c\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5e94\u8be5\u200b\u653e\u5728\u200b\u54ea\u91cc\u200b\uff1f</p> <p>\u200b\u4e00\u4e2a\u200b\u7ecf\u9a8c\u200b\u6cd5\u5219\u200b\u662f\u200b\u5c06\u200b\u5b83\u4eec\u200b\u653e\u5728\u200b\u9690\u85cf\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u4ee5\u53ca\u200b\u8f93\u51fa\u200b\u5c42\u200b\u4e4b\u540e\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u5e76\u200b\u6ca1\u6709\u200b\u4e00\u6210\u4e0d\u53d8\u200b\u7684\u200b\u89c4\u5b9a\u200b\u3002\u200b\u968f\u7740\u200b\u4f60\u200b\u5bf9\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e86\u89e3\u200b\u52a0\u6df1\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u6709\u200b\u5f88\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7ec4\u5408\u200b\u65b9\u5f0f\u200b\u3002\u200b\u5728\u6b64\u671f\u95f4\u200b\uff0c\u200b\u6700\u597d\u200b\u7684\u200b\u529e\u6cd5\u200b\u5c31\u662f\u200b\u4e0d\u65ad\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u518d\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u51c6\u5907\u5c31\u7eea\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4ee5\u53ca\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> In\u00a0[37]: Copied! <pre># Setup loss and optimizer \nloss_fn = nn.BCEWithLogitsLoss()\noptimizer = torch.optim.SGD(model_3.parameters(), lr=0.1)\n</pre> # Setup loss and optimizer  loss_fn = nn.BCEWithLogitsLoss() optimizer = torch.optim.SGD(model_3.parameters(), lr=0.1) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> In\u00a0[38]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\nepochs = 1000\n\n# Put all data on target device\nX_train, y_train = X_train.to(device), y_train.to(device)\nX_test, y_test = X_test.to(device), y_test.to(device)\n\nfor epoch in range(epochs):\n    # 1. Forward pass\n    y_logits = model_3(X_train).squeeze()\n    y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n    \n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits\n    acc = accuracy_fn(y_true=y_train, \n                      y_pred=y_pred)\n    \n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_3.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_3(X_test).squeeze()\n      test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels\n      # 2. Calcuate loss and accuracy\n      test_loss = loss_fn(test_logits, y_test)\n      test_acc = accuracy_fn(y_true=y_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42) epochs = 1000  # Put all data on target device X_train, y_train = X_train.to(device), y_train.to(device) X_test, y_test = X_test.to(device), y_test.to(device)  for epoch in range(epochs):     # 1. Forward pass     y_logits = model_3(X_train).squeeze()     y_pred = torch.round(torch.sigmoid(y_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels          # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_train) # BCEWithLogitsLoss calculates loss using logits     acc = accuracy_fn(y_true=y_train,                        y_pred=y_pred)          # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_3.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_3(X_test).squeeze()       test_pred = torch.round(torch.sigmoid(test_logits)) # logits -&gt; prediction probabilities -&gt; prediction labels       # 2. Calcuate loss and accuracy       test_loss = loss_fn(test_logits, y_test)       test_acc = accuracy_fn(y_true=y_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Accuracy: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Accuracy: {test_acc:.2f}%\") <pre>Epoch: 0 | Loss: 0.69295, Accuracy: 50.00% | Test Loss: 0.69319, Test Accuracy: 50.00%\nEpoch: 100 | Loss: 0.69115, Accuracy: 52.88% | Test Loss: 0.69102, Test Accuracy: 52.50%\nEpoch: 200 | Loss: 0.68977, Accuracy: 53.37% | Test Loss: 0.68940, Test Accuracy: 55.00%\nEpoch: 300 | Loss: 0.68795, Accuracy: 53.00% | Test Loss: 0.68723, Test Accuracy: 56.00%\nEpoch: 400 | Loss: 0.68517, Accuracy: 52.75% | Test Loss: 0.68411, Test Accuracy: 56.50%\nEpoch: 500 | Loss: 0.68102, Accuracy: 52.75% | Test Loss: 0.67941, Test Accuracy: 56.50%\nEpoch: 600 | Loss: 0.67515, Accuracy: 54.50% | Test Loss: 0.67285, Test Accuracy: 56.00%\nEpoch: 700 | Loss: 0.66659, Accuracy: 58.38% | Test Loss: 0.66322, Test Accuracy: 59.00%\nEpoch: 800 | Loss: 0.65160, Accuracy: 64.00% | Test Loss: 0.64757, Test Accuracy: 67.50%\nEpoch: 900 | Loss: 0.62362, Accuracy: 74.00% | Test Loss: 0.62145, Test Accuracy: 79.00%\n</pre> <p>\u200b\u5475\u5475\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u597d\u591a\u200b\u4e86\u200b\uff01</p> In\u00a0[39]: Copied! <pre># Make predictions\nmodel_3.eval()\nwith torch.inference_mode():\n    y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze()\ny_preds[:10], y[:10] # want preds in same format as truth labels\n</pre> # Make predictions model_3.eval() with torch.inference_mode():     y_preds = torch.round(torch.sigmoid(model_3(X_test))).squeeze() y_preds[:10], y[:10] # want preds in same format as truth labels Out[39]: <pre>(tensor([1., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0'),\n tensor([1., 1., 1., 1., 0., 1., 1., 1., 1., 0.]))</pre> In\u00a0[40]: Copied! <pre># Plot decision boundaries for training and test sets\nplt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity\n</pre> # Plot decision boundaries for training and test sets plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_1, X_train, y_train) # model_1 = no non-linearity plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_3, X_test, y_test) # model_3 = has non-linearity <p>\u200b\u5f88\u200b\u597d\u200b\uff01\u200b\u867d\u7136\u200b\u8fd8\u200b\u4e0d\u662f\u200b\u5b8c\u7f8e\u200b\uff0c\u200b\u4f46\u200b\u6bd4\u200b\u4e4b\u524d\u200b\u597d\u591a\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4f60\u200b\u6216\u8bb8\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u4e00\u4e9b\u200b\u6280\u5de7\u200b\u6765\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\uff1f\uff08\u200b\u63d0\u793a\u200b\uff1a\u200b\u56de\u5230\u200b\u7b2c\u200b5\u200b\u8282\u200b\u5bfb\u627e\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\u7684\u200b\u5efa\u8bae\u200b\uff09</p> In\u00a0[41]: Copied! <pre># Create a toy tensor (similar to the data going into our model(s))\nA = torch.arange(-10, 10, 1, dtype=torch.float32)\nA\n</pre> # Create a toy tensor (similar to the data going into our model(s)) A = torch.arange(-10, 10, 1, dtype=torch.float32) A Out[41]: <pre>tensor([-10.,  -9.,  -8.,  -7.,  -6.,  -5.,  -4.,  -3.,  -2.,  -1.,   0.,   1.,\n          2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.])</pre> <p>\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6765\u200b\u7ed8\u5236\u200b\u5b83\u200b\u3002</p> In\u00a0[42]: Copied! <pre># Visualize the toy tensor\nplt.plot(A);\n</pre> # Visualize the toy tensor plt.plot(A); <p>\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b ReLU \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u662f\u200b\u5982\u4f55\u200b\u5f71\u54cd\u200b\u5b83\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u81ea\u5df1\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b ReLU \u200b\u51fd\u6570\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4f7f\u7528\u200b PyTorch \u200b\u7684\u200b ReLU\uff08<code>torch.nn.ReLU</code>\uff09\u3002</p> <p>ReLU \u200b\u51fd\u6570\u200b\u5c06\u200b\u6240\u6709\u200b\u8d1f\u503c\u200b\u53d8\u4e3a\u200b 0\uff0c\u200b\u5e76\u200b\u4fdd\u6301\u200b\u6b63\u503c\u200b\u4e0d\u53d8\u200b\u3002</p> In\u00a0[43]: Copied! <pre># Create ReLU function by hand \ndef relu(x):\n  return torch.maximum(torch.tensor(0), x) # inputs must be tensors\n\n# Pass toy tensor through ReLU function\nrelu(A)\n</pre> # Create ReLU function by hand  def relu(x):   return torch.maximum(torch.tensor(0), x) # inputs must be tensors  # Pass toy tensor through ReLU function relu(A) Out[43]: <pre>tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 2., 3., 4., 5., 6., 7.,\n        8., 9.])</pre> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200bReLU\u200b\u51fd\u6570\u200b\u8d77\u200b\u4f5c\u7528\u200b\u4e86\u200b\uff0c\u200b\u6240\u6709\u200b\u8d1f\u503c\u200b\u90fd\u200b\u53d8\u6210\u200b\u4e86\u200b\u96f6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u7ed8\u5236\u200b\u4e00\u4e0b\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u3002</p> In\u00a0[44]: Copied! <pre># Plot ReLU activated toy tensor\nplt.plot(relu(A));\n</pre> # Plot ReLU activated toy tensor plt.plot(relu(A)); <p>\u200b\u4e0d\u9519\u200b\uff01\u200b\u8fd9\u200b\u770b\u8d77\u6765\u200b\u548c\u200b\u7ef4\u57fa\u767e\u79d1\u200b\u4e0a\u200b\u5173\u4e8e\u200bReLU)\u200b\u7684\u200bReLU\u200b\u51fd\u6570\u200b\u5f62\u72b6\u200b\u5b8c\u5168\u200b\u4e00\u6837\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u200b\u4e00\u76f4\u200b\u5728\u200b\u7528\u200b\u7684\u200bsigmoid\u200b\u51fd\u6570\u200b\u5982\u4f55\u200b\uff1f</p> <p>Sigmoid\u200b\u51fd\u6570\u200b\u7684\u200b\u516c\u5f0f\u200b\u5982\u4e0b\u200b\uff1a</p> <p>$$ out_i = \\frac{1}{1+e^{-input_i}} $$</p> <p>\u200b\u6216\u8005\u200b\u7528\u200b$x$\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\uff1a</p> <p>$$ S(x) = \\frac{1}{1+e^{-x_i}} $$</p> <p>\u200b\u5176\u4e2d\u200b$S$\u200b\u4ee3\u8868\u200bsigmoid\uff0c$e$\u200b\u4ee3\u8868\u200b\u6307\u6570\u51fd\u6570\u200b\uff08<code>torch.exp()</code>\uff09\uff0c$i$\u200b\u4ee3\u8868\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\u5143\u7d20\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200bPyTorch\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u590d\u73b0\u200bsigmoid\u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[45]: Copied! <pre># Create a custom sigmoid function\ndef sigmoid(x):\n  return 1 / (1 + torch.exp(-x))\n\n# Test custom sigmoid on toy tensor\nsigmoid(A)\n</pre> # Create a custom sigmoid function def sigmoid(x):   return 1 / (1 + torch.exp(-x))  # Test custom sigmoid on toy tensor sigmoid(A) Out[45]: <pre>tensor([4.5398e-05, 1.2339e-04, 3.3535e-04, 9.1105e-04, 2.4726e-03, 6.6929e-03,\n        1.7986e-02, 4.7426e-02, 1.1920e-01, 2.6894e-01, 5.0000e-01, 7.3106e-01,\n        8.8080e-01, 9.5257e-01, 9.8201e-01, 9.9331e-01, 9.9753e-01, 9.9909e-01,\n        9.9966e-01, 9.9988e-01])</pre> <p>\u200b\u54c7\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u770b\u8d77\u6765\u200b\u5f88\u200b\u50cf\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u89c1\u200b\u8fc7\u200b\u7684\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u53ef\u89c6\u5316\u200b\u540e\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u3002</p> In\u00a0[46]: Copied! <pre># Plot sigmoid activated toy tensor\nplt.plot(sigmoid(A));\n</pre> # Plot sigmoid activated toy tensor plt.plot(sigmoid(A)); <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff01\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\u53d8\u6210\u200b\u4e86\u200b\u66f2\u7ebf\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0cPyTorch \u200b\u4e2d\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u5c1d\u8bd5\u200b\u8fc7\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u662f\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u3002</p> <p>\u200b\u5173\u952e\u5728\u4e8e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7528\u200b\u65e0\u9650\u200b\u591a\u200b\u7684\u200b\u7ebf\u6027\u200b\uff08\u200b\u76f4\u7ebf\u200b\uff09\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\uff08\u200b\u975e\u200b\u76f4\u7ebf\u200b\uff09\u200b\u7ebf\u6761\u200b\u753b\u51fa\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u56fe\u6848\u200b\uff1f</p> <p>\u200b\u51e0\u4e4e\u200b\u4ec0\u4e48\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u5427\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u6b63\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u7ed3\u5408\u200b\u7ebf\u6027\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u65f6\u200b\u6a21\u578b\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e0d\u662f\u200b\u544a\u8bc9\u200b\u6a21\u578b\u200b\u8be5\u200b\u505a\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u800c\u662f\u200b\u7ed9\u200b\u5b83\u200b\u5de5\u5177\u200b\uff0c\u200b\u8ba9\u200b\u5b83\u200b\u81ea\u5df1\u200b\u627e\u51fa\u200b\u5982\u4f55\u200b\u6700\u597d\u200b\u5730\u200b\u53d1\u73b0\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u800c\u200b\u8fd9\u4e9b\u200b\u5de5\u5177\u200b\u5c31\u662f\u200b\u7ebf\u6027\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[47]: Copied! <pre># Import dependencies\nimport torch\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.model_selection import train_test_split\n\n# Set the hyperparameters for data creation\nNUM_CLASSES = 4\nNUM_FEATURES = 2\nRANDOM_SEED = 42\n\n# 1. Create multi-class data\nX_blob, y_blob = make_blobs(n_samples=1000,\n    n_features=NUM_FEATURES, # X features\n    centers=NUM_CLASSES, # y labels \n    cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)\n    random_state=RANDOM_SEED\n)\n\n# 2. Turn data into tensors\nX_blob = torch.from_numpy(X_blob).type(torch.float)\ny_blob = torch.from_numpy(y_blob).type(torch.LongTensor)\nprint(X_blob[:5], y_blob[:5])\n\n# 3. Split into train and test sets\nX_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,\n    y_blob,\n    test_size=0.2,\n    random_state=RANDOM_SEED\n)\n\n# 4. Plot data\nplt.figure(figsize=(10, 7))\nplt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu);\n</pre> # Import dependencies import torch import matplotlib.pyplot as plt from sklearn.datasets import make_blobs from sklearn.model_selection import train_test_split  # Set the hyperparameters for data creation NUM_CLASSES = 4 NUM_FEATURES = 2 RANDOM_SEED = 42  # 1. Create multi-class data X_blob, y_blob = make_blobs(n_samples=1000,     n_features=NUM_FEATURES, # X features     centers=NUM_CLASSES, # y labels      cluster_std=1.5, # give the clusters a little shake up (try changing this to 1.0, the default)     random_state=RANDOM_SEED )  # 2. Turn data into tensors X_blob = torch.from_numpy(X_blob).type(torch.float) y_blob = torch.from_numpy(y_blob).type(torch.LongTensor) print(X_blob[:5], y_blob[:5])  # 3. Split into train and test sets X_blob_train, X_blob_test, y_blob_train, y_blob_test = train_test_split(X_blob,     y_blob,     test_size=0.2,     random_state=RANDOM_SEED )  # 4. Plot data plt.figure(figsize=(10, 7)) plt.scatter(X_blob[:, 0], X_blob[:, 1], c=y_blob, cmap=plt.cm.RdYlBu); <pre>tensor([[-8.4134,  6.9352],\n        [-5.7665, -6.4312],\n        [-6.0421, -6.7661],\n        [ 3.9508,  0.6984],\n        [ 4.2505, -0.2815]]) tensor([3, 2, 2, 1, 1])\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e00\u4e9b\u200b\u591a\u200b\u7c7b\u522b\u200b\u6570\u636e\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5206\u79bb\u200b\u8fd9\u4e9b\u200b\u5f69\u8272\u200b\u6591\u70b9\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u9700\u8981\u200b\u975e\u7ebf\u6027\u200b\u5417\u200b\uff1f\u200b\u8fd8\u662f\u200b\u53ef\u4ee5\u200b\u7528\u200b\u4e00\u7cfb\u5217\u200b\u76f4\u7ebf\u200b\u6765\u200b\u5206\u79bb\u200b\u5b83\u200b\uff1f</p> In\u00a0[48]: Copied! <pre># Create device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Create device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[48]: <pre>'cuda'</pre> In\u00a0[49]: Copied! <pre>from torch import nn\n\n# Build model\nclass BlobModel(nn.Module):\n    def __init__(self, input_features, output_features, hidden_units=8):\n        \"\"\"Initializes all required hyperparameters for a multi-class classification model.\n\n        Args:\n            input_features (int): Number of input features to the model.\n            out_features (int): Number of output features of the model\n              (how many classes there are).\n            hidden_units (int): Number of hidden units between layers, default 8.\n        \"\"\"\n        super().__init__()\n        self.linear_layer_stack = nn.Sequential(\n            nn.Linear(in_features=input_features, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=hidden_units),\n            # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)\n            nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?\n        )\n    \n    def forward(self, x):\n        return self.linear_layer_stack(x)\n\n# Create an instance of BlobModel and send it to the target device\nmodel_4 = BlobModel(input_features=NUM_FEATURES, \n                    output_features=NUM_CLASSES, \n                    hidden_units=8).to(device)\nmodel_4\n</pre> from torch import nn  # Build model class BlobModel(nn.Module):     def __init__(self, input_features, output_features, hidden_units=8):         \"\"\"Initializes all required hyperparameters for a multi-class classification model.          Args:             input_features (int): Number of input features to the model.             out_features (int): Number of output features of the model               (how many classes there are).             hidden_units (int): Number of hidden units between layers, default 8.         \"\"\"         super().__init__()         self.linear_layer_stack = nn.Sequential(             nn.Linear(in_features=input_features, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=hidden_units),             # nn.ReLU(), # &lt;- does our dataset require non-linear layers? (try uncommenting and see if the results change)             nn.Linear(in_features=hidden_units, out_features=output_features), # how many classes are there?         )          def forward(self, x):         return self.linear_layer_stack(x)  # Create an instance of BlobModel and send it to the target device model_4 = BlobModel(input_features=NUM_FEATURES,                      output_features=NUM_CLASSES,                      hidden_units=8).to(device) model_4 Out[49]: <pre>BlobModel(\n  (linear_layer_stack): Sequential(\n    (0): Linear(in_features=2, out_features=8, bias=True)\n    (1): Linear(in_features=8, out_features=8, bias=True)\n    (2): Linear(in_features=8, out_features=4, bias=True)\n  )\n)</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u6211\u4eec\u200b\u7684\u200b\u591a\u200b\u7c7b\u522b\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b\u5b83\u200b\u521b\u5efa\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> In\u00a0[50]: Copied! <pre># Create loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model_4.parameters(), \n                            lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance\n</pre> # Create loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(model_4.parameters(),                              lr=0.1) # exercise: try changing the learning rate here and seeing what happens to the model's performance In\u00a0[51]: Copied! <pre># Perform a single forward pass on the data (we'll need to put it to the target device for it to work)\nmodel_4(X_blob_train.to(device))[:5]\n</pre> # Perform a single forward pass on the data (we'll need to put it to the target device for it to work) model_4(X_blob_train.to(device))[:5] Out[51]: <pre>tensor([[-1.2711, -0.6494, -1.4740, -0.7044],\n        [ 0.2210, -1.5439,  0.0420,  1.1531],\n        [ 2.8698,  0.9143,  3.3169,  1.4027],\n        [ 1.9576,  0.3125,  2.2244,  1.1324],\n        [ 0.5458, -1.2381,  0.4441,  1.1804]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)</pre> <p>\u200b\u8fd9\u91cc\u200b\u4f1a\u200b\u8f93\u51fa\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7279\u5f81\u200b\u90fd\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u503c\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u5f62\u72b6\u200b\u6765\u200b\u786e\u8ba4\u200b\u3002</p> In\u00a0[52]: Copied! <pre># How many elements in a single prediction sample?\nmodel_4(X_blob_train.to(device))[0].shape, NUM_CLASSES\n</pre> # How many elements in a single prediction sample? model_4(X_blob_train.to(device))[0].shape, NUM_CLASSES  Out[52]: <pre>(torch.Size([4]), 4)</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6b63\u5728\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u503c\u200b\u3002</p> <p>\u200b\u4f60\u200b\u8fd8\u200b\u8bb0\u5f97\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\u53eb\u200b\u4ec0\u4e48\u200b\u5417\u200b\uff1f</p> <p>\u200b\u63d0\u793a\u200b\uff1a\u200b\u5b83\u200b\u548c\u200b\u201c\u200b\u9752\u86d9\u200b\u5206\u88c2\u200b\u201d\u200b\u62bc\u97f5\u200b\uff08\u200b\u5728\u200b\u5236\u4f5c\u200b\u8fd9\u4e9b\u200b\u6750\u6599\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6ca1\u6709\u200b\u52a8\u7269\u200b\u53d7\u5230\u200b\u4f24\u5bb3\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u731c\u200b\u7684\u200b\u662f\u200blogits\uff0c\u200b\u90a3\u200b\u5c31\u200b\u5bf9\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6240\u4ee5\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6b63\u5728\u200b\u8f93\u51fa\u200blogits\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u200b\u786e\u5b9a\u200b\u5b83\u200b\u4e3a\u200b\u6837\u672c\u200b\u7ed9\u51fa\u200b\u7684\u200b\u5177\u4f53\u200b\u6807\u7b7e\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u4e2d\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4ece\u200b <code>logits -&gt; \u200b\u9884\u6d4b\u200b\u6982\u7387\u200b -&gt; \u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b</code> \u200b\u5462\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200bsoftmax\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u53d1\u6325\u4f5c\u7528\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p> <p>softmax\u200b\u51fd\u6570\u200b\u8ba1\u7b97\u200b\u6bcf\u4e2a\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u53ef\u80fd\u200b\u7c7b\u522b\u200b\u7684\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u7684\u200b\u6982\u7387\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u8fd9\u200b\u8fd8\u200b\u4e0d\u200b\u6e05\u695a\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4ee3\u7801\u200b\u6765\u770b\u200b\u4e00\u770b\u200b\u3002</p> In\u00a0[53]: Copied! <pre># Make prediction logits with model\ny_logits = model_4(X_blob_test.to(device))\n\n# Perform softmax calculation on logits across dimension 1 to get prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1) \nprint(y_logits[:5])\nprint(y_pred_probs[:5])\n</pre> # Make prediction logits with model y_logits = model_4(X_blob_test.to(device))  # Perform softmax calculation on logits across dimension 1 to get prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  print(y_logits[:5]) print(y_pred_probs[:5]) <pre>tensor([[-1.2549, -0.8112, -1.4795, -0.5696],\n        [ 1.7168, -1.2270,  1.7367,  2.1010],\n        [ 2.2400,  0.7714,  2.6020,  1.0107],\n        [-0.7993, -0.3723, -0.9138, -0.5388],\n        [-0.4332, -1.6117, -0.6891,  0.6852]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\ntensor([[0.1872, 0.2918, 0.1495, 0.3715],\n        [0.2824, 0.0149, 0.2881, 0.4147],\n        [0.3380, 0.0778, 0.4854, 0.0989],\n        [0.2118, 0.3246, 0.1889, 0.2748],\n        [0.1945, 0.0598, 0.1506, 0.5951]], device='cuda:0',\n       grad_fn=&lt;SliceBackward0&gt;)\n</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u53d1\u751f\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u5c3d\u7ba1\u200b\u770b\u8d77\u6765\u200bsoftmax\u200b\u51fd\u6570\u200b\u7684\u200b\u8f93\u51fa\u200b\u4ecd\u7136\u200b\u662f\u200b\u4e00\u5806\u200b\u6742\u4e71\u200b\u7684\u200b\u6570\u5b57\u200b\uff08\u200b\u786e\u5b9e\u200b\u5982\u6b64\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5c1a\u672a\u200b\u7ecf\u8fc7\u8bad\u7ec3\u200b\uff0c\u200b\u76ee\u524d\u200b\u662f\u200b\u57fa\u4e8e\u200b\u968f\u673a\u200b\u6a21\u5f0f\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff09\uff0c\u200b\u4f46\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u4e4b\u95f4\u200b\u6709\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u7279\u5b9a\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002</p> <p>\u200b\u5728\u200b\u5c06\u200blogits\u200b\u901a\u8fc7\u200bsoftmax\u200b\u51fd\u6570\u200b\u5904\u7406\u200b\u540e\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u6837\u672c\u200b\u73b0\u5728\u200b\u52a0\u200b\u8d77\u6765\u200b\u7b49\u4e8e\u200b1\uff08\u200b\u6216\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b1\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u9a8c\u8bc1\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[54]: Copied! <pre># Sum the first sample output of the softmax activation function \ntorch.sum(y_pred_probs[0])\n</pre> # Sum the first sample output of the softmax activation function  torch.sum(y_pred_probs[0]) Out[54]: <pre>tensor(1., device='cuda:0', grad_fn=&lt;SumBackward0&gt;)</pre> <p>\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u672c\u8d28\u200b\u4e0a\u200b\u662f\u200b\u5728\u200b\u8bf4\u660e\u200b\u6a21\u578b\u200b\u8ba4\u4e3a\u200b\u76ee\u6807\u200b\u6837\u672c\u200b <code>X</code>\uff08\u200b\u8f93\u5165\u200b\uff09\u200b\u4e0e\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u5bf9\u5e94\u200b\u7a0b\u5ea6\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b <code>y_pred_probs</code> \u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u503c\u200b\uff0c\u200b\u6700\u5927\u503c\u200b\u7684\u200b\u7d22\u5f15\u200b\u5c31\u662f\u200b\u6a21\u578b\u200b\u8ba4\u4e3a\u200b\u8be5\u200b\u7279\u5b9a\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u6700\u200b\u53ef\u80fd\u200b\u5c5e\u4e8e\u200b\u7684\u200b\u7c7b\u522b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.argmax()</code> \u200b\u6765\u200b\u68c0\u67e5\u200b\u54ea\u4e2a\u200b\u7d22\u5f15\u200b\u5177\u6709\u200b\u6700\u9ad8\u503c\u200b\u3002</p> In\u00a0[55]: Copied! <pre># Which class does the model think is *most* likely at the index 0 sample?\nprint(y_pred_probs[0])\nprint(torch.argmax(y_pred_probs[0]))\n</pre> # Which class does the model think is *most* likely at the index 0 sample? print(y_pred_probs[0]) print(torch.argmax(y_pred_probs[0])) <pre>tensor([0.1872, 0.2918, 0.1495, 0.3715], device='cuda:0',\n       grad_fn=&lt;SelectBackward0&gt;)\ntensor(3, device='cuda:0')\n</pre> <p>\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c<code>torch.argmax()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u662f\u200b 3\uff0c\u200b\u56e0\u6b64\u200b\u5bf9\u4e8e\u200b\u7d22\u5f15\u200b\u4e3a\u200b 0 \u200b\u7684\u200b\u6837\u672c\u200b\u7684\u200b\u7279\u5f81\u200b\uff08<code>X</code>\uff09\uff0c\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u6700\u6709\u200b\u53ef\u80fd\u200b\u7684\u200b\u7c7b\u522b\u200b\u503c\u200b\uff08<code>y</code>\uff09\u200b\u662f\u200b 3\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u76ee\u524d\u200b\u8fd9\u200b\u53ea\u662f\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6a21\u578b\u200b\u6709\u200b 25% \u200b\u7684\u200b\u6b63\u786e\u200b\u6982\u7387\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6709\u200b\u56db\u4e2a\u200b\u7c7b\u522b\u200b\uff09\u3002\u200b\u4f46\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6765\u200b\u63d0\u9ad8\u200b\u8fd9\u4e9b\u200b\u6982\u7387\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7efc\u4e0a\u6240\u8ff0\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\u88ab\u200b\u79f0\u4e3a\u200b logits\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8981\u200b\u5c06\u200b logits \u200b\u8f6c\u6362\u200b\u4e3a\u200b \u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b softmax \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08<code>torch.softmax</code>\uff09\u3002</p> <p>\u200b\u5177\u6709\u200b\u6700\u9ad8\u200b \u200b\u9884\u6d4b\u200b\u6982\u7387\u200b \u200b\u7684\u200b\u503c\u200b\u7684\u200b\u7d22\u5f15\u200b\u662f\u200b\u6a21\u578b\u200b\u8ba4\u4e3a\u200b\u5728\u200b\u8be5\u200b\u6837\u672c\u200b\u8f93\u5165\u200b\u7279\u5f81\u200b\u4e0b\u200b\u6700\u200b\u6709\u200b\u53ef\u80fd\u200b\u7684\u200b\u7c7b\u522b\u200b\u7f16\u53f7\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u4e00\u5b9a\u200b\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\uff09\u3002</p> In\u00a0[56]: Copied! <pre># Fit the model\ntorch.manual_seed(42)\n\n# Set number of epochs\nepochs = 100\n\n# Put data to target device\nX_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device)\nX_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_4.train()\n\n    # 1. Forward pass\n    y_logits = model_4(X_blob_train) # model outputs raw logits \n    y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels\n    # print(y_logits)\n    # 2. Calculate loss and accuracy\n    loss = loss_fn(y_logits, y_blob_train) \n    acc = accuracy_fn(y_true=y_blob_train,\n                      y_pred=y_pred)\n\n    # 3. Optimizer zero grad\n    optimizer.zero_grad()\n\n    # 4. Loss backwards\n    loss.backward()\n\n    # 5. Optimizer step\n    optimizer.step()\n\n    ### Testing\n    model_4.eval()\n    with torch.inference_mode():\n      # 1. Forward pass\n      test_logits = model_4(X_blob_test)\n      test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)\n      # 2. Calculate test loss and accuracy\n      test_loss = loss_fn(test_logits, y_blob_test)\n      test_acc = accuracy_fn(y_true=y_blob_test,\n                             y_pred=test_pred)\n\n    # Print out what's happening\n    if epoch % 10 == 0:\n        print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")\n</pre> # Fit the model torch.manual_seed(42)  # Set number of epochs epochs = 100  # Put data to target device X_blob_train, y_blob_train = X_blob_train.to(device), y_blob_train.to(device) X_blob_test, y_blob_test = X_blob_test.to(device), y_blob_test.to(device)  for epoch in range(epochs):     ### Training     model_4.train()      # 1. Forward pass     y_logits = model_4(X_blob_train) # model outputs raw logits      y_pred = torch.softmax(y_logits, dim=1).argmax(dim=1) # go from logits -&gt; prediction probabilities -&gt; prediction labels     # print(y_logits)     # 2. Calculate loss and accuracy     loss = loss_fn(y_logits, y_blob_train)      acc = accuracy_fn(y_true=y_blob_train,                       y_pred=y_pred)      # 3. Optimizer zero grad     optimizer.zero_grad()      # 4. Loss backwards     loss.backward()      # 5. Optimizer step     optimizer.step()      ### Testing     model_4.eval()     with torch.inference_mode():       # 1. Forward pass       test_logits = model_4(X_blob_test)       test_pred = torch.softmax(test_logits, dim=1).argmax(dim=1)       # 2. Calculate test loss and accuracy       test_loss = loss_fn(test_logits, y_blob_test)       test_acc = accuracy_fn(y_true=y_blob_test,                              y_pred=test_pred)      # Print out what's happening     if epoch % 10 == 0:         print(f\"Epoch: {epoch} | Loss: {loss:.5f}, Acc: {acc:.2f}% | Test Loss: {test_loss:.5f}, Test Acc: {test_acc:.2f}%\")  <pre>Epoch: 0 | Loss: 1.04324, Acc: 65.50% | Test Loss: 0.57861, Test Acc: 95.50%\nEpoch: 10 | Loss: 0.14398, Acc: 99.12% | Test Loss: 0.13037, Test Acc: 99.00%\nEpoch: 20 | Loss: 0.08062, Acc: 99.12% | Test Loss: 0.07216, Test Acc: 99.50%\nEpoch: 30 | Loss: 0.05924, Acc: 99.12% | Test Loss: 0.05133, Test Acc: 99.50%\nEpoch: 40 | Loss: 0.04892, Acc: 99.00% | Test Loss: 0.04098, Test Acc: 99.50%\nEpoch: 50 | Loss: 0.04295, Acc: 99.00% | Test Loss: 0.03486, Test Acc: 99.50%\nEpoch: 60 | Loss: 0.03910, Acc: 99.00% | Test Loss: 0.03083, Test Acc: 99.50%\nEpoch: 70 | Loss: 0.03643, Acc: 99.00% | Test Loss: 0.02799, Test Acc: 99.50%\nEpoch: 80 | Loss: 0.03448, Acc: 99.00% | Test Loss: 0.02587, Test Acc: 99.50%\nEpoch: 90 | Loss: 0.03300, Acc: 99.12% | Test Loss: 0.02423, Test Acc: 99.50%\n</pre> In\u00a0[57]: Copied! <pre># Make predictions\nmodel_4.eval()\nwith torch.inference_mode():\n    y_logits = model_4(X_blob_test)\n\n# View the first 10 predictions\ny_logits[:10]\n</pre> # Make predictions model_4.eval() with torch.inference_mode():     y_logits = model_4(X_blob_test)  # View the first 10 predictions y_logits[:10] Out[57]: <pre>tensor([[  4.3377,  10.3539, -14.8948,  -9.7642],\n        [  5.0142, -12.0371,   3.3860,  10.6699],\n        [ -5.5885, -13.3448,  20.9894,  12.7711],\n        [  1.8400,   7.5599,  -8.6016,  -6.9942],\n        [  8.0726,   3.2906, -14.5998,  -3.6186],\n        [  5.5844, -14.9521,   5.0168,  13.2890],\n        [ -5.9739, -10.1913,  18.8655,   9.9179],\n        [  7.0755,  -0.7601,  -9.5531,   0.1736],\n        [ -5.5918, -18.5990,  25.5309,  17.5799],\n        [  7.3142,   0.7197, -11.2017,  -1.2011]], device='cuda:0')</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4ecd\u7136\u200b\u662f\u200blogit\u200b\u5f62\u5f0f\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u8981\u200b\u8bc4\u4f30\u200b\u5b83\u4eec\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5fc5\u987b\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u6807\u7b7e\u200b\uff08<code>y_blob_test</code>\uff09\u200b\u5f62\u5f0f\u200b\u76f8\u540c\u200b\uff0c\u200b\u5373\u200b\u6574\u6570\u200b\u5f62\u5f0f\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200blogits\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff08\u200b\u4f7f\u7528\u200b<code>torch.softmax()</code>\uff09\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\uff08\u200b\u901a\u8fc7\u200b\u53d6\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b<code>argmax()</code>\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u8df3\u8fc7\u200b<code>torch.softmax()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ece\u200b<code>\u200b\u9884\u6d4b\u200blogits -&gt; \u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b</code>\uff0c\u200b\u901a\u8fc7\u200b\u76f4\u63a5\u200b\u5728\u200blogits\u200b\u4e0a\u200b\u8c03\u7528\u200b<code>torch.argmax()</code>\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c<code>y_preds = torch.argmax(y_logits, dim=1)</code>\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u8282\u7701\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u200b\u6b65\u9aa4\u200b\uff08\u200b\u4e0d\u200b\u9700\u8981\u200b<code>torch.softmax()</code>\uff09\uff0c\u200b\u4f46\u200b\u7ed3\u679c\u200b\u662f\u200b\u6ca1\u6709\u200b\u53ef\u7528\u200b\u7684\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</p> In\u00a0[58]: Copied! <pre># Turn predicted logits in prediction probabilities\ny_pred_probs = torch.softmax(y_logits, dim=1)\n\n# Turn prediction probabilities into prediction labels\ny_preds = y_pred_probs.argmax(dim=1)\n\n# Compare first 10 model preds and test labels\nprint(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\")\nprint(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\")\n</pre> # Turn predicted logits in prediction probabilities y_pred_probs = torch.softmax(y_logits, dim=1)  # Turn prediction probabilities into prediction labels y_preds = y_pred_probs.argmax(dim=1)  # Compare first 10 model preds and test labels print(f\"Predictions: {y_preds[:10]}\\nLabels: {y_blob_test[:10]}\") print(f\"Test accuracy: {accuracy_fn(y_true=y_blob_test, y_pred=y_preds)}%\") <pre>Predictions: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nLabels: tensor([1, 3, 2, 1, 0, 3, 2, 0, 2, 0], device='cuda:0')\nTest accuracy: 99.5%\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u73b0\u5728\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u6807\u7b7e\u200b\u7684\u200b\u683c\u5f0f\u200b\u4e00\u81f4\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>plot_decision_boundary()</code> \u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\u5b83\u4eec\u200b\u3002\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u5176\u200b\u79fb\u52a8\u200b\u5230\u200b CPU \u200b\u4e0a\u200b\u4ee5\u4fbf\u200b\u4f7f\u7528\u200b matplotlib\uff08<code>plot_decision_boundary()</code> \u200b\u4f1a\u200b\u81ea\u52a8\u200b\u4e3a\u200b\u6211\u4eec\u200b\u5b8c\u6210\u200b\u8fd9\u4e00\u200b\u64cd\u4f5c\u200b\uff09\u3002</p> In\u00a0[59]: Copied! <pre>plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.title(\"Train\")\nplot_decision_boundary(model_4, X_blob_train, y_blob_train)\nplt.subplot(1, 2, 2)\nplt.title(\"Test\")\nplot_decision_boundary(model_4, X_blob_test, y_blob_test)\n</pre> plt.figure(figsize=(12, 6)) plt.subplot(1, 2, 1) plt.title(\"Train\") plot_decision_boundary(model_4, X_blob_train, y_blob_train) plt.subplot(1, 2, 2) plt.title(\"Test\") plot_decision_boundary(model_4, X_blob_test, y_blob_test) In\u00a0[60]: Copied! <pre>try:\n    from torchmetrics import Accuracy\nexcept:\n    !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)\n    from torchmetrics import Accuracy\n\n# Setup metric and make sure it's on the target device\ntorchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)\n\n# Calculate accuracy\ntorchmetrics_accuracy(y_preds, y_blob_test)\n</pre> try:     from torchmetrics import Accuracy except:     !pip install torchmetrics==0.9.3 # this is the version we're using in this notebook (later versions exist here: https://torchmetrics.readthedocs.io/en/stable/generated/CHANGELOG.html#changelog)     from torchmetrics import Accuracy  # Setup metric and make sure it's on the target device torchmetrics_accuracy = Accuracy(task='multiclass', num_classes=4).to(device)  # Calculate accuracy torchmetrics_accuracy(y_preds, y_blob_test) Out[60]: <pre>tensor(0.9950, device='cuda:0')</pre>"},{"location":"02_pytorch_classification/#02-pytorch","title":"02. PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b\u200b\u00b6","text":""},{"location":"02_pytorch_classification/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff1f\u00b6","text":"<p>\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b \u200b\u6d89\u53ca\u200b\u9884\u6d4b\u200b\u67d0\u200b\u4e8b\u7269\u200b\u662f\u200b\u67d0\u7269\u200b\u8fd8\u662f\u200b\u5176\u4ed6\u200b\u7269\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u60f3\u8981\u200b\uff1a</p> \u200b\u95ee\u9898\u200b\u7c7b\u578b\u200b \u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f \u200b\u793a\u4f8b\u200b \u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b \u200b\u76ee\u6807\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4e24\u4e2a\u200b\u9009\u9879\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u4f8b\u5982\u200b\u662f\u200b\u6216\u5426\u200b \u200b\u6839\u636e\u200b\u5065\u5eb7\u200b\u53c2\u6570\u200b\u9884\u6d4b\u200b\u67d0\u4eba\u200b\u662f\u5426\u200b\u60a3\u6709\u200b\u5fc3\u810f\u75c5\u200b\u3002 \u200b\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b \u200b\u76ee\u6807\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4e24\u4e2a\u200b\u4ee5\u4e0a\u200b\u9009\u9879\u200b\u4e4b\u4e00\u200b \u200b\u5224\u65ad\u200b\u4e00\u5f20\u200b\u7167\u7247\u200b\u662f\u200b\u98df\u7269\u200b\u3001\u200b\u4eba\u7269\u200b\u8fd8\u662f\u200b\u72d7\u200b\u3002 \u200b\u591a\u200b\u6807\u7b7e\u200b\u5206\u7c7b\u200b \u200b\u76ee\u6807\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u5206\u914d\u200b\u591a\u4e2a\u200b\u9009\u9879\u200b \u200b\u9884\u6d4b\u200b\u7ef4\u57fa\u767e\u79d1\u200b\u6587\u7ae0\u200b\u5e94\u200b\u5206\u914d\u200b\u54ea\u4e9b\u200b\u7c7b\u522b\u200b\uff08\u200b\u4f8b\u5982\u200b\u6570\u5b66\u200b\u3001\u200b\u79d1\u5b66\u200b\u548c\u200b\u54f2\u5b66\u200b\uff09\u3002 <p>\u200b\u5206\u7c7b\u200b\u548c\u200b\u56de\u5f52\u200b\uff08\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u6570\u503c\u200b\uff0c\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b01\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\uff09\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e24\u79cd\u200b\u95ee\u9898\u200b\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u5728\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u51e0\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u6765\u200b\u4f7f\u7528\u200b PyTorch \u200b\u8fdb\u884c\u200b\u5b9e\u8df5\u200b\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b\u6839\u636e\u200b\u4e00\u7ec4\u200b\u8f93\u5165\u200b\u6765\u200b\u9884\u6d4b\u200b\u8fd9\u7ec4\u200b\u8f93\u5165\u200b\u5c5e\u4e8e\u200b\u54ea\u4e2a\u200b\u7c7b\u522b\u200b\u3002</p>"},{"location":"02_pytorch_classification/","title":"\u6211\u4eec\u200b\u5c06\u8981\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u56de\u987e\u200b\u5728\u200b01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200b PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002</p> <p></p> <p>\u200b\u4e0d\u540c\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u518d\u200b\u5c1d\u8bd5\u200b\u9884\u6d4b\u200b\u4e00\u6761\u200b\u76f4\u7ebf\u200b\uff08\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u6570\u5b57\u200b\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\uff09\uff0c\u200b\u800c\u662f\u200b\u5904\u7406\u200b\u4e00\u4e2a\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. \u200b\u5206\u7c7b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u67b6\u6784\u200b \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u6709\u200b\u4efb\u4f55\u200b\u5f62\u72b6\u200b\u6216\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u901a\u5e38\u200b\u9075\u5faa\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u5e03\u5c40\u200b\u3002 1. \u200b\u51c6\u5907\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b \u200b\u6570\u636e\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u5165\u95e8\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 2. \u200b\u6784\u5efa\u200b PyTorch \u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b \u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u5e76\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\u4e8e\u200b\u5206\u7c7b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u3002 3. \u200b\u5c06\u200b\u6a21\u578b\u200b\u62df\u5408\u200b\u5230\u200b\u6570\u636e\u200b\uff08\u200b\u8bad\u7ec3\u200b\uff09 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8ba9\u200b\u6a21\u578b\u200b\uff08\u200b\u5c1d\u8bd5\u200b\uff09\u200b\u5728\u200b\uff08\u200b\u8bad\u7ec3\u200b\uff09\u200b\u6570\u636e\u200b\u4e2d\u200b\u627e\u5230\u200b\u6a21\u5f0f\u200b\u3002 4. \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\uff08\u200b\u63a8\u7406\u200b\uff09 \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u5728\u200b\u6570\u636e\u200b\u4e2d\u200b\u627e\u5230\u200b\u4e86\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u53d1\u73b0\u200b\u4e0e\u200b\u5b9e\u9645\u200b\u7684\u200b\uff08\u200b\u6d4b\u8bd5\u200b\uff09\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002 5. \u200b\u6539\u8fdb\u200b\u6a21\u578b\u200b\uff08\u200b\u4ece\u200b\u6a21\u578b\u200b\u89d2\u5ea6\u200b\uff09 \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u5e76\u200b\u8bc4\u4f30\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u6ca1\u6709\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e9b\u200b\u65b9\u6cd5\u200b\u6765\u200b\u6539\u8fdb\u200b\u5b83\u200b\u3002 6. \u200b\u975e\u7ebf\u6027\u200b \u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53ea\u80fd\u200b\u5efa\u6a21\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u90a3\u4e48\u200b\u975e\u7ebf\u6027\u200b\uff08\u200b\u975e\u200b\u76f4\u7ebf\u200b\uff09\u200b\u7684\u200b\u7ebf\u5462\u200b\uff1f 7. \u200b\u590d\u5236\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b \u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u6765\u200b\u5e2e\u52a9\u200b\u5efa\u6a21\u200b\u975e\u7ebf\u6027\u200b\u6570\u636e\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\uff1f 8. \u200b\u7ed3\u5408\u200b\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u76ee\u524d\u4e3a\u6b62\u200b\u4e3a\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e00\u5207\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u89e3\u51b3\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002"},{"location":"02_pytorch_classification/","title":"\u5982\u4f55\u200b\u83b7\u53d6\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u5b58\u653e\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b \u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b \u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u89e3\u51b3\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u597d\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"02_pytorch_classification/#0","title":"0. \u200b\u5206\u7c7b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u67b6\u6784\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u6765\u770b\u200b\u4e00\u4e0b\u200b\u5206\u7c7b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u4e00\u822c\u200b\u67b6\u6784\u200b\u3002</p> \u200b\u8d85\u200b\u53c2\u6570\u200b \u200b\u4e8c\u200b\u5206\u7c7b\u200b \u200b\u591a\u200b\u5206\u7c7b\u200b \u200b\u8f93\u5165\u200b\u5c42\u200b\u5f62\u72b6\u200b (<code>in_features</code>) \u200b\u4e0e\u200b\u7279\u5f81\u200b\u6570\u91cf\u200b\u76f8\u540c\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5fc3\u810f\u75c5\u200b\u9884\u6d4b\u200b\u4e2d\u200b\u7684\u200b\u5e74\u9f84\u200b\u3001\u200b\u6027\u522b\u200b\u3001\u200b\u8eab\u9ad8\u200b\u3001\u200b\u4f53\u91cd\u200b\u3001\u200b\u5438\u70df\u200b\u72b6\u51b5\u200b\u5171\u200b5\u200b\u4e2a\u200b\u7279\u5f81\u200b\uff09 \u200b\u4e0e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u76f8\u540c\u200b \u200b\u9690\u85cf\u200b\u5c42\u200b \u200b\u95ee\u9898\u200b\u7279\u5b9a\u200b\uff0c\u200b\u6700\u5c11\u200b = 1\uff0c\u200b\u6700\u200b\u591a\u200b = \u200b\u65e0\u9650\u200b \u200b\u4e0e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u76f8\u540c\u200b \u200b\u6bcf\u5c42\u200b\u9690\u85cf\u200b\u5c42\u4e2d\u200b\u7684\u200b\u795e\u7ecf\u5143\u200b\u6570\u91cf\u200b \u200b\u95ee\u9898\u200b\u7279\u5b9a\u200b\uff0c\u200b\u901a\u5e38\u200b\u4e3a\u200b 10 \u200b\u5230\u200b 512 \u200b\u4e0e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u76f8\u540c\u200b \u200b\u8f93\u51fa\u200b\u5c42\u200b\u5f62\u72b6\u200b (<code>out_features</code>) 1\uff08\u200b\u4e00\u4e2a\u200b\u7c7b\u522b\u200b\u6216\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7c7b\u522b\u200b\uff09 \u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b 1 \u200b\u4e2a\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u98df\u7269\u200b\u3001\u200b\u4eba\u7269\u200b\u6216\u200b\u72d7\u200b\u7684\u200b\u7167\u7247\u200b\u5171\u200b3\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09 \u200b\u9690\u85cf\u200b\u5c42\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b \u200b\u901a\u5e38\u200b\u4e3a\u200b ReLU\uff08\u200b\u4fee\u6b63\u200b\u7ebf\u6027\u200b\u5355\u5143\u200b\uff09\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u662f\u200b\u5176\u4ed6\u200b\u591a\u79cd\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b \u200b\u4e0e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u76f8\u540c\u200b \u200b\u8f93\u51fa\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b Sigmoid\uff08\u200b\u5728\u200b PyTorch \u200b\u4e2d\u4e3a\u200b <code>torch.sigmoid</code>\uff09 Softmax\uff08\u200b\u5728\u200b PyTorch \u200b\u4e2d\u4e3a\u200b <code>torch.softmax</code>\uff09 \u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\uff08\u200b\u5728\u200b PyTorch \u200b\u4e2d\u4e3a\u200b <code>torch.nn.BCELoss</code>\uff09 \u200b\u4ea4\u53c9\u200b\u71b5\u200b\uff08\u200b\u5728\u200b PyTorch \u200b\u4e2d\u4e3a\u200b <code>torch.nn.CrossEntropyLoss</code>\uff09 \u200b\u4f18\u5316\u200b\u5668\u200b SGD\uff08\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09\uff0cAdam\uff08\u200b\u66f4\u200b\u591a\u200b\u9009\u9879\u200b\u89c1\u200b <code>torch.optim</code>\uff09 \u200b\u4e0e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u76f8\u540c\u200b <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5206\u7c7b\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7ec4\u4ef6\u200b\u7684\u200b\u6e05\u5355\u200b\u4f1a\u200b\u6839\u636e\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u800c\u200b\u6709\u6240\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u5df2\u7ecf\u200b\u8db3\u591f\u200b\u8ba9\u200b\u4f60\u200b\u5165\u95e8\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u901a\u8fc7\u200b\u8fd9\u4e2a\u200b\u8bbe\u7f6e\u200b\u8fdb\u884c\u200b\u5b9e\u8df5\u200b\u64cd\u4f5c\u200b\u3002</p>"},{"location":"02_pytorch_classification/#1","title":"1. \u200b\u521b\u5efa\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u5e76\u200b\u51c6\u5907\u5c31\u7eea\u200b\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u751f\u6210\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Scikit-Learn \u200b\u5e93\u4e2d\u200b\u7684\u200b <code>make_circles()</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u751f\u6210\u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u989c\u8272\u200b\u70b9\u200b\u7684\u200b\u5706\u200b\u3002</p>"},{"location":"02_pytorch_classification/#11","title":"1.1 \u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u00b6","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u9519\u8bef\u200b\u4e4b\u4e00\u200b\u662f\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u5f20\u91cf\u200b\u53ca\u5176\u200b\u64cd\u4f5c\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\uff0c\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u6a21\u578b\u200b\u51fa\u73b0\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u5728\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u591a\u6b21\u200b\u9047\u5230\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u6ca1\u6709\u200b\u7edd\u5bf9\u200b\u7684\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u786e\u4fdd\u200b\u5b83\u4eec\u200b\u4e0d\u4f1a\u200b\u53d1\u751f\u200b\uff0c\u200b\u5b83\u4eec\u200b\u603b\u4f1a\u200b\u53d1\u751f\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u505a\u200b\u7684\u200b\u662f\u200b\u4e0d\u65ad\u200b\u719f\u6089\u200b\u4f60\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u6211\u200b\u559c\u6b22\u200b\u5c06\u200b\u5176\u200b\u79f0\u4e3a\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u95ee\u95ee\u200b\u81ea\u5df1\u200b\uff1a</p> <p>\u201c\u200b\u6211\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\u201d</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u627e\u51fa\u200b\u7b54\u6848\u200b\u3002</p>"},{"location":"02_pytorch_classification/#12","title":"1.2 \u200b\u5c06\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u5e76\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u7814\u7a76\u200b\u4e86\u200b\u6570\u636e\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u4ee5\u4fbf\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u4f7f\u7528\u200b\u5e76\u200b\u8fdb\u884c\u200b\u5efa\u6a21\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\uff1a</p> <ol> <li>\u200b\u5c06\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff08\u200b\u76ee\u524d\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b NumPy \u200b\u6570\u7ec4\u200b\uff0c\u200b\u800c\u200b PyTorch \u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b\u4f7f\u7528\u200b PyTorch \u200b\u5f20\u91cf\u200b\uff09\u3002</li> <li>\u200b\u5c06\u200b\u6570\u636e\u200b\u5206\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ee5\u200b\u5b66\u4e60\u200b <code>X</code> \u200b\u548c\u200b <code>y</code> \u200b\u4e4b\u95f4\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u8fd9\u4e9b\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff09\u3002</li> </ol>"},{"location":"02_pytorch_classification/#2","title":"2. \u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u200b\u5206\u89e3\u200b\u4e3a\u200b\u51e0\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ol> <li>\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\uff08\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5728\u200bCPU\u200b\u6216\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u5982\u679c\u200bGPU\u200b\u53ef\u7528\u200b\u7684\u8bdd\u200b\uff09\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5b50\u200b\u7c7b\u5316\u200b <code>nn.Module</code> \u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff08\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u5c06\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u4e2d\u200b\u8fdb\u884c\u200b\uff09\u3002</li> </ol> <p>\u200b\u597d\u6d88\u606f\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b01\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u4e0a\u8ff0\u200b\u6240\u6709\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c06\u200b\u8c03\u6574\u200b\u5b83\u4eec\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u9002\u7528\u200b\u4e8e\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u5bfc\u5165\u200bPyTorch\u200b\u548c\u200b <code>torch.nn</code> \u200b\u4ee5\u53ca\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\u5f00\u59cb\u200b\u3002</p>"},{"location":"02_pytorch_classification/#21","title":"2.1 \u200b\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b01\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u8bbe\u7f6e\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u51c6\u5219\u200b\u6216\u200b\u6210\u672c\u200b\u51fd\u6570\u200b\uff09\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u4f46\u662f\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u95ee\u9898\u200b\u9700\u8981\u200b\u4e0d\u540c\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\uff08\u200b\u9884\u6d4b\u200b\u4e00\u4e2a\u200b\u6570\u503c\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\uff08MAE\uff09\u200b\u635f\u5931\u200b\u3002</p> <p>\u200b\u800c\u200b\u5bf9\u4e8e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff08\u200b\u5982\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff09\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u4f5c\u4e3a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u76f8\u540c\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u51fd\u6570\u200b\u901a\u5e38\u200b\u53ef\u4ee5\u200b\u7528\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u95ee\u9898\u200b\u7a7a\u95f4\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u4f18\u5316\u200b\u5668\u200b\uff08SGD\uff0c<code>torch.optim.SGD()</code>\uff09\u200b\u53ef\u4ee5\u200b\u7528\u4e8e\u200b\u591a\u79cd\u200b\u95ee\u9898\u200b\uff0c\u200b\u540c\u6837\u200b\u9002\u7528\u200b\u4e8e\u200bAdam\u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>torch.optim.Adam()</code>\uff09\u3002</p> \u200b\u635f\u5931\u200b\u51fd\u6570\u200b/\u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u95ee\u9898\u200b\u7c7b\u578b\u200b PyTorch\u200b\u4ee3\u7801\u200b \u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff08SGD\uff09\u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u5206\u7c7b\u200b\u3001\u200b\u56de\u5f52\u200b\u3001\u200b\u5176\u4ed6\u200b\u591a\u79cd\u200b\u95ee\u9898\u200b <code>torch.optim.SGD()</code> Adam\u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u5206\u7c7b\u200b\u3001\u200b\u56de\u5f52\u200b\u3001\u200b\u5176\u4ed6\u200b\u591a\u79cd\u200b\u95ee\u9898\u200b <code>torch.optim.Adam()</code> \u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b \u200b\u4e8c\u200b\u5206\u7c7b\u200b <code>torch.nn.BCELossWithLogits</code> \u200b\u6216\u200b <code>torch.nn.BCELoss</code> \u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b \u200b\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b <code>torch.nn.CrossEntropyLoss</code> \u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\uff08MAE\uff09\u200b\u6216\u200bL1\u200b\u635f\u5931\u200b \u200b\u56de\u5f52\u200b <code>torch.nn.L1Loss</code> \u200b\u5747\u65b9\u200b\u8bef\u5dee\u200b\uff08MSE\uff09\u200b\u6216\u200bL2\u200b\u635f\u5931\u200b \u200b\u56de\u5f52\u200b <code>torch.nn.MSELoss</code> <p>\u200b\u5404\u79cd\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u8868\u683c\u200b\uff0c\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u4f60\u200b\u4f1a\u200b\u770b\u5230\u200b\u7684\u200b\u4e00\u4e9b\u200b\u5e38\u89c1\u200b\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u4e00\u4e2a\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u662f\u200b\u8861\u91cf\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u6709\u200b\u591a\u200b\u9519\u8bef\u200b\u7684\u200b\u6307\u6807\u200b\uff0c\u200b\u635f\u5931\u200b\u8d8a\u9ad8\u200b\uff0c\u200b\u6a21\u578b\u200b\u8d8a\u5dee\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0cPyTorch\u200b\u6587\u6863\u200b\u901a\u5e38\u200b\u5c06\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u79f0\u4e3a\u200b\u201c\u200b\u635f\u5931\u200b\u51c6\u5219\u200b\u201d\u200b\u6216\u200b\u201c\u200b\u51c6\u5219\u200b\u201d\uff0c\u200b\u8fd9\u4e9b\u200b\u90fd\u200b\u662f\u200b\u63cf\u8ff0\u200b\u540c\u4e00\u200b\u4e8b\u7269\u200b\u7684\u200b\u4e0d\u540c\u200b\u65b9\u5f0f\u200b\u3002</p> <p>PyTorch\u200b\u6709\u200b\u4e24\u79cd\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u5b9e\u73b0\u200b\uff1a</p> <ol> <li><code>torch.nn.BCELoss()</code> - \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8861\u91cf\u200b\u76ee\u6807\u200b\uff08\u200b\u6807\u7b7e\u200b\uff09\u200b\u548c\u200b\u8f93\u5165\u200b\uff08\u200b\u7279\u5f81\u200b\uff09\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u3002</li> <li><code>torch.nn.BCEWithLogitsLoss()</code> - \u200b\u8fd9\u200b\u4e0e\u200b\u4e0a\u8ff0\u200b\u76f8\u540c\u200b\uff0c\u200b\u53ea\u662f\u200b\u5b83\u200b\u5185\u7f6e\u200b\u4e86\u200b\u4e00\u4e2a\u200bSigmoid\u200b\u5c42\u200b\uff08<code>nn.Sigmoid</code>\uff09\uff08\u200b\u6211\u4eec\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4ec0\u4e48\u200b\uff09\u3002</li> </ol> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u54ea\u200b\u4e00\u4e2a\u200b\uff1f</p> <p><code>torch.nn.BCEWithLogitsLoss()</code>\u200b\u7684\u200b\u6587\u6863\u200b\u6307\u51fa\u200b\uff0c\u200b\u5b83\u200b\u5728\u200b\u6570\u503c\u200b\u4e0a\u200b\u6bd4\u200b\u5728\u200b<code>nn.Sigmoid</code>\u200b\u5c42\u200b\u4e4b\u540e\u200b\u4f7f\u7528\u200b<code>torch.nn.BCELoss()</code>\u200b\u66f4\u200b\u7a33\u5b9a\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u7b2c\u4e8c\u79cd\u200b\u5b9e\u73b0\u200b\u662f\u200b\u4e00\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u9009\u62e9\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u9ad8\u7ea7\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u5206\u79bb\u200b<code>nn.Sigmoid</code>\u200b\u548c\u200b<code>torch.nn.BCELoss()</code>\u200b\u7684\u200b\u7ec4\u5408\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u8d85\u51fa\u200b\u4e86\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u8303\u56f4\u200b\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u540e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>torch.optim.SGD()</code>\u200b\u4ee5\u200b\u5b66\u4e60\u200b\u7387\u200b0.1\u200b\u6765\u200b\u4f18\u5316\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200bPyTorch\u200b\u8bba\u575b\u200b\u4e0a\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b<code>nn.BCELoss</code>\u200b\u4e0e\u200b<code>nn.BCEWithLogitsLoss</code>\u200b\u7684\u200b\u4f7f\u7528\u200b\u7684\u200b\u8ba8\u8bba\u200b\u3002\u200b\u4e00\u200b\u5f00\u59cb\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u611f\u5230\u200b\u56f0\u60d1\u200b\uff0c\u200b\u4f46\u200b\u968f\u7740\u200b\u5b9e\u8df5\u200b\u7684\u200b\u589e\u591a\u200b\uff0c\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u66f4\u200b\u5bb9\u6613\u200b\u7406\u89e3\u200b\u3002</p>"},{"location":"02_pytorch_classification/#3","title":"3. \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u8fd8\u200b\u8bb0\u5f97\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u7684\u200b\u6b65\u9aa4\u200b\u5417\u200b\uff1f</p> <p>\u200b\u5982\u679c\u200b\u8bb0\u4e0d\u6e05\u200b\u4e86\u200b\uff0c\u200b\u8fd9\u91cc\u200b\u6709\u200b\u4e00\u4e2a\u200b\u63d0\u9192\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\uff1a</p> PyTorch\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u6b65\u9aa4\u200b <ol> <li>\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b - \u200b\u6a21\u578b\u200b\u904d\u5386\u200b\u6240\u6709\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e00\u6b21\u200b\uff0c\u200b\u6267\u884c\u200b\u5176\u200b             <code>forward()</code> \u200b\u51fd\u6570\u200b             \u200b\u8ba1\u7b97\u200b\uff08<code>model(x_train)</code>\uff09\u3002         </li> <li>\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b - \u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u9884\u6d4b\u503c\u200b\uff09\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u503c\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u5e76\u200b\u8bc4\u4f30\u200b             \u200b\u5b83\u4eec\u200b\u7684\u200b\u8bef\u5dee\u200b\uff08<code>loss = loss_fn(y_pred, y_train)</code>\uff09\u3002</li> <li>\u200b\u6e05\u96f6\u200b\u68af\u5ea6\u200b - \u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u68af\u5ea6\u200b\u88ab\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u96f6\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5b83\u4eec\u200b\u4f1a\u200b\u7d2f\u79ef\u200b\uff09\uff0c\u200b\u4ee5\u4fbf\u200b             \u200b\u53ef\u4ee5\u200b\u4e3a\u200b\u7279\u5b9a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\u91cd\u65b0\u200b\u8ba1\u7b97\u200b\uff08<code>optimizer.zero_grad()</code>\uff09\u3002</li> <li>\u200b\u5bf9\u200b\u635f\u5931\u200b\u8fdb\u884c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b - \u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u6bcf\u4e2a\u200b\u9700\u8981\u200b\u66f4\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b             \u200b\u53c2\u6570\u200b\u7684\u200b\u68af\u5ea6\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u53c2\u6570\u200b             \u200b\u5e26\u6709\u200b <code>requires_grad=True</code>\uff09\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u56e0\u6b64\u200b\u79f0\u4e3a\u200b\u201c\u200b\u53cd\u5411\u200b\u201d             \uff08<code>loss.backward()</code>\uff09\u3002</li> <li>\u200b\u66f4\u65b0\u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff09 - \u200b\u6839\u636e\u200b\u635f\u5931\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u5e26\u6709\u200b <code>requires_grad=True</code>             \u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u200b\u6539\u8fdb\u200b\u5b83\u4eec\u200b\uff08<code>optimizer.step()</code>\uff09\u3002</li> </ol>"},{"location":"02_pytorch_classification/#31-logits-","title":"3.1 \u200b\u4ece\u200b\u539f\u59cb\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\u5230\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\uff08logits -&gt; \u200b\u9884\u6d4b\u200b\u6982\u7387\u200b -&gt; \u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\uff09\u00b6","text":"<p>\u200b\u5728\u200b\u8fdb\u5165\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u6b65\u9aa4\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u770b\u770b\u200b\u5728\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff08\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u7531\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u5b9a\u4e49\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5411\u200b\u6a21\u578b\u200b\u4f20\u9012\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"02_pytorch_classification/#32","title":"3.2 \u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u5982\u4f55\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5148\u200b\u8fdb\u884c\u200b100\u200b\u4e2a\u200b\u5468\u671f\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6bcf\u200b10\u200b\u4e2a\u200b\u5468\u671f\u200b\u8f93\u51fa\u200b\u4e00\u6b21\u200b\u6a21\u578b\u200b\u7684\u200b\u8fdb\u5ea6\u200b\u3002</p>"},{"location":"02_pytorch_classification/#4","title":"4. \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u4ece\u200b\u6307\u6807\u200b\u6765\u770b\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4f3c\u4e4e\u200b\u662f\u200b\u5728\u200b\u968f\u673a\u200b\u731c\u6d4b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u8fdb\u4e00\u6b65\u200b\u8c03\u67e5\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u200b\u6709\u200b\u4e00\u4e2a\u200b\u60f3\u6cd5\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff01</p> <p>\u201c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01\u201d</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ed8\u5236\u200b\u4e00\u4e2a\u200b\u56fe\u8868\u200b\uff0c\u200b\u5c55\u793a\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3001\u200b\u5b83\u200b\u8bd5\u56fe\u200b\u9884\u6d4b\u200b\u7684\u200b\u6570\u636e\u200b\u4ee5\u53ca\u200b\u5b83\u200b\u4e3a\u200b\u533a\u5206\u200b\u7c7b\u522b\u200b0\u200b\u548c\u200b\u7c7b\u522b\u200b1\u200b\u6240\u200b\u521b\u5efa\u200b\u7684\u200b\u51b3\u7b56\u200b\u8fb9\u754c\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ece\u200bLearn PyTorch for Deep Learning\u200b\u4ed3\u5e93\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u5bfc\u5165\u200b<code>helper_functions.py</code>\u200b\u811a\u672c\u200b\u3002</p> <p>\u200b\u8be5\u200b\u811a\u672c\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>plot_decision_boundary()</code>\u200b\u7684\u200b\u6709\u7528\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200bNumPy\u200b\u7f51\u683c\u200b\uff0c\u200b\u4ee5\u200b\u53ef\u89c6\u5316\u200b\u6a21\u578b\u200b\u5728\u200b\u4e0d\u540c\u70b9\u200b\u4e0a\u200b\u9884\u6d4b\u200b\u7279\u5b9a\u200b\u7c7b\u522b\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u5bfc\u5165\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b01\u200b\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b<code>plot_predictions()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u5907\u200b\u540e\u200b\u7528\u200b\u3002</p>"},{"location":"02_pytorch_classification/#5","title":"5. \u200b\u6539\u8fdb\u200b\u6a21\u578b\u200b\uff08\u200b\u4ece\u200b\u6a21\u578b\u200b\u89d2\u5ea6\u200b\uff09\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u89e3\u51b3\u200b\u6a21\u578b\u200b\u6b20\u200b\u62df\u5408\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u5177\u4f53\u200b\u4ece\u200b\u6a21\u578b\u200b\uff08\u200b\u800c\u200b\u975e\u200b\u6570\u636e\u200b\uff09\u200b\u7684\u200b\u89d2\u5ea6\u200b\uff0c\u200b\u6709\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u6539\u8fdb\u200b\u3002</p> \u200b\u6a21\u578b\u200b\u6539\u8fdb\u200b\u6280\u672f\u200b* \u200b\u4f5c\u7528\u200b \u200b\u589e\u52a0\u200b\u66f4\u200b\u591a\u5c42\u200b \u200b\u6bcf\u5c42\u200b\u53ef\u80fd\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\uff0c\u200b\u6bcf\u5c42\u200b\u80fd\u591f\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u4e2d\u200b\u67d0\u79cd\u200b\u65b0\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u66f4\u200b\u591a\u5c42\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u4f7f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u66f4\u6df1\u200b\u3002 \u200b\u589e\u52a0\u200b\u66f4\u200b\u591a\u200b\u9690\u85cf\u200b\u5355\u5143\u200b \u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4e0a\u8ff0\u200b\uff0c\u200b\u6bcf\u5c42\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u610f\u5473\u7740\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u63d0\u9ad8\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u4f7f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u66f4\u200b\u5bbd\u200b\u3002 \u200b\u5ef6\u957f\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u5468\u671f\u200b\uff09 \u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u673a\u4f1a\u200b\u89c2\u5bdf\u200b\u6570\u636e\u200b\uff0c\u200b\u5b83\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5b66\u5230\u200b\u66f4\u200b\u591a\u200b\u3002 \u200b\u6539\u53d8\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b \u200b\u67d0\u4e9b\u200b\u6570\u636e\u200b\u65e0\u6cd5\u200b\u4ec5\u7528\u200b\u76f4\u7ebf\u200b\u62df\u5408\u200b\uff08\u200b\u5982\u200b\u6211\u4eec\u200b\u6240\u200b\u89c1\u200b\uff09\uff0c\u200b\u4f7f\u7528\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff08\u200b\u63d0\u793a\u200b\uff0c\u200b\u63d0\u793a\u200b\uff09\u3002 \u200b\u6539\u53d8\u200b\u5b66\u4e60\u200b\u7387\u200b \u200b\u867d\u7136\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u9488\u5bf9\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u4ecd\u7136\u200b\u76f8\u5173\u200b\uff0c\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u51b3\u5b9a\u200b\u4e86\u200b\u6a21\u578b\u200b\u6bcf\u200b\u4e00\u6b65\u200b\u5e94\u200b\u6539\u53d8\u200b\u53c2\u6570\u200b\u7684\u200b\u7a0b\u5ea6\u200b\uff0c\u200b\u592a\u200b\u5927\u200b\u6a21\u578b\u200b\u4f1a\u200b\u8fc7\u5ea6\u200b\u6821\u6b63\u200b\uff0c\u200b\u592a\u5c0f\u200b\u5219\u200b\u5b66\u200b\u4e0d\u5230\u200b\u8db3\u591f\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002 \u200b\u6539\u53d8\u200b\u635f\u5931\u200b\u51fd\u6570\u200b \u200b\u540c\u6837\u200b\uff0c\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u9488\u5bf9\u200b\u6a21\u578b\u200b\u4f46\u200b\u4ecd\u7136\u200b\u91cd\u8981\u200b\uff0c\u200b\u4e0d\u540c\u200b\u95ee\u9898\u200b\u9700\u8981\u200b\u4e0d\u540c\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e0d\u9002\u200b\u7528\u4e8e\u200b\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002 \u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u91c7\u7528\u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u4f60\u200b\u95ee\u9898\u200b\u9886\u57df\u200b\u76f8\u4f3c\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u8c03\u6574\u200b\u5b83\u200b\u4ee5\u200b\u9002\u5e94\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u6211\u4eec\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b06\u200b\u4e2d\u200b\u8ba8\u8bba\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u3002 <p>\u200b\u6ce8\u610f\u200b\uff1a *\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u624b\u52a8\u200b\u8c03\u6574\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\uff0c\u200b\u5b83\u4eec\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e00\u534a\u200b\u827a\u672f\u200b\u4e00\u534a\u200b\u79d1\u5b66\u200b\u7684\u200b\u5730\u65b9\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4f60\u200b\u7684\u200b\u9879\u76ee\u200b\uff0c\u200b\u6ca1\u6709\u200b\u771f\u6b63\u200b\u7684\u200b\u65b9\u6cd5\u200b\u77e5\u9053\u200b\u6700\u4f73\u200b\u7684\u200b\u53c2\u6570\u200b\u7ec4\u5408\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u6700\u597d\u200b\u7684\u200b\u529e\u6cd5\u200b\u662f\u200b\u9075\u5faa\u200b\u6570\u636e\u200b\u79d1\u5b66\u5bb6\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff1a\u201c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\u201d\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5411\u200b\u6a21\u578b\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u5ef6\u957f\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff08\u200b\u4ece\u200b<code>epochs=100</code>\u200b\u6539\u4e3a\u200b<code>epochs=1000</code>\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u7684\u200b\u6570\u91cf\u200b\u4ece\u200b<code>5</code>\u200b\u589e\u52a0\u200b\u5230\u200b<code>10</code>\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9075\u5faa\u200b\u4e0a\u8ff0\u200b\u76f8\u540c\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4f46\u200b\u8c03\u6574\u200b\u4e00\u4e9b\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"02_pytorch_classification/#51","title":"5.1 \u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u4ee5\u200b\u9a8c\u8bc1\u200b\u6a21\u578b\u200b\u80fd\u5426\u200b\u62df\u5408\u200b\u76f4\u7ebf\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u7ebf\u6027\u200b\u6570\u636e\u200b\uff0c\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u80fd\u591f\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u5efa\u6a21\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u6839\u672c\u65e0\u6cd5\u200b\u5b66\u4e60\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"02_pytorch_classification/#52-model_1","title":"5.2 \u200b\u8c03\u6574\u200b <code>model_1</code> \u200b\u4ee5\u200b\u62df\u5408\u200b\u76f4\u7ebf\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b <code>model_1</code>\uff0c\u200b\u4f46\u200b\u4f7f\u7528\u200b\u9002\u5408\u200b\u56de\u5f52\u200b\u6570\u636e\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"02_pytorch_classification/#6","title":"6. \u200b\u7f3a\u5931\u200b\u7684\u200b\u4e00\u73af\u200b\uff1a\u200b\u975e\u7ebf\u6027\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\uff0c\u200b\u7531\u4e8e\u200b\u5176\u200b\u7ebf\u6027\u200b\u5c42\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u7ed8\u5236\u200b\u76f4\u7ebf\u200b\uff08\u200b\u7ebf\u6027\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8ba9\u200b\u5b83\u200b\u80fd\u591f\u200b\u7ed8\u5236\u200b\u975e\u200b\u76f4\u7ebf\u200b\uff08\u200b\u975e\u7ebf\u6027\u200b\uff09\u200b\u7684\u200b\u7ebf\u6761\u200b\uff0c\u200b\u4f1a\u200b\u600e\u4e48\u6837\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u63a2\u200b\u7a76\u7adf\u200b\u3002</p>"},{"location":"02_pytorch_classification/#61","title":"6.1 \u200b\u91cd\u6784\u200b\u975e\u7ebf\u6027\u200b\u6570\u636e\u200b\uff08\u200b\u7ea2\u8272\u200b\u548c\u200b\u84dd\u8272\u200b\u5706\u5708\u200b\uff09\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\uff0c\u200b\u4ee5\u200b\u5168\u65b0\u200b\u7684\u200b\u72b6\u6001\u200b\u5f00\u59cb\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u91c7\u7528\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u76f8\u540c\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u3002</p>"},{"location":"02_pytorch_classification/#62","title":"6.2 \u200b\u6784\u5efa\u200b\u5177\u6709\u200b\u975e\u7ebf\u6027\u200b\u7279\u6027\u200b\u7684\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u5230\u200b\u4e86\u200b\u6709\u8da3\u200b\u7684\u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u7528\u200b\u65e0\u9650\u200b\u7684\u200b\u76f4\u7ebf\u200b\uff08\u200b\u7ebf\u6027\u200b\uff09\u200b\u548c\u200b\u975e\u200b\u76f4\u7ebf\u200b\uff08\u200b\u975e\u7ebf\u6027\u200b\uff09\u200b\u7ebf\u6761\u200b\u80fd\u753b\u200b\u51fa\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u56fe\u6848\u200b\uff1f</p> <p>\u200b\u6211\u6562\u200b\u6253\u8d4c\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u53d8\u5f97\u200b\u975e\u5e38\u200b\u6709\u200b\u521b\u610f\u200b\u3002</p> <p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u53ea\u200b\u4f7f\u7528\u200b\u4e86\u200b\u7ebf\u6027\u200b\uff08\u200b\u76f4\u7ebf\u200b\uff09\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u975e\u7ebf\u6027\u200b\u7684\u200b\uff08\u200b\u5706\u5f62\u200b\uff09\u3002</p> <p>\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u5f53\u200b\u6211\u4eec\u200b\u5f15\u5165\u200b\u8ba9\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u80fd\u529b\u200b\u65f6\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u3002</p> <p>PyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u73b0\u6210\u200b\u7684\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u5b83\u4eec\u200b\u505a\u200b\u7c7b\u4f3c\u200b\u4f46\u200b\u4e0d\u540c\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u4e14\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u662f\u200b ReLU)\uff08\u200b\u4fee\u6b63\u200b\u7ebf\u6027\u200b\u5355\u5143\u200b\uff0c<code>torch.nn.ReLU()</code>\uff09\u3002</p> <p>\u200b\u4e0e\u5176\u200b\u8c08\u8bba\u200b\u5b83\u200b\uff0c\u200b\u4e0d\u5982\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u524d\u200b\u5411\u200b\u4f20\u64ad\u200b\u4e2d\u200b\u9690\u85cf\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u52a0\u5165\u200b ReLU \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p>"},{"location":"02_pytorch_classification/#63","title":"6.3 \u200b\u4f7f\u7528\u200b\u975e\u7ebf\u6027\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u719f\u6089\u200b\u6d41\u7a0b\u200b\u4e86\u200b\uff0c\u200b\u6a21\u578b\u200b\u3001\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u3002</p>"},{"location":"02_pytorch_classification/#64","title":"6.4 \u200b\u8bc4\u4f30\u200b\u4f7f\u7528\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u8fd8\u200b\u8bb0\u5f97\u200b\u6211\u4eec\u200b\u7684\u200b\u5706\u5f62\u200b\u6570\u636e\u200b\u662f\u200b\u5982\u4f55\u200b\u5448\u73b0\u200b\u975e\u7ebf\u6027\u200b\u7279\u5f81\u200b\u7684\u200b\u5417\u200b\uff1f\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5728\u200b\u4f7f\u7528\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u3002</p>"},{"location":"02_pytorch_classification/#7","title":"7. \u200b\u590d\u5236\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u4e86\u89e3\u200b\u5230\u200b\uff0c\u200b\u5728\u200b\u6a21\u578b\u200b\u4e2d\u200b\u52a0\u5165\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u6709\u52a9\u4e8e\u200b\u5176\u200b\u62df\u5408\u200b\u975e\u7ebf\u6027\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u5728\u200b\u5b9e\u9645\u200b\u5de5\u4f5c\u200b\u4e2d\u200b\u9047\u5230\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u6570\u636e\u200b\u90fd\u200b\u662f\u200b\u975e\u7ebf\u6027\u200b\u7684\u200b\uff08\u200b\u6216\u8005\u200b\u662f\u200b\u7ebf\u6027\u200b\u4e0e\u200b\u975e\u7ebf\u6027\u200b\u7684\u200b\u7ec4\u5408\u200b\uff09\u3002\u200b\u76ee\u524d\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u5904\u7406\u200b\u4e8c\u7ef4\u200b\u56fe\u4e0a\u200b\u7684\u200b\u70b9\u200b\u3002\u200b\u4f46\u200b\u60f3\u8c61\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u60f3\u8981\u200b\u5206\u7c7b\u200b\u7684\u200b\u690d\u7269\u200b\u56fe\u50cf\u200b\uff0c\u200b\u690d\u7269\u200b\u7684\u200b\u5f62\u72b6\u200b\u5343\u5dee\u4e07\u522b\u200b\u3002\u200b\u6216\u8005\u200b\u4f60\u200b\u60f3\u200b\u603b\u7ed3\u200b\u7ef4\u57fa\u767e\u79d1\u200b\u4e0a\u200b\u7684\u200b\u6587\u672c\u200b\uff0c\u200b\u6587\u5b57\u200b\u7684\u200b\u7ec4\u5408\u200b\u65b9\u5f0f\u200b\u591a\u79cd\u591a\u6837\u200b\uff08\u200b\u7ebf\u6027\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\u6a21\u5f0f\u200b\uff09\u3002</p> <p>\u200b\u4f46\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u770b\u8d77\u6765\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u4e0d\u59a8\u200b\u5c1d\u8bd5\u200b\u590d\u5236\u200b\u4e00\u4e9b\u200b\uff0c\u200b\u5e76\u200b\u4e86\u89e3\u200b\u5b83\u4eec\u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u5c0f\u90e8\u5206\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"02_pytorch_classification/#8-pytorch","title":"8. \u200b\u901a\u8fc7\u200b\u6784\u5efa\u200b\u591a\u200b\u7c7b\u200b PyTorch \u200b\u6a21\u578b\u200b\u5c06\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u6574\u5408\u200b\u8d77\u6765\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6db5\u76d6\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u4f46\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u5c06\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u6574\u5408\u200b\u8d77\u6765\u200b\u3002</p> <p>\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u6d89\u53ca\u200b\u5c06\u200b\u4e8b\u7269\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u9009\u9879\u200b\u4e4b\u4e00\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5c06\u200b\u7167\u7247\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u732b\u200b\u7167\u7247\u200b\u6216\u200b\u72d7\u200b\u7167\u7247\u200b\uff09\uff0c\u200b\u800c\u591a\u7c7b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u6d89\u53ca\u200b\u5c06\u200b\u4e8b\u7269\u200b\u4ece\u200b\u4e24\u4e2a\u200b\u4ee5\u4e0a\u200b\u7684\u200b\u9009\u9879\u200b\u5217\u8868\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5c06\u200b\u7167\u7247\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u732b\u200b\u3001\u200b\u72d7\u200b\u6216\u200b\u9e21\u200b\uff09\u3002</p> <p> \u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u4e0e\u200b\u591a\u7c7b\u200b\u5206\u7c7b\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u6d89\u53ca\u200b\u4e24\u4e2a\u200b\u7c7b\u522b\u200b\uff08\u200b\u4e00\u4e2a\u200b\u4e8b\u7269\u200b\u6216\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u4e8b\u7269\u200b\uff09\uff0c\u200b\u800c\u591a\u7c7b\u200b\u5206\u7c7b\u200b\u53ef\u4ee5\u200b\u5904\u7406\u200b\u4e24\u4e2a\u200b\u4ee5\u4e0a\u200b\u7684\u200b\u4efb\u610f\u200b\u6570\u91cf\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6d41\u884c\u200b\u7684\u200b ImageNet-1k \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u88ab\u200b\u7528\u4f5c\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u57fa\u51c6\u200b\uff0c\u200b\u5305\u542b\u200b 1000 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002</p>"},{"location":"02_pytorch_classification/#81","title":"8.1 \u200b\u521b\u5efa\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u8981\u200b\u5f00\u59cb\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u591a\u200b\u7c7b\u522b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b Scikit-Learn \u200b\u7684\u200b <code>make_blobs()</code> \u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u7684\u200b\u4efb\u610f\u200b\u6570\u91cf\u200b\u7684\u200b\u7c7b\u522b\u200b\uff08\u200b\u901a\u8fc7\u200b <code>centers</code> \u200b\u53c2\u6570\u200b\uff09\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6267\u884c\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b <code>make_blobs()</code> \u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u591a\u200b\u7c7b\u522b\u200b\u6570\u636e\u200b\u3002</li> <li>\u200b\u5c06\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff08<code>make_blobs()</code> \u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200b NumPy \u200b\u6570\u7ec4\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>train_test_split()</code> \u200b\u5c06\u200b\u6570\u636e\u200b\u5206\u5272\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</li> <li>\u200b\u53ef\u89c6\u5316\u200b\u6570\u636e\u200b\u3002</li> </ol>"},{"location":"02_pytorch_classification/#82-pytorch","title":"8.2 \u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u6784\u5efa\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u521b\u5efa\u200b\u4e86\u200b\u51e0\u4e2a\u200bPyTorch\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4e5f\u200b\u5f00\u59cb\u200b\u610f\u8bc6\u200b\u5230\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u7075\u6d3b\u6027\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b<code>model_3</code>\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u4ecd\u7136\u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u591a\u200b\u7c7b\u522b\u200b\u6570\u636e\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b<code>nn.Module</code>\u200b\u7684\u200b\u5b50\u7c7b\u200b\uff0c\u200b\u8be5\u200b\u5b50\u7c7b\u200b\u63a5\u6536\u200b\u4e09\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li><code>input_features</code> - \u200b\u8fdb\u5165\u200b\u6a21\u578b\u200b\u7684\u200b<code>X</code>\u200b\u7279\u5f81\u200b\u7684\u200b\u6570\u91cf\u200b\u3002</li> <li><code>output_features</code> - \u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u7684\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\u6570\u91cf\u200b\uff08\u200b\u8fd9\u200b\u5c06\u200b\u4e0e\u200b<code>NUM_CLASSES</code>\u200b\u6216\u200b\u4f60\u200b\u7684\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u4e2d\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\u76f8\u7b49\u200b\uff09\u3002</li> <li><code>hidden_units</code> - \u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6bcf\u4e2a\u200b\u9690\u85cf\u200b\u5c42\u200b\u4f7f\u7528\u200b\u7684\u200b\u9690\u85cf\u200b\u795e\u7ecf\u5143\u200b\u6570\u91cf\u200b\u3002</li> </ul> <p>\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u8981\u200b\u628a\u200b\u8fd9\u4e9b\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u4e00\u4e9b\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\uff08\u200b\u6211\u4eec\u200b\u5728\u200b\u540c\u4e00\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u4e0d\u5fc5\u200b\u518d\u6b21\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u8fd9\u200b\u53ea\u662f\u200b\u4e2a\u200b\u63d0\u9192\u200b\uff09\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e0a\u8ff0\u200b\u8d85\u200b\u53c2\u6570\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u7c7b\u200b\u3002</p>"},{"location":"02_pytorch_classification/#83-pytorch","title":"8.3 \u200b\u4e3a\u200b\u591a\u200b\u7c7b\u522b\u200bPyTorch\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>nn.CrossEntropyLoss()</code> \u200b\u65b9\u6cd5\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u4f7f\u7528\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b0.1\u200b\u7684\u200bSGD\u200b\u6765\u200b\u4f18\u5316\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_4</code> \u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"02_pytorch_classification/#84-pytorch","title":"8.4 \u200b\u83b7\u53d6\u200b\u591a\u200b\u7c7b\u522b\u200bPyTorch\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u5e76\u4e14\u200b\u51c6\u5907\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u4f46\u200b\u5728\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u7528\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u662f\u5426\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\u3002</p>"},{"location":"02_pytorch_classification/#85-pytorch","title":"8.5 \u200b\u4e3a\u200b\u591a\u7c7b\u200bPyTorch\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u6240\u6709\u200b\u7684\u200b\u51c6\u5907\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u6765\u200b\u6539\u8fdb\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u5df2\u7ecf\u200b\u505a\u8fc7\u200b\u5f88\u591a\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8fd9\u90e8\u5206\u200b\u5185\u5bb9\u200b\u4e3b\u8981\u200b\u662f\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u552f\u4e00\u200b\u7684\u200b\u4e0d\u540c\u4e4b\u5904\u200b\u5728\u4e8e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8c03\u6574\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u5c06\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\uff08logits\uff09\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff08\u200b\u4f7f\u7528\u200bsoftmax\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u53d6\u200bsoftmax\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8f93\u51fa\u200b\u7684\u200bargmax\u200b\u6765\u200b\u5f97\u5230\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b<code>epochs=100</code>\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u200b\u6bcf\u200b10\u200b\u4e2a\u200bepoch\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u8bc4\u4f30\u200b\u3002</p>"},{"location":"02_pytorch_classification/#86-pytorch","title":"8.6 \u200b\u4f7f\u7528\u200bPyTorch\u200b\u591a\u200b\u7c7b\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u8bc4\u4f30\u200b\u00b6","text":"<p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u786e\u8ba4\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u53ef\u89c6\u5316\u200b\u3002</p>"},{"location":"02_pytorch_classification/#9","title":"9. \u200b\u66f4\u200b\u591a\u200b\u5206\u7c7b\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u00b6","text":"<p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u6db5\u76d6\u200b\u4e86\u200b\u51e0\u79cd\u200b\u8bc4\u4f30\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff08\u200b\u51c6\u786e\u6027\u200b\u3001\u200b\u635f\u5931\u200b\u548c\u200b\u53ef\u89c6\u5316\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u4f60\u200b\u4f1a\u200b\u9047\u5230\u200b\u7684\u200b\u4e00\u4e9b\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u8d77\u70b9\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6307\u6807\u200b\u6765\u200b\u8bc4\u4f30\u200b\u4f60\u200b\u7684\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4ee5\u4e0b\u200b\u51e0\u79cd\u200b\uff1a</p> \u200b\u6307\u6807\u200b\u540d\u79f0\u200b/\u200b\u8bc4\u4f30\u200b\u65b9\u6cd5\u200b \u200b\u5b9a\u4e49\u200b \u200b\u4ee3\u7801\u200b \u200b\u51c6\u786e\u6027\u200b \u200b\u5728\u200b100\u200b\u4e2a\u200b\u9884\u6d4b\u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u80fd\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u591a\u5c11\u200b\u4e2a\u200b\uff1f\u200b\u4f8b\u5982\u200b\uff0c95%\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u610f\u5473\u7740\u200b\u6a21\u578b\u200b\u80fd\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b95/100\u200b\u4e2a\u200b\u9884\u6d4b\u200b\u3002 <code>torchmetrics.Accuracy()</code> \u200b\u6216\u200b <code>sklearn.metrics.accuracy_score()</code> \u200b\u7cbe\u786e\u5ea6\u200b \u200b\u771f\u200b\u9633\u6027\u200b\u6837\u672c\u200b\u5360\u200b\u6240\u6709\u200b\u6837\u672c\u200b\u7684\u200b\u6bd4\u4f8b\u200b\u3002\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u7cbe\u786e\u5ea6\u200b\u610f\u5473\u7740\u200b\u66f4\u5c11\u200b\u7684\u200b\u5047\u200b\u9633\u6027\u200b\uff08\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u4e3a\u200b1\u200b\u4f46\u200b\u5b9e\u9645\u4e0a\u200b\u5e94\u8be5\u200b\u662f\u200b0\uff09\u3002 <code>torchmetrics.Precision()</code> \u200b\u6216\u200b <code>sklearn.metrics.precision_score()</code> \u200b\u53ec\u56de\u200b\u7387\u200b \u200b\u771f\u200b\u9633\u6027\u200b\u6837\u672c\u200b\u5360\u200b\u6240\u6709\u200b\u771f\u200b\u9633\u6027\u200b\u548c\u200b\u5047\u200b\u9634\u6027\u200b\u6837\u672c\u200b\u7684\u200b\u6bd4\u4f8b\u200b\uff08\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u4e3a\u200b0\u200b\u4f46\u200b\u5b9e\u9645\u4e0a\u200b\u5e94\u8be5\u200b\u662f\u200b1\uff09\u3002\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u53ec\u56de\u200b\u7387\u200b\u610f\u5473\u7740\u200b\u66f4\u5c11\u200b\u7684\u200b\u5047\u200b\u9634\u6027\u200b\u3002 <code>torchmetrics.Recall()</code> \u200b\u6216\u200b <code>sklearn.metrics.recall_score()</code> F1\u200b\u5206\u6570\u200b \u200b\u5c06\u200b\u7cbe\u786e\u5ea6\u200b\u548c\u200b\u53ec\u56de\u200b\u7387\u200b\u7ed3\u5408\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u6307\u6807\u200b\u30021\u200b\u662f\u200b\u6700\u597d\u200b\u7684\u200b\uff0c0\u200b\u662f\u200b\u6700\u5dee\u200b\u7684\u200b\u3002 <code>torchmetrics.F1Score()</code> \u200b\u6216\u200b <code>sklearn.metrics.f1_score()</code> \u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b \u200b\u4ee5\u200b\u8868\u683c\u200b\u65b9\u5f0f\u200b\u6bd4\u8f83\u200b\u9884\u6d4b\u503c\u200b\u548c\u200b\u771f\u5b9e\u200b\u503c\u200b\uff0c\u200b\u5982\u679c\u200b100%\u200b\u6b63\u786e\u200b\uff0c\u200b\u77e9\u9635\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u503c\u200b\u5c06\u200b\u4ece\u200b\u5de6\u4e0a\u89d2\u200b\u5230\u200b\u53f3\u4e0b\u89d2\u200b\uff08\u200b\u5bf9\u89d2\u7ebf\u200b\uff09\u3002 <code>torchmetrics.ConfusionMatrix</code> \u200b\u6216\u200b <code>sklearn.metrics.plot_confusion_matrix()</code> \u200b\u5206\u7c7b\u200b\u62a5\u544a\u200b \u200b\u6536\u96c6\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u4e3b\u8981\u200b\u7684\u200b\u5206\u7c7b\u200b\u6307\u6807\u200b\uff0c\u200b\u5982\u200b\u7cbe\u786e\u5ea6\u200b\u3001\u200b\u53ec\u56de\u200b\u7387\u200b\u548c\u200bF1\u200b\u5206\u6570\u200b\u3002 <code>sklearn.metrics.classification_report()</code> <p>Scikit-Learn\uff08\u200b\u4e00\u4e2a\u200b\u6d41\u884c\u200b\u4e14\u200b\u4e16\u754c\u7ea7\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5e93\u200b\uff09\u200b\u6709\u200b\u8bb8\u591a\u200b\u4e0a\u8ff0\u200b\u6307\u6807\u200b\u7684\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b\u5bfb\u627e\u200b\u7c7b\u4f3c\u200bPyTorch\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200bTorchMetrics\uff0c\u200b\u7279\u522b\u200b\u662f\u200bTorchMetrics\u200b\u5206\u7c7b\u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b<code>torchmetrics.Accuracy</code>\u200b\u6307\u6807\u200b\u3002</p>"},{"location":"02_pytorch_classification/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u96c6\u4e2d\u200b\u5728\u200b\u7ec3\u4e60\u200b\u4e0a\u8ff0\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u7ae0\u8282\u200b\u6216\u200b\u9075\u5faa\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>02\u200b\u7ae0\u8282\u200b\u7684\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b</li> <li>02\u200b\u7ae0\u8282\u200b\u7684\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\u4e4b\u524d\u200b\u67e5\u770b\u200b\u6b64\u200b\u5185\u5bb9\u200b\uff09</li> </ul> <ol> <li>\u200b\u4f7f\u7528\u200bScikit-Learn\u200b\u7684\u200b<code>make_moons()</code>\u200b\u51fd\u6570\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002<ul> <li>\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u4e00\u81f4\u6027\u200b\uff0c\u200b\u6570\u636e\u200b\u96c6\u200b\u5e94\u8be5\u200b\u6709\u200b1000\u200b\u4e2a\u200b\u6837\u672c\u200b\uff0c\u200b\u5e76\u4e14\u200b<code>random_state=42</code>\u3002</li> <li>\u200b\u5c06\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200bPyTorch\u200b\u5f20\u91cf\u200b\u3002\u200b\u4f7f\u7528\u200b<code>train_test_split</code>\u200b\u5c06\u200b\u6570\u636e\u200b\u5206\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff0c\u200b\u5176\u4e2d\u200b80%\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff0c20%\u200b\u4e3a\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</li> </ul> </li> <li>\u200b\u901a\u8fc7\u200b\u5b50\u200b\u7c7b\u5316\u200b<code>nn.Module</code>\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8be5\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u62df\u5408\u200b\u4f60\u200b\u5728\u200b\u7b2c\u200b1\u200b\u6b65\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u3002<ul> <li>\u200b\u53ef\u4ee5\u200b\u81ea\u7531\u200b\u4f7f\u7528\u200b\u4efb\u4f55\u200bPyTorch\u200b\u5c42\u200b\u7684\u200b\u7ec4\u5408\u200b\uff08\u200b\u7ebf\u6027\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\uff09\u3002</li> </ul> </li> <li>\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u9002\u7528\u200b\u4e8e\u200b\u4e8c\u200b\u5206\u7c7b\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u65f6\u200b\u4f7f\u7528\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\uff0c\u200b\u5c06\u200b\u4f60\u200b\u5728\u200b\u7b2c\u200b2\u200b\u6b65\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u62df\u5408\u200b\u5230\u200b\u4f60\u200b\u5728\u200b\u7b2c\u200b1\u200b\u6b65\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u3002<ul> <li>\u200b\u4e3a\u4e86\u200b\u6d4b\u91cf\u200b\u6a21\u578b\u200b\u51c6\u786e\u6027\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u51fd\u6570\u200b\u6216\u200b\u4f7f\u7528\u200bTorchMetrics\u200b\u4e2d\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u51fd\u6570\u200b\u3002</li> <li>\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f4\u5230\u200b\u5176\u200b\u51c6\u786e\u6027\u200b\u8d85\u8fc7\u200b96%\u3002</li> <li>\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u5e94\u8be5\u200b\u6bcf\u200b10\u200b\u4e2a\u200b\u5468\u671f\u200b\u8f93\u51fa\u200b\u4e00\u6b21\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u635f\u5931\u200b\u548c\u200b\u51c6\u786e\u6027\u200b\u3002</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>plot_decision_boundary()</code>\u200b\u51fd\u6570\u200b\u7ed8\u5236\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</li> <li>\u200b\u5728\u200b\u7eaf\u200bPyTorch\u200b\u4e2d\u200b\u590d\u5236\u200bTanh\uff08\u200b\u53cc\u66f2\u200b\u6b63\u5207\u200b\uff09\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002<ul> <li>\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200bML cheatsheet\u200b\u7f51\u7ad9\u200b\u7684\u200b\u516c\u5f0f\u200b\u3002</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200bCS231n\u200b\u7684\u200b\u87ba\u65cb\u200b\u6570\u636e\u200b\u521b\u5efa\u200b\u51fd\u6570\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u591a\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u89c1\u200b\u4e0b\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\uff09\u3002<ul> <li>\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u62df\u5408\u200b\u6570\u636e\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u7ebf\u6027\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\u5c42\u200b\u7684\u200b\u7ec4\u5408\u200b\uff09\u3002</li> <li>\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u591a\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u53ef\u200b\u9009\u200b\u6269\u5c55\u200b\uff1a\u200b\u4f7f\u7528\u200bAdam\u200b\u4f18\u5316\u200b\u5668\u200b\u800c\u200b\u4e0d\u662f\u200bSGD\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u503c\u200b\u4ee5\u200b\u4f7f\u200b\u5176\u200b\u5de5\u4f5c\u200b\uff09\u3002</li> <li>\u200b\u4e3a\u200b\u591a\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5176\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u4ee5\u200b\u8fbe\u5230\u200b\u8d85\u8fc7\u200b95%\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u6027\u200b\uff08\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4efb\u4f55\u200b\u4f60\u200b\u559c\u6b22\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u6d4b\u91cf\u200b\u51fd\u6570\u200b\uff09\u3002</li> <li>\u200b\u5728\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u4e2d\u200b\u7ed8\u5236\u200b\u87ba\u65cb\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u51b3\u7b56\u200b\u8fb9\u754c\u200b\uff0c<code>plot_decision_boundary()</code>\u200b\u51fd\u6570\u200b\u5e94\u8be5\u200b\u4e5f\u200b\u9002\u7528\u200b\u4e8e\u200b\u6b64\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</li> </ul> </li> </ol> <pre># \u200b\u521b\u5efa\u200b\u87ba\u65cb\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u4ee3\u7801\u200b\u6765\u81ea\u200bCS231n\nimport numpy as np\nN = 100 # \u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u7684\u200b\u70b9\u6570\u200b\nD = 2 # \u200b\u7ef4\u5ea6\u200b\nK = 3 # \u200b\u7c7b\u522b\u200b\u6570\u200b\nX = np.zeros((N*K,D)) # \u200b\u6570\u636e\u200b\u77e9\u9635\u200b\uff08\u200b\u6bcf\u884c\u200b = \u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\uff09\ny = np.zeros(N*K, dtype='uint8') # \u200b\u7c7b\u522b\u200b\u6807\u7b7e\u200b\nfor j in range(K):\n  ix = range(N*j,N*(j+1))\n  r = np.linspace(0.0,1,N) # \u200b\u534a\u5f84\u200b\n  t = np.linspace(j*4,(j+1)*4,N) + np.random.randn(N)*0.2 # \u200b\u89d2\u5ea6\u200b\n  X[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n  y[ix] = j\n# \u200b\u8ba9\u200b\u6211\u4eec\u200b\u53ef\u89c6\u5316\u200b\u6570\u636e\u200b\nplt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap=plt.cm.Spectral)\nplt.show()\n</pre>"},{"location":"02_pytorch_classification/","title":"\u989d\u5916\u200b\u8bfe\u7a0b\u200b\u00b6","text":"<ul> <li>\u200b\u5199\u4e0b\u200b3\u200b\u4e2a\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u673a\u5668\u200b\u5206\u7c7b\u200b\u53ef\u80fd\u200b\u6709\u7528\u200b\u7684\u200b\u573a\u666f\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\uff0c\u200b\u5c3d\u60c5\u200b\u53d1\u6325\u200b\u521b\u610f\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6839\u636e\u200b\u8d2d\u4e70\u200b\u91d1\u989d\u200b\u548c\u200b\u8d2d\u4e70\u200b\u5730\u70b9\u200b\u7279\u5f81\u200b\u5c06\u200b\u4fe1\u7528\u5361\u200b\u4ea4\u6613\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u6b3a\u8bc8\u200b\u6216\u975e\u200b\u6b3a\u8bc8\u200b\uff09\u3002</li> <li>\u200b\u7814\u7a76\u200b\u57fa\u4e8e\u200b\u68af\u5ea6\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u5982\u200bSGD\u200b\u6216\u200bAdam\uff09\u200b\u4e2d\u200b\u7684\u200b\u201c\u200b\u52a8\u91cf\u200b\u201d\u200b\u6982\u5ff5\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4ec0\u4e48\u200b\uff1f</li> <li>\u200b\u82b1\u200b10\u200b\u5206\u949f\u200b\u9605\u8bfb\u200b\u4e0d\u540c\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7684\u200bWikipedia\u200b\u9875\u9762\u200b\uff0c\u200b\u4f60\u200b\u80fd\u200b\u5c06\u200b\u5176\u4e2d\u200b\u591a\u5c11\u200b\u4e0e\u200bPyTorch\u200b\u7684\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u5bf9\u5e94\u200b\u8d77\u6765\u200b\uff1f</li> <li>\u200b\u7814\u7a76\u200b\u4f55\u65f6\u200b\u51c6\u786e\u6027\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7cdf\u7cd5\u200b\u7684\u200b\u6307\u6807\u200b\uff08\u200b\u63d0\u793a\u200b\uff1a\u200b\u9605\u8bfb\u200b\"Beyond Accuracy\" by Will Koehrsen\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u60f3\u6cd5\u200b\uff09\u3002</li> <li>\u200b\u89c2\u770b\u200b\uff1a \u200b\u4e3a\u4e86\u200b\u4e86\u89e3\u200b\u6211\u4eec\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5185\u90e8\u200b\u53d1\u751f\u200b\u4e86\u200b\u4ec0\u4e48\u200b\u4ee5\u53ca\u200b\u5b83\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u5b66\u4e60\u200b\u7684\u200b\uff0c\u200b\u89c2\u770b\u200bMIT\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4ecb\u7ecd\u200b\u89c6\u9891\u200b\u3002</li> </ul>"},{"location":"03_pytorch_computer_vision/","title":"03. PyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b | \u200b\u89c2\u770b\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b</p> In\u00a0[1]: Copied! <pre># Import PyTorch\nimport torch\nfrom torch import nn\n\n# Import torchvision \nimport torchvision\nfrom torchvision import datasets\nfrom torchvision.transforms import ToTensor\n\n# Import matplotlib for visualization\nimport matplotlib.pyplot as plt\n\n# Check versions\n# Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11\nprint(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\")\n</pre> # Import PyTorch import torch from torch import nn  # Import torchvision  import torchvision from torchvision import datasets from torchvision.transforms import ToTensor  # Import matplotlib for visualization import matplotlib.pyplot as plt  # Check versions # Note: your PyTorch version shouldn't be lower than 1.10.0 and torchvision version shouldn't be lower than 0.11 print(f\"PyTorch version: {torch.__version__}\\ntorchvision version: {torchvision.__version__}\") <pre>PyTorch version: 2.0.1+cu118\ntorchvision version: 0.15.2+cu118\n</pre> In\u00a0[2]: Copied! <pre># Setup training data\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=ToTensor()\n)\n</pre> # Setup training data train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Setup testing data test_data = datasets.FashionMNIST(     root=\"data\",     train=False, # get test data     download=True,     transform=ToTensor() ) <pre>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 26421880/26421880 [00:01&lt;00:00, 16189161.14it/s]\n</pre> <pre>Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 29515/29515 [00:00&lt;00:00, 269809.67it/s]\n</pre> <pre>Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4422102/4422102 [00:00&lt;00:00, 4950701.58it/s]\n</pre> <pre>Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5148/5148 [00:00&lt;00:00, 4744512.63it/s]</pre> <pre>Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\n</pre> <pre>\n</pre> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u6837\u672c\u200b\u3002</p> In\u00a0[3]: Copied! <pre># See first training sample\nimage, label = train_data[0]\nimage, label\n</pre> # See first training sample image, label = train_data[0] image, label Out[3]: <pre>(tensor([[[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.0000, 0.0510,\n           0.2863, 0.0000, 0.0000, 0.0039, 0.0157, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0118, 0.0000, 0.1412, 0.5333,\n           0.4980, 0.2431, 0.2118, 0.0000, 0.0000, 0.0000, 0.0039, 0.0118,\n           0.0157, 0.0000, 0.0000, 0.0118],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0235, 0.0000, 0.4000, 0.8000,\n           0.6902, 0.5255, 0.5647, 0.4824, 0.0902, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0471, 0.0392, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6078, 0.9255,\n           0.8118, 0.6980, 0.4196, 0.6118, 0.6314, 0.4275, 0.2510, 0.0902,\n           0.3020, 0.5098, 0.2824, 0.0588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0039, 0.0000, 0.2706, 0.8118, 0.8745,\n           0.8549, 0.8471, 0.8471, 0.6392, 0.4980, 0.4745, 0.4784, 0.5725,\n           0.5529, 0.3451, 0.6745, 0.2588],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0039, 0.0039, 0.0000, 0.7843, 0.9098, 0.9098,\n           0.9137, 0.8980, 0.8745, 0.8745, 0.8431, 0.8353, 0.6431, 0.4980,\n           0.4824, 0.7686, 0.8980, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7176, 0.8824, 0.8471,\n           0.8745, 0.8941, 0.9216, 0.8902, 0.8784, 0.8706, 0.8784, 0.8667,\n           0.8745, 0.9608, 0.6784, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.7569, 0.8941, 0.8549,\n           0.8353, 0.7765, 0.7059, 0.8314, 0.8235, 0.8275, 0.8353, 0.8745,\n           0.8627, 0.9529, 0.7922, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0039, 0.0118, 0.0000, 0.0471, 0.8588, 0.8627, 0.8314,\n           0.8549, 0.7529, 0.6627, 0.8902, 0.8157, 0.8549, 0.8784, 0.8314,\n           0.8863, 0.7725, 0.8196, 0.2039],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0235, 0.0000, 0.3882, 0.9569, 0.8706, 0.8627,\n           0.8549, 0.7961, 0.7765, 0.8667, 0.8431, 0.8353, 0.8706, 0.8627,\n           0.9608, 0.4667, 0.6549, 0.2196],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0157, 0.0000, 0.0000, 0.2157, 0.9255, 0.8941, 0.9020,\n           0.8941, 0.9412, 0.9098, 0.8353, 0.8549, 0.8745, 0.9176, 0.8510,\n           0.8510, 0.8196, 0.3608, 0.0000],\n          [0.0000, 0.0000, 0.0039, 0.0157, 0.0235, 0.0275, 0.0078, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.9294, 0.8863, 0.8510, 0.8745,\n           0.8706, 0.8588, 0.8706, 0.8667, 0.8471, 0.8745, 0.8980, 0.8431,\n           0.8549, 1.0000, 0.3020, 0.0000],\n          [0.0000, 0.0118, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.2431, 0.5686, 0.8000, 0.8941, 0.8118, 0.8353, 0.8667,\n           0.8549, 0.8157, 0.8275, 0.8549, 0.8784, 0.8745, 0.8588, 0.8431,\n           0.8784, 0.9569, 0.6235, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0706, 0.1725, 0.3216, 0.4196,\n           0.7412, 0.8941, 0.8627, 0.8706, 0.8510, 0.8863, 0.7843, 0.8039,\n           0.8275, 0.9020, 0.8784, 0.9176, 0.6902, 0.7373, 0.9804, 0.9725,\n           0.9137, 0.9333, 0.8431, 0.0000],\n          [0.0000, 0.2235, 0.7333, 0.8157, 0.8784, 0.8667, 0.8784, 0.8157,\n           0.8000, 0.8392, 0.8157, 0.8196, 0.7843, 0.6235, 0.9608, 0.7569,\n           0.8078, 0.8745, 1.0000, 1.0000, 0.8667, 0.9176, 0.8667, 0.8275,\n           0.8627, 0.9098, 0.9647, 0.0000],\n          [0.0118, 0.7922, 0.8941, 0.8784, 0.8667, 0.8275, 0.8275, 0.8392,\n           0.8039, 0.8039, 0.8039, 0.8627, 0.9412, 0.3137, 0.5882, 1.0000,\n           0.8980, 0.8667, 0.7373, 0.6039, 0.7490, 0.8235, 0.8000, 0.8196,\n           0.8706, 0.8941, 0.8824, 0.0000],\n          [0.3843, 0.9137, 0.7765, 0.8235, 0.8706, 0.8980, 0.8980, 0.9176,\n           0.9765, 0.8627, 0.7608, 0.8431, 0.8510, 0.9451, 0.2549, 0.2863,\n           0.4157, 0.4588, 0.6588, 0.8588, 0.8667, 0.8431, 0.8510, 0.8745,\n           0.8745, 0.8784, 0.8980, 0.1137],\n          [0.2941, 0.8000, 0.8314, 0.8000, 0.7569, 0.8039, 0.8275, 0.8824,\n           0.8471, 0.7255, 0.7725, 0.8078, 0.7765, 0.8353, 0.9412, 0.7647,\n           0.8902, 0.9608, 0.9373, 0.8745, 0.8549, 0.8314, 0.8196, 0.8706,\n           0.8627, 0.8667, 0.9020, 0.2627],\n          [0.1882, 0.7961, 0.7176, 0.7608, 0.8353, 0.7725, 0.7255, 0.7451,\n           0.7608, 0.7529, 0.7922, 0.8392, 0.8588, 0.8667, 0.8627, 0.9255,\n           0.8824, 0.8471, 0.7804, 0.8078, 0.7294, 0.7098, 0.6941, 0.6745,\n           0.7098, 0.8039, 0.8078, 0.4510],\n          [0.0000, 0.4784, 0.8588, 0.7569, 0.7020, 0.6706, 0.7176, 0.7686,\n           0.8000, 0.8235, 0.8353, 0.8118, 0.8275, 0.8235, 0.7843, 0.7686,\n           0.7608, 0.7490, 0.7647, 0.7490, 0.7765, 0.7529, 0.6902, 0.6118,\n           0.6549, 0.6941, 0.8235, 0.3608],\n          [0.0000, 0.0000, 0.2902, 0.7412, 0.8314, 0.7490, 0.6863, 0.6745,\n           0.6863, 0.7098, 0.7255, 0.7373, 0.7412, 0.7373, 0.7569, 0.7765,\n           0.8000, 0.8196, 0.8235, 0.8235, 0.8275, 0.7373, 0.7373, 0.7608,\n           0.7529, 0.8471, 0.6667, 0.0000],\n          [0.0078, 0.0000, 0.0000, 0.0000, 0.2588, 0.7843, 0.8706, 0.9294,\n           0.9373, 0.9490, 0.9647, 0.9529, 0.9569, 0.8667, 0.8627, 0.7569,\n           0.7490, 0.7020, 0.7137, 0.7137, 0.7098, 0.6902, 0.6510, 0.6588,\n           0.3882, 0.2275, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1569,\n           0.2392, 0.1725, 0.2824, 0.1608, 0.1373, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000],\n          [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n           0.0000, 0.0000, 0.0000, 0.0000]]]),\n 9)</pre> In\u00a0[4]: Copied! <pre># What's the shape of the image?\nimage.shape\n</pre> # What's the shape of the image? image.shape Out[4]: <pre>torch.Size([1, 28, 28])</pre> <p>\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[1, 28, 28]</code>\uff0c\u200b\u66f4\u200b\u5177\u4f53\u5730\u8bf4\u200b\uff1a</p> <pre><code>[\u200b\u989c\u8272\u200b\u901a\u9053\u200b=1, \u200b\u9ad8\u5ea6\u200b=28, \u200b\u5bbd\u5ea6\u200b=28]\n</code></pre> <p><code>\u200b\u989c\u8272\u200b\u901a\u9053\u200b=1</code> \u200b\u610f\u5473\u7740\u200b\u56fe\u50cf\u200b\u662f\u200b\u7070\u5ea6\u200b\u56fe\u50cf\u200b\u3002</p> <p> \u200b\u4e0d\u540c\u200b\u7684\u200b\u95ee\u9898\u200b\u4f1a\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002\u200b\u4f46\u200b\u524d\u63d0\u200b\u662f\u200b\uff1a\u200b\u5c06\u200b\u6570\u636e\u7f16\u7801\u200b\u4e3a\u200b\u6570\u5b57\u200b\uff0c\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u4ee5\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u6570\u5b57\u200b\u4e2d\u200b\u5bfb\u627e\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6a21\u5f0f\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b <code>\u200b\u989c\u8272\u200b\u901a\u9053\u200b=3</code>\uff0c\u200b\u56fe\u50cf\u200b\u5c06\u200b\u5305\u542b\u200b\u7ea2\u8272\u200b\u3001\u200b\u7eff\u8272\u200b\u548c\u200b\u84dd\u8272\u200b\u7684\u200b\u50cf\u7d20\u200b\u503c\u200b\uff08\u200b\u8fd9\u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200bRGB \u200b\u8272\u5f69\u200b\u6a21\u578b\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u5f20\u91cf\u200b\u7684\u200b\u987a\u5e8f\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b <code>CHW</code>\uff08\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u3001\u200b\u9ad8\u5ea6\u200b\u3001\u200b\u5bbd\u5ea6\u200b\uff09\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u56fe\u50cf\u200b\u5e94\u8be5\u200b\u8868\u793a\u200b\u4e3a\u200b <code>CHW</code>\uff08\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u5728\u200b\u524d\u200b\uff09\u200b\u8fd8\u662f\u200b <code>HWC</code>\uff08\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u5728\u200b\u540e\u200b\uff09\u200b\u5b58\u5728\u200b\u4e89\u8bae\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u8fd8\u200b\u4f1a\u200b\u770b\u5230\u200b <code>NCHW</code> \u200b\u548c\u200b <code>NHWC</code> \u200b\u683c\u5f0f\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>N</code> \u200b\u4ee3\u8868\u200b\u56fe\u50cf\u200b\u7684\u200b\u6570\u91cf\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u4e00\u4e2a\u200b <code>batch_size=32</code>\uff0c\u200b\u4f60\u200b\u7684\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b\u53ef\u80fd\u200b\u662f\u200b <code>[32, 1, 28, 28]</code>\u3002\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u8ba8\u8bba\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3002</p> <p>PyTorch \u200b\u901a\u5e38\u200b\u63a5\u53d7\u200b <code>NCHW</code>\uff08\u200b\u901a\u9053\u200b\u5728\u200b\u524d\u200b\uff09\u200b\u4f5c\u4e3a\u200b\u8bb8\u591a\u200b\u64cd\u4f5c\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0cPyTorch \u200b\u4e5f\u200b\u89e3\u91ca\u200b\u8bf4\u200b <code>NHWC</code>\uff08\u200b\u901a\u9053\u200b\u5728\u200b\u540e\u200b\uff09\u200b\u6027\u80fd\u200b\u66f4\u597d\u200b\uff0c\u200b\u5e76\u4e14\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\u3002</p> <p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6a21\u578b\u200b\u76f8\u5bf9\u200b\u8f83\u200b\u5c0f\u200b\uff0c\u200b\u8fd9\u200b\u4e0d\u4f1a\u200b\u4ea7\u751f\u200b\u592a\u200b\u5927\u200b\u5f71\u54cd\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u5904\u7406\u200b\u66f4\u5927\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u96c6\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65f6\u200b\uff08\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u4e9b\u200b\uff09\uff0c\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u5f88\u200b\u91cd\u8981\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u6570\u636e\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u5f62\u72b6\u200b\u3002</p> In\u00a0[5]: Copied! <pre># How many samples are there? \nlen(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets)\n</pre> # How many samples are there?  len(train_data.data), len(train_data.targets), len(test_data.data), len(test_data.targets) Out[5]: <pre>(60000, 60000, 10000, 10000)</pre> <p>\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u6709\u200b60,000\u200b\u4e2a\u200b\u8bad\u7ec3\u6837\u672c\u200b\u548c\u200b10,000\u200b\u4e2a\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u3002</p> <p>\u200b\u6709\u200b\u54ea\u4e9b\u200b\u7c7b\u522b\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>.classes</code>\u200b\u5c5e\u6027\u200b\u6765\u200b\u627e\u5230\u200b\u8fd9\u4e9b\u200b\u7c7b\u522b\u200b\u3002</p> In\u00a0[6]: Copied! <pre># See classes\nclass_names = train_data.classes\nclass_names\n</pre> # See classes class_names = train_data.classes class_names Out[6]: <pre>['T-shirt/top',\n 'Trouser',\n 'Pullover',\n 'Dress',\n 'Coat',\n 'Sandal',\n 'Shirt',\n 'Sneaker',\n 'Bag',\n 'Ankle boot']</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u6709\u200b10\u200b\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u8863\u7269\u200b\u7c7b\u522b\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b10\u200b\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u76f4\u89c2\u200b\u5730\u200b\u770b\u770b\u200b\u3002</p> In\u00a0[7]: Copied! <pre>import matplotlib.pyplot as plt\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape}\")\nplt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width)\nplt.title(label);\n</pre> import matplotlib.pyplot as plt image, label = train_data[0] print(f\"Image shape: {image.shape}\") plt.imshow(image.squeeze()) # image shape is [1, 28, 28] (colour channels, height, width) plt.title(label); <pre>Image shape: torch.Size([1, 28, 28])\n</pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>plt.imshow()</code> \u200b\u7684\u200b <code>cmap</code> \u200b\u53c2\u6570\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7070\u5ea6\u200b\u56fe\u50cf\u200b\u3002</p> In\u00a0[8]: Copied! <pre>plt.imshow(image.squeeze(), cmap=\"gray\")\nplt.title(class_names[label]);\n</pre> plt.imshow(image.squeeze(), cmap=\"gray\") plt.title(class_names[label]); <p>\u200b\u6f02\u4eae\u200b\uff0c\u200b\u597d\u200b\u5427\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u4e00\u4e2a\u200b\u50cf\u7d20\u200b\u5316\u200b\u7684\u200b\u7070\u5ea6\u200b\u811a\u8e1d\u200b\u9774\u6240\u80fd\u200b\u8fbe\u5230\u200b\u7684\u200b\u90a3\u6837\u200b\u7f8e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u518d\u200b\u6765\u770b\u200b\u51e0\u4e2a\u200b\u3002</p> In\u00a0[9]: Copied! <pre># Plot more images\ntorch.manual_seed(42)\nfig = plt.figure(figsize=(9, 9))\nrows, cols = 4, 4\nfor i in range(1, rows * cols + 1):\n    random_idx = torch.randint(0, len(train_data), size=[1]).item()\n    img, label = train_data[random_idx]\n    fig.add_subplot(rows, cols, i)\n    plt.imshow(img.squeeze(), cmap=\"gray\")\n    plt.title(class_names[label])\n    plt.axis(False);\n</pre> # Plot more images torch.manual_seed(42) fig = plt.figure(figsize=(9, 9)) rows, cols = 4, 4 for i in range(1, rows * cols + 1):     random_idx = torch.randint(0, len(train_data), size=[1]).item()     img, label = train_data[random_idx]     fig.add_subplot(rows, cols, i)     plt.imshow(img.squeeze(), cmap=\"gray\")     plt.title(class_names[label])     plt.axis(False); <p>\u200b\u55ef\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u770b\u8d77\u6765\u200b\u4e0d\u592a\u200b\u7f8e\u89c2\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u5373\u5c06\u200b\u5b66\u4e60\u200b\u7684\u200b\u5982\u4f55\u200b\u4e3a\u6b64\u200b\u6570\u636e\u200b\u96c6\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u5219\u200b\uff0c\u200b\u5c06\u200b\u9002\u7528\u200b\u4e8e\u200b\u5e7f\u6cdb\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u5c31\u662f\u200b\u5229\u7528\u200b\u50cf\u7d20\u200b\u503c\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ece\u4e2d\u200b\u5bfb\u627e\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5e94\u7528\u200b\u4e8e\u200b\u672a\u6765\u200b\u7684\u200b\u50cf\u7d20\u200b\u503c\u200b\u3002</p> <p>\u200b\u800c\u4e14\u200b\uff0c\u200b\u5373\u4f7f\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u5c0f\u578b\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u662f\u200b\u7684\u200b\uff0c\u200b\u5373\u4f7f\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c60,000\u200b\u5f20\u200b\u56fe\u50cf\u200b\u4e5f\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u76f8\u5f53\u200b\u5c0f\u200b\u7684\u200b\uff09\uff0c\u200b\u4f60\u200b\u80fd\u200b\u7f16\u5199\u7a0b\u5e8f\u200b\u5bf9\u200b\u6bcf\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u5417\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u5927\u6982\u200b\u53ef\u4ee5\u200b\u505a\u5230\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u200b\u8ba4\u4e3a\u200b\u7528\u200bPyTorch\u200b\u7f16\u5199\u200b\u6a21\u578b\u200b\u4f1a\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u4e0a\u8ff0\u200b\u6570\u636e\u200b\u4ec5\u7528\u200b\u76f4\u7ebf\u200b\uff08\u200b\u7ebf\u6027\u200b\uff09\u200b\u5c31\u200b\u80fd\u200b\u5efa\u6a21\u200b\u5417\u200b\uff1f\u200b\u8fd8\u662f\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u4e5f\u200b\u9700\u8981\u200b\u975e\u200b\u76f4\u7ebf\u200b\uff08\u200b\u975e\u7ebf\u6027\u200b\uff09\u200b\u7684\u200b\u7ebf\u6761\u200b\uff1f</p> In\u00a0[10]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Setup the batch size hyperparameter\nBATCH_SIZE = 32\n\n# Turn datasets into iterables (batches)\ntrain_dataloader = DataLoader(train_data, # dataset to turn into iterable\n    batch_size=BATCH_SIZE, # how many samples per batch? \n    shuffle=True # shuffle data every epoch?\n)\n\ntest_dataloader = DataLoader(test_data,\n    batch_size=BATCH_SIZE,\n    shuffle=False # don't necessarily have to shuffle the testing data\n)\n\n# Let's check out what we've created\nprint(f\"Dataloaders: {train_dataloader, test_dataloader}\") \nprint(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\")\nprint(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\")\n</pre> from torch.utils.data import DataLoader  # Setup the batch size hyperparameter BATCH_SIZE = 32  # Turn datasets into iterables (batches) train_dataloader = DataLoader(train_data, # dataset to turn into iterable     batch_size=BATCH_SIZE, # how many samples per batch?      shuffle=True # shuffle data every epoch? )  test_dataloader = DataLoader(test_data,     batch_size=BATCH_SIZE,     shuffle=False # don't necessarily have to shuffle the testing data )  # Let's check out what we've created print(f\"Dataloaders: {train_dataloader, test_dataloader}\")  print(f\"Length of train dataloader: {len(train_dataloader)} batches of {BATCH_SIZE}\") print(f\"Length of test dataloader: {len(test_dataloader)} batches of {BATCH_SIZE}\") <pre>Dataloaders: (&lt;torch.utils.data.dataloader.DataLoader object at 0x7fc991463cd0&gt;, &lt;torch.utils.data.dataloader.DataLoader object at 0x7fc991475120&gt;)\nLength of train dataloader: 1875 batches of 32\nLength of test dataloader: 313 batches of 32\n</pre> In\u00a0[11]: Copied! <pre># Check out what's inside the training dataloader\ntrain_features_batch, train_labels_batch = next(iter(train_dataloader))\ntrain_features_batch.shape, train_labels_batch.shape\n</pre> # Check out what's inside the training dataloader train_features_batch, train_labels_batch = next(iter(train_dataloader)) train_features_batch.shape, train_labels_batch.shape Out[11]: <pre>(torch.Size([32, 1, 28, 28]), torch.Size([32]))</pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u68c0\u67e5\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\u770b\u5230\u200b\u6570\u636e\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> In\u00a0[12]: Copied! <pre># Show a sample\ntorch.manual_seed(42)\nrandom_idx = torch.randint(0, len(train_features_batch), size=[1]).item()\nimg, label = train_features_batch[random_idx], train_labels_batch[random_idx]\nplt.imshow(img.squeeze(), cmap=\"gray\")\nplt.title(class_names[label])\nplt.axis(\"Off\");\nprint(f\"Image size: {img.shape}\")\nprint(f\"Label: {label}, label size: {label.shape}\")\n</pre> # Show a sample torch.manual_seed(42) random_idx = torch.randint(0, len(train_features_batch), size=[1]).item() img, label = train_features_batch[random_idx], train_labels_batch[random_idx] plt.imshow(img.squeeze(), cmap=\"gray\") plt.title(class_names[label]) plt.axis(\"Off\"); print(f\"Image size: {img.shape}\") print(f\"Label: {label}, label size: {label.shape}\") <pre>Image size: torch.Size([1, 28, 28])\nLabel: 6, label size: torch.Size([])\n</pre> In\u00a0[13]: Copied! <pre># Create a flatten layer\nflatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)\n\n# Get a single sample\nx = train_features_batch[0]\n\n# Flatten the sample\noutput = flatten_model(x) # perform forward pass\n\n# Print out what happened\nprint(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")\n\n# Try uncommenting below and see what happens\n#print(x)\n#print(output)\n</pre> # Create a flatten layer flatten_model = nn.Flatten() # all nn modules function as a model (can do a forward pass)  # Get a single sample x = train_features_batch[0]  # Flatten the sample output = flatten_model(x) # perform forward pass  # Print out what happened print(f\"Shape before flattening: {x.shape} -&gt; [color_channels, height, width]\") print(f\"Shape after flattening: {output.shape} -&gt; [color_channels, height*width]\")  # Try uncommenting below and see what happens #print(x) #print(output) <pre>Shape before flattening: torch.Size([1, 28, 28]) -&gt; [color_channels, height, width]\nShape after flattening: torch.Size([1, 784]) -&gt; [color_channels, height*width]\n</pre> <p><code>nn.Flatten()</code> \u200b\u5c42\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u5f62\u72b6\u200b\u4ece\u200b <code>[\u200b\u989c\u8272\u200b\u901a\u9053\u200b, \u200b\u9ad8\u5ea6\u200b, \u200b\u5bbd\u5ea6\u200b]</code> \u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>[\u200b\u989c\u8272\u200b\u901a\u9053\u200b, \u200b\u9ad8\u5ea6\u200b*\u200b\u5bbd\u5ea6\u200b]</code>\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5c06\u200b\u50cf\u7d20\u200b\u6570\u636e\u200b\u4ece\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u7ef4\u5ea6\u200b\u8f6c\u6362\u6210\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u957f\u957f\u7684\u200b\u7279\u5f81\u5411\u91cf\u200b\u3002</p> <p>\u200b\u800c\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u559c\u6b22\u200b\u5b83\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u662f\u200b\u7279\u5f81\u5411\u91cf\u200b\u7684\u200b\u5f62\u5f0f\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>nn.Flatten()</code> \u200b\u4f5c\u4e3a\u200b\u7b2c\u4e00\u5c42\u200b\u6765\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[14]: Copied! <pre>from torch import nn\nclass FashionMNISTModelV0(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # neural networks like their inputs in vector form\n            nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)\n            nn.Linear(in_features=hidden_units, out_features=output_shape)\n        )\n    \n    def forward(self, x):\n        return self.layer_stack(x)\n</pre> from torch import nn class FashionMNISTModelV0(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # neural networks like their inputs in vector form             nn.Linear(in_features=input_shape, out_features=hidden_units), # in_features = number of features in a data sample (784 pixels)             nn.Linear(in_features=hidden_units, out_features=output_shape)         )          def forward(self, x):         return self.layer_stack(x) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u7684\u200b\u57fa\u7ebf\u200b\u6a21\u578b\u200b\u7c7b\u200b\uff0c\u200b\u73b0\u5728\u200b\u6765\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u4ee5\u4e0b\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li><code>input_shape=784</code> - \u200b\u8fd9\u662f\u200b\u8f93\u5165\u200b\u6a21\u578b\u200b\u7684\u200b\u7279\u5f81\u200b\u6570\u91cf\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u50cf\u7d20\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u7279\u5f81\u200b\uff08\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u4e3a\u200b 28 \u200b\u50cf\u7d20\u200b\u9ad8\u200b x 28 \u200b\u50cf\u7d20\u200b\u5bbd\u200b = 784 \u200b\u4e2a\u200b\u7279\u5f81\u200b\uff09\u3002</li> <li><code>hidden_units=10</code> - \u200b\u9690\u85cf\u200b\u5c42\u4e2d\u200b\u7684\u200b\u5355\u5143\u200b/\u200b\u795e\u7ecf\u5143\u200b\u6570\u91cf\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6570\u5b57\u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u610f\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u6a21\u578b\u200b\u8f83\u200b\u5c0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u4ece\u200b <code>10</code> \u200b\u5f00\u59cb\u200b\u3002</li> <li><code>output_shape=len(class_names)</code> - \u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e3a\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u795e\u7ecf\u5143\u200b\u3002</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u6682\u65f6\u200b\u5c06\u200b\u5176\u200b\u53d1\u9001\u5230\u200b CPU\uff08\u200b\u6211\u4eec\u200b\u5f88\u5feb\u200b\u4f1a\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u6bd4\u8f83\u200b <code>model_0</code> \u200b\u5728\u200b CPU \u200b\u548c\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u7684\u200b\u6027\u80fd\u200b\uff09\u3002</p> In\u00a0[15]: Copied! <pre>torch.manual_seed(42)\n\n# Need to setup model with input parameters\nmodel_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)\n    hidden_units=10, # how many units in the hiden layer\n    output_shape=len(class_names) # one for every class\n)\nmodel_0.to(\"cpu\") # keep model on CPU to begin with\n</pre> torch.manual_seed(42)  # Need to setup model with input parameters model_0 = FashionMNISTModelV0(input_shape=784, # one for every pixel (28x28)     hidden_units=10, # how many units in the hiden layer     output_shape=len(class_names) # one for every class ) model_0.to(\"cpu\") # keep model on CPU to begin with  Out[15]: <pre>FashionMNISTModelV0(\n  (layer_stack): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=784, out_features=10, bias=True)\n    (2): Linear(in_features=10, out_features=10, bias=True)\n  )\n)</pre> In\u00a0[16]: Copied! <pre>import requests\nfrom pathlib import Path \n\n# Download helper functions from Learn PyTorch repo (if not already downloaded)\nif Path(\"helper_functions.py\").is_file():\n  print(\"helper_functions.py already exists, skipping download\")\nelse:\n  print(\"Downloading helper_functions.py\")\n  # Note: you need the \"raw\" GitHub URL for this to work\n  request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n  with open(\"helper_functions.py\", \"wb\") as f:\n    f.write(request.content)\n</pre> import requests from pathlib import Path   # Download helper functions from Learn PyTorch repo (if not already downloaded) if Path(\"helper_functions.py\").is_file():   print(\"helper_functions.py already exists, skipping download\") else:   print(\"Downloading helper_functions.py\")   # Note: you need the \"raw\" GitHub URL for this to work   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")   with open(\"helper_functions.py\", \"wb\") as f:     f.write(request.content) <pre>Downloading helper_functions.py\n</pre> In\u00a0[17]: Copied! <pre># Import accuracy metric\nfrom helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places\noptimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n</pre> # Import accuracy metric from helper_functions import accuracy_fn # Note: could also use torchmetrics.Accuracy(task = 'multiclass', num_classes=len(class_names)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() # this is also called \"criterion\"/\"cost function\" in some places optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1) In\u00a0[18]: Copied! <pre>from timeit import default_timer as timer \ndef print_train_time(start: float, end: float, device: torch.device = None):\n    \"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format). \n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"Train time on {device}: {total_time:.3f} seconds\")\n    return total_time\n</pre> from timeit import default_timer as timer  def print_train_time(start: float, end: float, device: torch.device = None):     \"\"\"Prints difference between start and end time.      Args:         start (float): Start time of computation (preferred in timeit format).          end (float): End time of computation.         device ([type], optional): Device that compute is running on. Defaults to None.      Returns:         float: time between start and end in seconds (higher is longer).     \"\"\"     total_time = end - start     print(f\"Train time on {device}: {total_time:.3f} seconds\")     return total_time In\u00a0[19]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# Set the seed and start the timer\ntorch.manual_seed(42)\ntrain_time_start_on_cpu = timer()\n\n# Set the number of epochs (we'll keep this small for faster training times)\nepochs = 3\n\n# Create training and testing loop\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n-------\")\n    ### Training\n    train_loss = 0\n    # Add a loop to loop through training batches\n    for batch, (X, y) in enumerate(train_dataloader):\n        model_0.train() \n        # 1. Forward pass\n        y_pred = model_0(X)\n\n        # 2. Calculate loss (per batch)\n        loss = loss_fn(y_pred, y)\n        train_loss += loss # accumulatively add up the loss per epoch \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Print out how many samples have been seen\n        if batch % 400 == 0:\n            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n\n    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n    train_loss /= len(train_dataloader)\n    \n    ### Testing\n    # Setup variables for accumulatively adding up loss and accuracy \n    test_loss, test_acc = 0, 0 \n    model_0.eval()\n    with torch.inference_mode():\n        for X, y in test_dataloader:\n            # 1. Forward pass\n            test_pred = model_0(X)\n           \n            # 2. Calculate loss (accumatively)\n            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n\n            # 3. Calculate accuracy (preds need to be same as y_true)\n            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n        \n        # Calculations on test metrics need to happen inside torch.inference_mode()\n        # Divide total test loss by length of test dataloader (per batch)\n        test_loss /= len(test_dataloader)\n\n        # Divide total accuracy by length of test dataloader (per batch)\n        test_acc /= len(test_dataloader)\n\n    ## Print out what's happening\n    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n\n# Calculate training time      \ntrain_time_end_on_cpu = timer()\ntotal_train_time_model_0 = print_train_time(start=train_time_start_on_cpu, \n                                           end=train_time_end_on_cpu,\n                                           device=str(next(model_0.parameters()).device))\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # Set the seed and start the timer torch.manual_seed(42) train_time_start_on_cpu = timer()  # Set the number of epochs (we'll keep this small for faster training times) epochs = 3  # Create training and testing loop for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n-------\")     ### Training     train_loss = 0     # Add a loop to loop through training batches     for batch, (X, y) in enumerate(train_dataloader):         model_0.train()          # 1. Forward pass         y_pred = model_0(X)          # 2. Calculate loss (per batch)         loss = loss_fn(y_pred, y)         train_loss += loss # accumulatively add up the loss per epoch           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Print out how many samples have been seen         if batch % 400 == 0:             print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")      # Divide total train loss by length of train dataloader (average loss per batch per epoch)     train_loss /= len(train_dataloader)          ### Testing     # Setup variables for accumulatively adding up loss and accuracy      test_loss, test_acc = 0, 0      model_0.eval()     with torch.inference_mode():         for X, y in test_dataloader:             # 1. Forward pass             test_pred = model_0(X)                         # 2. Calculate loss (accumatively)             test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch              # 3. Calculate accuracy (preds need to be same as y_true)             test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))                  # Calculations on test metrics need to happen inside torch.inference_mode()         # Divide total test loss by length of test dataloader (per batch)         test_loss /= len(test_dataloader)          # Divide total accuracy by length of test dataloader (per batch)         test_acc /= len(test_dataloader)      ## Print out what's happening     print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")  # Calculate training time       train_time_end_on_cpu = timer() total_train_time_model_0 = print_train_time(start=train_time_start_on_cpu,                                             end=train_time_end_on_cpu,                                            device=str(next(model_0.parameters()).device)) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.59039 | Test loss: 0.50954, Test acc: 82.04%\n\nEpoch: 1\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.47633 | Test loss: 0.47989, Test acc: 83.20%\n\nEpoch: 2\n-------\nLooked at 0/60000 samples\nLooked at 12800/60000 samples\nLooked at 25600/60000 samples\nLooked at 38400/60000 samples\nLooked at 51200/60000 samples\n\nTrain loss: 0.45503 | Test loss: 0.47664, Test acc: 83.43%\n\nTrain time on cpu: 32.349 seconds\n</pre> <p>\u200b\u4e0d\u9519\u200b\uff01\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u57fa\u7ebf\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u597d\u200b\u3002</p> <p>\u200b\u5373\u4fbf\u200b\u53ea\u662f\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4e5f\u200b\u6ca1\u6709\u200b\u82b1\u8d39\u200b\u592a\u200b\u591a\u200b\u65f6\u95f4\u200b\uff0c\u200b\u4e0d\u200b\u77e5\u9053\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u4f1a\u200b\u4e0d\u4f1a\u200b\u66f4\u5feb\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\u6765\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[20]: Copied! <pre>torch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn):\n    \"\"\"Returns a dictionary containing the results of model predicting on data_loader.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Make predictions with the model\n            y_pred = model(X)\n            \n            # Accumulate the loss and accuracy values per batch\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, \n                                y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)\n        \n        # Scale loss and acc to find the average loss/acc per batch\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n        \n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 0 results on test dataset\nmodel_0_results = eval_model(model=model_0, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn\n)\nmodel_0_results\n</pre> torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn):     \"\"\"Returns a dictionary containing the results of model predicting on data_loader.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Make predictions with the model             y_pred = model(X)                          # Accumulate the loss and accuracy values per batch             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # For accuracy, need the prediction labels (logits -&gt; pred_prob -&gt; pred_labels)                  # Scale loss and acc to find the average loss/acc per batch         loss /= len(data_loader)         acc /= len(data_loader)              return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 0 results on test dataset model_0_results = eval_model(model=model_0, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn ) model_0_results Out[20]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}</pre> <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u8fd9\u4e2a\u200b\u5b57\u5178\u200b\u6765\u200b\u5c06\u200b\u57fa\u7ebf\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p> In\u00a0[21]: Copied! <pre># Setup device agnostic code\nimport torch\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code import torch device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[21]: <pre>'cuda'</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u518d\u200b\u6765\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[22]: Copied! <pre># Create a model with non-linear and linear layers\nclass FashionMNISTModelV1(nn.Module):\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.layer_stack = nn.Sequential(\n            nn.Flatten(), # flatten inputs into single vector\n            nn.Linear(in_features=input_shape, out_features=hidden_units),\n            nn.ReLU(),\n            nn.Linear(in_features=hidden_units, out_features=output_shape),\n            nn.ReLU()\n        )\n    \n    def forward(self, x: torch.Tensor):\n        return self.layer_stack(x)\n</pre> # Create a model with non-linear and linear layers class FashionMNISTModelV1(nn.Module):     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.layer_stack = nn.Sequential(             nn.Flatten(), # flatten inputs into single vector             nn.Linear(in_features=input_shape, out_features=hidden_units),             nn.ReLU(),             nn.Linear(in_features=hidden_units, out_features=output_shape),             nn.ReLU()         )          def forward(self, x: torch.Tensor):         return self.layer_stack(x) <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b\u4e4b\u524d\u200b\u4f7f\u7528\u200b\u7684\u200b\u76f8\u540c\u200b\u8bbe\u7f6e\u200b\u6765\u200b\u5b9e\u4f8b\u200b\u5316\u5b83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b <code>input_shape=784</code>\uff08\u200b\u7b49\u4e8e\u200b\u6211\u4eec\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u7684\u200b\u7279\u5f81\u200b\u6570\u91cf\u200b\uff09\uff0c<code>hidden_units=10</code>\uff08\u200b\u4ece\u5c0f\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u57fa\u7ebf\u200b\u6a21\u578b\u200b\u76f8\u540c\u200b\uff09\u200b\u548c\u200b <code>output_shape=len(class_names)</code>\uff08\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u5355\u5143\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bf7\u200b\u6ce8\u610f\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4fdd\u6301\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u8bbe\u7f6e\u200b\u76f8\u540c\u200b\uff0c\u200b\u9664\u4e86\u200b\u4e00\u4e2a\u200b\u53d8\u5316\u200b\uff1a\u200b\u6dfb\u52a0\u200b\u975e\u7ebf\u6027\u200b\u5c42\u200b\u3002\u200b\u8fd9\u662f\u200b\u8fdb\u884c\u200b\u4e00\u7cfb\u5217\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u6807\u51c6\u200b\u505a\u6cd5\u200b\uff0c\u200b\u6539\u53d8\u200b\u4e00\u4e2a\u200b\u5730\u65b9\u200b\u5e76\u200b\u89c2\u5bdf\u200b\u7ed3\u679c\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u6b21\u200b\u91cd\u590d\u200b\u3002</p> In\u00a0[23]: Copied! <pre>torch.manual_seed(42)\nmodel_1 = FashionMNISTModelV1(input_shape=784, # number of input features\n    hidden_units=10,\n    output_shape=len(class_names) # number of output classes desired\n).to(device) # send model to GPU if it's available\nnext(model_1.parameters()).device # check model device\n</pre> torch.manual_seed(42) model_1 = FashionMNISTModelV1(input_shape=784, # number of input features     hidden_units=10,     output_shape=len(class_names) # number of output classes desired ).to(device) # send model to GPU if it's available next(model_1.parameters()).device # check model device Out[23]: <pre>device(type='cuda', index=0)</pre> In\u00a0[24]: Copied! <pre>from helper_functions import accuracy_fn\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_1.parameters(), \n                            lr=0.1)\n</pre> from helper_functions import accuracy_fn loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_1.parameters(),                              lr=0.1) In\u00a0[25]: Copied! <pre>def train_step(model: torch.nn.Module,\n               data_loader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               accuracy_fn,\n               device: torch.device = device):\n    train_loss, train_acc = 0, 0\n    model.to(device)\n    for batch, (X, y) in enumerate(data_loader):\n        # Send data to GPU\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n        train_acc += accuracy_fn(y_true=y,\n                                 y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n    # Calculate loss and accuracy per epoch and print out what's happening\n    train_loss /= len(data_loader)\n    train_acc /= len(data_loader)\n    print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n\ndef test_step(data_loader: torch.utils.data.DataLoader,\n              model: torch.nn.Module,\n              loss_fn: torch.nn.Module,\n              accuracy_fn,\n              device: torch.device = device):\n    test_loss, test_acc = 0, 0\n    model.to(device)\n    model.eval() # put model in eval mode\n    # Turn on inference context manager\n    with torch.inference_mode(): \n        for X, y in data_loader:\n            # Send data to GPU\n            X, y = X.to(device), y.to(device)\n            \n            # 1. Forward pass\n            test_pred = model(X)\n            \n            # 2. Calculate loss and accuracy\n            test_loss += loss_fn(test_pred, y)\n            test_acc += accuracy_fn(y_true=y,\n                y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels\n            )\n        \n        # Adjust metrics and print out\n        test_loss /= len(data_loader)\n        test_acc /= len(data_loader)\n        print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n</pre> def train_step(model: torch.nn.Module,                data_loader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,                optimizer: torch.optim.Optimizer,                accuracy_fn,                device: torch.device = device):     train_loss, train_acc = 0, 0     model.to(device)     for batch, (X, y) in enumerate(data_loader):         # Send data to GPU         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate loss         loss = loss_fn(y_pred, y)         train_loss += loss         train_acc += accuracy_fn(y_true=y,                                  y_pred=y_pred.argmax(dim=1)) # Go from logits -&gt; pred labels          # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()      # Calculate loss and accuracy per epoch and print out what's happening     train_loss /= len(data_loader)     train_acc /= len(data_loader)     print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")  def test_step(data_loader: torch.utils.data.DataLoader,               model: torch.nn.Module,               loss_fn: torch.nn.Module,               accuracy_fn,               device: torch.device = device):     test_loss, test_acc = 0, 0     model.to(device)     model.eval() # put model in eval mode     # Turn on inference context manager     with torch.inference_mode():          for X, y in data_loader:             # Send data to GPU             X, y = X.to(device), y.to(device)                          # 1. Forward pass             test_pred = model(X)                          # 2. Calculate loss and accuracy             test_loss += loss_fn(test_pred, y)             test_acc += accuracy_fn(y_true=y,                 y_pred=test_pred.argmax(dim=1) # Go from logits -&gt; pred labels             )                  # Adjust metrics and print out         test_loss /= len(data_loader)         test_acc /= len(data_loader)         print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\") <p>\u200b\u54c7\u547c\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u8fd0\u884c\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u6bcf\u4e2a\u200b epoch \u200b\u4e2d\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u5faa\u73af\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6bcf\u4e2a\u200b epoch \u200b\u6211\u4eec\u200b\u90fd\u200b\u4f1a\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u8bad\u7ec3\u200b\u548c\u200b\u4e00\u6b21\u200b\u6d4b\u8bd5\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u81ea\u5b9a\u4e49\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u6b65\u9aa4\u200b\u7684\u200b\u9891\u7387\u200b\u3002\u200b\u6709\u65f6\u200b\u4eba\u4eec\u200b\u6bcf\u200b\u4e94\u4e2a\u200b epoch \u200b\u6216\u200b\u5341\u4e2a\u200b epoch \u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u6216\u8005\u200b\u50cf\u200b\u6211\u4eec\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6bcf\u4e2a\u200b epoch \u200b\u90fd\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8ba1\u65f6\u200b\uff0c\u200b\u770b\u770b\u200b\u4ee3\u7801\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u9700\u8981\u200b\u591a\u957f\u65f6\u95f4\u200b\u3002</p> In\u00a0[26]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_on_gpu = timer()\n\nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_1, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_1,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn\n    )\n\ntrain_time_end_on_gpu = timer()\ntotal_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,\n                                            end=train_time_end_on_gpu,\n                                            device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_on_gpu = timer()  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_1,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn     )     test_step(data_loader=test_dataloader,         model=model_1,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn     )  train_time_end_on_gpu = timer() total_train_time_model_1 = print_train_time(start=train_time_start_on_gpu,                                             end=train_time_end_on_gpu,                                             device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 1.09199 | Train accuracy: 61.34%\nTest loss: 0.95636 | Test accuracy: 65.00%\n\nEpoch: 1\n---------\nTrain loss: 0.78101 | Train accuracy: 71.93%\nTest loss: 0.72227 | Test accuracy: 73.91%\n\nEpoch: 2\n---------\nTrain loss: 0.67027 | Train accuracy: 75.94%\nTest loss: 0.68500 | Test accuracy: 75.02%\n\nTrain time on cuda: 36.878 seconds\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\uff0c\u200b\u4f46\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u6bd4\u200b\u9884\u671f\u200b\u8981\u957f\u200b\uff1f</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b CUDA \u200b\u548c\u200b CPU \u200b\u4e0a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u5f88\u5927\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b CPU/GPU \u200b\u7684\u200b\u8d28\u91cf\u200b\u3002\u200b\u7ee7\u7eed\u200b\u9605\u8bfb\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u7684\u200b\u89e3\u91ca\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\uff1a \"\u200b\u6211\u200b\u4f7f\u7528\u200b\u4e86\u200b GPU\uff0c\u200b\u4f46\u200b\u6211\u200b\u7684\u200b\u6a21\u578b\u200b\u5e76\u200b\u6ca1\u6709\u200b\u8bad\u7ec3\u200b\u5f97\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u4ec0\u4e48\u200b\u539f\u56e0\u200b\uff1f\"</p> <p>\u200b\u56de\u7b54\u200b\uff1a \u200b\u55ef\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u53ef\u80fd\u200b\u7684\u200b\u539f\u56e0\u200b\u662f\u200b\u60a8\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6a21\u578b\u200b\u90fd\u200b\u975e\u5e38\u200b\u5c0f\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6a21\u578b\u200b\u4e00\u6837\u200b\uff09\uff0c\u200b\u4f7f\u7528\u200b GPU \u200b\u7684\u200b\u597d\u5904\u200b\u88ab\u200b\u5b9e\u9645\u200b\u4f20\u8f93\u6570\u636e\u200b\u5230\u200b GPU \u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u6240\u200b\u62b5\u6d88\u200b\u3002</p> <p>\u200b\u5728\u200b\u5c06\u200b\u6570\u636e\u200b\u4ece\u200b CPU \u200b\u5185\u5b58\u200b\uff08\u200b\u9ed8\u8ba4\u200b\uff09\u200b\u590d\u5236\u5230\u200b GPU \u200b\u5185\u5b58\u200b\u4e4b\u95f4\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u74f6\u9888\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u96c6\u200b\uff0cCPU \u200b\u5b9e\u9645\u4e0a\u200b\u53ef\u80fd\u200b\u662f\u200b\u6700\u4f73\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5730\u70b9\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u8f83\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6a21\u578b\u200b\uff0cGPU \u200b\u63d0\u4f9b\u200b\u7684\u200b\u8ba1\u7b97\u901f\u5ea6\u200b\u901a\u5e38\u200b\u8fdc\u8fdc\u200b\u8d85\u8fc7\u200b\u5c06\u200b\u6570\u636e\u4f20\u8f93\u200b\u5230\u200b GPU \u200b\u7684\u200b\u6210\u672c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u200b\u5f88\u5927\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u786c\u4ef6\u200b\u3002\u200b\u901a\u8fc7\u200b\u5b9e\u8df5\u200b\uff0c\u200b\u60a8\u200b\u4f1a\u200b\u4e60\u60ef\u4e8e\u200b\u627e\u5230\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u6700\u4f73\u200b\u5730\u70b9\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>eval_model()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b <code>model_1</code>\uff0c\u200b\u770b\u770b\u200b\u6548\u679c\u200b\u5982\u4f55\u200b\u3002</p> In\u00a0[27]: Copied! <pre>torch.manual_seed(42)\n\n# Note: This will error due to `eval_model()` not using device agnostic code \nmodel_1_results = eval_model(model=model_1, \n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn) \nmodel_1_results\n</pre> torch.manual_seed(42)  # Note: This will error due to `eval_model()` not using device agnostic code  model_1_results = eval_model(model=model_1,      data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn)  model_1_results  <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-27-93fed76e63a5&gt; in &lt;cell line: 4&gt;()\n      2 \n      3 # Note: This will error due to `eval_model()` not using device agnostic code\n----&gt; 4 model_1_results = eval_model(model=model_1, \n      5     data_loader=test_dataloader,\n      6     loss_fn=loss_fn,\n\n&lt;ipython-input-20-885bc9be9cde&gt; in eval_model(model, data_loader, loss_fn, accuracy_fn)\n     20         for X, y in data_loader:\n     21             # Make predictions with the model\n---&gt; 22             y_pred = model(X)\n     23 \n     24             # Accumulate the loss and accuracy values per batch\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501             return forward_call(*args, **kwargs)\n   1502         # Do not call functions when jit is used\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\n\n&lt;ipython-input-22-a46e692b8bdd&gt; in forward(self, x)\n     12 \n     13     def forward(self, x: torch.Tensor):\n---&gt; 14         return self.layer_stack(x)\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501             return forward_call(*args, **kwargs)\n   1502         # Do not call functions when jit is used\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py in forward(self, input)\n    215     def forward(self, input):\n    216         for module in self:\n--&gt; 217             input = module(input)\n    218         return input\n    219 \n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py in _call_impl(self, *args, **kwargs)\n   1499                 or _global_backward_pre_hooks or _global_backward_hooks\n   1500                 or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1501             return forward_call(*args, **kwargs)\n   1502         # Do not call functions when jit is used\n   1503         full_backward_hooks, non_full_backward_hooks = [], []\n\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)</pre> <p>\u200b\u54ce\u5440\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b <code>eval_model()</code> \u200b\u51fd\u6570\u200b\u51fa\u9519\u200b\u4e86\u200b\uff0c\u200b\u9519\u8bef\u4fe1\u606f\u200b\u5982\u4e0b\u200b\uff1a</p> <p><code>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</code></p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u867d\u7136\u200b\u8bbe\u7f6e\u200b\u4e86\u200b\u8ba9\u200b\u6570\u636e\u200b\u548c\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u5bf9\u200b\u8bc4\u4f30\u200b\u51fd\u6570\u200b\u505a\u200b\u540c\u6837\u200b\u7684\u200b\u5904\u7406\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5411\u200b <code>eval_model()</code> \u200b\u51fd\u6570\u200b\u4f20\u9012\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b <code>device</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u518d\u200b\u5c1d\u8bd5\u200b\u91cd\u65b0\u200b\u8ba1\u7b97\u7ed3\u679c\u200b\u3002</p> In\u00a0[28]: Copied! <pre># Move values to device\ntorch.manual_seed(42)\ndef eval_model(model: torch.nn.Module, \n               data_loader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               accuracy_fn, \n               device: torch.device = device):\n    \"\"\"Evaluates a given model on a given dataset.\n\n    Args:\n        model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.\n        data_loader (torch.utils.data.DataLoader): The target dataset to predict on.\n        loss_fn (torch.nn.Module): The loss function of model.\n        accuracy_fn: An accuracy function to compare the models predictions to the truth labels.\n        device (str, optional): Target device to compute on. Defaults to device.\n\n    Returns:\n        (dict): Results of model making predictions on data_loader.\n    \"\"\"\n    loss, acc = 0, 0\n    model.eval()\n    with torch.inference_mode():\n        for X, y in data_loader:\n            # Send data to the target device\n            X, y = X.to(device), y.to(device)\n            y_pred = model(X)\n            loss += loss_fn(y_pred, y)\n            acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))\n        \n        # Scale loss and acc\n        loss /= len(data_loader)\n        acc /= len(data_loader)\n    return {\"model_name\": model.__class__.__name__, # only works when model was created with a class\n            \"model_loss\": loss.item(),\n            \"model_acc\": acc}\n\n# Calculate model 1 results with device-agnostic code \nmodel_1_results = eval_model(model=model_1, data_loader=test_dataloader,\n    loss_fn=loss_fn, accuracy_fn=accuracy_fn,\n    device=device\n)\nmodel_1_results\n</pre> # Move values to device torch.manual_seed(42) def eval_model(model: torch.nn.Module,                 data_loader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 accuracy_fn,                 device: torch.device = device):     \"\"\"Evaluates a given model on a given dataset.      Args:         model (torch.nn.Module): A PyTorch model capable of making predictions on data_loader.         data_loader (torch.utils.data.DataLoader): The target dataset to predict on.         loss_fn (torch.nn.Module): The loss function of model.         accuracy_fn: An accuracy function to compare the models predictions to the truth labels.         device (str, optional): Target device to compute on. Defaults to device.      Returns:         (dict): Results of model making predictions on data_loader.     \"\"\"     loss, acc = 0, 0     model.eval()     with torch.inference_mode():         for X, y in data_loader:             # Send data to the target device             X, y = X.to(device), y.to(device)             y_pred = model(X)             loss += loss_fn(y_pred, y)             acc += accuracy_fn(y_true=y, y_pred=y_pred.argmax(dim=1))                  # Scale loss and acc         loss /= len(data_loader)         acc /= len(data_loader)     return {\"model_name\": model.__class__.__name__, # only works when model was created with a class             \"model_loss\": loss.item(),             \"model_acc\": acc}  # Calculate model 1 results with device-agnostic code  model_1_results = eval_model(model=model_1, data_loader=test_dataloader,     loss_fn=loss_fn, accuracy_fn=accuracy_fn,     device=device ) model_1_results Out[28]: <pre>{'model_name': 'FashionMNISTModelV1',\n 'model_loss': 0.6850008964538574,\n 'model_acc': 75.01996805111821}</pre> In\u00a0[29]: Copied! <pre># Check baseline results\nmodel_0_results\n</pre> # Check baseline results model_0_results Out[29]: <pre>{'model_name': 'FashionMNISTModelV0',\n 'model_loss': 0.47663894295692444,\n 'model_acc': 83.42651757188499}</pre> <p>\u200b\u54c7\u200b\uff0c\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e2d\u200b\u52a0\u5165\u200b\u975e\u7ebf\u6027\u200b\u56e0\u7d20\u200b\u53cd\u800c\u200b\u4f7f\u200b\u5176\u200b\u8868\u73b0\u200b\u4e0d\u5982\u200b\u57fa\u51c6\u7ebf\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u4e00\u70b9\u200b\uff0c\u200b\u6709\u65f6\u5019\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u5e94\u8be5\u200b\u6709\u6548\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5374\u200b\u5e76\u200b\u4e0d\u200b\u594f\u6548\u200b\u3002</p> <p>\u200b\u800c\u200b\u6709\u65f6\u5019\u200b\uff0c\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u53ef\u80fd\u200b\u4e0d\u200b\u594f\u6548\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5374\u200b\u80fd\u200b\u53d1\u6325\u4f5c\u7528\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u65e2\u200b\u662f\u200b\u79d1\u5b66\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u827a\u672f\u200b\u3002</p> <p>\u200b\u4ece\u200b\u76ee\u524d\u200b\u7684\u200b\u60c5\u51b5\u200b\u6765\u770b\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4f3c\u4e4e\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fc7\u200b\u62df\u5408\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8fc7\u200b\u62df\u5408\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u5b66\u4e60\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u6a21\u5f0f\u200b\u5e76\u200b\u6ca1\u6709\u200b\u6cdb\u5316\u200b\u5230\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u89e3\u51b3\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u65b9\u6cd5\u200b\u5305\u62ec\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5c0f\u200b\u6216\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u67d0\u4e9b\u200b\u6a21\u578b\u200b\u5bf9\u200b\u7279\u5b9a\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\u62df\u5408\u200b\u5f97\u200b\u66f4\u597d\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u66f4\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u6570\u636e\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u6a21\u578b\u200b\u5b66\u4e60\u200b\u5230\u200b\u53ef\u200b\u6cdb\u5316\u200b\u6a21\u5f0f\u200b\u7684\u200b\u673a\u4f1a\u200b\u5c31\u200b\u8d8a\u200b\u5927\u200b\uff09\u3002</li> </ol> <p>\u200b\u8fd8\u6709\u200b\u5176\u4ed6\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u6211\u200b\u5c06\u200b\u8fd9\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u6311\u6218\u200b\u7559\u7ed9\u200b\u4f60\u200b\u53bb\u200b\u63a2\u7d22\u200b\u3002</p> <p>\u200b\u8bd5\u7740\u200b\u5728\u200b\u7f51\u4e0a\u200b\u641c\u7d22\u200b\u201c\u200b\u9632\u6b62\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b\u201d\uff0c\u200b\u770b\u770b\u200b\u80fd\u200b\u627e\u5230\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u4e0e\u6b64\u540c\u65f6\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u7b2c\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\uff1a\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[30]: Copied! <pre># Create a convolutional neural network \nclass FashionMNISTModelV2(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n        super().__init__()\n        self.block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, 3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*7*7, \n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.block_1(x)\n        # print(x.shape)\n        x = self.block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n\ntorch.manual_seed(42)\nmodel_2 = FashionMNISTModelV2(input_shape=1, \n    hidden_units=10, \n    output_shape=len(class_names)).to(device)\nmodel_2\n</pre> # Create a convolutional neural network  class FashionMNISTModelV2(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int):         super().__init__()         self.block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1),# options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, 3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*7*7,                        out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.block_1(x)         # print(x.shape)         x = self.block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x  torch.manual_seed(42) model_2 = FashionMNISTModelV2(input_shape=1,      hidden_units=10,      output_shape=len(class_names)).to(device) model_2 Out[30]: <pre>FashionMNISTModelV2(\n  (block_1): Sequential(\n    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=490, out_features=10, bias=True)\n  )\n)</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u5e38\u89c1\u200b\u505a\u6cd5\u200b\u3002</p> <p>\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff0c\u200b\u5e76\u7528\u200b\u4ee3\u7801\u200b\u590d\u73b0\u200b\u5b83\u200b\u3002</p> In\u00a0[31]: Copied! <pre>torch.manual_seed(42)\n\n# Create sample batch of random numbers with same size as image batch\nimages = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width]\ntest_image = images[0] # get a single image for testing\nprint(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\") \nprint(f\"Single image pixel values:\\n{test_image}\")\n</pre> torch.manual_seed(42)  # Create sample batch of random numbers with same size as image batch images = torch.randn(size=(32, 3, 64, 64)) # [batch_size, color_channels, height, width] test_image = images[0] # get a single image for testing print(f\"Image batch shape: {images.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Single image shape: {test_image.shape} -&gt; [color_channels, height, width]\")  print(f\"Single image pixel values:\\n{test_image}\") <pre>Image batch shape: torch.Size([32, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nSingle image shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nSingle image pixel values:\ntensor([[[ 1.9269,  1.4873,  0.9007,  ...,  1.8446, -1.1845,  1.3835],\n         [ 1.4451,  0.8564,  2.2181,  ...,  0.3399,  0.7200,  0.4114],\n         [ 1.9312,  1.0119, -1.4364,  ..., -0.5558,  0.7043,  0.7099],\n         ...,\n         [-0.5610, -0.4830,  0.4770,  ..., -0.2713, -0.9537, -0.6737],\n         [ 0.3076, -0.1277,  0.0366,  ..., -2.0060,  0.2824, -0.8111],\n         [-1.5486,  0.0485, -0.7712,  ..., -0.1403,  0.9416, -0.0118]],\n\n        [[-0.5197,  1.8524,  1.8365,  ...,  0.8935, -1.5114, -0.8515],\n         [ 2.0818,  1.0677, -1.4277,  ...,  1.6612, -2.6223, -0.4319],\n         [-0.1010, -0.4388, -1.9775,  ...,  0.2106,  0.2536, -0.7318],\n         ...,\n         [ 0.2779,  0.7342, -0.3736,  ..., -0.4601,  0.1815,  0.1850],\n         [ 0.7205, -0.2833,  0.0937,  ..., -0.1002, -2.3609,  2.2465],\n         [-1.3242, -0.1973,  0.2920,  ...,  0.5409,  0.6940,  1.8563]],\n\n        [[-0.7978,  1.0261,  1.1465,  ...,  1.2134,  0.9354, -0.0780],\n         [-1.4647, -1.9571,  0.1017,  ..., -1.9986, -0.7409,  0.7011],\n         [-1.3938,  0.8466, -1.7191,  ..., -1.1867,  0.1320,  0.3407],\n         ...,\n         [ 0.8206, -0.3745,  1.2499,  ..., -0.0676,  0.0385,  0.6335],\n         [-0.5589, -0.3393,  0.2347,  ...,  2.1181,  2.4569,  1.3083],\n         [-0.4092,  1.5199,  0.2401,  ..., -0.2558,  0.7870,  0.9924]]])\n</pre> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5e26\u6709\u200b\u5404\u79cd\u200b\u53c2\u6570\u200b\u7684\u200b <code>nn.Conv2d()</code> \u200b\u793a\u4f8b\u200b\uff1a</p> <ul> <li><code>in_channels</code> (int) - \u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u3002</li> <li><code>out_channels</code> (int) - \u200b\u5377\u79ef\u200b\u4ea7\u751f\u200b\u7684\u200b\u901a\u9053\u200b\u6570\u200b\u3002</li> <li><code>kernel_size</code> (int \u200b\u6216\u200b tuple) - \u200b\u5377\u79ef\u200b\u6838\u200b/\u200b\u6ee4\u6ce2\u5668\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002</li> <li><code>stride</code> (int \u200b\u6216\u200b tuple, \u200b\u53ef\u200b\u9009\u200b) - \u200b\u5377\u79ef\u200b\u6838\u200b\u6bcf\u6b21\u200b\u79fb\u52a8\u200b\u7684\u200b\u6b65\u957f\u200b\u3002\u200b\u9ed8\u8ba4\u503c\u200b\uff1a1\u3002</li> <li><code>padding</code> (int, tuple, str) - \u200b\u6dfb\u52a0\u200b\u5230\u200b\u8f93\u5165\u200b\u56db\u5468\u200b\u7684\u200b\u586b\u5145\u200b\u3002\u200b\u9ed8\u8ba4\u503c\u200b\uff1a0\u3002</li> </ul> <p></p> <p>\u200b\u6539\u53d8\u200b <code>nn.Conv2d()</code> \u200b\u5c42\u8d85\u200b\u53c2\u6570\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002</p> In\u00a0[32]: Copied! <pre>torch.manual_seed(42)\n\n# Create a convolutional layer with same dimensions as TinyVGG \n# (try changing any of the parameters and see what happens)\nconv_layer = nn.Conv2d(in_channels=3,\n                       out_channels=10,\n                       kernel_size=3,\n                       stride=1,\n                       padding=0) # also try using \"valid\" or \"same\" here \n\n# Pass the data through the convolutional layer\nconv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)\n</pre> torch.manual_seed(42)  # Create a convolutional layer with same dimensions as TinyVGG  # (try changing any of the parameters and see what happens) conv_layer = nn.Conv2d(in_channels=3,                        out_channels=10,                        kernel_size=3,                        stride=1,                        padding=0) # also try using \"valid\" or \"same\" here   # Pass the data through the convolutional layer conv_layer(test_image) # Note: If running PyTorch &lt;1.11.0, this will error because of shape issues (nn.Conv.2d() expects a 4d tensor as input)  Out[32]: <pre>tensor([[[ 1.5396,  0.0516,  0.6454,  ..., -0.3673,  0.8711,  0.4256],\n         [ 0.3662,  1.0114, -0.5997,  ...,  0.8983,  0.2809, -0.2741],\n         [ 1.2664, -1.4054,  0.3727,  ..., -0.3409,  1.2191, -0.0463],\n         ...,\n         [-0.1541,  0.5132, -0.3624,  ..., -0.2360, -0.4609, -0.0035],\n         [ 0.2981, -0.2432,  1.5012,  ..., -0.6289, -0.7283, -0.5767],\n         [-0.0386, -0.0781, -0.0388,  ...,  0.2842,  0.4228, -0.1802]],\n\n        [[-0.2840, -0.0319, -0.4455,  ..., -0.7956,  1.5599, -1.2449],\n         [ 0.2753, -0.1262, -0.6541,  ..., -0.2211,  0.1999, -0.8856],\n         [-0.5404, -1.5489,  0.0249,  ..., -0.5932, -1.0913, -0.3849],\n         ...,\n         [ 0.3870, -0.4064, -0.8236,  ...,  0.1734, -0.4330, -0.4951],\n         [-0.1984, -0.6386,  1.0263,  ..., -0.9401, -0.0585, -0.7833],\n         [-0.6306, -0.2052, -0.3694,  ..., -1.3248,  0.2456, -0.7134]],\n\n        [[ 0.4414,  0.5100,  0.4846,  ..., -0.8484,  0.2638,  1.1258],\n         [ 0.8117,  0.3191, -0.0157,  ...,  1.2686,  0.2319,  0.5003],\n         [ 0.3212,  0.0485, -0.2581,  ...,  0.2258,  0.2587, -0.8804],\n         ...,\n         [-0.1144, -0.1869,  0.0160,  ..., -0.8346,  0.0974,  0.8421],\n         [ 0.2941,  0.4417,  0.5866,  ..., -0.1224,  0.4814, -0.4799],\n         [ 0.6059, -0.0415, -0.2028,  ...,  0.1170,  0.2521, -0.4372]],\n\n        ...,\n\n        [[-0.2560, -0.0477,  0.6380,  ...,  0.6436,  0.7553, -0.7055],\n         [ 1.5595, -0.2209, -0.9486,  ..., -0.4876,  0.7754,  0.0750],\n         [-0.0797,  0.2471,  1.1300,  ...,  0.1505,  0.2354,  0.9576],\n         ...,\n         [ 1.1065,  0.6839,  1.2183,  ...,  0.3015, -0.1910, -0.1902],\n         [-0.3486, -0.7173, -0.3582,  ...,  0.4917,  0.7219,  0.1513],\n         [ 0.0119,  0.1017,  0.7839,  ..., -0.3752, -0.8127, -0.1257]],\n\n        [[ 0.3841,  1.1322,  0.1620,  ...,  0.7010,  0.0109,  0.6058],\n         [ 0.1664,  0.1873,  1.5924,  ...,  0.3733,  0.9096, -0.5399],\n         [ 0.4094, -0.0861, -0.7935,  ..., -0.1285, -0.9932, -0.3013],\n         ...,\n         [ 0.2688, -0.5630, -1.1902,  ...,  0.4493,  0.5404, -0.0103],\n         [ 0.0535,  0.4411,  0.5313,  ...,  0.0148, -1.0056,  0.3759],\n         [ 0.3031, -0.1590, -0.1316,  ..., -0.5384, -0.4271, -0.4876]],\n\n        [[-1.1865, -0.7280, -1.2331,  ..., -0.9013, -0.0542, -1.5949],\n         [-0.6345, -0.5920,  0.5326,  ..., -1.0395, -0.7963, -0.0647],\n         [-0.1132,  0.5166,  0.2569,  ...,  0.5595, -1.6881,  0.9485],\n         ...,\n         [-0.0254, -0.2669,  0.1927,  ..., -0.2917,  0.1088, -0.4807],\n         [-0.2609, -0.2328,  0.1404,  ..., -0.1325, -0.8436, -0.7524],\n         [-1.1399, -0.1751, -0.8705,  ...,  0.1589,  0.3377,  0.3493]]],\n       grad_fn=&lt;SqueezeBackward1&gt;)</pre> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4f20\u9012\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u4f1a\u200b\u9047\u5230\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\u7684\u200b\u9519\u8bef\u200b\uff1a</p> <p><code>RuntimeError: Expected 4-dimensional input for 4-dimensional weight [10, 3, 3, 3], but got 3-dimensional input of size [3, 64, 64] instead</code></p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b PyTorch 1.11.0 \u200b\u53ca\u200b\u4ee5\u4e0a\u200b\u7248\u672c\u200b\uff0c\u200b\u5219\u200b\u4e0d\u4f1a\u200b\u51fa\u73b0\u200b\u6b64\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>nn.Conv2d()</code> \u200b\u5c42\u200b\u671f\u671b\u200b\u8f93\u5165\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(N, C, H, W)</code> \u200b\u6216\u200b <code>[batch_size, color_channels, height, width]</code> \u200b\u7684\u200b 4 \u200b\u7ef4\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u76ee\u524d\u200b\u6211\u4eec\u200b\u7684\u200b\u5355\u5f20\u200b\u56fe\u7247\u200b <code>test_image</code> \u200b\u53ea\u6709\u200b <code>[color_channels, height, width]</code> \u200b\u6216\u200b <code>[3, 64, 64]</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b <code>test_image.unsqueeze(dim=0)</code> \u200b\u4e3a\u200b <code>N</code> \u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> In\u00a0[33]: Copied! <pre># Add extra dimension to test image\ntest_image.unsqueeze(dim=0).shape\n</pre> # Add extra dimension to test image test_image.unsqueeze(dim=0).shape Out[33]: <pre>torch.Size([1, 3, 64, 64])</pre> In\u00a0[34]: Copied! <pre># Pass test image with extra dimension through conv_layer\nconv_layer(test_image.unsqueeze(dim=0)).shape\n</pre> # Pass test image with extra dimension through conv_layer conv_layer(test_image.unsqueeze(dim=0)).shape Out[34]: <pre>torch.Size([1, 10, 62, 62])</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u6ce8\u610f\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u5f62\u72b6\u200b\uff08\u200b\u4e0e\u200bCNN Explainer\u200b\u4e0a\u200bTinyVGG\u200b\u7684\u200b\u7b2c\u4e00\u5c42\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\uff09\u200b\u53d1\u751f\u200b\u4e86\u200b\u53d8\u5316\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b\u901a\u9053\u200b\u5927\u5c0f\u200b\u4ee5\u53ca\u200b\u4e0d\u540c\u200b\u7684\u200b\u50cf\u7d20\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6539\u53d8\u200b<code>conv_layer</code>\u200b\u7684\u200b\u503c\u4f1a\u200b\u600e\u6837\u200b\u5462\u200b\uff1f</p> In\u00a0[35]: Copied! <pre>torch.manual_seed(42)\n# Create a new conv_layer with different values (try setting these to whatever you like)\nconv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image\n                         out_channels=10,\n                         kernel_size=(5, 5), # kernel is usually a square so a tuple also works\n                         stride=2,\n                         padding=0)\n\n# Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input)\nconv_layer_2(test_image.unsqueeze(dim=0)).shape\n</pre> torch.manual_seed(42) # Create a new conv_layer with different values (try setting these to whatever you like) conv_layer_2 = nn.Conv2d(in_channels=3, # same number of color channels as our input image                          out_channels=10,                          kernel_size=(5, 5), # kernel is usually a square so a tuple also works                          stride=2,                          padding=0)  # Pass single image through new conv_layer_2 (this calls nn.Conv2d()'s forward() method on the input) conv_layer_2(test_image.unsqueeze(dim=0)).shape Out[35]: <pre>torch.Size([1, 10, 30, 30])</pre> <p>\u200b\u54c7\u200b\uff0c\u200b\u6211\u4eec\u200b\u53c8\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u5f62\u72b6\u200b\u53d8\u5316\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u662f\u200b <code>[1, 10, 30, 30]</code>\uff08\u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5f62\u72b6\u200b\u4f1a\u200b\u6709\u6240\u4e0d\u540c\u200b\uff09\uff0c\u200b\u6216\u8005\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u4e3a\u200b <code>[batch_size=1, color_channels=10, height=30, width=30]</code>\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u600e\u4e48\u56de\u4e8b\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5728\u200b\u5e55\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b <code>nn.Conv2d()</code> \u200b\u6b63\u5728\u200b\u538b\u7f29\u200b\u5b58\u50a8\u200b\u5728\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u5b83\u200b\u901a\u8fc7\u200b\u5728\u200b\u8f93\u5165\u200b\uff08\u200b\u6211\u4eec\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff09\u200b\u4e0a\u200b\u6267\u884c\u200b\u4e0e\u5176\u200b\u5185\u90e8\u200b\u53c2\u6570\u200b\u76f8\u5173\u200b\u7684\u200b\u64cd\u4f5c\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u8fd9\u4e00\u200b\u76ee\u6807\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u6784\u5efa\u200b\u7684\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7c7b\u4f3c\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u8f93\u5165\u200b\u540e\u200b\uff0c\u200b\u5404\u5c42\u200b\u5c1d\u8bd5\u200b\u66f4\u65b0\u200b\u5176\u200b\u5185\u90e8\u200b\u53c2\u6570\u200b\uff08\u200b\u6a21\u5f0f\u200b\uff09\uff0c\u200b\u4ee5\u200b\u501f\u52a9\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5e2e\u52a9\u200b\u964d\u4f4e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u552f\u4e00\u200b\u7684\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b\u4e0d\u540c\u200b\u5c42\u200b\u5982\u4f55\u200b\u8ba1\u7b97\u200b\u5176\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\uff0c\u200b\u6216\u8005\u200b\u7528\u200b PyTorch \u200b\u7684\u200b\u672f\u8bed\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5c42\u4e2d\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u6240\u200b\u6267\u884c\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u67e5\u770b\u200b <code>conv_layer_2.state_dict()</code>\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u7f6e\u200b\u8bbe\u7f6e\u200b\u3002</p> In\u00a0[36]: Copied! <pre># Check out the conv_layer_2 internal parameters\nprint(conv_layer_2.state_dict())\n</pre> # Check out the conv_layer_2 internal parameters print(conv_layer_2.state_dict()) <pre>OrderedDict([('weight', tensor([[[[ 0.0883,  0.0958, -0.0271,  0.1061, -0.0253],\n          [ 0.0233, -0.0562,  0.0678,  0.1018, -0.0847],\n          [ 0.1004,  0.0216,  0.0853,  0.0156,  0.0557],\n          [-0.0163,  0.0890,  0.0171, -0.0539,  0.0294],\n          [-0.0532, -0.0135, -0.0469,  0.0766, -0.0911]],\n\n         [[-0.0532, -0.0326, -0.0694,  0.0109, -0.1140],\n          [ 0.1043, -0.0981,  0.0891,  0.0192, -0.0375],\n          [ 0.0714,  0.0180,  0.0933,  0.0126, -0.0364],\n          [ 0.0310, -0.0313,  0.0486,  0.1031,  0.0667],\n          [-0.0505,  0.0667,  0.0207,  0.0586, -0.0704]],\n\n         [[-0.1143, -0.0446, -0.0886,  0.0947,  0.0333],\n          [ 0.0478,  0.0365, -0.0020,  0.0904, -0.0820],\n          [ 0.0073, -0.0788,  0.0356, -0.0398,  0.0354],\n          [-0.0241,  0.0958, -0.0684, -0.0689, -0.0689],\n          [ 0.1039,  0.0385,  0.1111, -0.0953, -0.1145]]],\n\n\n        [[[-0.0903, -0.0777,  0.0468,  0.0413,  0.0959],\n          [-0.0596, -0.0787,  0.0613, -0.0467,  0.0701],\n          [-0.0274,  0.0661, -0.0897, -0.0583,  0.0352],\n          [ 0.0244, -0.0294,  0.0688,  0.0785, -0.0837],\n          [-0.0616,  0.1057, -0.0390, -0.0409, -0.1117]],\n\n         [[-0.0661,  0.0288, -0.0152, -0.0838,  0.0027],\n          [-0.0789, -0.0980, -0.0636, -0.1011, -0.0735],\n          [ 0.1154,  0.0218,  0.0356, -0.1077, -0.0758],\n          [-0.0384,  0.0181, -0.1016, -0.0498, -0.0691],\n          [ 0.0003, -0.0430, -0.0080, -0.0782, -0.0793]],\n\n         [[-0.0674, -0.0395, -0.0911,  0.0968, -0.0229],\n          [ 0.0994,  0.0360, -0.0978,  0.0799, -0.0318],\n          [-0.0443, -0.0958, -0.1148,  0.0330, -0.0252],\n          [ 0.0450, -0.0948,  0.0857, -0.0848, -0.0199],\n          [ 0.0241,  0.0596,  0.0932,  0.1052, -0.0916]]],\n\n\n        [[[ 0.0291, -0.0497, -0.0127, -0.0864,  0.1052],\n          [-0.0847,  0.0617,  0.0406,  0.0375, -0.0624],\n          [ 0.1050,  0.0254,  0.0149, -0.1018,  0.0485],\n          [-0.0173, -0.0529,  0.0992,  0.0257, -0.0639],\n          [-0.0584, -0.0055,  0.0645, -0.0295, -0.0659]],\n\n         [[-0.0395, -0.0863,  0.0412,  0.0894, -0.1087],\n          [ 0.0268,  0.0597,  0.0209, -0.0411,  0.0603],\n          [ 0.0607,  0.0432, -0.0203, -0.0306,  0.0124],\n          [-0.0204, -0.0344,  0.0738,  0.0992, -0.0114],\n          [-0.0259,  0.0017, -0.0069,  0.0278,  0.0324]],\n\n         [[-0.1049, -0.0426,  0.0972,  0.0450, -0.0057],\n          [-0.0696, -0.0706, -0.1034, -0.0376,  0.0390],\n          [ 0.0736,  0.0533, -0.1021, -0.0694, -0.0182],\n          [ 0.1117,  0.0167, -0.0299,  0.0478, -0.0440],\n          [-0.0747,  0.0843, -0.0525, -0.0231, -0.1149]]],\n\n\n        [[[ 0.0773,  0.0875,  0.0421, -0.0805, -0.1140],\n          [-0.0938,  0.0861,  0.0554,  0.0972,  0.0605],\n          [ 0.0292, -0.0011, -0.0878, -0.0989, -0.1080],\n          [ 0.0473, -0.0567, -0.0232, -0.0665, -0.0210],\n          [-0.0813, -0.0754,  0.0383, -0.0343,  0.0713]],\n\n         [[-0.0370, -0.0847, -0.0204, -0.0560, -0.0353],\n          [-0.1099,  0.0646, -0.0804,  0.0580,  0.0524],\n          [ 0.0825, -0.0886,  0.0830, -0.0546,  0.0428],\n          [ 0.1084, -0.0163, -0.0009, -0.0266, -0.0964],\n          [ 0.0554, -0.1146,  0.0717,  0.0864,  0.1092]],\n\n         [[-0.0272, -0.0949,  0.0260,  0.0638, -0.1149],\n          [-0.0262, -0.0692, -0.0101, -0.0568, -0.0472],\n          [-0.0367, -0.1097,  0.0947,  0.0968, -0.0181],\n          [-0.0131, -0.0471, -0.1043, -0.1124,  0.0429],\n          [-0.0634, -0.0742, -0.0090, -0.0385, -0.0374]]],\n\n\n        [[[ 0.0037, -0.0245, -0.0398, -0.0553, -0.0940],\n          [ 0.0968, -0.0462,  0.0306, -0.0401,  0.0094],\n          [ 0.1077,  0.0532, -0.1001,  0.0458,  0.1096],\n          [ 0.0304,  0.0774,  0.1138, -0.0177,  0.0240],\n          [-0.0803, -0.0238,  0.0855,  0.0592, -0.0731]],\n\n         [[-0.0926, -0.0789, -0.1140, -0.0891, -0.0286],\n          [ 0.0779,  0.0193, -0.0878, -0.0926,  0.0574],\n          [-0.0859, -0.0142,  0.0554, -0.0534, -0.0126],\n          [-0.0101, -0.0273, -0.0585, -0.1029, -0.0933],\n          [-0.0618,  0.1115, -0.0558, -0.0775,  0.0280]],\n\n         [[ 0.0318,  0.0633,  0.0878,  0.0643, -0.1145],\n          [ 0.0102,  0.0699, -0.0107, -0.0680,  0.1101],\n          [-0.0432, -0.0657, -0.1041,  0.0052,  0.0512],\n          [ 0.0256,  0.0228, -0.0876, -0.1078,  0.0020],\n          [ 0.1053,  0.0666, -0.0672, -0.0150, -0.0851]]],\n\n\n        [[[-0.0557,  0.0209,  0.0629,  0.0957, -0.1060],\n          [ 0.0772, -0.0814,  0.0432,  0.0977,  0.0016],\n          [ 0.1051, -0.0984, -0.0441,  0.0673, -0.0252],\n          [-0.0236, -0.0481,  0.0796,  0.0566,  0.0370],\n          [-0.0649, -0.0937,  0.0125,  0.0342, -0.0533]],\n\n         [[-0.0323,  0.0780,  0.0092,  0.0052, -0.0284],\n          [-0.1046, -0.1086, -0.0552, -0.0587,  0.0360],\n          [-0.0336, -0.0452,  0.1101,  0.0402,  0.0823],\n          [-0.0559, -0.0472,  0.0424, -0.0769, -0.0755],\n          [-0.0056, -0.0422, -0.0866,  0.0685,  0.0929]],\n\n         [[ 0.0187, -0.0201, -0.1070, -0.0421,  0.0294],\n          [ 0.0544, -0.0146, -0.0457,  0.0643, -0.0920],\n          [ 0.0730, -0.0448,  0.0018, -0.0228,  0.0140],\n          [-0.0349,  0.0840, -0.0030,  0.0901,  0.1110],\n          [-0.0563, -0.0842,  0.0926,  0.0905, -0.0882]]],\n\n\n        [[[-0.0089, -0.1139, -0.0945,  0.0223,  0.0307],\n          [ 0.0245, -0.0314,  0.1065,  0.0165, -0.0681],\n          [-0.0065,  0.0277,  0.0404, -0.0816,  0.0433],\n          [-0.0590, -0.0959, -0.0631,  0.1114,  0.0987],\n          [ 0.1034,  0.0678,  0.0872, -0.0155, -0.0635]],\n\n         [[ 0.0577, -0.0598, -0.0779, -0.0369,  0.0242],\n          [ 0.0594, -0.0448, -0.0680,  0.0156, -0.0681],\n          [-0.0752,  0.0602, -0.0194,  0.1055,  0.1123],\n          [ 0.0345,  0.0397,  0.0266,  0.0018, -0.0084],\n          [ 0.0016,  0.0431,  0.1074, -0.0299, -0.0488]],\n\n         [[-0.0280, -0.0558,  0.0196,  0.0862,  0.0903],\n          [ 0.0530, -0.0850, -0.0620, -0.0254, -0.0213],\n          [ 0.0095, -0.1060,  0.0359, -0.0881, -0.0731],\n          [-0.0960,  0.1006, -0.1093,  0.0871, -0.0039],\n          [-0.0134,  0.0722, -0.0107,  0.0724,  0.0835]]],\n\n\n        [[[-0.1003,  0.0444,  0.0218,  0.0248,  0.0169],\n          [ 0.0316, -0.0555, -0.0148,  0.1097,  0.0776],\n          [-0.0043, -0.1086,  0.0051, -0.0786,  0.0939],\n          [-0.0701, -0.0083, -0.0256,  0.0205,  0.1087],\n          [ 0.0110,  0.0669,  0.0896,  0.0932, -0.0399]],\n\n         [[-0.0258,  0.0556, -0.0315,  0.0541, -0.0252],\n          [-0.0783,  0.0470,  0.0177,  0.0515,  0.1147],\n          [ 0.0788,  0.1095,  0.0062, -0.0993, -0.0810],\n          [-0.0717, -0.1018, -0.0579, -0.1063, -0.1065],\n          [-0.0690, -0.1138, -0.0709,  0.0440,  0.0963]],\n\n         [[-0.0343, -0.0336,  0.0617, -0.0570, -0.0546],\n          [ 0.0711, -0.1006,  0.0141,  0.1020,  0.0198],\n          [ 0.0314, -0.0672, -0.0016,  0.0063,  0.0283],\n          [ 0.0449,  0.1003, -0.0881,  0.0035, -0.0577],\n          [-0.0913, -0.0092, -0.1016,  0.0806,  0.0134]]],\n\n\n        [[[-0.0622,  0.0603, -0.1093, -0.0447, -0.0225],\n          [-0.0981, -0.0734, -0.0188,  0.0876,  0.1115],\n          [ 0.0735, -0.0689, -0.0755,  0.1008,  0.0408],\n          [ 0.0031,  0.0156, -0.0928, -0.0386,  0.1112],\n          [-0.0285, -0.0058, -0.0959, -0.0646, -0.0024]],\n\n         [[-0.0717, -0.0143,  0.0470, -0.1130,  0.0343],\n          [-0.0763, -0.0564,  0.0443,  0.0918, -0.0316],\n          [-0.0474, -0.1044, -0.0595, -0.1011, -0.0264],\n          [ 0.0236, -0.1082,  0.1008,  0.0724, -0.1130],\n          [-0.0552,  0.0377, -0.0237, -0.0126, -0.0521]],\n\n         [[ 0.0927, -0.0645,  0.0958,  0.0075,  0.0232],\n          [ 0.0901, -0.0190, -0.0657, -0.0187,  0.0937],\n          [-0.0857,  0.0262, -0.1135,  0.0605,  0.0427],\n          [ 0.0049,  0.0496,  0.0001,  0.0639, -0.0914],\n          [-0.0170,  0.0512,  0.1150,  0.0588, -0.0840]]],\n\n\n        [[[ 0.0888, -0.0257, -0.0247, -0.1050, -0.0182],\n          [ 0.0817,  0.0161, -0.0673,  0.0355, -0.0370],\n          [ 0.1054, -0.1002, -0.0365, -0.1115, -0.0455],\n          [ 0.0364,  0.1112,  0.0194,  0.1132,  0.0226],\n          [ 0.0667,  0.0926,  0.0965, -0.0646,  0.1062]],\n\n         [[ 0.0699, -0.0540, -0.0551, -0.0969,  0.0290],\n          [-0.0936,  0.0488,  0.0365, -0.1003,  0.0315],\n          [-0.0094,  0.0527,  0.0663, -0.1148,  0.1059],\n          [ 0.0968,  0.0459, -0.1055, -0.0412, -0.0335],\n          [-0.0297,  0.0651,  0.0420,  0.0915, -0.0432]],\n\n         [[ 0.0389,  0.0411, -0.0961, -0.1120, -0.0599],\n          [ 0.0790, -0.1087, -0.1005,  0.0647,  0.0623],\n          [ 0.0950, -0.0872, -0.0845,  0.0592,  0.1004],\n          [ 0.0691,  0.0181,  0.0381,  0.1096, -0.0745],\n          [-0.0524,  0.0808, -0.0790, -0.0637,  0.0843]]]])), ('bias', tensor([ 0.0364,  0.0373, -0.0489, -0.0016,  0.1057, -0.0693,  0.0009,  0.0549,\n        -0.0797,  0.1121]))])\n</pre> <p>\u200b\u770b\u200b\u90a3\u200b\uff01\u200b\u4e00\u5806\u200b\u968f\u673a\u200b\u7684\u200b\u6570\u5b57\u200b\uff0c\u200b\u5bf9\u5e94\u200b\u7740\u200b\u6743\u91cd\u200b\u548c\u200b\u504f\u7f6e\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\u7531\u200b\u6211\u4eec\u200b\u5728\u200b\u8bbe\u7f6e\u200b <code>nn.Conv2d()</code> \u200b\u65f6\u200b\u4f20\u5165\u200b\u7684\u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u51b3\u5b9a\u200b\u7684\u200b\u3002</p> <p>\u200b\u4e00\u200b\u8d77\u6765\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u5427\u200b\u3002</p> In\u00a0[37]: Copied! <pre># Get shapes of weight and bias tensors within conv_layer_2\nprint(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\")\nprint(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\")\n</pre> # Get shapes of weight and bias tensors within conv_layer_2 print(f\"conv_layer_2 weight shape: \\n{conv_layer_2.weight.shape} -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\") print(f\"\\nconv_layer_2 bias shape: \\n{conv_layer_2.bias.shape} -&gt; [out_channels=10]\") <pre>conv_layer_2 weight shape: \ntorch.Size([10, 3, 5, 5]) -&gt; [out_channels=10, in_channels=3, kernel_size=5, kernel_size=5]\n\nconv_layer_2 bias shape: \ntorch.Size([10]) -&gt; [out_channels=10]\n</pre> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u5982\u4f55\u200b\u8bbe\u7f6e\u200b <code>nn.Conv2d()</code> \u200b\u5c42\u200b\u7684\u200b\u53c2\u6570\u200b\uff1f</p> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u597d\u200b\u95ee\u9898\u200b\u3002\u200b\u4f46\u200b\u4e0e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u4e8b\u7269\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u7684\u200b\u503c\u200b\u5e76\u975e\u200b\u56fa\u5b9a\u200b\u4e0d\u53d8\u200b\uff08\u200b\u5e76\u4e14\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u662f\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u7684\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5b83\u4eec\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u8d85\u200b\u53c2\u6570\u200b\u201d\uff09\u3002</p> <p>\u200b\u627e\u51fa\u200b\u6700\u4f73\u200b\u65b9\u6cd5\u200b\u662f\u200b\u901a\u8fc7\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u5982\u4f55\u200b\u5f71\u54cd\u200b\u6a21\u578b\u200b\u7684\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u66f4\u597d\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u4f60\u200b\u9762\u4e34\u200b\u7684\u200b\u95ee\u9898\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u6709\u6548\u200b\u793a\u4f8b\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b TinyVGG \u200b\u4e2d\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff09\u200b\u5e76\u200b\u8fdb\u884c\u200b\u590d\u5236\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u4e00\u79cd\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u770b\u5230\u200b\u7684\u200b\u4e0d\u540c\u200b\u7684\u200b\u5c42\u200b\u3002</p> <p>\u200b\u4f46\u200b\u524d\u63d0\u200b\u4ecd\u7136\u200b\u76f8\u540c\u200b\uff1a\u200b\u4ece\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u66f4\u65b0\u200b\u5b83\u4eec\u200b\u4ee5\u200b\u66f4\u597d\u200b\u5730\u200b\u8868\u793a\u200b\u6570\u636e\u200b\u3002</p> In\u00a0[38]: Copied! <pre># Print out original image shape without and with unsqueezed dimension\nprint(f\"Test image original shape: {test_image.shape}\")\nprint(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")\n\n# Create a sample nn.MaxPoo2d() layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2)\n\n# Pass data through just the conv_layer\ntest_image_through_conv = conv_layer(test_image.unsqueeze(dim=0))\nprint(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")\n\n# Pass data through the max pool layer\ntest_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv)\nprint(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\")\n</pre> # Print out original image shape without and with unsqueezed dimension print(f\"Test image original shape: {test_image.shape}\") print(f\"Test image with unsqueezed dimension: {test_image.unsqueeze(dim=0).shape}\")  # Create a sample nn.MaxPoo2d() layer max_pool_layer = nn.MaxPool2d(kernel_size=2)  # Pass data through just the conv_layer test_image_through_conv = conv_layer(test_image.unsqueeze(dim=0)) print(f\"Shape after going through conv_layer(): {test_image_through_conv.shape}\")  # Pass data through the max pool layer test_image_through_conv_and_max_pool = max_pool_layer(test_image_through_conv) print(f\"Shape after going through conv_layer() and max_pool_layer(): {test_image_through_conv_and_max_pool.shape}\") <pre>Test image original shape: torch.Size([3, 64, 64])\nTest image with unsqueezed dimension: torch.Size([1, 3, 64, 64])\nShape after going through conv_layer(): torch.Size([1, 10, 62, 62])\nShape after going through conv_layer() and max_pool_layer(): torch.Size([1, 10, 31, 31])\n</pre> <p>\u200b\u6ce8\u610f\u200b\u89c2\u5bdf\u200b\u5728\u200b <code>nn.MaxPool2d()</code> \u200b\u5c42\u200b\u5185\u5916\u200b\u53d1\u751f\u200b\u7684\u200b\u53d8\u5316\u200b\u3002</p> <p><code>nn.MaxPool2d()</code> \u200b\u5c42\u200b\u7684\u200b <code>kernel_size</code> \u200b\u4f1a\u200b\u5f71\u54cd\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u5f62\u72b6\u200b\u4ece\u200b <code>62x62</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\u51cf\u534a\u200b\u4e3a\u200b <code>31x31</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u5f20\u91cf\u200b\u6765\u200b\u770b\u770b\u200b\u8fd9\u4e2a\u200b\u8fc7\u7a0b\u200b\u3002</p> In\u00a0[39]: Copied! <pre>torch.manual_seed(42)\n# Create a random tensor with a similiar number of dimensions to our images\nrandom_tensor = torch.randn(size=(1, 1, 2, 2))\nprint(f\"Random tensor:\\n{random_tensor}\")\nprint(f\"Random tensor shape: {random_tensor.shape}\")\n\n# Create a max pool layer\nmax_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value \n\n# Pass the random tensor through the max pool layer\nmax_pool_tensor = max_pool_layer(random_tensor)\nprint(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\")\nprint(f\"Max pool tensor shape: {max_pool_tensor.shape}\")\n</pre> torch.manual_seed(42) # Create a random tensor with a similiar number of dimensions to our images random_tensor = torch.randn(size=(1, 1, 2, 2)) print(f\"Random tensor:\\n{random_tensor}\") print(f\"Random tensor shape: {random_tensor.shape}\")  # Create a max pool layer max_pool_layer = nn.MaxPool2d(kernel_size=2) # see what happens when you change the kernel_size value   # Pass the random tensor through the max pool layer max_pool_tensor = max_pool_layer(random_tensor) print(f\"\\nMax pool tensor:\\n{max_pool_tensor} &lt;- this is the maximum value from random_tensor\") print(f\"Max pool tensor shape: {max_pool_tensor.shape}\") <pre>Random tensor:\ntensor([[[[0.3367, 0.1288],\n          [0.2345, 0.2303]]]])\nRandom tensor shape: torch.Size([1, 1, 2, 2])\n\nMax pool tensor:\ntensor([[[[0.3367]]]]) &lt;- this is the maximum value from random_tensor\nMax pool tensor shape: torch.Size([1, 1, 1, 1])\n</pre> <p>\u200b\u6ce8\u610f\u200b<code>random_tensor</code>\u200b\u548c\u200b<code>max_pool_tensor</code>\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6700\u540e\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4ece\u200b<code>[2, 2]</code>\u200b\u53d8\u6210\u200b\u4e86\u200b<code>[1, 1]</code>\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u5b83\u4eec\u200b\u88ab\u200b\u51cf\u534a\u200b\u4e86\u200b\u3002</p> <p>\u200b\u800c\u200b\u5bf9\u4e8e\u200b<code>nn.MaxPool2d()</code>\u200b\u7684\u200b\u4e0d\u540c\u200b<code>kernel_size</code>\u200b\u503c\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u53d8\u5316\u200b\u4e5f\u200b\u4f1a\u200b\u6709\u6240\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u8fd8\u8981\u200b\u6ce8\u610f\u200b\uff0c<code>max_pool_tensor</code>\u200b\u4e2d\u200b\u5269\u4e0b\u200b\u7684\u200b\u503c\u200b\u662f\u200b<code>random_tensor</code>\u200b\u4e2d\u200b\u7684\u200b\u6700\u5927\u503c\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u53d1\u751f\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u8fd9\u662f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u62fc\u56fe\u200b\u4e2d\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u90fd\u200b\u5728\u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u9ad8\u200b\u7ef4\u7a7a\u95f4\u200b\u7684\u200b\u6570\u636e\u538b\u7f29\u200b\u5230\u200b\u4f4e\u200b\u7ef4\u7a7a\u95f4\u200b\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5c31\u200b\u662f\u4ece\u200b\u5927\u91cf\u200b\u7684\u200b\u6570\u5b57\u200b\uff08\u200b\u539f\u59cb\u6570\u636e\u200b\uff09\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u8fd9\u4e9b\u200b\u6570\u5b57\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6a21\u5f0f\u200b\u65e2\u200b\u5177\u6709\u200b\u9884\u6d4b\u6027\u200b\uff0c\u200b\u540c\u65f6\u200b\u53c8\u200b\u6bd4\u200b\u539f\u59cb\u200b\u503c\u200b\u66f4\u200b\u5c0f\u200b\u3002</p> <p>\u200b\u4ece\u200b\u4eba\u5de5\u667a\u80fd\u200b\u7684\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6574\u4e2a\u200b\u76ee\u6807\u200b\u5c31\u662f\u200b\u538b\u7f29\u200b\u4fe1\u606f\u200b\u3002</p> <p></p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\uff0c\u200b\u4ece\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0c\u200b\u667a\u80fd\u200b\u5c31\u662f\u200b\u538b\u7f29\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u4f7f\u7528\u200b<code>nn.MaxPool2d()</code>\u200b\u5c42\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff1a\u200b\u4ece\u200b\u5f20\u91cf\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u4e2d\u53d6\u200b\u6700\u5927\u503c\u200b\uff0c\u200b\u800c\u200b\u5ffd\u7565\u200b\u5176\u4f59\u90e8\u5206\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u964d\u4f4e\u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u540c\u65f6\u200b\u4ecd\u7136\u200b\u4fdd\u7559\u200b\uff08\u200b\u5e0c\u671b\u200b\uff09\u200b\u5927\u90e8\u5206\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b<code>nn.Conv2d()</code>\u200b\u5c42\u200b\u4e5f\u200b\u662f\u200b\u5982\u6b64\u200b\u3002</p> <p>\u200b\u53ea\u4e0d\u8fc7\u200b\u4e0d\u662f\u200b\u53ea\u53d6\u200b\u6700\u5927\u503c\u200b\uff0c<code>nn.Conv2d()</code>\u200b\u5c42\u200b\u5bf9\u200b\u6570\u636e\u200b\u6267\u884c\u200b\u5377\u79ef\u200b\u64cd\u4f5c\u200b\uff08\u200b\u5728\u200bCNN Explainer\u200b\u7f51\u9875\u200b\u4e0a\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8fd9\u4e2a\u200b\u64cd\u4f5c\u200b\uff09\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u4f60\u200b\u8ba4\u4e3a\u200b<code>nn.AvgPool2d()</code>\u200b\u5c42\u200b\u662f\u200b\u505a\u200b\u4ec0\u4e48\u200b\u7684\u200b\uff1f\u200b\u5c1d\u8bd5\u200b\u50cf\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u90a3\u6837\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\u5e76\u200b\u4f20\u9012\u200b\u7ed9\u200b\u5b83\u200b\u3002\u200b\u68c0\u67e5\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u5f62\u72b6\u200b\u4ee5\u53ca\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u503c\u200b\u3002</p> <p>\u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b\uff1a \u200b\u67e5\u627e\u200b\u201c\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u201d\uff0c\u200b\u4f60\u200b\u53d1\u73b0\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u67b6\u6784\u200b\uff1f\u200b\u5176\u4e2d\u200b\u6709\u6ca1\u6709\u200b\u5305\u542b\u200b\u5728\u200b<code>torchvision.models</code>\u200b\u5e93\u4e2d\u200b\u7684\u200b\uff1f\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7528\u200b\u8fd9\u4e9b\u200b\u505a\u200b\u4ec0\u4e48\u200b\uff1f</p> In\u00a0[40]: Copied! <pre># Setup loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(params=model_2.parameters(), \n                             lr=0.1)\n</pre> # Setup loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.SGD(params=model_2.parameters(),                               lr=0.1) In\u00a0[41]: Copied! <pre>torch.manual_seed(42)\n\n# Measure time\nfrom timeit import default_timer as timer\ntrain_time_start_model_2 = timer()\n\n# Train and test model \nepochs = 3\nfor epoch in tqdm(range(epochs)):\n    print(f\"Epoch: {epoch}\\n---------\")\n    train_step(data_loader=train_dataloader, \n        model=model_2, \n        loss_fn=loss_fn,\n        optimizer=optimizer,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n    test_step(data_loader=test_dataloader,\n        model=model_2,\n        loss_fn=loss_fn,\n        accuracy_fn=accuracy_fn,\n        device=device\n    )\n\ntrain_time_end_model_2 = timer()\ntotal_train_time_model_2 = print_train_time(start=train_time_start_model_2,\n                                           end=train_time_end_model_2,\n                                           device=device)\n</pre> torch.manual_seed(42)  # Measure time from timeit import default_timer as timer train_time_start_model_2 = timer()  # Train and test model  epochs = 3 for epoch in tqdm(range(epochs)):     print(f\"Epoch: {epoch}\\n---------\")     train_step(data_loader=train_dataloader,          model=model_2,          loss_fn=loss_fn,         optimizer=optimizer,         accuracy_fn=accuracy_fn,         device=device     )     test_step(data_loader=test_dataloader,         model=model_2,         loss_fn=loss_fn,         accuracy_fn=accuracy_fn,         device=device     )  train_time_end_model_2 = timer() total_train_time_model_2 = print_train_time(start=train_time_start_model_2,                                            end=train_time_end_model_2,                                            device=device) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0\n---------\nTrain loss: 0.59302 | Train accuracy: 78.41%\nTest loss: 0.39771 | Test accuracy: 86.01%\n\nEpoch: 1\n---------\nTrain loss: 0.36149 | Train accuracy: 87.00%\nTest loss: 0.35713 | Test accuracy: 87.00%\n\nEpoch: 2\n---------\nTrain loss: 0.32354 | Train accuracy: 88.28%\nTest loss: 0.32857 | Test accuracy: 88.38%\n\nTrain time on cuda: 44.250 seconds\n</pre> <p>\u200b\u54c7\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u5377\u79ef\u200b\u5c42\u200b\u548c\u200b\u6700\u5927\u200b\u6c60\u5316\u5c42\u200b\u5e2e\u52a9\u200b\u6027\u80fd\u200b\u6709\u6240\u200b\u63d0\u5347\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>eval_model()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u8bc4\u4f30\u200b <code>model_2</code> \u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> In\u00a0[42]: Copied! <pre># Get model_2 results \nmodel_2_results = eval_model(\n    model=model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn,\n    accuracy_fn=accuracy_fn\n)\nmodel_2_results\n</pre> # Get model_2 results  model_2_results = eval_model(     model=model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,     accuracy_fn=accuracy_fn ) model_2_results Out[42]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.3285697102546692,\n 'model_acc': 88.37859424920129}</pre> In\u00a0[43]: Copied! <pre>import pandas as pd\ncompare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results])\ncompare_results\n</pre> import pandas as pd compare_results = pd.DataFrame([model_0_results, model_1_results, model_2_results]) compare_results Out[43]: model_name model_loss model_acc 0 FashionMNISTModelV0 0.476639 83.426518 1 FashionMNISTModelV1 0.685001 75.019968 2 FashionMNISTModelV2 0.328570 88.378594 <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u6dfb\u52a0\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u503c\u200b\u3002</p> In\u00a0[44]: Copied! <pre># Add training times to results comparison\ncompare_results[\"training_time\"] = [total_train_time_model_0,\n                                    total_train_time_model_1,\n                                    total_train_time_model_2]\ncompare_results\n</pre> # Add training times to results comparison compare_results[\"training_time\"] = [total_train_time_model_0,                                     total_train_time_model_1,                                     total_train_time_model_2] compare_results Out[44]: model_name model_loss model_acc training_time 0 FashionMNISTModelV0 0.476639 83.426518 32.348722 1 FashionMNISTModelV1 0.685001 75.019968 36.877976 2 FashionMNISTModelV2 0.328570 88.378594 44.249765 <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200bCNN\u200b\u6a21\u578b\u200b\uff08<code>FashionMNISTModelV2</code>\uff09\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\uff08\u200b\u635f\u5931\u200b\u6700\u4f4e\u200b\uff0c\u200b\u51c6\u786e\u7387\u200b\u6700\u9ad8\u200b\uff09\uff0c\u200b\u4f46\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u6700\u957f\u200b\u3002</p> <p>\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\uff08<code>FashionMNISTModelV0</code>\uff09\u200b\u8868\u73b0\u200b\u4f18\u4e8e\u200b<code>model_1</code>\uff08<code>FashionMNISTModelV1</code>\uff09\u3002</p> In\u00a0[45]: Copied! <pre># Visualize our model results\ncompare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\")\nplt.xlabel(\"accuracy (%)\")\nplt.ylabel(\"model\");\n</pre> # Visualize our model results compare_results.set_index(\"model_name\")[\"model_acc\"].plot(kind=\"barh\") plt.xlabel(\"accuracy (%)\") plt.ylabel(\"model\"); In\u00a0[46]: Copied! <pre>def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):\n    pred_probs = []\n    model.eval()\n    with torch.inference_mode():\n        for sample in data:\n            # Prepare sample\n            sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device\n\n            # Forward pass (model outputs raw logit)\n            pred_logit = model(sample)\n\n            # Get prediction probability (logit -&gt; prediction probability)\n            pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)\n\n            # Get pred_prob off GPU for further calculations\n            pred_probs.append(pred_prob.cpu())\n            \n    # Stack the pred_probs to turn list into a tensor\n    return torch.stack(pred_probs)\n</pre> def make_predictions(model: torch.nn.Module, data: list, device: torch.device = device):     pred_probs = []     model.eval()     with torch.inference_mode():         for sample in data:             # Prepare sample             sample = torch.unsqueeze(sample, dim=0).to(device) # Add an extra dimension and send sample to device              # Forward pass (model outputs raw logit)             pred_logit = model(sample)              # Get prediction probability (logit -&gt; prediction probability)             pred_prob = torch.softmax(pred_logit.squeeze(), dim=0) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 1, so can perform on dim=0)              # Get pred_prob off GPU for further calculations             pred_probs.append(pred_prob.cpu())                  # Stack the pred_probs to turn list into a tensor     return torch.stack(pred_probs) In\u00a0[47]: Copied! <pre>import random\nrandom.seed(42)\ntest_samples = []\ntest_labels = []\nfor sample, label in random.sample(list(test_data), k=9):\n    test_samples.append(sample)\n    test_labels.append(label)\n\n# View the first test sample shape and label\nprint(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\")\n</pre> import random random.seed(42) test_samples = [] test_labels = [] for sample, label in random.sample(list(test_data), k=9):     test_samples.append(sample)     test_labels.append(label)  # View the first test sample shape and label print(f\"Test sample image shape: {test_samples[0].shape}\\nTest sample label: {test_labels[0]} ({class_names[test_labels[0]]})\") <pre>Test sample image shape: torch.Size([1, 28, 28])\nTest sample label: 5 (Sandal)\n</pre> In\u00a0[48]: Copied! <pre># Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n</pre> # Make predictions on test samples with model 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # View first two prediction probabilities list pred_probs[:2] Out[48]: <pre>tensor([[2.4012e-07, 6.5406e-08, 4.8069e-08, 2.1070e-07, 1.4175e-07, 9.9992e-01,\n         2.1711e-07, 1.6177e-05, 3.7849e-05, 2.7548e-05],\n        [1.5646e-02, 8.9752e-01, 3.6928e-04, 6.7402e-02, 1.2920e-02, 4.9539e-05,\n         5.6485e-03, 1.9456e-04, 2.0808e-04, 3.7861e-05]])</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>make_predictions()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5bf9\u200b <code>test_samples</code> \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[49]: Copied! <pre># Make predictions on test samples with model 2\npred_probs= make_predictions(model=model_2, \n                             data=test_samples)\n\n# View first two prediction probabilities list\npred_probs[:2]\n</pre> # Make predictions on test samples with model 2 pred_probs= make_predictions(model=model_2,                               data=test_samples)  # View first two prediction probabilities list pred_probs[:2] Out[49]: <pre>tensor([[2.4012e-07, 6.5406e-08, 4.8069e-08, 2.1070e-07, 1.4175e-07, 9.9992e-01,\n         2.1711e-07, 1.6177e-05, 3.7849e-05, 2.7548e-05],\n        [1.5646e-02, 8.9752e-01, 3.6928e-04, 6.7402e-02, 1.2920e-02, 4.9539e-05,\n         5.6485e-03, 1.9456e-04, 2.0808e-04, 3.7861e-05]])</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u53d6\u200b <code>torch.softmax()</code> \u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u8f93\u51fa\u200b\u7684\u200b <code>torch.argmax()</code> \u200b\u6765\u200b\u4ece\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u8f6c\u6362\u200b\u5230\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</p> In\u00a0[50]: Copied! <pre># Turn the prediction probabilities into prediction labels by taking the argmax()\npred_classes = pred_probs.argmax(dim=1)\npred_classes\n</pre> # Turn the prediction probabilities into prediction labels by taking the argmax() pred_classes = pred_probs.argmax(dim=1) pred_classes Out[50]: <pre>tensor([5, 1, 7, 4, 3, 0, 4, 7, 1])</pre> In\u00a0[51]: Copied! <pre># Are our predictions in the same form as our test labels? \ntest_labels, pred_classes\n</pre> # Are our predictions in the same form as our test labels?  test_labels, pred_classes Out[51]: <pre>([5, 1, 7, 4, 3, 0, 4, 7, 1], tensor([5, 1, 7, 4, 3, 0, 4, 7, 1]))</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u6807\u7b7e\u200b\u683c\u5f0f\u200b\u4e00\u81f4\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u4e86\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u575a\u5b88\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\u3002</p> <p>\u201c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01\u201d</p> In\u00a0[52]: Copied! <pre># Plot predictions\nplt.figure(figsize=(9, 9))\nnrows = 3\nncols = 3\nfor i, sample in enumerate(test_samples):\n  # Create a subplot\n  plt.subplot(nrows, ncols, i+1)\n\n  # Plot the target image\n  plt.imshow(sample.squeeze(), cmap=\"gray\")\n\n  # Find the prediction label (in text form, e.g. \"Sandal\")\n  pred_label = class_names[pred_classes[i]]\n\n  # Get the truth label (in text form, e.g. \"T-shirt\")\n  truth_label = class_names[test_labels[i]] \n\n  # Create the title text of the plot\n  title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"\n  \n  # Check for equality and change title colour accordingly\n  if pred_label == truth_label:\n      plt.title(title_text, fontsize=10, c=\"g\") # green text if correct\n  else:\n      plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong\n  plt.axis(False);\n</pre> # Plot predictions plt.figure(figsize=(9, 9)) nrows = 3 ncols = 3 for i, sample in enumerate(test_samples):   # Create a subplot   plt.subplot(nrows, ncols, i+1)    # Plot the target image   plt.imshow(sample.squeeze(), cmap=\"gray\")    # Find the prediction label (in text form, e.g. \"Sandal\")   pred_label = class_names[pred_classes[i]]    # Get the truth label (in text form, e.g. \"T-shirt\")   truth_label = class_names[test_labels[i]]     # Create the title text of the plot   title_text = f\"Pred: {pred_label} | Truth: {truth_label}\"      # Check for equality and change title colour accordingly   if pred_label == truth_label:       plt.title(title_text, fontsize=10, c=\"g\") # green text if correct   else:       plt.title(title_text, fontsize=10, c=\"r\") # red text if wrong   plt.axis(False); <p>\u200b\u54c7\u200b\u54e6\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u4ec5\u4ec5\u200b\u51e0\u5341\u200b\u884c\u200b PyTorch \u200b\u4ee3\u7801\u200b\u5c31\u200b\u80fd\u200b\u505a\u5230\u200b\u8fd9\u6837\u200b\uff0c\u200b\u5df2\u7ecf\u200b\u5f88\u200b\u4e0d\u9519\u200b\u4e86\u200b\uff01</p> In\u00a0[53]: Copied! <pre># Import tqdm for progress bar\nfrom tqdm.auto import tqdm\n\n# 1. Make predictions with trained model\ny_preds = []\nmodel_2.eval()\nwith torch.inference_mode():\n  for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):\n    # Send data and targets to target device\n    X, y = X.to(device), y.to(device)\n    # Do the forward pass\n    y_logit = model_2(X)\n    # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels\n    y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)\n    # Put predictions on CPU for evaluation\n    y_preds.append(y_pred.cpu())\n# Concatenate list of predictions into a tensor\ny_pred_tensor = torch.cat(y_preds)\n</pre> # Import tqdm for progress bar from tqdm.auto import tqdm  # 1. Make predictions with trained model y_preds = [] model_2.eval() with torch.inference_mode():   for X, y in tqdm(test_dataloader, desc=\"Making predictions\"):     # Send data and targets to target device     X, y = X.to(device), y.to(device)     # Do the forward pass     y_logit = model_2(X)     # Turn predictions from logits -&gt; prediction probabilities -&gt; predictions labels     y_pred = torch.softmax(y_logit, dim=1).argmax(dim=1) # note: perform softmax on the \"logits\" dimension, not \"batch\" dimension (in this case we have a batch size of 32, so can perform on dim=1)     # Put predictions on CPU for evaluation     y_preds.append(y_pred.cpu()) # Concatenate list of predictions into a tensor y_pred_tensor = torch.cat(y_preds) <pre>Making predictions:   0%|          | 0/313 [00:00&lt;?, ?it/s]</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5f97\u5230\u200b\u4e86\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u6267\u884c\u200b\u6b65\u9aa4\u200b 2 \u200b\u548c\u200b 3\uff1a 2. \u200b\u4f7f\u7528\u200b <code>torchmetrics.ConfusionMatrix</code> \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002 3. \u200b\u4f7f\u7528\u200b <code>mlxtend.plotting.plot_confusion_matrix()</code> \u200b\u7ed8\u5236\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u4e86\u200b <code>torchmetrics</code> \u200b\u548c\u200b <code>mlxtend</code>\uff08\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u5e93\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u548c\u200b\u53ef\u89c6\u5316\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u9ed8\u8ba4\u200b\u5b89\u88c5\u200b\u7684\u200b <code>mlxtend</code> \u200b\u7248\u672c\u200b\u662f\u200b 0.14.0\uff08\u200b\u622a\u81f3\u200b 2022 \u200b\u5e74\u200b 3 \u200b\u6708\u200b\uff09\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4f7f\u7528\u200b <code>plot_confusion_matrix()</code> \u200b\u51fd\u6570\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b 0.19.0 \u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\u3002</p> In\u00a0[54]: Copied! <pre># See if torchmetrics exists, if not, install it\ntry:\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n    assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\"\nexcept:\n    !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime\n    import torchmetrics, mlxtend\n    print(f\"mlxtend version: {mlxtend.__version__}\")\n</pre> # See if torchmetrics exists, if not, install it try:     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\")     assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19, \"mlxtend verison should be 0.19.0 or higher\" except:     !pip install -q torchmetrics -U mlxtend # &lt;- Note: If you're using Google Colab, this may require restarting the runtime     import torchmetrics, mlxtend     print(f\"mlxtend version: {mlxtend.__version__}\") <pre>     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 519.2/519.2 kB 10.8 MB/s eta 0:00:00\n     \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 1.4/1.4 MB 54.9 MB/s eta 0:00:00\nmlxtend version: 0.22.0\n</pre> <p>\u200b\u8981\u200b\u7ed8\u5236\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u5b89\u88c5\u200b\u4e86\u200b 0.19.0 \u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\u7684\u200b <code>mlxtend</code>\u3002</p> In\u00a0[55]: Copied! <pre># Import mlxtend upgraded version\nimport mlxtend \nprint(mlxtend.__version__)\nassert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher\n</pre> # Import mlxtend upgraded version import mlxtend  print(mlxtend.__version__) assert int(mlxtend.__version__.split(\".\")[1]) &gt;= 19 # should be version 0.19.0 or higher <pre>0.22.0\n</pre> <p><code>torchmetrics</code> \u200b\u548c\u200b <code>mlxtend</code> \u200b\u5df2\u200b\u5b89\u88c5\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff01</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>torchmetrics.ConfusionMatrix</code> \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b <code>num_classes=len(class_names)</code> \u200b\u6765\u200b\u544a\u8bc9\u200b\u5b83\u200b\u6211\u4eec\u200b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u4f20\u9012\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\uff08<code>preds=y_pred_tensor</code>\uff09\u200b\u548c\u200b\u76ee\u6807\u200b\uff08<code>target=test_data.targets</code>\uff09\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4ee5\u200b\u5f20\u91cf\u200b\u683c\u5f0f\u200b\u8868\u793a\u200b\u7684\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>mlxtend.plotting</code> \u200b\u4e2d\u200b\u7684\u200b <code>plot_confusion_matrix()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u7ed8\u5236\u200b\u6211\u4eec\u200b\u7684\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</p> In\u00a0[56]: Copied! <pre>from torchmetrics import ConfusionMatrix\nfrom mlxtend.plotting import plot_confusion_matrix\n\n# 2. Setup confusion matrix instance and compare predictions to targets\nconfmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass')\nconfmat_tensor = confmat(preds=y_pred_tensor,\n                         target=test_data.targets)\n\n# 3. Plot the confusion matrix\nfig, ax = plot_confusion_matrix(\n    conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy \n    class_names=class_names, # turn the row and column labels into class names\n    figsize=(10, 7)\n);\n</pre> from torchmetrics import ConfusionMatrix from mlxtend.plotting import plot_confusion_matrix  # 2. Setup confusion matrix instance and compare predictions to targets confmat = ConfusionMatrix(num_classes=len(class_names), task='multiclass') confmat_tensor = confmat(preds=y_pred_tensor,                          target=test_data.targets)  # 3. Plot the confusion matrix fig, ax = plot_confusion_matrix(     conf_mat=confmat_tensor.numpy(), # matplotlib likes working with NumPy      class_names=class_names, # turn the row and column labels into class names     figsize=(10, 7) ); <p>\u200b\u54c7\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\u5427\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u5f97\u200b\u76f8\u5f53\u200b\u597d\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5927\u591a\u6570\u200b\u6df1\u8272\u200b\u65b9\u5757\u200b\u90fd\u200b\u6cbf\u7740\u200b\u4ece\u200b\u5de6\u4e0a\u89d2\u200b\u5230\u200b\u53f3\u4e0b\u89d2\u200b\u7684\u200b\u5bf9\u89d2\u7ebf\u200b\u5206\u5e03\u200b\uff08\u200b\u7406\u60f3\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u65b9\u5757\u200b\u4e2d\u200b\u53ea\u6709\u200b\u503c\u200b\uff0c\u200b\u5176\u4ed6\u200b\u5730\u65b9\u200b\u90fd\u200b\u662f\u200b0\uff09\u3002</p> <p>\u200b\u6a21\u578b\u200b\u5728\u200b\u76f8\u4f3c\u200b\u7c7b\u522b\u200b\u4e0a\u200b\u6700\u200b\u5bb9\u6613\u200b\u6df7\u6dc6\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5c06\u200b\u5b9e\u9645\u200b\u6807\u8bb0\u200b\u4e3a\u200b\u201c\u200b\u886c\u886b\u200b\u201d\u200b\u7684\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u201c\u200b\u5957\u5934\u886b\u200b\u201d\u3002</p> <p>\u200b\u540c\u6837\u200b\u5730\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u5b9e\u9645\u200b\u6807\u8bb0\u200b\u4e3a\u200b\u201cT\u200b\u6064\u200b/\u200b\u4e0a\u8863\u200b\u201d\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6a21\u578b\u200b\u4e5f\u200b\u4f1a\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u201c\u200b\u886c\u886b\u200b\u201d\u3002</p> <p>\u200b\u8fd9\u79cd\u200b\u4fe1\u606f\u200b\u901a\u5e38\u200b\u6bd4\u200b\u5355\u4e00\u200b\u7684\u200b\u51c6\u786e\u5ea6\u200b\u6307\u6807\u200b\u66f4\u200b\u6709\u200b\u5e2e\u52a9\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u5728\u200b\u54ea\u4e9b\u5730\u65b9\u200b\u51fa\u9519\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5b83\u200b\u8fd8\u200b\u6697\u793a\u200b\u4e86\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u5728\u200b\u54ea\u4e9b\u5730\u65b9\u200b\u51fa\u9519\u200b\u7684\u200b\u539f\u56e0\u200b\u3002</p> <p>\u200b\u6a21\u578b\u200b\u6709\u65f6\u200b\u5c06\u200b\u6807\u8bb0\u200b\u4e3a\u200b\u201cT\u200b\u6064\u200b/\u200b\u4e0a\u8863\u200b\u201d\u200b\u7684\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\u4e3a\u200b\u201c\u200b\u886c\u886b\u200b\u201d\u200b\u662f\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u8fd9\u79cd\u200b\u4fe1\u606f\u200b\u8fdb\u4e00\u6b65\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\uff0c\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u6539\u8fdb\u200b\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b <code>model_2</code> \u200b\u5bf9\u200b\u6d4b\u8bd5\u200b FashionMNIST \u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002\u200b\u7136\u540e\u200b\u7ed8\u5236\u200b\u4e00\u4e9b\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u9519\u8bef\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5e76\u200b\u663e\u793a\u200b\u56fe\u50cf\u200b\u5e94\u6709\u200b\u7684\u200b\u6807\u7b7e\u200b\u3002\u200b\u5728\u200b\u53ef\u89c6\u5316\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u6a21\u578b\u200b\u9519\u8bef\u200b\u8fd8\u662f\u200b\u6570\u636e\u200b\u9519\u8bef\u200b\uff1f\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6a21\u578b\u200b\u80fd\u5426\u200b\u505a\u200b\u5f97\u200b\u66f4\u597d\u200b\uff0c\u200b\u8fd8\u662f\u200b\u6570\u636e\u200b\u7684\u200b\u6807\u7b7e\u200b\u8fc7\u4e8e\u200b\u63a5\u8fd1\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u201c\u200b\u886c\u886b\u200b\u201d\u200b\u6807\u7b7e\u200b\u4e0e\u200b\u201cT\u200b\u6064\u200b/\u200b\u4e0a\u8863\u200b\u201d\u200b\u8fc7\u4e8e\u200b\u63a5\u8fd1\u200b\uff09\uff1f</p> In\u00a0[57]: Copied! <pre>from pathlib import Path\n\n# Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir\nMODEL_PATH = Path(\"models\")\nMODEL_PATH.mkdir(parents=True, # create parent directories if needed\n                 exist_ok=True # if models directory already exists, don't error\n)\n\n# Create model save path\nMODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\"\nMODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME\n\n# Save the model state dict\nprint(f\"Saving model to: {MODEL_SAVE_PATH}\")\ntorch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters\n           f=MODEL_SAVE_PATH)\n</pre> from pathlib import Path  # Create models directory (if it doesn't already exist), see: https://docs.python.org/3/library/pathlib.html#pathlib.Path.mkdir MODEL_PATH = Path(\"models\") MODEL_PATH.mkdir(parents=True, # create parent directories if needed                  exist_ok=True # if models directory already exists, don't error )  # Create model save path MODEL_NAME = \"03_pytorch_computer_vision_model_2.pth\" MODEL_SAVE_PATH = MODEL_PATH / MODEL_NAME  # Save the model state dict print(f\"Saving model to: {MODEL_SAVE_PATH}\") torch.save(obj=model_2.state_dict(), # only saving the state_dict() only saves the learned parameters            f=MODEL_SAVE_PATH) <pre>Saving model to: models/03_pytorch_computer_vision_model_2.pth\n</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b <code>state_dict()</code>\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>load_state_dict()</code> \u200b\u548c\u200b <code>torch.load()</code> \u200b\u7684\u200b\u7ec4\u5408\u200b\u5c06\u200b\u5176\u200b\u52a0\u8f7d\u200b\u56de\u6765\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b <code>load_state_dict()</code>\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b <code>FashionMNISTModelV2()</code> \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b <code>state_dict()</code> \u200b\u76f8\u540c\u200b\u7684\u200b\u8f93\u5165\u200b\u53c2\u6570\u200b\u3002</p> In\u00a0[58]: Copied! <pre># Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict())\n# Note: loading model will error if the shapes here aren't the same as the saved version\nloaded_model_2 = FashionMNISTModelV2(input_shape=1, \n                                    hidden_units=10, # try changing this to 128 and seeing what happens \n                                    output_shape=10) \n\n# Load in the saved state_dict()\nloaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))\n\n# Send model to GPU\nloaded_model_2 = loaded_model_2.to(device)\n</pre> # Create a new instance of FashionMNISTModelV2 (the same class as our saved state_dict()) # Note: loading model will error if the shapes here aren't the same as the saved version loaded_model_2 = FashionMNISTModelV2(input_shape=1,                                      hidden_units=10, # try changing this to 128 and seeing what happens                                      output_shape=10)   # Load in the saved state_dict() loaded_model_2.load_state_dict(torch.load(f=MODEL_SAVE_PATH))  # Send model to GPU loaded_model_2 = loaded_model_2.to(device) <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u52a0\u8f7d\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>eval_model()</code> \u200b\u6765\u200b\u8bc4\u4f30\u200b\u5b83\u200b\uff0c\u200b\u786e\u4fdd\u200b\u5176\u200b\u53c2\u6570\u200b\u5728\u200b\u4fdd\u5b58\u200b\u4e4b\u524d\u200b\u4e0e\u200b <code>model_2</code> \u200b\u7684\u200b\u5de5\u4f5c\u200b\u65b9\u5f0f\u200b\u76f8\u4f3c\u200b\u3002</p> <pre># \u200b\u793a\u4f8b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5047\u8bbe\u200b eval_model \u200b\u662f\u200b\u4e00\u4e2a\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b\u8bc4\u4f30\u200b\u51fd\u6570\u200b\ndef eval_model(model):\n    # \u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u7684\u200b\u4ee3\u7801\u200b\n    pass\n\neval_model(loaded_model)\n</pre> In\u00a0[59]: Copied! <pre># Evaluate loaded model\ntorch.manual_seed(42)\n\nloaded_model_2_results = eval_model(\n    model=loaded_model_2,\n    data_loader=test_dataloader,\n    loss_fn=loss_fn, \n    accuracy_fn=accuracy_fn\n)\n\nloaded_model_2_results\n</pre> # Evaluate loaded model torch.manual_seed(42)  loaded_model_2_results = eval_model(     model=loaded_model_2,     data_loader=test_dataloader,     loss_fn=loss_fn,      accuracy_fn=accuracy_fn )  loaded_model_2_results Out[59]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.3285697102546692,\n 'model_acc': 88.37859424920129}</pre> <p>\u200b\u8fd9\u4e9b\u200b\u7ed3\u679c\u200b\u770b\u8d77\u6765\u200b\u548c\u200b <code>model_2_results</code> \u200b\u4e00\u6837\u200b\u5417\u200b\uff1f</p> In\u00a0[60]: Copied! <pre>model_2_results\n</pre> model_2_results Out[60]: <pre>{'model_name': 'FashionMNISTModelV2',\n 'model_loss': 0.3285697102546692,\n 'model_acc': 88.37859424920129}</pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.isclose()</code> \u200b\u6765\u200b\u5224\u65ad\u200b\u4e24\u4e2a\u200b\u5f20\u91cf\u200b\u662f\u5426\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u53c2\u6570\u200b <code>atol</code>\uff08\u200b\u7edd\u5bf9\u200b\u5bb9\u5dee\u200b\uff09\u200b\u548c\u200b <code>rtol</code>\uff08\u200b\u76f8\u5bf9\u200b\u5bb9\u5dee\u200b\uff09\u200b\u4f20\u9012\u200b\u63a5\u8fd1\u200b\u7a0b\u5ea6\u200b\u7684\u200b\u5bb9\u5dee\u200b\u6c34\u5e73\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u63a5\u8fd1\u200b\uff0c<code>torch.isclose()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u5e94\u8be5\u200b\u662f\u200b true\u3002</p> In\u00a0[61]: Copied! <pre># Check to see if results are close to each other (if they are very far away, there may be an error)\ntorch.isclose(torch.tensor(model_2_results[\"model_loss\"]), \n              torch.tensor(loaded_model_2_results[\"model_loss\"]),\n              atol=1e-08, # absolute tolerance\n              rtol=0.0001) # relative tolerance\n</pre> # Check to see if results are close to each other (if they are very far away, there may be an error) torch.isclose(torch.tensor(model_2_results[\"model_loss\"]),                torch.tensor(loaded_model_2_results[\"model_loss\"]),               atol=1e-08, # absolute tolerance               rtol=0.0001) # relative tolerance Out[61]: <pre>tensor(True)</pre>"},{"location":"03_pytorch_computer_vision/#03-pytorch","title":"03. PyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u00b6","text":"<p>\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u662f\u200b\u6559\u200b\u8ba1\u7b97\u673a\u200b\u5b66\u4f1a\u200b\u770b\u200b\u7684\u200b\u6280\u672f\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5305\u62ec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5206\u7c7b\u200b\u4e00\u5f20\u200b\u7167\u7247\u200b\u662f\u200b\u732b\u200b\u8fd8\u662f\u200b\u72d7\u200b\uff08\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\uff09\u3002</p> <p>\u200b\u6216\u8005\u200b\u662f\u200b\u732b\u200b\u3001\u200b\u72d7\u200b\u8fd8\u662f\u200b\u9e21\u200b\u7684\u200b\u7167\u7247\u200b\uff08\u200b\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b\uff09\u3002</p> <p>\u200b\u6216\u8005\u200b\u662f\u200b\u8bc6\u522b\u200b\u89c6\u9891\u200b\u5e27\u200b\u4e2d\u200b\u6c7d\u8f66\u200b\u51fa\u73b0\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff08\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\uff09\u3002</p> <p>\u200b\u6216\u8005\u200b\u662f\u200b\u627e\u51fa\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u4e0d\u540c\u200b\u7269\u4f53\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u5206\u79bb\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff08\u200b\u5168\u666f\u200b\u5206\u5272\u200b\uff09\u3002</p> <p> \u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u3001\u200b\u591a\u200b\u7c7b\u200b\u5206\u7c7b\u200b\u3001\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u548c\u200b\u5206\u5272\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\u793a\u4f8b\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/","title":"\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u5e94\u7528\u9886\u57df\u200b\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u667a\u80fd\u624b\u673a\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u63a5\u89e6\u200b\u8fc7\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6280\u672f\u200b\u4e86\u200b\u3002</p> <p>\u200b\u76f8\u673a\u200b\u548c\u200b\u7167\u7247\u200b\u5e94\u7528\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6280\u672f\u200b\u6765\u200b\u589e\u5f3a\u200b\u548c\u200b\u5206\u7c7b\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u73b0\u4ee3\u200b\u6c7d\u8f66\u200b\u5229\u7528\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6280\u672f\u200b\u6765\u200b\u907f\u514d\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u8f66\u8f86\u200b\u78b0\u649e\u200b\u5e76\u200b\u4fdd\u6301\u200b\u5728\u200b\u8f66\u9053\u200b\u5185\u200b\u884c\u9a76\u200b\u3002</p> <p>\u200b\u5236\u9020\u5546\u200b\u4f7f\u7528\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6280\u672f\u200b\u6765\u200b\u8bc6\u522b\u200b\u5404\u79cd\u200b\u4ea7\u54c1\u200b\u4e2d\u200b\u7684\u200b\u7f3a\u9677\u200b\u3002</p> <p>\u200b\u76d1\u63a7\u200b\u6444\u50cf\u5934\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6280\u672f\u200b\u6765\u200b\u68c0\u6d4b\u200b\u6f5c\u5728\u200b\u7684\u200b\u5165\u4fb5\u8005\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u4efb\u4f55\u200b\u53ef\u4ee5\u200b\u7528\u200b\u89c6\u89c9\u200b\u65b9\u5f0f\u200b\u63cf\u8ff0\u200b\u7684\u200b\u4e8b\u7269\u200b\u90fd\u200b\u6709\u200b\u53ef\u80fd\u200b\u6210\u4e3a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6280\u672f\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/","title":"\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u4f1a\u200b\u628a\u200b\u5728\u200b\u524d\u200b\u51e0\u8282\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u5e94\u7528\u200b\u4e8e\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\u3002</p> <p></p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. PyTorch \u200b\u4e2d\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b PyTorch \u200b\u5185\u7f6e\u200b\u4e86\u200b\u8bb8\u591a\u200b\u6709\u7528\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u3002 1. \u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b \u200b\u4e3a\u4e86\u200b\u7ec3\u4e60\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b FashionMNIST \u200b\u4e2d\u200b\u83b7\u53d6\u200b\u4e00\u4e9b\u200b\u4e0d\u540c\u200b\u670d\u88c5\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002 2. \u200b\u51c6\u5907\u200b\u6570\u636e\u200b \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b PyTorch <code>DataLoader</code> \u200b\u52a0\u8f7d\u200b\u5b83\u4eec\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u3002 3. \u200b\u6a21\u578b\u200b 0\uff1a\u200b\u6784\u5efa\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b \u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u6765\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u9009\u62e9\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u5e76\u200b\u6784\u5efa\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u3002 4. \u200b\u9884\u6d4b\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b 0 \u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8bc4\u4f30\u200b\u5b83\u4eec\u200b\u3002 5. \u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u4ee5\u4f9b\u200b\u672a\u6765\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200b \u200b\u7f16\u5199\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u662f\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u5b83\u200b\u3002 6. \u200b\u6a21\u578b\u200b 1\uff1a\u200b\u589e\u52a0\u200b\u975e\u7ebf\u6027\u200b \u200b\u5b9e\u9a8c\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u91cd\u8981\u200b\u90e8\u5206\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u901a\u8fc7\u200b\u589e\u52a0\u200b\u975e\u7ebf\u6027\u200b\u5c42\u6765\u200b\u6539\u8fdb\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u3002 7. \u200b\u6a21\u578b\u200b 2\uff1a\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b (CNN) \u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\uff0c\u200b\u5e76\u200b\u5f15\u5165\u200b\u5f3a\u5927\u200b\u7684\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\u4e86\u200b\u3002 8. \u200b\u6bd4\u8f83\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6784\u5efa\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u6bd4\u8f83\u200b\u5b83\u4eec\u200b\u3002 9. \u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bf9\u200b\u968f\u673a\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u3002 10. \u200b\u5236\u4f5c\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b \u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u662f\u200b\u8bc4\u4f30\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u7684\u200b\u597d\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u5236\u4f5c\u200b\u4e00\u4e2a\u200b\u3002 11. \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u8868\u73b0\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b \u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4ee5\u540e\u200b\u4f1a\u200b\u7528\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4fdd\u5b58\u200b\u5b83\u200b\u5e76\u200b\u786e\u4fdd\u200b\u5b83\u200b\u80fd\u200b\u6b63\u786e\u200b\u52a0\u8f7d\u200b\u56de\u6765\u200b\u3002"},{"location":"03_pytorch_computer_vision/","title":"\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u5b58\u653e\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub Discussions \u200b\u9875\u9762\u200b \u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u6587\u6863\u200b \u200b\u548c\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#0-pytorch","title":"0. PyTorch \u200b\u4e2d\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u5f00\u59cb\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u6765\u200b\u4e86\u89e3\u200b\u4e00\u4e0b\u200b\u4f60\u200b\u5e94\u8be5\u200b\u77e5\u9053\u200b\u7684\u200b\u4e00\u4e9b\u200b PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b\u3002</p> PyTorch \u200b\u6a21\u5757\u200b \u200b\u529f\u80fd\u200b\u63cf\u8ff0\u200b <code>torchvision</code> \u200b\u5305\u542b\u200b\u5e38\u7528\u200b\u4e8e\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3001\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u548c\u200b\u56fe\u50cf\u200b\u53d8\u6362\u200b\u3002 <code>torchvision.datasets</code> \u200b\u8fd9\u91cc\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u8bb8\u591a\u200b\u7528\u4e8e\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u3001\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u3001\u200b\u56fe\u50cf\u200b\u6807\u6ce8\u200b\u3001\u200b\u89c6\u9891\u5206\u7c7b\u200b\u7b49\u200b\u95ee\u9898\u200b\u7684\u200b\u793a\u4f8b\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u5b83\u200b\u8fd8\u200b\u5305\u542b\u200b\u4e00\u7cfb\u5217\u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u57fa\u7c7b\u200b\u3002 <code>torchvision.models</code> \u200b\u8fd9\u4e2a\u200b\u6a21\u5757\u200b\u5305\u542b\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u5b9e\u73b0\u200b\u7684\u200b\u6027\u80fd\u200b\u826f\u597d\u200b\u4e14\u200b\u5e38\u7528\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u7528\u4e8e\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002 <code>torchvision.transforms</code> \u200b\u901a\u5e38\u200b\u5728\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\uff0c\u200b\u56fe\u50cf\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\uff08\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6570\u5b57\u200b/\u200b\u5904\u7406\u200b/\u200b\u589e\u5f3a\u200b\uff09\uff0c\u200b\u5e38\u89c1\u200b\u7684\u200b\u56fe\u50cf\u200b\u53d8\u6362\u200b\u65b9\u6cd5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u3002 <code>torch.utils.data.Dataset</code> PyTorch \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u57fa\u7c7b\u200b\u3002 <code>torch.utils.data.DataLoader</code> \u200b\u5728\u200b <code>torch.utils.data.Dataset</code> \u200b\u521b\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b Python \u200b\u53ef\u200b\u8fed\u4ee3\u200b\u5bf9\u8c61\u200b\u3002 <p>\u200b\u6ce8\u610f\u200b\uff1a <code>torch.utils.data.Dataset</code> \u200b\u548c\u200b <code>torch.utils.data.DataLoader</code> \u200b\u7c7b\u200b\u4e0d\u4ec5\u200b\u9002\u7528\u200b\u4e8e\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\uff0c\u200b\u5b83\u4eec\u200b\u8fd8\u200b\u80fd\u200b\u5904\u7406\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u5bfc\u5165\u200b\u76f8\u5173\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u8981\u200b\u5f00\u59cb\u200b\u5904\u7406\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\uff0c\u200b\u9996\u5148\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b FashionMNIST \u200b\u5f00\u59cb\u200b\u3002</p> <p>MNIST \u200b\u4ee3\u8868\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b\u7f8e\u56fd\u200b\u56fd\u5bb6\u6807\u51c6\u200b\u4e0e\u200b\u6280\u672f\u200b\u7814\u7a76\u9662\u200b\uff08Modified National Institute of Standards and Technology\uff09\u3002</p> <p>\u200b\u539f\u59cb\u200b\u7684\u200b MNIST \u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b\u6210\u5343\u4e0a\u4e07\u200b\u7684\u200b\u624b\u5199\u200b\u6570\u5b57\u200b\uff08\u200b\u4ece\u200b0\u200b\u5230\u200b9\uff09\u200b\u7684\u200b\u793a\u4f8b\u200b\uff0c\u200b\u66fe\u200b\u88ab\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ee5\u200b\u8bc6\u522b\u200b\u90ae\u5c40\u200b\u4e2d\u200b\u7684\u200b\u6570\u5b57\u200b\u3002</p> <p>FashionMNIST \u200b\u7531\u200b Zalando \u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u5236\u4f5c\u200b\uff0c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u3002</p> <p>\u200b\u53ea\u4e0d\u8fc7\u200b\u5b83\u200b\u5305\u542b\u200b10\u200b\u79cd\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u670d\u88c5\u200b\u7684\u200b\u7070\u5ea6\u200b\u56fe\u50cf\u200b\u3002</p> <p> <code>torchvision.datasets</code> \u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u793a\u4f8b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u7ec3\u4e60\u200b\u7f16\u5199\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4ee3\u7801\u200b\u3002FashionMNIST \u200b\u5c31\u662f\u200b\u5176\u4e2d\u200b\u4e4b\u4e00\u200b\u3002\u200b\u7531\u4e8e\u200b\u5b83\u200b\u6709\u200b10\u200b\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u7c7b\u522b\u200b\uff08\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u670d\u88c5\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u7a0d\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u795e\u7ecf\u200b\u7f51\u7edc\u200b\u6765\u200b\u8bc6\u522b\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u540c\u200b\u670d\u88c5\u200b\u98ce\u683c\u200b\u3002</p> <p>PyTorch \u200b\u5728\u200b <code>torchvision.datasets</code> \u200b\u4e2d\u200b\u5b58\u50a8\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5e38\u89c1\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u5305\u62ec\u200b\u5728\u200b <code>torchvision.datasets.FashionMNIST()</code> \u200b\u4e2d\u200b\u7684\u200b FashionMNIST\u3002</p> <p>\u200b\u8981\u200b\u4e0b\u8f7d\u200b\u5b83\u200b\uff0c\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4ee5\u4e0b\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li><code>root: str</code> - \u200b\u4f60\u200b\u60f3\u200b\u5c06\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u5230\u200b\u54ea\u4e2a\u200b\u6587\u4ef6\u5939\u200b\uff1f</li> <li><code>train: Bool</code> - \u200b\u4f60\u200b\u60f3\u8981\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u8fd8\u662f\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff1f</li> <li><code>download: Bool</code> - \u200b\u662f\u5426\u200b\u5e94\u8be5\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\uff1f</li> <li><code>transform: torchvision.transforms</code> - \u200b\u4f60\u200b\u5e0c\u671b\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u54ea\u4e9b\u200b\u53d8\u6362\u200b\uff1f</li> <li><code>target_transform</code> - \u200b\u5982\u679c\u200b\u4f60\u200b\u613f\u610f\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u76ee\u6807\u200b\uff08\u200b\u6807\u7b7e\u200b\uff09\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\u3002</li> </ul> <p><code>torchvision</code> \u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u6570\u636e\u200b\u96c6\u200b\u4e5f\u200b\u6709\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u9009\u9879\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#11","title":"1.1 \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u5165\u200b\u4e0e\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u5927\u91cf\u200b\u503c\u200b\u7684\u200b\u5f20\u91cf\u200b\uff08\u200b\u56fe\u50cf\u200b\uff09\uff0c\u200b\u6700\u7ec8\u200b\u8f93\u51fa\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u503c\u200b\uff08\u200b\u6807\u7b7e\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u56fe\u50cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#12","title":"1.2 \u200b\u53ef\u89c6\u5316\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u00b6","text":""},{"location":"03_pytorch_computer_vision/#2-dataloader","title":"2. \u200b\u51c6\u5907\u200b DataLoader\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u662f\u200b\u7528\u200b <code>torch.utils.data.DataLoader</code> \u200b\u6216\u200b\u7b80\u79f0\u200b <code>DataLoader</code> \u200b\u6765\u200b\u51c6\u5907\u200b\u5b83\u200b\u3002</p> <p><code>DataLoader</code> \u200b\u7684\u200b\u4f5c\u7528\u200b\u6b63\u5982\u200b\u5176\u540d\u200b\u3002</p> <p>\u200b\u5b83\u200b\u5e2e\u52a9\u200b\u5c06\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5230\u200b\u6a21\u578b\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u65e0\u8bba\u662f\u200b\u8bad\u7ec3\u200b\u8fd8\u662f\u200b\u63a8\u7406\u200b\u3002</p> <p>\u200b\u5b83\u200b\u5c06\u200b\u4e00\u4e2a\u200b\u5927\u200b\u7684\u200b <code>Dataset</code> \u200b\u8f6c\u6362\u6210\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6570\u636e\u200b\u5757\u200b\u7684\u200b Python \u200b\u53ef\u200b\u8fed\u4ee3\u200b\u5bf9\u8c61\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6570\u636e\u200b\u5757\u200b\u88ab\u200b\u79f0\u4e3a\u200b \u200b\u6279\u6b21\u200b \u200b\u6216\u200b \u200b\u5c0f\u200b\u6279\u6b21\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>batch_size</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u8fd9\u6837\u200b\u5728\u200b\u8ba1\u7b97\u200b\u4e0a\u200b\u66f4\u200b\u9ad8\u6548\u200b\u3002</p> <p>\u200b\u5728\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4e00\u6b21\u6027\u200b\u5bf9\u200b\u6240\u6709\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u548c\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e00\u65e6\u200b\u4f60\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\u975e\u5e38\u200b\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u9664\u975e\u200b\u4f60\u200b\u6709\u200b\u65e0\u9650\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\uff0c\u200b\u5426\u5219\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5206\u6210\u200b\u6279\u6b21\u200b\u4f1a\u200b\u66f4\u200b\u5bb9\u6613\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e5f\u200b\u7ed9\u200b\u6a21\u578b\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u6539\u8fdb\u200b\u7684\u200b\u673a\u4f1a\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b \u200b\u5c0f\u200b\u6279\u6b21\u200b\uff08\u200b\u6570\u636e\u200b\u7684\u200b\u4e00\u5c0f\u90e8\u5206\u200b\uff09\uff0c\u200b\u6bcf\u4e2a\u200b epoch \u200b\u4e2d\u200b\u4f1a\u200b\u8fdb\u884c\u200b\u66f4\u200b\u9891\u7e41\u200b\u7684\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u5c0f\u200b\u6279\u6b21\u200b\u4e00\u6b21\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6bcf\u4e2a\u200b epoch \u200b\u4e00\u6b21\u200b\uff09\u3002</p> <p>\u200b\u4ec0\u4e48\u200b\u662f\u200b\u597d\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff1f</p> <p>32 \u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e0d\u9519\u200b\u7684\u200b\u8d77\u70b9\u200b \u200b\u5bf9\u4e8e\u200b\u8bb8\u591a\u200b\u95ee\u9898\u200b\u6765\u8bf4\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u503c\u200b\uff08\u200b\u4e00\u4e2a\u200b \u200b\u8d85\u200b\u53c2\u6570\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u5404\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b 2 \u200b\u7684\u200b\u5e42\u6b21\u200b\uff08\u200b\u4f8b\u5982\u200b 32, 64, 128, 256, 512\uff09\u3002</p> <p> \u200b\u4f7f\u7528\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u4e3a\u200b 32 \u200b\u5e76\u200b\u5f00\u542f\u200b shuffle \u200b\u7684\u200b FashionMNIST \u200b\u6279\u5904\u7406\u200b\u793a\u4f8b\u200b\u3002\u200b\u5176\u4ed6\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6279\u5904\u7406\u200b\u8fc7\u7a0b\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4f46\u4f1a\u200b\u6839\u636e\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u800c\u200b\u6709\u6240\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u521b\u5efa\u200b <code>DataLoader</code>\u3002</p>"},{"location":"03_pytorch_computer_vision/#3-0","title":"3. \u200b\u6a21\u578b\u200b0\uff1a\u200b\u6784\u5efa\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6570\u636e\u200b\u5df2\u200b\u52a0\u8f7d\u200b\u5e76\u200b\u51c6\u5907\u200b\u597d\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u901a\u8fc7\u200b\u5b50\u200b\u7c7b\u5316\u200b <code>nn.Module</code> \u200b\u6765\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u662f\u200b\u4f60\u200b\u80fd\u200b\u60f3\u8c61\u200b\u5230\u200b\u7684\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u6a21\u578b\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5c06\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u4f5c\u4e3a\u200b\u8d77\u70b9\u200b\uff0c\u200b\u5e76\u200b\u5c1d\u8bd5\u200b\u901a\u8fc7\u200b\u540e\u7eed\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u6a21\u578b\u200b\u6765\u200b\u6539\u8fdb\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u5c06\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u8fd9\u6837\u200b\u505a\u8fc7\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u4f1a\u200b\u6709\u200b\u4e00\u70b9\u200b\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u5c42\u200b\u6765\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u90a3\u200b\u5c31\u662f\u200b <code>nn.Flatten()</code> \u200b\u5c42\u200b\u3002</p> <p><code>nn.Flatten()</code> \u200b\u5c06\u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u538b\u7f29\u6210\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u5411\u91cf\u200b\u3002</p> <p>\u200b\u5f53\u200b\u4f60\u200b\u770b\u5230\u200b\u5b83\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u200b\u66f4\u200b\u5bb9\u6613\u200b\u7406\u89e3\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#31","title":"3.1 \u200b\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u4e00\u4e2a\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f15\u5165\u200b\u6211\u4eec\u200b\u7684\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b 02 \u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u7684\u200b <code>accuracy_fn()</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0e\u5176\u200b\u5bfc\u5165\u200b\u548c\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u51fd\u6570\u200b\u6216\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4ece\u200b TorchMetrics \u200b\u5305\u200b \u200b\u5bfc\u5165\u200b\u5404\u79cd\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#32","title":"3.2 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u8ba1\u65f6\u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u00b6","text":"<p>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u59a8\u200b\u505a\u200b\u4e00\u4e9b\u200b\u5c0f\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u6211\u200b\u7684\u200b\u610f\u601d\u200b\u662f\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8ba1\u65f6\u200b\u51fd\u6570\u200b\uff0c\u200b\u6765\u200b\u6d4b\u91cf\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200bCPU\u200b\u548c\u200bGPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5148\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u8ba1\u65f6\u200b\u51fd\u6570\u200b\u5c06\u200b\u5bfc\u5165\u200bPython\u200b\u7684\u200b<code>timeit</code>\u200b\u6a21\u5757\u200b\u4e2d\u200b\u7684\u200b<code>timeit.default_timer()</code>\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#33","title":"3.3 \u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u5e76\u200b\u5728\u200b\u6570\u636e\u200b\u6279\u6b21\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u6240\u6709\u200b\u7684\u200b\u62fc\u56fe\u200b\u788e\u7247\u200b\u4e86\u200b\uff0c\u200b\u5305\u62ec\u200b\u8ba1\u65f6\u5668\u200b\u3001\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u3001\u200b\u6a21\u578b\u200b\uff0c\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8fd8\u6709\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u548c\u200b\u4e00\u4e2a\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u6765\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u76f8\u540c\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u73b0\u5728\u200b\u662f\u200b\u6279\u6b21\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6dfb\u52a0\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5faa\u73af\u200b\u6765\u200b\u904d\u5386\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u6279\u6b21\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u6279\u6b21\u200b\u5305\u542b\u200b\u5728\u200b <code>DataLoader</code> \u200b\u4e2d\u200b\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u5206\u5272\u200b\u7684\u200b <code>train_dataloader</code> \u200b\u548c\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u5206\u5272\u200b\u7684\u200b <code>test_dataloader</code>\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u662f\u200b <code>BATCH_SIZE</code> \u200b\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b <code>X</code>\uff08\u200b\u7279\u5f81\u200b\uff09\u200b\u548c\u200b <code>y</code>\uff08\u200b\u6807\u7b7e\u200b\uff09\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>BATCH_SIZE=32</code>\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6279\u6b21\u200b\u5305\u542b\u200b 32 \u200b\u4e2a\u200b\u56fe\u50cf\u200b\u6837\u672c\u200b\u548c\u200b\u76ee\u6807\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200b\u6279\u6b21\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u5c06\u200b\u6309\u200b\u6279\u6b21\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6574\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u635f\u5931\u200b\u548c\u200b\u51c6\u786e\u6027\u200b\u503c\u200b\u9664\u4ee5\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b <code>DataLoader</code> \u200b\u4e2d\u200b\u7684\u200b\u6279\u6b21\u200b\u6570\u91cf\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u6b65\u6b65\u200b\u6765\u200b\uff1a</p> <ol> <li>\u200b\u904d\u5386\u200b epochs\u3002</li> <li>\u200b\u904d\u5386\u200b\u8bad\u7ec3\u200b\u6279\u6b21\u200b\uff0c\u200b\u6267\u884c\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u6bcf\u4e2a\u200b\u6279\u6b21\u200b\u7684\u200b\u8bad\u7ec3\u200b\u635f\u5931\u200b\u3002</li> <li>\u200b\u904d\u5386\u200b\u6d4b\u8bd5\u200b\u6279\u6b21\u200b\uff0c\u200b\u6267\u884c\u200b\u6d4b\u8bd5\u6b65\u9aa4\u200b\uff0c\u200b\u8ba1\u7b97\u200b\u6bcf\u4e2a\u200b\u6279\u6b21\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u3002</li> <li>\u200b\u8f93\u51fa\u200b\u6b63\u5728\u200b\u53d1\u751f\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</li> <li>\u200b\u8ba1\u65f6\u200b\uff08\u200b\u4e3a\u4e86\u200b\u597d\u73a9\u200b\uff09\u3002</li> </ol> <p>\u200b\u867d\u7136\u200b\u6b65\u9aa4\u200b\u4e0d\u5c11\u200b\uff0c\u200b\u4f46\u662f\u200b...</p> <p>...\u200b\u5982\u679c\u200b\u6709\u200b\u7591\u95ee\u200b\uff0c\u200b\u5c31\u200b\u5199\u200b\u4ee3\u7801\u200b\u89e3\u51b3\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#4-0","title":"4. \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b0\u200b\u7684\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u591a\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\u4ee5\u200b\u76f8\u4f3c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8bc4\u4f30\u200b\u5b83\u4eec\u200b\u662f\u200b\u4e00\u4e2a\u200b\u597d\u200b\u4e3b\u610f\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u5df2\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b<code>DataLoader</code>\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u51c6\u786e\u7387\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u8be5\u200b\u51fd\u6570\u200b\u5c06\u200b\u4f7f\u7528\u200b\u6a21\u578b\u200b\u5bf9\u200b<code>DataLoader</code>\u200b\u4e2d\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u51c6\u786e\u7387\u200b\u51fd\u6570\u200b\u6765\u200b\u8bc4\u4f30\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#5-gpu","title":"5. \u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\uff08\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u6709\u200bGPU\u200b\u65f6\u200b\u4f7f\u7528\u200b\uff09\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u5230\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b60,000\u200b\u4e2a\u200b\u6837\u672c\u200b\u7684\u200bPyTorch\u200b\u6a21\u578b\u200b\u9700\u8981\u200b\u591a\u957f\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u53d6\u51b3\u4e8e\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u786c\u4ef6\u200b\u3002\u200b\u901a\u5e38\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5904\u7406\u5668\u200b\u610f\u5473\u7740\u200b\u66f4\u5feb\u200b\u7684\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\uff0c\u200b\u800c\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u901a\u5e38\u200b\u4f1a\u200b\u6bd4\u200b\u5927\u578b\u200b\u6a21\u578b\u200b\u548c\u200b\u5927\u578b\u200b\u6570\u636e\u200b\u96c6\u200b\u8bad\u7ec3\u200b\u5f97\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u8bbe\u7f6e\u200b\u4e00\u4e9b\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200bGPU\u200b\u53ef\u7528\u200b\u65f6\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200bGoogle Colab\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u5f00\u542f\u200bGPU\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u901a\u8fc7\u200b<code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code>\u200b\u6765\u200b\u5f00\u542f\u200b\u4e00\u4e2a\u200bGPU\u200b\u4e86\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u8fd0\u884c\u200b\u65f6\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u91cd\u7f6e\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f60\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b<code>Runtime -&gt; Run before</code>\u200b\u6765\u200b\u91cd\u65b0\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u6240\u6709\u200b\u5355\u5143\u683c\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#6-1","title":"6. \u200b\u6a21\u578b\u200b1\uff1a\u200b\u901a\u8fc7\u200b\u975e\u7ebf\u6027\u200b\u6784\u5efa\u200b\u66f4\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b02\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u4e86\u200b\u975e\u7ebf\u6027\u200b\u7684\u200b\u529b\u91cf\u200b\u3002</p> <p>\u200b\u89c2\u5bdf\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u5b83\u200b\u9700\u8981\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u5417\u200b\uff1f</p> <p>\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u7ebf\u6027\u200b\u610f\u5473\u7740\u200b\u76f4\u7ebf\u200b\uff0c\u200b\u800c\u200b\u975e\u7ebf\u6027\u200b\u610f\u5473\u7740\u200b\u975e\u200b\u76f4\u7ebf\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u9a8c\u8bc1\u200b\u4e00\u4e0b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u6a21\u578b\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u52a0\u5165\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\uff08<code>nn.ReLU()</code>\uff09\u3002</p>"},{"location":"03_pytorch_computer_vision/#61","title":"6.1 \u200b\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u00b6","text":"<p>\u200b\u4e0e\u200b\u5f80\u5e38\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3001\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u4e00\u4e2a\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff08\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u591a\u4e2a\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff0c\u200b\u4f46\u200b\u76ee\u524d\u200b\u6211\u4eec\u200b\u53ea\u200b\u4f7f\u7528\u200b\u51c6\u786e\u6027\u200b\uff09\u3002</p>"},{"location":"03_pytorch_computer_vision/#62","title":"6.2 \u200b\u5c06\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u51fd\u6570\u200b\u5316\u200b\u00b6","text":"<p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u591a\u6b21\u200b\u7f16\u5199\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u3002</p> <p>\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u518d\u6b21\u200b\u7f16\u5199\u200b\u5b83\u4eec\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u628a\u200b\u5b83\u4eec\u200b\u653e\u5728\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5b83\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u53cd\u590d\u200b\u8c03\u7528\u200b\u3002</p> <p>\u200b\u800c\u4e14\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u786e\u4fdd\u200b\u5728\u200b\u7279\u5f81\u200b\uff08<code>X</code>\uff09\u200b\u548c\u200b\u76ee\u6807\u200b\uff08<code>y</code>\uff09\u200b\u5f20\u91cf\u200b\u4e0a\u200b\u8c03\u7528\u200b <code>.to(device)</code>\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>train_step()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u5b83\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b <code>DataLoader</code>\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u5c06\u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u5c06\u200b\u88ab\u200b\u547d\u540d\u200b\u4e3a\u200b <code>test_step()</code>\uff0c\u200b\u5e76\u4e14\u200b\u5b83\u200b\u5c06\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b <code>DataLoader</code>\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u8bc4\u4f30\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7531\u4e8e\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u51fd\u6570\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u81ea\u5b9a\u4e49\u200b\u5b83\u4eec\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u5236\u4f5c\u200b\u7684\u200b\u662f\u200b\u7279\u5b9a\u200b\u5206\u7c7b\u200b\u7528\u4f8b\u200b\u7684\u200b\u57fa\u672c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u51fd\u6570\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#7-2cnn","title":"7. \u200b\u6a21\u578b\u200b2\uff1a\u200b\u6784\u5efa\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08CNN\uff09\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u63d0\u5347\u200b\u4e00\u4e2a\u200b\u6863\u6b21\u200b\u4e86\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08CNN \u200b\u6216\u200b ConvNet\uff09\u3002</p> <p>CNN \u200b\u4ee5\u200b\u5176\u200b\u8bc6\u522b\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u4e2d\u200b\u6a21\u5f0f\u200b\u7684\u200b\u80fd\u529b\u200b\u800c\u200b\u95fb\u540d\u200b\u3002</p> <p>\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u4f7f\u7528\u200b CNN \u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u80fd\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u7684\u200b\u57fa\u7ebf\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u7684\u200b CNN \u200b\u6a21\u578b\u200b\u79f0\u4e3a\u200b TinyVGG\uff0c\u200b\u6765\u81ea\u200bCNN Explainer\u200b\u7f51\u7ad9\u200b\u3002</p> <p>\u200b\u5b83\u200b\u9075\u5faa\u200b\u5178\u578b\u200b\u7684\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7ed3\u6784\u200b\uff1a</p> <p><code>\u200b\u8f93\u5165\u200b\u5c42\u200b -&gt; [\u200b\u5377\u79ef\u200b\u5c42\u200b -&gt; \u200b\u6fc0\u6d3b\u200b\u5c42\u200b -&gt; \u200b\u6c60\u5316\u5c42\u200b] -&gt; \u200b\u8f93\u51fa\u200b\u5c42\u200b</code></p> <p>\u200b\u5176\u4e2d\u200b\uff0c<code>[\u200b\u5377\u79ef\u200b\u5c42\u200b -&gt; \u200b\u6fc0\u6d3b\u200b\u5c42\u200b -&gt; \u200b\u6c60\u5316\u5c42\u200b]</code>\u200b\u7684\u200b\u5185\u5bb9\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u548c\u200b\u91cd\u590d\u200b\u591a\u6b21\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/","title":"\u6211\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u4ec0\u4e48\u200b\u6a21\u578b\u200b\uff1f\u00b6","text":"<p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u7b49\u7b49\u200b\uff0c\u200b\u4f60\u200b\u8bf4\u200bCNN\u200b\u9002\u5408\u200b\u56fe\u50cf\u5904\u7406\u200b\uff0c\u200b\u8fd8\u6709\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u6211\u200b\u5e94\u8be5\u200b\u4e86\u89e3\u200b\u5417\u200b\uff1f</p> <p>\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u8868\u683c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u901a\u7528\u200b\u6307\u5357\u200b\uff0c\u200b\u544a\u8bc9\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u54ea\u200b\u79cd\u200b\u6a21\u578b\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u4e5f\u200b\u6709\u200b\u4f8b\u5916\u200b\uff09\u3002</p> \u200b\u95ee\u9898\u200b\u7c7b\u578b\u200b \u200b\u4e00\u822c\u200b\u4f7f\u7528\u200b\u7684\u200b\u6a21\u578b\u200b \u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b \u200b\u7ed3\u6784\u5316\u200b\u6570\u636e\u200b\uff08Excel\u200b\u8868\u683c\u200b\uff0c\u200b\u884c\u548c\u5217\u200b\u6570\u636e\u200b\uff09 \u200b\u68af\u5ea6\u200b\u63d0\u5347\u200b\u6a21\u578b\u200b\uff0c\u200b\u968f\u673a\u200b\u68ee\u6797\u200b\uff0cXGBoost <code>sklearn.ensemble</code>, XGBoost\u200b\u5e93\u200b \u200b\u975e\u200b\u7ed3\u6784\u5316\u200b\u6570\u636e\u200b\uff08\u200b\u56fe\u50cf\u200b\uff0c\u200b\u97f3\u9891\u200b\uff0c\u200b\u8bed\u8a00\u200b\uff09 \u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0cTransformer <code>torchvision.models</code>, HuggingFace Transformers <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u683c\u200b\u4ec5\u4f9b\u53c2\u8003\u200b\uff0c\u200b\u4f60\u200b\u6700\u7ec8\u200b\u4f7f\u7528\u200b\u7684\u200b\u6a21\u578b\u200b\u5c06\u200b\u9ad8\u5ea6\u200b\u4f9d\u8d56\u4e8e\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u4ee5\u53ca\u200b\u4f60\u200b\u6240\u200b\u9762\u4e34\u200b\u7684\u200b\u7ea6\u675f\u200b\uff08\u200b\u6570\u636e\u91cf\u200b\uff0c\u200b\u5ef6\u8fdf\u200b\u8981\u6c42\u200b\uff09\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u8ba8\u8bba\u200b\u5c31\u200b\u5230\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200bCNN\u200b\u6a21\u578b\u200b\uff0c\u200b\u590d\u5236\u200bCNN Explainer\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p></p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5229\u7528\u200b<code>torch.nn</code>\u200b\u4e2d\u200b\u7684\u200b<code>nn.Conv2d()</code>\u200b\u548c\u200b<code>nn.MaxPool2d()</code>\u200b\u5c42\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#71-nnconv2d","title":"7.1 \u200b\u9010\u6b65\u200b\u89e3\u6790\u200b <code>nn.Conv2d()</code>\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u4e0a\u9762\u200b\u7684\u200b\u6a21\u578b\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\u5e76\u200b\u89c2\u5bdf\u200b\u5176\u200b\u8868\u73b0\u200b\uff0c\u200b\u4f46\u200b\u9996\u5148\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9010\u6b65\u200b\u4e86\u89e3\u200b\u6211\u4eec\u200b\u65b0\u589e\u200b\u7684\u200b\u4e24\u4e2a\u200b\u5c42\u200b\uff1a</p> <ul> <li><code>nn.Conv2d()</code>\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u5377\u79ef\u200b\u5c42\u200b\u3002</li> <li><code>nn.MaxPool2d()</code>\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u6700\u5927\u200b\u6c60\u5316\u5c42\u200b\u3002</li> </ul> <p>\u200b\u95ee\u9898\u200b\uff1a <code>nn.Conv2d()</code> \u200b\u4e2d\u200b\u7684\u200b \"2d\" \u200b\u4ee3\u8868\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\"2d\" \u200b\u4ee3\u8868\u200b\u4e8c\u7ef4\u200b\u6570\u636e\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u6709\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\uff1a\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u4e5f\u200b\u6709\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u4f46\u200b\u6bcf\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u540c\u6837\u200b\u5177\u6709\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u5176\u4ed6\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6570\u636e\u200b\uff08\u200b\u4f8b\u5982\u200b\u6587\u672c\u200b\u7684\u200b\u4e00\u7ef4\u200b\u6570\u636e\u200b\u6216\u200b\u4e09\u7ef4\u200b\u7269\u4f53\u200b\u7684\u200b\u4e09\u7ef4\u200b\u6570\u636e\u200b\uff09\uff0c\u200b\u8fd8\u6709\u200b <code>nn.Conv1d()</code> \u200b\u548c\u200b <code>nn.Conv3d()</code>\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u6d4b\u8bd5\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b CNN Explainer \u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u793a\u4f8b\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#72-nnmaxpool2d","title":"7.2 \u200b\u9010\u6b65\u200b\u89e3\u6790\u200b <code>nn.MaxPool2d()</code>\u00b6","text":"<p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5f53\u200b\u6570\u636e\u200b\u901a\u8fc7\u200b <code>nn.MaxPool2d()</code> \u200b\u65f6\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#73-model_2","title":"7.3 \u200b\u4e3a\u200b <code>model_2</code> \u200b\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5145\u5206\u200b\u4e86\u89e3\u200b\u4e86\u200b\u7b2c\u4e00\u4e2a\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5404\u4e2a\u200b\u5c42\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u5982\u679c\u200b\u67d0\u4e9b\u200b\u5185\u5bb9\u200b\u4ecd\u7136\u200b\u4e0d\u200b\u6e05\u6670\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u4ece\u200b\u5c0f\u5904\u7740\u624b\u200b\u3002</p> <p>\u200b\u9009\u62e9\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u5c42\u200b\uff0c\u200b\u4f20\u9012\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u7ed9\u200b\u5b83\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u7ee7\u7eed\u524d\u8fdb\u200b\u5e76\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c<code>nn.CrossEntropyLoss()</code> \u200b\u4f5c\u4e3a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\uff09\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u4f7f\u7528\u200b <code>torch.optim.SGD()</code> \u200b\u4f5c\u4e3a\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u4ee5\u200b\u5b66\u4e60\u200b\u7387\u200b <code>0.1</code> \u200b\u6765\u200b\u4f18\u5316\u200b <code>model_2.parameters()</code>\u3002</p>"},{"location":"03_pytorch_computer_vision/#74-model_2","title":"7.4 \u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u51fd\u6570\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b <code>model_2</code>\u00b6","text":"<p>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u5df2\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b <code>train_step()</code> \u200b\u548c\u200b <code>test_step()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u8bb0\u5f55\u65f6\u95f4\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#8","title":"8. \u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u548c\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <ol> <li><code>model_0</code> - \u200b\u6211\u4eec\u200b\u7684\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u3002</li> <li><code>model_1</code> - \u200b\u4e0e\u200b\u57fa\u51c6\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u4e4b\u95f4\u200b\u52a0\u5165\u200b\u4e86\u200b <code>nn.ReLU()</code> \u200b\u5c42\u200b\u3002</li> <li><code>model_2</code> - \u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b CNN \u200b\u6a21\u578b\u200b\uff0c\u200b\u6a21\u4eff\u200b CNN Explainer \u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u7684\u200b TinyVGG \u200b\u67b6\u6784\u200b\u3002</li> </ol> <p>\u200b\u8fd9\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u5e38\u89c4\u200b\u505a\u6cd5\u200b\u3002</p> <p>\u200b\u6784\u5efa\u200b\u591a\u4e2a\u200b\u6a21\u578b\u200b\u5e76\u200b\u8fdb\u884c\u200b\u591a\u6b21\u200b\u8bad\u7ec3\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4ee5\u200b\u627e\u51fa\u200b\u6027\u80fd\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u5b57\u5178\u200b\u5408\u5e76\u200b\u5230\u200b\u4e00\u4e2a\u200b DataFrame \u200b\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u627e\u51fa\u200b\u7ed3\u679c\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#-","title":"\u6027\u80fd\u200b-\u200b\u901f\u5ea6\u200b\u6743\u8861\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\u6027\u80fd\u200b-\u200b\u901f\u5ea6\u200b\u6743\u8861\u200b\u3002</p> <p>\u200b\u901a\u5e38\u200b\uff0c\u200b\u4ece\u200b\u66f4\u200b\u5927\u200b\u3001\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u6a21\u578b\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u6027\u80fd\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b<code>model_2</code>\u200b\u4e2d\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff09\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u6027\u80fd\u200b\u63d0\u5347\u200b\u5f80\u5f80\u200b\u4ee5\u200b\u727a\u7272\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\u548c\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u4e3a\u200b\u4ee3\u4ef7\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u5f97\u5230\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u5c06\u200b\u5f88\u5927\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u786c\u4ef6\u200b\u3002</p> <p>\u200b\u901a\u5e38\u200b\uff0cCPU\u200b\u6838\u5fc3\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u6a21\u578b\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\u8d8a\u200b\u5feb\u200b\u3002GPU\u200b\u4e5f\u200b\u662f\u200b\u5982\u6b64\u200b\u3002</p> <p>\u200b\u8f83\u200b\u65b0\u200b\u7684\u200b\u786c\u4ef6\u200b\uff08\u200b\u6309\u200b\u5e74\u9f84\u200b\u8ba1\u7b97\u200b\uff09\u200b\u901a\u5e38\u200b\u7531\u4e8e\u200b\u91c7\u7528\u200b\u4e86\u200b\u6280\u672f\u200b\u8fdb\u6b65\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u5982\u4f55\u200b\uff1f</p>"},{"location":"03_pytorch_computer_vision/#9","title":"9. \u200b\u4f7f\u7528\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u968f\u673a\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8bc4\u4f30\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u76f8\u4e92\u200b\u6bd4\u8f83\u200b\u8fc7\u200b\u4e86\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fdb\u4e00\u6b65\u200b\u8bc4\u4f30\u200b\u8868\u73b0\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b <code>model_2</code>\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b <code>make_predictions()</code>\uff0c\u200b\u5728\u200b\u5176\u4e2d\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f20\u5165\u200b\u6a21\u578b\u200b\u548c\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#10","title":"10. \u200b\u5236\u4f5c\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u8bc4\u4f30\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u6700\u200b\u76f4\u89c2\u200b\u7684\u200b\u4e00\u79cd\u200b\u662f\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</p> <p>\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u5c55\u793a\u200b\u4e86\u200b\u4f60\u200b\u7684\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u5728\u200b\u9884\u6d4b\u200b\u548c\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6df7\u6dc6\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u8981\u200b\u5236\u4f5c\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u4e09\u4e2a\u200b\u6b65\u9aa4\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b <code>model_2</code> \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u6bd4\u8f83\u200b\u9884\u6d4b\u503c\u200b\u548c\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torchmetrics.ConfusionMatrix</code> \u200b\u5236\u4f5c\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>mlxtend.plotting.plot_confusion_matrix()</code> \u200b\u7ed8\u5236\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\u3002</li> </ol> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/#11","title":"11. \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6700\u4f73\u200b\u6027\u80fd\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6211\u4eec\u200b\u6027\u80fd\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u6765\u200b\u7ed3\u675f\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>\u200b\u56de\u987e\u200b\u7b14\u8bb0\u672c\u200b01\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u7ec4\u5408\u200b\u6765\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200bPyTorch\u200b\u6a21\u578b\u200b\uff1a</p> <ul> <li><code>torch.save</code> - \u200b\u7528\u4e8e\u200b\u4fdd\u5b58\u200b\u6574\u4e2a\u200bPyTorch\u200b\u6a21\u578b\u200b\u6216\u200b\u6a21\u578b\u200b\u7684\u200b<code>state_dict()</code>\u3002</li> <li><code>torch.load</code> - \u200b\u7528\u4e8e\u200b\u52a0\u8f7d\u200b\u5df2\u200b\u4fdd\u5b58\u200b\u7684\u200bPyTorch\u200b\u5bf9\u8c61\u200b\u3002</li> <li><code>torch.nn.Module.load_state_dict()</code> - \u200b\u7528\u4e8e\u200b\u5c06\u200b\u5df2\u200b\u4fdd\u5b58\u200b\u7684\u200b<code>state_dict()</code>\u200b\u52a0\u8f7d\u200b\u5230\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\u4e2d\u200b\u3002</li> </ul> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200bPyTorch\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u6587\u6863\u200b\u4e2d\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u8fd9\u200b\u4e09\u8005\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4fdd\u5b58\u200b<code>model_2</code>\u200b\u7684\u200b<code>state_dict()</code>\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5176\u200b\u52a0\u8f7d\u200b\u56de\u6765\u200b\u5e76\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u8fc7\u7a0b\u200b\u6b63\u786e\u200b\u65e0\u8bef\u200b\u3002</p>"},{"location":"03_pytorch_computer_vision/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u5b9e\u8df5\u200b\u4e0a\u8ff0\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u7ae0\u8282\u200b\u6216\u200b\u9075\u5faa\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u5e94\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u7b2c\u200b03\u200b\u7ae0\u200b\u7684\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b</li> <li>\u200b\u7b2c\u200b03\u200b\u7ae0\u200b\u7684\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5728\u200b\u67e5\u770b\u200b\u6b64\u200b\u5185\u5bb9\u200b\u4e4b\u524d\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\uff09</li> </ul> <ol> <li>\u200b\u5217\u51fa\u200b\u5f53\u524d\u200b\u5de5\u4e1a\u754c\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u4e09\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u9886\u57df\u200b\u3002</li> <li>\u200b\u641c\u7d22\u200b\u201c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u8fc7\u200b\u62df\u5408\u200b\u662f\u200b\u4ec0\u4e48\u200b\u201d\uff0c\u200b\u5e76\u200b\u5199\u4e0b\u200b\u4f60\u200b\u627e\u5230\u200b\u7684\u200b\u53e5\u5b50\u200b\u3002</li> <li>\u200b\u641c\u7d22\u200b\u201c\u200b\u9632\u6b62\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b\u201d\uff0c\u200b\u5199\u4e0b\u200b\u4f60\u200b\u627e\u5230\u200b\u7684\u200b\u4e09\u79cd\u200b\u65b9\u6cd5\u200b\u53ca\u5176\u200b\u7b80\u8981\u200b\u63cf\u8ff0\u200b\u3002\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u6709\u200b\u5f88\u591a\u200b\uff0c\u200b\u4e0d\u5fc5\u200b\u62c5\u5fc3\u200b\u5168\u90e8\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u9009\u62e9\u200b\u4e09\u79cd\u200b\u5e76\u200b\u4ece\u200b\u5b83\u4eec\u200b\u5f00\u59cb\u200b\u3002</li> <li>\u200b\u82b1\u200b20\u200b\u5206\u949f\u200b\u9605\u8bfb\u200b\u5e76\u200b\u70b9\u51fb\u200bCNN\u200b\u89e3\u91ca\u5668\u200b\u7f51\u7ad9\u200b\u3002<ul> <li>\u200b\u4f7f\u7528\u200b\u201c\u200b\u4e0a\u4f20\u200b\u201d\u200b\u6309\u94ae\u200b\u4e0a\u4f20\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u89c2\u5bdf\u200b\u56fe\u50cf\u200b\u901a\u8fc7\u200bCNN\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u65f6\u200b\u53d1\u751f\u200b\u7684\u200b\u53d8\u5316\u200b\u3002</li> </ul> </li> <li>\u200b\u52a0\u8f7d\u200b<code>torchvision.datasets.MNIST()</code>\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u3002</li> <li>\u200b\u53ef\u89c6\u5316\u200bMNIST\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u81f3\u5c11\u200b\u4e94\u4e2a\u200b\u4e0d\u540c\u200b\u6837\u672c\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b<code>torch.utils.data.DataLoader</code>\u200b\u5c06\u200bMNIST\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\uff0c\u200b\u8bbe\u7f6e\u200b<code>batch_size=32</code>\u3002</li> <li>\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b<code>model_2</code>\uff08\u200b\u6765\u81ea\u200bCNN\u200b\u89e3\u91ca\u5668\u200b\u7f51\u7ad9\u200b\u7684\u200b\u76f8\u540c\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200bTinyVGG\uff09\uff0c\u200b\u4f7f\u200b\u5176\u200b\u80fd\u591f\u200b\u9002\u5e94\u200bMNIST\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</li> <li>\u200b\u5728\u200b\u4f60\u200b\u6784\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5206\u522b\u200b\u5728\u200bCPU\u200b\u548c\u200bGPU\u200b\u4e0a\u200b\u8fdb\u884c\u200b\uff0c\u200b\u5e76\u200b\u6bd4\u8f83\u200b\u4e24\u8005\u200b\u7684\u200b\u8017\u65f6\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u4f60\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5e76\u200b\u53ef\u89c6\u5316\u200b\u81f3\u5c11\u200b\u4e94\u4e2a\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6bd4\u8f83\u200b\u9884\u6d4b\u503c\u200b\u4e0e\u200b\u76ee\u6807\u200b\u6807\u7b7e\u200b\u3002</li> <li>\u200b\u7ed8\u5236\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff0c\u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>[1, 3, 64, 64]</code>\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u901a\u8fc7\u200b<code>nn.Conv2d()</code>\u200b\u5c42\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u8bbe\u7f6e\u200b\u53ef\u4ee5\u200b\u4efb\u610f\u200b\u9009\u62e9\u200b\uff09\uff0c\u200b\u5982\u679c\u200b<code>kernel_size</code>\u200b\u53c2\u6570\u200b\u589e\u52a0\u200b\u6216\u200b\u51cf\u5c11\u200b\uff0c\u200b\u4f60\u200b\u6ce8\u610f\u200b\u5230\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff1f</li> <li>\u200b\u4f7f\u7528\u200b\u7c7b\u4f3c\u200b\u4e8e\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\u7684\u200b<code>model_2</code>\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b<code>torchvision.datasets.FashionMNIST</code>\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002<ul> <li>\u200b\u7136\u540e\u200b\u7ed8\u5236\u200b\u4e00\u4e9b\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u9519\u8bef\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5e76\u200b\u5c55\u793a\u200b\u56fe\u50cf\u200b\u5e94\u6709\u200b\u7684\u200b\u6807\u7b7e\u200b\u3002</li> <li>\u200b\u5728\u200b\u53ef\u89c6\u5316\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u8fd9\u662f\u200b\u6a21\u578b\u200b\u9519\u8bef\u200b\u8fd8\u662f\u200b\u6570\u636e\u200b\u9519\u8bef\u200b\uff1f</li> <li>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u6a21\u578b\u200b\u80fd\u5426\u200b\u505a\u200b\u5f97\u200b\u66f4\u597d\u200b\uff0c\u200b\u8fd8\u662f\u200b\u6570\u636e\u200b\u7684\u200b\u6807\u7b7e\u200b\u8fc7\u4e8e\u200b\u63a5\u8fd1\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u201c\u200b\u886c\u886b\u200b\u201d\u200b\u6807\u7b7e\u200b\u4e0e\u200b\u201cT\u200b\u6064\u200b/\u200b\u4e0a\u8863\u200b\u201d\u200b\u8fc7\u4e8e\u200b\u63a5\u8fd1\u200b\uff09\uff1f</li> </ul> </li> </ol>"},{"location":"03_pytorch_computer_vision/","title":"\u989d\u5916\u200b\u8bfe\u7a0b\u200b\u00b6","text":"<ul> <li>\u200b\u89c2\u770b\u200b\uff1a MIT\u200b\u7684\u200b\u6df1\u5ea6\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4ecb\u7ecd\u200b\u8bb2\u5ea7\u200b\u3002\u200b\u8fd9\u200b\u5c06\u200b\u4e3a\u200b\u4f60\u200b\u63d0\u4f9b\u200b\u5173\u4e8e\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6df1\u523b\u200b\u76f4\u89c9\u200b\u3002</li> <li>\u200b\u82b1\u200b10\u200b\u5206\u949f\u200b\u70b9\u51fb\u200bPyTorch\u200b\u89c6\u89c9\u200b\u5e93\u200b\u7684\u200b\u4e0d\u540c\u200b\u9009\u9879\u200b\uff0c\u200b\u6709\u200b\u54ea\u4e9b\u200b\u6a21\u5757\u200b\u53ef\u7528\u200b\uff1f</li> <li>\u200b\u67e5\u627e\u200b\u201c\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u201d\uff0c\u200b\u4f60\u200b\u53d1\u73b0\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u67b6\u6784\u200b\uff1f\u200b\u5176\u4e2d\u200b\u662f\u5426\u200b\u6709\u200b\u5305\u542b\u200b\u5728\u200b<code>torchvision.models</code>\u200b\u5e93\u4e2d\u200b\u7684\u200b\uff1f\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u53ef\u4ee5\u200b\u7528\u200b\u8fd9\u4e9b\u200b\u505a\u200b\u4ec0\u4e48\u200b\uff1f</li> <li>\u200b\u5bf9\u4e8e\u200b\u5927\u91cf\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bPyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u4ee5\u53ca\u200b\u8bb8\u591a\u200bPyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u529f\u80fd\u200b\u7684\u200b\u6269\u5c55\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200bRoss Wightman\u200b\u7684\u200bPyTorch\u200b\u56fe\u50cf\u200b\u6a21\u578b\u5e93\u200b<code>timm</code>\uff08Torch Image Models\uff09\u3002</li> </ul>"},{"location":"04_pytorch_custom_datasets/","title":"04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b | \u200b\u89c2\u770b\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b</p> In\u00a0[1]: Copied! <pre>import torch\nfrom torch import nn\n\n# Note: this notebook requires torch &gt;= 1.10.0\ntorch.__version__\n</pre> import torch from torch import nn  # Note: this notebook requires torch &gt;= 1.10.0 torch.__version__ Out[1]: <pre>'1.12.1+cu113'</pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9075\u5faa\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u4e0e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u542f\u7528\u200b GPU\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u901a\u8fc7\u200b <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code> \u200b\u6765\u200b\u542f\u7528\u200b\u4e00\u4e2a\u200b GPU \u200b\u4e86\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u8fd0\u884c\u200b\u65f6\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u91cd\u7f6e\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b <code>Runtime -&gt; Run before</code> \u200b\u6765\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u6240\u6709\u200b\u5355\u5143\u683c\u200b\u3002</p> In\u00a0[2]: Copied! <pre># Setup device-agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device-agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[2]: <pre>'cuda'</pre> In\u00a0[3]: Copied! <pre>import requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n</pre> import requests import zipfile from pathlib import Path  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path) <pre>data/pizza_steak_sushi directory exists.\n</pre> In\u00a0[4]: Copied! <pre>import os\ndef walk_through_dir(dir_path):\n  \"\"\"\n  Walks through dir_path returning its contents.\n  Args:\n    dir_path (str or pathlib.Path): target directory\n  \n  Returns:\n    A print out of:\n      number of subdiretories in dir_path\n      number of images (files) in each subdirectory\n      name of each subdirectory\n  \"\"\"\n  for dirpath, dirnames, filenames in os.walk(dir_path):\n    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")\n</pre> import os def walk_through_dir(dir_path):   \"\"\"   Walks through dir_path returning its contents.   Args:     dir_path (str or pathlib.Path): target directory      Returns:     A print out of:       number of subdiretories in dir_path       number of images (files) in each subdirectory       name of each subdirectory   \"\"\"   for dirpath, dirnames, filenames in os.walk(dir_path):     print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\") In\u00a0[5]: Copied! <pre>walk_through_dir(image_path)\n</pre> walk_through_dir(image_path) <pre>There are 2 directories and 1 images in 'data/pizza_steak_sushi'.\nThere are 3 directories and 0 images in 'data/pizza_steak_sushi/test'.\nThere are 0 directories and 19 images in 'data/pizza_steak_sushi/test/steak'.\nThere are 0 directories and 31 images in 'data/pizza_steak_sushi/test/sushi'.\nThere are 0 directories and 25 images in 'data/pizza_steak_sushi/test/pizza'.\nThere are 3 directories and 0 images in 'data/pizza_steak_sushi/train'.\nThere are 0 directories and 75 images in 'data/pizza_steak_sushi/train/steak'.\nThere are 0 directories and 72 images in 'data/pizza_steak_sushi/train/sushi'.\nThere are 0 directories and 78 images in 'data/pizza_steak_sushi/train/pizza'.\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u6bcf\u4e2a\u200b\u8bad\u7ec3\u200b\u7c7b\u522b\u200b\u5927\u7ea6\u200b\u6709\u200b 75 \u200b\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6d4b\u8bd5\u200b\u7c7b\u522b\u200b\u5927\u7ea6\u200b\u6709\u200b 25 \u200b\u5f20\u200b\u56fe\u7247\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u662f\u200b\u539f\u59cb\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5b50\u96c6\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6570\u636e\u200b\u521b\u5efa\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u67e5\u770b\u200b\u5b83\u4eec\u200b\u7684\u200b\u521b\u5efa\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u8d81\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u8def\u5f84\u200b\u5427\u200b\u3002</p> In\u00a0[6]: Copied! <pre># Setup train and testing paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\ntrain_dir, test_dir\n</pre> # Setup train and testing paths train_dir = image_path / \"train\" test_dir = image_path / \"test\"  train_dir, test_dir Out[6]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> In\u00a0[7]: Copied! <pre>import random\nfrom PIL import Image\n\n# Set seed\nrandom.seed(42) # &lt;- try changing this and see what happens\n\n# 1. Get all image paths (* means \"any combination\")\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# 2. Get random image path\nrandom_image_path = random.choice(image_path_list)\n\n# 3. Get image class from path name (the image class is the name of the directory where the image is stored)\nimage_class = random_image_path.parent.stem\n\n# 4. Open image\nimg = Image.open(random_image_path)\n\n# 5. Print metadata\nprint(f\"Random image path: {random_image_path}\")\nprint(f\"Image class: {image_class}\")\nprint(f\"Image height: {img.height}\") \nprint(f\"Image width: {img.width}\")\nimg\n</pre> import random from PIL import Image  # Set seed random.seed(42) # &lt;- try changing this and see what happens  # 1. Get all image paths (* means \"any combination\") image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # 2. Get random image path random_image_path = random.choice(image_path_list)  # 3. Get image class from path name (the image class is the name of the directory where the image is stored) image_class = random_image_path.parent.stem  # 4. Open image img = Image.open(random_image_path)  # 5. Print metadata print(f\"Random image path: {random_image_path}\") print(f\"Image class: {image_class}\") print(f\"Image height: {img.height}\")  print(f\"Image width: {img.width}\") img <pre>Random image path: data/pizza_steak_sushi/test/pizza/2124579.jpg\nImage class: pizza\nImage height: 384\nImage width: 512\n</pre> Out[7]: <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b <code>matplotlib.pyplot.imshow()</code> \u200b\u505a\u200b\u540c\u6837\u200b\u7684\u200b\u4e8b\u60c5\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b NumPy \u200b\u6570\u7ec4\u200b\u3002</p> In\u00a0[8]: Copied! <pre>import numpy as np\nimport matplotlib.pyplot as plt\n\n# Turn the image into an array\nimg_as_array = np.asarray(img)\n\n# Plot the image with matplotlib\nplt.figure(figsize=(10, 7))\nplt.imshow(img_as_array)\nplt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\")\nplt.axis(False);\n</pre> import numpy as np import matplotlib.pyplot as plt  # Turn the image into an array img_as_array = np.asarray(img)  # Plot the image with matplotlib plt.figure(figsize=(10, 7)) plt.imshow(img_as_array) plt.title(f\"Image class: {image_class} | Image shape: {img_as_array.shape} -&gt; [height, width, color_channels]\") plt.axis(False); In\u00a0[9]: Copied! <pre>import torch\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets, transforms\n</pre> import torch from torch.utils.data import DataLoader from torchvision import datasets, transforms In\u00a0[10]: Copied! <pre># Write transform for image\ndata_transform = transforms.Compose([\n    # Resize the images to 64x64\n    transforms.Resize(size=(64, 64)),\n    # Flip the images randomly on the horizontal\n    transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance\n    # Turn the image into a torch.Tensor\n    transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0 \n])\n</pre> # Write transform for image data_transform = transforms.Compose([     # Resize the images to 64x64     transforms.Resize(size=(64, 64)),     # Flip the images randomly on the horizontal     transforms.RandomHorizontalFlip(p=0.5), # p = probability of flip, 0.5 = 50% chance     # Turn the image into a torch.Tensor     transforms.ToTensor() # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0  ]) <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53d8\u6362\u200b\u7684\u200b\u7ec4\u5408\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u53d8\u6362\u200b\u5e94\u7528\u200b\u4e8e\u200b\u5404\u79cd\u200b\u56fe\u50cf\u200b\u3002</p> In\u00a0[11]: Copied! <pre>def plot_transformed_images(image_paths, transform, n=3, seed=42):\n    \"\"\"Plots a series of random images from image_paths.\n\n    Will open n image paths from image_paths, transform them\n    with transform and plot them side by side.\n\n    Args:\n        image_paths (list): List of target image paths. \n        transform (PyTorch Transforms): Transforms to apply to images.\n        n (int, optional): Number of images to plot. Defaults to 3.\n        seed (int, optional): Random seed for the random generator. Defaults to 42.\n    \"\"\"\n    random.seed(seed)\n    random_image_paths = random.sample(image_paths, k=n)\n    for image_path in random_image_paths:\n        with Image.open(image_path) as f:\n            fig, ax = plt.subplots(1, 2)\n            ax[0].imshow(f) \n            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n            ax[0].axis(\"off\")\n\n            # Transform and plot image\n            # Note: permute() will change shape of image to suit matplotlib \n            # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])\n            transformed_image = transform(f).permute(1, 2, 0) \n            ax[1].imshow(transformed_image) \n            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n            ax[1].axis(\"off\")\n\n            fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)\n\nplot_transformed_images(image_path_list, \n                        transform=data_transform, \n                        n=3)\n</pre> def plot_transformed_images(image_paths, transform, n=3, seed=42):     \"\"\"Plots a series of random images from image_paths.      Will open n image paths from image_paths, transform them     with transform and plot them side by side.      Args:         image_paths (list): List of target image paths.          transform (PyTorch Transforms): Transforms to apply to images.         n (int, optional): Number of images to plot. Defaults to 3.         seed (int, optional): Random seed for the random generator. Defaults to 42.     \"\"\"     random.seed(seed)     random_image_paths = random.sample(image_paths, k=n)     for image_path in random_image_paths:         with Image.open(image_path) as f:             fig, ax = plt.subplots(1, 2)             ax[0].imshow(f)              ax[0].set_title(f\"Original \\nSize: {f.size}\")             ax[0].axis(\"off\")              # Transform and plot image             # Note: permute() will change shape of image to suit matplotlib              # (PyTorch default is [C, H, W] but Matplotlib is [H, W, C])             transformed_image = transform(f).permute(1, 2, 0)              ax[1].imshow(transformed_image)              ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")             ax[1].axis(\"off\")              fig.suptitle(f\"Class: {image_path.parent.stem}\", fontsize=16)  plot_transformed_images(image_path_list,                          transform=data_transform,                          n=3) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchvision.transforms</code> \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u9700\u8981\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u5b83\u4eec\u200b\u7684\u200b\u5927\u5c0f\u200b\u548c\u200b\u65b9\u5411\u200b\uff08\u200b\u6709\u4e9b\u200b\u6a21\u578b\u200b\u504f\u597d\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u548c\u200b\u5f62\u72b6\u200b\u7684\u200b\u56fe\u50cf\u200b\uff09\u3002</p> <p>\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u56fe\u50cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u8d8a\u5927\u200b\uff0c\u200b\u6a21\u578b\u200b\u80fd\u200b\u6062\u590d\u200b\u7684\u200b\u4fe1\u606f\u200b\u5c31\u200b\u8d8a\u200b\u591a\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b <code>[256, 256, 3]</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\u5c06\u200b\u6bd4\u200b\u5927\u5c0f\u200b\u4e3a\u200b <code>[64, 64, 3]</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\u591a\u200b 16 \u200b\u500d\u200b\u7684\u200b\u50cf\u7d20\u200b\uff08<code>(256*256*3)/(64*64*3)=16</code>\uff09\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6743\u8861\u200b\u4e4b\u200b\u5904\u200b\u5728\u4e8e\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u50cf\u7d20\u200b\u9700\u8981\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u5c1d\u8bd5\u200b\u6ce8\u91ca\u200b\u6389\u200b <code>data_transform</code> \u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u53d8\u6362\u200b\uff0c\u200b\u7136\u540e\u200b\u518d\u6b21\u200b\u8fd0\u884c\u200b\u7ed8\u56fe\u200b\u51fd\u6570\u200b <code>plot_transformed_images()</code>\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</p> In\u00a0[12]: Copied! <pre># Use ImageFolder to create dataset(s)\nfrom torchvision import datasets\ntrain_data = datasets.ImageFolder(root=train_dir, # target folder of images\n                                  transform=data_transform, # transforms to perform on data (images)\n                                  target_transform=None) # transforms to perform on labels (if necessary)\n\ntest_data = datasets.ImageFolder(root=test_dir, \n                                 transform=data_transform)\n\nprint(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\")\n</pre> # Use ImageFolder to create dataset(s) from torchvision import datasets train_data = datasets.ImageFolder(root=train_dir, # target folder of images                                   transform=data_transform, # transforms to perform on data (images)                                   target_transform=None) # transforms to perform on labels (if necessary)  test_data = datasets.ImageFolder(root=test_dir,                                   transform=data_transform)  print(f\"Train data:\\n{train_data}\\nTest data:\\n{test_data}\") <pre>Train data:\nDataset ImageFolder\n    Number of datapoints: 225\n    Root location: data/pizza_steak_sushi/train\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\nTest data:\nDataset ImageFolder\n    Number of datapoints: 75\n    Root location: data/pizza_steak_sushi/test\n    StandardTransform\nTransform: Compose(\n               Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n               RandomHorizontalFlip(p=0.5)\n               ToTensor()\n           )\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b PyTorch \u200b\u5df2\u7ecf\u200b\u6ce8\u518c\u200b\u4e86\u200b\u6211\u4eec\u200b\u7684\u200b <code>Dataset</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u67e5\u770b\u200b <code>classes</code> \u200b\u548c\u200b <code>class_to_idx</code> \u200b\u5c5e\u6027\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u957f\u5ea6\u200b\u6765\u200b\u68c0\u67e5\u200b\u5b83\u4eec\u200b\u3002</p> In\u00a0[13]: Copied! <pre># Get class names as a list\nclass_names = train_data.classes\nclass_names\n</pre> # Get class names as a list class_names = train_data.classes class_names Out[13]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[14]: Copied! <pre># Can also get class names as a dict\nclass_dict = train_data.class_to_idx\nclass_dict\n</pre> # Can also get class names as a dict class_dict = train_data.class_to_idx class_dict Out[14]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> In\u00a0[15]: Copied! <pre># Check the lengths\nlen(train_data), len(test_data)\n</pre> # Check the lengths len(train_data), len(test_data) Out[15]: <pre>(225, 75)</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7528\u200b\u8fd9\u4e9b\u200b\u4f5c\u4e3a\u200b\u53c2\u8003\u200b\uff0c\u200b\u4ee5\u5907\u200b\u540e\u200b\u7528\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u548c\u200b\u6807\u7b7e\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5b83\u4eec\u200b\u770b\u8d77\u6765\u200b\u600e\u4e48\u6837\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bf9\u200b <code>train_data</code> \u200b\u548c\u200b <code>test_data</code> \u200b\u7684\u200b <code>Dataset</code> \u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\uff0c\u200b\u4ee5\u200b\u627e\u5230\u200b\u6837\u672c\u200b\u53ca\u5176\u200b\u76ee\u6807\u200b\u6807\u7b7e\u200b\u3002</p> In\u00a0[16]: Copied! <pre>img, label = train_data[0][0], train_data[0][1]\nprint(f\"Image tensor:\\n{img}\")\nprint(f\"Image shape: {img.shape}\")\nprint(f\"Image datatype: {img.dtype}\")\nprint(f\"Image label: {label}\")\nprint(f\"Label datatype: {type(label)}\")\n</pre> img, label = train_data[0][0], train_data[0][1] print(f\"Image tensor:\\n{img}\") print(f\"Image shape: {img.shape}\") print(f\"Image datatype: {img.dtype}\") print(f\"Image label: {label}\") print(f\"Label datatype: {type(label)}\") <pre>Image tensor:\ntensor([[[0.1137, 0.1020, 0.0980,  ..., 0.1255, 0.1216, 0.1176],\n         [0.1059, 0.0980, 0.0980,  ..., 0.1294, 0.1294, 0.1294],\n         [0.1020, 0.0980, 0.0941,  ..., 0.1333, 0.1333, 0.1333],\n         ...,\n         [0.1098, 0.1098, 0.1255,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0941, 0.1098,  ..., 0.1686, 0.1647, 0.1686],\n         [0.0863, 0.0863, 0.0980,  ..., 0.1686, 0.1647, 0.1647]],\n\n        [[0.0745, 0.0706, 0.0745,  ..., 0.0588, 0.0588, 0.0588],\n         [0.0706, 0.0706, 0.0745,  ..., 0.0627, 0.0627, 0.0627],\n         [0.0706, 0.0745, 0.0745,  ..., 0.0706, 0.0706, 0.0706],\n         ...,\n         [0.1255, 0.1333, 0.1373,  ..., 0.2510, 0.2392, 0.2392],\n         [0.1098, 0.1176, 0.1255,  ..., 0.2510, 0.2392, 0.2314],\n         [0.1020, 0.1059, 0.1137,  ..., 0.2431, 0.2353, 0.2275]],\n\n        [[0.0941, 0.0902, 0.0902,  ..., 0.0196, 0.0196, 0.0196],\n         [0.0902, 0.0863, 0.0902,  ..., 0.0196, 0.0157, 0.0196],\n         [0.0902, 0.0902, 0.0902,  ..., 0.0157, 0.0157, 0.0196],\n         ...,\n         [0.1294, 0.1333, 0.1490,  ..., 0.1961, 0.1882, 0.1804],\n         [0.1098, 0.1137, 0.1255,  ..., 0.1922, 0.1843, 0.1804],\n         [0.1059, 0.1020, 0.1059,  ..., 0.1843, 0.1804, 0.1765]]])\nImage shape: torch.Size([3, 64, 64])\nImage datatype: torch.float32\nImage label: 0\nLabel datatype: &lt;class 'int'&gt;\n</pre> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u73b0\u5728\u200b\u662f\u200b\u4ee5\u200b\u5f20\u91cf\u200b\u5f62\u5f0f\u200b\u5b58\u5728\u200b\u7684\u200b\uff08\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[3, 64, 64]</code>\uff09\uff0c\u200b\u800c\u200b\u6807\u7b7e\u200b\u662f\u200b\u4ee5\u200b\u4e0e\u200b\u7279\u5b9a\u200b\u7c7b\u522b\u200b\u76f8\u5173\u200b\u7684\u200b\u6574\u6570\u200b\u5f62\u5f0f\u200b\u5b58\u5728\u200b\u7684\u200b\uff08\u200b\u901a\u8fc7\u200b <code>class_to_idx</code> \u200b\u5c5e\u6027\u200b\u5f15\u7528\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b <code>matplotlib</code> \u200b\u7ed8\u5236\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u5462\u200b\uff1f</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u7ef4\u5ea6\u200b\u91cd\u6392\u200b\uff08\u200b\u91cd\u65b0\u6392\u5217\u200b\u5176\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u987a\u5e8f\u200b\uff09\uff0c\u200b\u4f7f\u200b\u5176\u200b\u517c\u5bb9\u200b\u3002</p> <p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u7ef4\u5ea6\u200b\u683c\u5f0f\u200b\u662f\u200b <code>CHW</code>\uff08\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff0c\u200b\u9ad8\u5ea6\u200b\uff0c\u200b\u5bbd\u5ea6\u200b\uff09\uff0c\u200b\u4f46\u200b <code>matplotlib</code> \u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b <code>HWC</code>\uff08\u200b\u9ad8\u5ea6\u200b\uff0c\u200b\u5bbd\u5ea6\u200b\uff0c\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff09\u3002</p> In\u00a0[17]: Copied! <pre># Rearrange the order of dimensions\nimg_permute = img.permute(1, 2, 0)\n\n# Print out different shapes (before and after permute)\nprint(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\")\nprint(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")\n\n# Plot the image\nplt.figure(figsize=(10, 7))\nplt.imshow(img.permute(1, 2, 0))\nplt.axis(\"off\")\nplt.title(class_names[label], fontsize=14);\n</pre> # Rearrange the order of dimensions img_permute = img.permute(1, 2, 0)  # Print out different shapes (before and after permute) print(f\"Original shape: {img.shape} -&gt; [color_channels, height, width]\") print(f\"Image permute shape: {img_permute.shape} -&gt; [height, width, color_channels]\")  # Plot the image plt.figure(figsize=(10, 7)) plt.imshow(img.permute(1, 2, 0)) plt.axis(\"off\") plt.title(class_names[label], fontsize=14); <pre>Original shape: torch.Size([3, 64, 64]) -&gt; [color_channels, height, width]\nImage permute shape: torch.Size([64, 64, 3]) -&gt; [height, width, color_channels]\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u56fe\u50cf\u200b\u73b0\u5728\u200b\u53d8\u5f97\u200b\u66f4\u52a0\u200b\u50cf\u7d20\u200b\u5316\u200b\uff08\u200b\u8d28\u91cf\u200b\u964d\u4f4e\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u7531\u4e8e\u200b\u56fe\u50cf\u200b\u4ece\u200b <code>512x512</code> \u200b\u50cf\u7d20\u200b\u8c03\u6574\u200b\u5230\u200b\u4e86\u200b <code>64x64</code> \u200b\u50cf\u7d20\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u76f4\u89c9\u200b\u662f\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u89c9\u5f97\u200b\u56fe\u50cf\u200b\u66f4\u200b\u96be\u4ee5\u200b\u8bc6\u522b\u200b\u5176\u4e2d\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6a21\u578b\u200b\u4e5f\u200b\u5f88\u200b\u53ef\u80fd\u200b\u66f4\u200b\u96be\u4ee5\u200b\u7406\u89e3\u200b\u5b83\u200b\u3002</p> In\u00a0[18]: Copied! <pre># Turn train and test Datasets into DataLoaders\nfrom torch.utils.data import DataLoader\ntrain_dataloader = DataLoader(dataset=train_data, \n                              batch_size=1, # how many samples per batch?\n                              num_workers=1, # how many subprocesses to use for data loading? (higher = more)\n                              shuffle=True) # shuffle the data?\n\ntest_dataloader = DataLoader(dataset=test_data, \n                             batch_size=1, \n                             num_workers=1, \n                             shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader, test_dataloader\n</pre> # Turn train and test Datasets into DataLoaders from torch.utils.data import DataLoader train_dataloader = DataLoader(dataset=train_data,                                batch_size=1, # how many samples per batch?                               num_workers=1, # how many subprocesses to use for data loading? (higher = more)                               shuffle=True) # shuffle the data?  test_dataloader = DataLoader(dataset=test_data,                               batch_size=1,                               num_workers=1,                               shuffle=False) # don't usually need to shuffle testing data  train_dataloader, test_dataloader Out[18]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9dca0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9de50&gt;)</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u53ef\u200b\u8fed\u4ee3\u200b\u7684\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u4e00\u8bd5\u200b\u5e76\u200b\u68c0\u67e5\u200b\u5f62\u72b6\u200b\u3002</p> In\u00a0[19]: Copied! <pre>img, label = next(iter(train_dataloader))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label.shape}\")\n</pre> img, label = next(iter(train_dataloader))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b <code>DataLoader</code> \u200b\u5728\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u4e2d\u6765\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5728\u200b\u6211\u4eec\u200b\u8fd9\u6837\u200b\u505a\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u53e6\u200b\u4e00\u79cd\u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\uff08\u200b\u6216\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\uff09\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> In\u00a0[20]: Copied! <pre>import os\nimport pathlib\nimport torch\n\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom typing import Tuple, Dict, List\n</pre> import os import pathlib import torch  from PIL import Image from torch.utils.data import Dataset from torchvision import transforms from typing import Tuple, Dict, List <p>\u200b\u8fd8\u200b\u8bb0\u5f97\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u7684\u200b\u5b9e\u4f8b\u200b\u65f6\u200b\uff0c\u200b\u662f\u200b\u5982\u4f55\u200b\u5229\u7528\u200b <code>classes</code> \u200b\u548c\u200b <code>class_to_idx</code> \u200b\u5c5e\u6027\u200b\u7684\u200b\u5417\u200b\uff1f</p> In\u00a0[21]: Copied! <pre># Instance of torchvision.datasets.ImageFolder()\ntrain_data.classes, train_data.class_to_idx\n</pre> # Instance of torchvision.datasets.ImageFolder() train_data.classes, train_data.class_to_idx Out[21]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> In\u00a0[22]: Copied! <pre># Setup path for target directory\ntarget_directory = train_dir\nprint(f\"Target directory: {target_directory}\")\n\n# Get the class names from the target directory\nclass_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))])\nprint(f\"Class names found: {class_names_found}\")\n</pre> # Setup path for target directory target_directory = train_dir print(f\"Target directory: {target_directory}\")  # Get the class names from the target directory class_names_found = sorted([entry.name for entry in list(os.scandir(image_path / \"train\"))]) print(f\"Class names found: {class_names_found}\") <pre>Target directory: data/pizza_steak_sushi/train\nClass names found: ['pizza', 'steak', 'sushi']\n</pre> <p>\u200b\u975e\u5e38\u200b\u597d\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u5c06\u200b\u5176\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u7684\u200b\u51fd\u6570\u200b\u5462\u200b\uff1f</p> In\u00a0[23]: Copied! <pre># Make function to find classes in target directory\ndef find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:\n    \"\"\"Finds the class folder names in a target directory.\n    \n    Assumes target directory is in standard image classification format.\n\n    Args:\n        directory (str): target directory to load classnames from.\n\n    Returns:\n        Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))\n    \n    Example:\n        find_classes(\"food_images/train\")\n        &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})\n    \"\"\"\n    # 1. Get the class names by scanning the target directory\n    classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())\n    \n    # 2. Raise an error if class names not found\n    if not classes:\n        raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")\n        \n    # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)\n    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n    return classes, class_to_idx\n</pre> # Make function to find classes in target directory def find_classes(directory: str) -&gt; Tuple[List[str], Dict[str, int]]:     \"\"\"Finds the class folder names in a target directory.          Assumes target directory is in standard image classification format.      Args:         directory (str): target directory to load classnames from.      Returns:         Tuple[List[str], Dict[str, int]]: (list_of_class_names, dict(class_name: idx...))          Example:         find_classes(\"food_images/train\")         &gt;&gt;&gt; ([\"class_1\", \"class_2\"], {\"class_1\": 0, ...})     \"\"\"     # 1. Get the class names by scanning the target directory     classes = sorted(entry.name for entry in os.scandir(directory) if entry.is_dir())          # 2. Raise an error if class names not found     if not classes:         raise FileNotFoundError(f\"Couldn't find any classes in {directory}.\")              # 3. Create a dictionary of index labels (computers prefer numerical rather than string labels)     class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}     return classes, class_to_idx <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200b <code>find_classes()</code> \u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[24]: Copied! <pre>find_classes(train_dir)\n</pre> find_classes(train_dir) Out[24]: <pre>(['pizza', 'steak', 'sushi'], {'pizza': 0, 'steak': 1, 'sushi': 2})</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff01</p> In\u00a0[25]: Copied! <pre># Write a custom dataset class (inherits from torch.utils.data.Dataset)\nfrom torch.utils.data import Dataset\n\n# 1. Subclass torch.utils.data.Dataset\nclass ImageFolderCustom(Dataset):\n    \n    # 2. Initialize with a targ_dir and transform (optional) parameter\n    def __init__(self, targ_dir: str, transform=None) -&gt; None:\n        \n        # 3. Create class attributes\n        # Get all image paths\n        self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's\n        # Setup transforms\n        self.transform = transform\n        # Create classes and class_to_idx attributes\n        self.classes, self.class_to_idx = find_classes(targ_dir)\n\n    # 4. Make function to load images\n    def load_image(self, index: int) -&gt; Image.Image:\n        \"Opens an image via a path and returns it.\"\n        image_path = self.paths[index]\n        return Image.open(image_path) \n    \n    # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)\n    def __len__(self) -&gt; int:\n        \"Returns the total number of samples.\"\n        return len(self.paths)\n    \n    # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)\n    def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:\n        \"Returns one sample of data, data and label (X, y).\"\n        img = self.load_image(index)\n        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n        class_idx = self.class_to_idx[class_name]\n\n        # Transform if necessary\n        if self.transform:\n            return self.transform(img), class_idx # return data, label (X, y)\n        else:\n            return img, class_idx # return data, label (X, y)\n</pre> # Write a custom dataset class (inherits from torch.utils.data.Dataset) from torch.utils.data import Dataset  # 1. Subclass torch.utils.data.Dataset class ImageFolderCustom(Dataset):          # 2. Initialize with a targ_dir and transform (optional) parameter     def __init__(self, targ_dir: str, transform=None) -&gt; None:                  # 3. Create class attributes         # Get all image paths         self.paths = list(pathlib.Path(targ_dir).glob(\"*/*.jpg\")) # note: you'd have to update this if you've got .png's or .jpeg's         # Setup transforms         self.transform = transform         # Create classes and class_to_idx attributes         self.classes, self.class_to_idx = find_classes(targ_dir)      # 4. Make function to load images     def load_image(self, index: int) -&gt; Image.Image:         \"Opens an image via a path and returns it.\"         image_path = self.paths[index]         return Image.open(image_path)           # 5. Overwrite the __len__() method (optional but recommended for subclasses of torch.utils.data.Dataset)     def __len__(self) -&gt; int:         \"Returns the total number of samples.\"         return len(self.paths)          # 6. Overwrite the __getitem__() method (required for subclasses of torch.utils.data.Dataset)     def __getitem__(self, index: int) -&gt; Tuple[torch.Tensor, int]:         \"Returns one sample of data, data and label (X, y).\"         img = self.load_image(index)         class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg         class_idx = self.class_to_idx[class_name]          # Transform if necessary         if self.transform:             return self.transform(img), class_idx # return data, label (X, y)         else:             return img, class_idx # return data, label (X, y) <p>\u200b\u54c7\u200b\uff01\u200b\u4e00\u5927\u5806\u200b\u4ee3\u7801\u200b\u6765\u200b\u52a0\u8f7d\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u521b\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u7684\u200b\u4e00\u4e2a\u200b\u7f3a\u70b9\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5199\u200b\u4e86\u200b\u4e00\u6b21\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u79fb\u200b\u5230\u200b\u4e00\u4e2a\u200b <code>.py</code> \u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>data_loader.py</code>\uff0c\u200b\u5e76\u200b\u52a0\u5165\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u6709\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4ee5\u540e\u200b\u91cd\u590d\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u6d4b\u8bd5\u200b\u65b0\u200b\u7684\u200b <code>ImageFolderCustom</code> \u200b\u7c7b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u53d8\u6362\u200b\u6765\u200b\u51c6\u5907\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> In\u00a0[26]: Copied! <pre># Augment train data\ntrain_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor()\n])\n\n# Don't augment test data, only reshape\ntest_transforms = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Augment train data train_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.RandomHorizontalFlip(p=0.5),     transforms.ToTensor() ])  # Don't augment test data, only reshape test_transforms = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>\u200b\u73b0\u5728\u200b\u5230\u200b\u4e86\u200b\u63ed\u6653\u200b\u771f\u76f8\u200b\u7684\u200b\u65f6\u523b\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\uff08\u200b\u5305\u542b\u200b\u5728\u200b <code>train_dir</code> \u200b\u4e2d\u200b\uff09\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff08\u200b\u5305\u542b\u200b\u5728\u200b <code>test_dir</code> \u200b\u4e2d\u200b\uff09\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b <code>ImageFolderCustom</code> \u200b\u7c7b\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>Dataset</code>\u3002</p> In\u00a0[27]: Copied! <pre>train_data_custom = ImageFolderCustom(targ_dir=train_dir, \n                                      transform=train_transforms)\ntest_data_custom = ImageFolderCustom(targ_dir=test_dir, \n                                     transform=test_transforms)\ntrain_data_custom, test_data_custom\n</pre> train_data_custom = ImageFolderCustom(targ_dir=train_dir,                                        transform=train_transforms) test_data_custom = ImageFolderCustom(targ_dir=test_dir,                                       transform=test_transforms) train_data_custom, test_data_custom Out[27]: <pre>(&lt;__main__.ImageFolderCustom at 0x7f5461f70c70&gt;,\n &lt;__main__.ImageFolderCustom at 0x7f5461f70c40&gt;)</pre> <p>\u200b\u55ef\u200b... \u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\uff0c\u200b\u6210\u529f\u200b\u4e86\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u65b0\u200b\u7684\u200b <code>Dataset</code> \u200b\u8c03\u7528\u200b <code>len()</code>\uff0c\u200b\u5e76\u200b\u67e5\u627e\u200b <code>classes</code> \u200b\u548c\u200b <code>class_to_idx</code> \u200b\u5c5e\u6027\u200b\u3002</p> In\u00a0[28]: Copied! <pre>len(train_data_custom), len(test_data_custom)\n</pre> len(train_data_custom), len(test_data_custom) Out[28]: <pre>(225, 75)</pre> In\u00a0[29]: Copied! <pre>train_data_custom.classes\n</pre> train_data_custom.classes Out[29]: <pre>['pizza', 'steak', 'sushi']</pre> In\u00a0[30]: Copied! <pre>train_data_custom.class_to_idx\n</pre> train_data_custom.class_to_idx Out[30]: <pre>{'pizza': 0, 'steak': 1, 'sushi': 2}</pre> <p><code>len(test_data_custom) == len(test_data)</code> \u200b\u5e76\u4e14\u200b <code>len(test_data_custom) == len(test_data)</code> \u200b\u662f\u200b\u7684\u200b\uff01\uff01\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u5b83\u200b\u8d77\u200b\u4f5c\u7528\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u68c0\u67e5\u200b\u7531\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u7c7b\u200b\u521b\u5efa\u200b\u7684\u200b <code>Dataset</code> \u200b\u662f\u5426\u200b\u76f8\u7b49\u200b\u3002</p> In\u00a0[31]: Copied! <pre># Check for equality amongst our custom Dataset and ImageFolder Dataset\nprint((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data)))\nprint(train_data_custom.classes == train_data.classes)\nprint(train_data_custom.class_to_idx == train_data.class_to_idx)\n</pre> # Check for equality amongst our custom Dataset and ImageFolder Dataset print((len(train_data_custom) == len(train_data)) &amp; (len(test_data_custom) == len(test_data))) print(train_data_custom.classes == train_data.classes) print(train_data_custom.class_to_idx == train_data.class_to_idx) <pre>True\nTrue\nTrue\n</pre> <p>\u200b\u5475\u5475\u200b\uff01</p> <p>\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u8fdb\u5c55\u200b\u5f97\u200b\u591a\u200b\u987a\u5229\u200b\uff01</p> <p>\u200b\u4e09\u4e2a\u200b <code>True</code>\uff01</p> <p>\u200b\u8fd9\u200b\u5df2\u7ecf\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e0d\u5982\u200b\u6211\u4eec\u200b\u518d\u200b\u63d0\u5347\u200b\u4e00\u4e2a\u200b\u6863\u6b21\u200b\uff0c\u200b\u7ed8\u5236\u200b\u4e00\u4e9b\u200b\u968f\u673a\u200b\u56fe\u50cf\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u6211\u4eec\u200b\u7684\u200b <code>__getitem__</code> \u200b\u91cd\u5199\u200b\uff1f</p> In\u00a0[32]: Copied! <pre># 1. Take in a Dataset as well as a list of class names\ndef display_random_images(dataset: torch.utils.data.dataset.Dataset,\n                          classes: List[str] = None,\n                          n: int = 10,\n                          display_shape: bool = True,\n                          seed: int = None):\n    \n    # 2. Adjust display if n too high\n    if n &gt; 10:\n        n = 10\n        display_shape = False\n        print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")\n    \n    # 3. Set random seed\n    if seed:\n        random.seed(seed)\n\n    # 4. Get random sample indexes\n    random_samples_idx = random.sample(range(len(dataset)), k=n)\n\n    # 5. Setup plot\n    plt.figure(figsize=(16, 8))\n\n    # 6. Loop through samples and display random samples \n    for i, targ_sample in enumerate(random_samples_idx):\n        targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]\n\n        # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]\n        targ_image_adjust = targ_image.permute(1, 2, 0)\n\n        # Plot adjusted samples\n        plt.subplot(1, n, i+1)\n        plt.imshow(targ_image_adjust)\n        plt.axis(\"off\")\n        if classes:\n            title = f\"class: {classes[targ_label]}\"\n            if display_shape:\n                title = title + f\"\\nshape: {targ_image_adjust.shape}\"\n        plt.title(title)\n</pre> # 1. Take in a Dataset as well as a list of class names def display_random_images(dataset: torch.utils.data.dataset.Dataset,                           classes: List[str] = None,                           n: int = 10,                           display_shape: bool = True,                           seed: int = None):          # 2. Adjust display if n too high     if n &gt; 10:         n = 10         display_shape = False         print(f\"For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\")          # 3. Set random seed     if seed:         random.seed(seed)      # 4. Get random sample indexes     random_samples_idx = random.sample(range(len(dataset)), k=n)      # 5. Setup plot     plt.figure(figsize=(16, 8))      # 6. Loop through samples and display random samples      for i, targ_sample in enumerate(random_samples_idx):         targ_image, targ_label = dataset[targ_sample][0], dataset[targ_sample][1]          # 7. Adjust image tensor shape for plotting: [color_channels, height, width] -&gt; [color_channels, height, width]         targ_image_adjust = targ_image.permute(1, 2, 0)          # Plot adjusted samples         plt.subplot(1, n, i+1)         plt.imshow(targ_image_adjust)         plt.axis(\"off\")         if classes:             title = f\"class: {classes[targ_label]}\"             if display_shape:                 title = title + f\"\\nshape: {targ_image_adjust.shape}\"         plt.title(title) <p>\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5148\u200b\u5728\u200b\u7528\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u521b\u5efa\u200b\u7684\u200b <code>Dataset</code> \u200b\u4e0a\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[33]: Copied! <pre># Display random images from ImageFolder created Dataset\ndisplay_random_images(train_data, \n                      n=5, \n                      classes=class_names,\n                      seed=None)\n</pre> # Display random images from ImageFolder created Dataset display_random_images(train_data,                        n=5,                        classes=class_names,                       seed=None) <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b <code>ImageFolderCustom</code> \u200b\u521b\u5efa\u200b\u7684\u200b <code>Dataset</code>\u3002</p> In\u00a0[34]: Copied! <pre># Display random images from ImageFolderCustom Dataset\ndisplay_random_images(train_data_custom, \n                      n=12, \n                      classes=class_names,\n                      seed=None) # Try setting the seed for reproducible images\n</pre> # Display random images from ImageFolderCustom Dataset display_random_images(train_data_custom,                        n=12,                        classes=class_names,                       seed=None) # Try setting the seed for reproducible images <pre>For display purposes, n shouldn't be larger than 10, setting to 10 and removing shape display.\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b <code>ImageFolderCustom</code> \u200b\u8fd0\u884c\u200b\u5f97\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u6240\u200b\u671f\u671b\u200b\u7684\u200b\u90a3\u6837\u200b\u3002</p> In\u00a0[35]: Copied! <pre># Turn train and test custom Dataset's into DataLoader's\nfrom torch.utils.data import DataLoader\ntrain_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset\n                                     batch_size=1, # how many samples per batch?\n                                     num_workers=0, # how many subprocesses to use for data loading? (higher = more)\n                                     shuffle=True) # shuffle the data?\n\ntest_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset\n                                    batch_size=1, \n                                    num_workers=0, \n                                    shuffle=False) # don't usually need to shuffle testing data\n\ntrain_dataloader_custom, test_dataloader_custom\n</pre> # Turn train and test custom Dataset's into DataLoader's from torch.utils.data import DataLoader train_dataloader_custom = DataLoader(dataset=train_data_custom, # use custom created train Dataset                                      batch_size=1, # how many samples per batch?                                      num_workers=0, # how many subprocesses to use for data loading? (higher = more)                                      shuffle=True) # shuffle the data?  test_dataloader_custom = DataLoader(dataset=test_data_custom, # use custom created test Dataset                                     batch_size=1,                                      num_workers=0,                                      shuffle=False) # don't usually need to shuffle testing data  train_dataloader_custom, test_dataloader_custom Out[35]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ab8400&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ab8490&gt;)</pre> <p>\u200b\u6837\u54c1\u200b\u7684\u200b\u5f62\u72b6\u200b\u770b\u8d77\u6765\u200b\u662f\u5426\u200b\u76f8\u540c\u200b\uff1f</p> In\u00a0[36]: Copied! <pre># Get image and label from custom DataLoader\nimg_custom, label_custom = next(iter(train_dataloader_custom))\n\n# Batch size will now be 1, try changing the batch_size parameter above and see what happens\nprint(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\")\nprint(f\"Label shape: {label_custom.shape}\")\n</pre> # Get image and label from custom DataLoader img_custom, label_custom = next(iter(train_dataloader_custom))  # Batch size will now be 1, try changing the batch_size parameter above and see what happens print(f\"Image shape: {img_custom.shape} -&gt; [batch_size, color_channels, height, width]\") print(f\"Label shape: {label_custom.shape}\") <pre>Image shape: torch.Size([1, 3, 64, 64]) -&gt; [batch_size, color_channels, height, width]\nLabel shape: torch.Size([1])\n</pre> <p>\u200b\u786e\u5b9e\u200b\u5982\u6b64\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u5176\u4ed6\u200b\u5f62\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u3002</p> In\u00a0[37]: Copied! <pre>from torchvision import transforms\n\ntrain_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense \n    transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1\n])\n\n# Don't need to perform augmentation on the test data\ntest_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), \n    transforms.ToTensor()\n])\n</pre> from torchvision import transforms  train_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.TrivialAugmentWide(num_magnitude_bins=31), # how intense      transforms.ToTensor() # use ToTensor() last to get everything between 0 &amp; 1 ])  # Don't need to perform augmentation on the test data test_transforms = transforms.Compose([     transforms.Resize((224, 224)),      transforms.ToTensor() ]) <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f60\u200b\u4e0d\u4f1a\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u901a\u8fc7\u200b\u4eba\u5de5\u200b\u589e\u52a0\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u591a\u6837\u6027\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u66f4\u597d\u200b\u5730\u200b\u9884\u6d4b\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u56fe\u50cf\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u7684\u200b\u5927\u5c0f\u200b\u8c03\u6574\u200b\u4e3a\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u76f8\u540c\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u5fc5\u8981\u200b\uff0c\u200b\u63a8\u7406\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8fdb\u884c\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f71\u54cd\u200b\u6027\u80fd\u200b\uff09\u3002</p> <p>\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u53d8\u6362\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4e0d\u200b\u5305\u542b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u53d8\u6362\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u6548\u679c\u200b\u5427\u200b\uff01</p> In\u00a0[38]: Copied! <pre># Get all image paths\nimage_path_list = list(image_path.glob(\"*/*/*.jpg\"))\n\n# Plot random images\nplot_transformed_images(\n    image_paths=image_path_list,\n    transform=train_transforms,\n    n=3,\n    seed=None\n)\n</pre> # Get all image paths image_path_list = list(image_path.glob(\"*/*/*.jpg\"))  # Plot random images plot_transformed_images(     image_paths=image_path_list,     transform=train_transforms,     n=3,     seed=None ) <p>\u200b\u5c1d\u8bd5\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u5728\u200b\u53d8\u6362\u200b\u8fc7\u7a0b\u200b\u4e2d\u662f\u200b\u5982\u4f55\u200b\u53d8\u5316\u200b\u7684\u200b\u3002</p> In\u00a0[39]: Copied! <pre># Create simple transform\nsimple_transform = transforms.Compose([ \n    transforms.Resize((64, 64)),\n    transforms.ToTensor(),\n])\n</pre> # Create simple transform simple_transform = transforms.Compose([      transforms.Resize((64, 64)),     transforms.ToTensor(), ]) <p>\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u8f6c\u6362\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\uff1a</p> <ol> <li>\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\uff0c\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u6587\u4ef6\u5939\u200b\u9996\u5148\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>Dataset</code>\uff0c\u200b\u4f7f\u7528\u200b <code>torchvision.datasets.ImageFolder()</code></li> <li>\u200b\u7136\u540e\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\uff0c\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader()</code>\u3002<ul> <li>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b <code>batch_size=32</code>\uff0c\u200b\u5e76\u200b\u5c06\u200b <code>num_workers</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u673a\u5668\u200b\u4e0a\u200b\u7684\u200b CPU \u200b\u6570\u91cf\u200b\uff08\u200b\u8fd9\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u673a\u5668\u200b\uff09\u3002</li> </ul> </li> </ol> In\u00a0[40]: Copied! <pre># 1. Load and transform data\nfrom torchvision import datasets\ntrain_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform)\ntest_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)\n\n# 2. Turn data into DataLoaders\nimport os\nfrom torch.utils.data import DataLoader\n\n# Setup batch size and number of workers \nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\nprint(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")\n\n# Create DataLoader's\ntrain_dataloader_simple = DataLoader(train_data_simple, \n                                     batch_size=BATCH_SIZE, \n                                     shuffle=True, \n                                     num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_simple, test_dataloader_simple\n</pre> # 1. Load and transform data from torchvision import datasets train_data_simple = datasets.ImageFolder(root=train_dir, transform=simple_transform) test_data_simple = datasets.ImageFolder(root=test_dir, transform=simple_transform)  # 2. Turn data into DataLoaders import os from torch.utils.data import DataLoader  # Setup batch size and number of workers  BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count() print(f\"Creating DataLoader's with batch size {BATCH_SIZE} and {NUM_WORKERS} workers.\")  # Create DataLoader's train_dataloader_simple = DataLoader(train_data_simple,                                       batch_size=BATCH_SIZE,                                       shuffle=True,                                       num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_simple, test_dataloader_simple <pre>Creating DataLoader's with batch size 32 and 16 workers.\n</pre> Out[40]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ad2f70&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f5460ad23d0&gt;)</pre> <p><code>DataLoader</code> \u200b\u521b\u5efa\u200b\u597d\u200b\u4e86\u200b\uff01</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[41]: Copied! <pre>class TinyVGG(nn.Module):\n    \"\"\"\n    Model architecture copying TinyVGG from: \n    https://poloclub.github.io/cnn-explainer/\n    \"\"\"\n    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n        super().__init__()\n        self.conv_block_1 = nn.Sequential(\n            nn.Conv2d(in_channels=input_shape, \n                      out_channels=hidden_units, \n                      kernel_size=3, # how big is the square that's going over the image?\n                      stride=1, # default\n                      padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number \n            nn.ReLU(),\n            nn.Conv2d(in_channels=hidden_units, \n                      out_channels=hidden_units,\n                      kernel_size=3,\n                      stride=1,\n                      padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2,\n                         stride=2) # default stride value is same as kernel_size\n        )\n        self.conv_block_2 = nn.Sequential(\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(2)\n        )\n        self.classifier = nn.Sequential(\n            nn.Flatten(),\n            # Where did this in_features shape come from? \n            # It's because each layer of our network compresses and changes the shape of our inputs data.\n            nn.Linear(in_features=hidden_units*16*16,\n                      out_features=output_shape)\n        )\n    \n    def forward(self, x: torch.Tensor):\n        x = self.conv_block_1(x)\n        # print(x.shape)\n        x = self.conv_block_2(x)\n        # print(x.shape)\n        x = self.classifier(x)\n        # print(x.shape)\n        return x\n        # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n\ntorch.manual_seed(42)\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\nmodel_0\n</pre> class TinyVGG(nn.Module):     \"\"\"     Model architecture copying TinyVGG from:      https://poloclub.github.io/cnn-explainer/     \"\"\"     def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:         super().__init__()         self.conv_block_1 = nn.Sequential(             nn.Conv2d(in_channels=input_shape,                        out_channels=hidden_units,                        kernel_size=3, # how big is the square that's going over the image?                       stride=1, # default                       padding=1), # options = \"valid\" (no padding) or \"same\" (output has same shape as input) or int for specific number              nn.ReLU(),             nn.Conv2d(in_channels=hidden_units,                        out_channels=hidden_units,                       kernel_size=3,                       stride=1,                       padding=1),             nn.ReLU(),             nn.MaxPool2d(kernel_size=2,                          stride=2) # default stride value is same as kernel_size         )         self.conv_block_2 = nn.Sequential(             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),             nn.ReLU(),             nn.MaxPool2d(2)         )         self.classifier = nn.Sequential(             nn.Flatten(),             # Where did this in_features shape come from?              # It's because each layer of our network compresses and changes the shape of our inputs data.             nn.Linear(in_features=hidden_units*16*16,                       out_features=output_shape)         )          def forward(self, x: torch.Tensor):         x = self.conv_block_1(x)         # print(x.shape)         x = self.conv_block_2(x)         # print(x.shape)         x = self.classifier(x)         # print(x.shape)         return x         # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion  torch.manual_seed(42) model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device) model_0 Out[41]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u52a0\u901f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8ba1\u7b97\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4e4b\u4e00\u200b\u662f\u200b\u5229\u7528\u200b \u200b\u7b97\u5b50\u200b\u878d\u5408\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5728\u200b\u6211\u4eec\u200b\u4e0a\u8ff0\u200b\u6a21\u578b\u200b\u7684\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u662f\u200b\u6bcf\u6b21\u200b\u8c03\u7528\u200b\u4e00\u4e2a\u200b\u5c42\u5757\u200b\u5e76\u200b\u91cd\u65b0\u200b\u8d4b\u503c\u200b <code>x</code>\uff0c\u200b\u800c\u662f\u200b\u8fde\u7eed\u200b\u8c03\u7528\u200b\u6bcf\u4e2a\u200b\u5757\u200b\uff08\u200b\u53c2\u89c1\u200b\u4e0a\u8ff0\u200b\u6a21\u578b\u200b\u4e2d\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u884c\u200b\u4f5c\u4e3a\u200b\u793a\u4f8b\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u8282\u7701\u200b\u91cd\u65b0\u200b\u8d4b\u503c\u200b <code>x</code>\uff08\u200b\u5185\u5b58\u200b\u5bc6\u96c6\u578b\u200b\uff09\u200b\u6240\u200b\u82b1\u8d39\u200b\u7684\u200b\u65f6\u95f4\u200b\uff0c\u200b\u5e76\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u4ec5\u200b\u5bf9\u200b <code>x</code> \u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u66f4\u200b\u591a\u200b\u52a0\u901f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b Horace He \u200b\u7684\u200b\u6587\u7ae0\u200b Making Deep Learning Go Brrrr From First Principles\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u770b\u8d77\u6765\u200b\u5f88\u68d2\u200b\u7684\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u901a\u8fc7\u200b\u5728\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u5462\u200b\uff1f</p> In\u00a0[42]: Copied! <pre># 1. Get a batch of images and labels from the DataLoader\nimg_batch, label_batch = next(iter(train_dataloader_simple))\n\n# 2. Get a single image from the batch and unsqueeze the image so its shape fits the model\nimg_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0]\nprint(f\"Single image shape: {img_single.shape}\\n\")\n\n# 3. Perform a forward pass on a single image\nmodel_0.eval()\nwith torch.inference_mode():\n    pred = model_0(img_single.to(device))\n    \n# 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label\nprint(f\"Output logits:\\n{pred}\\n\")\nprint(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\")\nprint(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\")\nprint(f\"Actual label:\\n{label_single}\")\n</pre> # 1. Get a batch of images and labels from the DataLoader img_batch, label_batch = next(iter(train_dataloader_simple))  # 2. Get a single image from the batch and unsqueeze the image so its shape fits the model img_single, label_single = img_batch[0].unsqueeze(dim=0), label_batch[0] print(f\"Single image shape: {img_single.shape}\\n\")  # 3. Perform a forward pass on a single image model_0.eval() with torch.inference_mode():     pred = model_0(img_single.to(device))      # 4. Print out what's happening and convert model logits -&gt; pred probs -&gt; pred label print(f\"Output logits:\\n{pred}\\n\") print(f\"Output prediction probabilities:\\n{torch.softmax(pred, dim=1)}\\n\") print(f\"Output prediction label:\\n{torch.argmax(torch.softmax(pred, dim=1), dim=1)}\\n\") print(f\"Actual label:\\n{label_single}\") <pre>Single image shape: torch.Size([1, 3, 64, 64])\n\nOutput logits:\ntensor([[0.0578, 0.0634, 0.0352]], device='cuda:0')\n\nOutput prediction probabilities:\ntensor([[0.3352, 0.3371, 0.3277]], device='cuda:0')\n\nOutput prediction label:\ntensor([1], device='cuda:0')\n\nActual label:\n2\n</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\u7684\u200b\u7ed3\u679c\u200b\u548c\u200b\u6211\u4eec\u200b\u9884\u671f\u200b\u7684\u200b\u4e00\u81f4\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u90fd\u200b\u4f1a\u200b\u5bf9\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5f80\u5f80\u200b\u662f\u200b\u4e0d\u200b\u6b63\u786e\u200b\u7684\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u6b63\u5e38\u200b\u7684\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6a21\u578b\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u7ecf\u8fc7\u8bad\u7ec3\u200b\uff0c\u200b\u5b83\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u4f7f\u7528\u200b\u968f\u673a\u200b\u6743\u91cd\u200b\u8fdb\u884c\u200b\u731c\u6d4b\u200b\u3002</p> In\u00a0[43]: Copied! <pre># Install torchinfo if it's not available, import it if it is\ntry: \n    import torchinfo\nexcept:\n    !pip install torchinfo\n    import torchinfo\n    \nfrom torchinfo import summary\nsummary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size\n</pre> # Install torchinfo if it's not available, import it if it is try:      import torchinfo except:     !pip install torchinfo     import torchinfo      from torchinfo import summary summary(model_0, input_size=[1, 3, 64, 64]) # do a test pass through of an example input size  Out[43]: <pre>==========================================================================================\nLayer (type:depth-idx)                   Output Shape              Param #\n==========================================================================================\nTinyVGG                                  [1, 3]                    --\n\u251c\u2500Sequential: 1-1                        [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-1                       [1, 10, 64, 64]           280\n\u2502    \u2514\u2500ReLU: 2-2                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500Conv2d: 2-3                       [1, 10, 64, 64]           910\n\u2502    \u2514\u2500ReLU: 2-4                         [1, 10, 64, 64]           --\n\u2502    \u2514\u2500MaxPool2d: 2-5                    [1, 10, 32, 32]           --\n\u251c\u2500Sequential: 1-2                        [1, 10, 16, 16]           --\n\u2502    \u2514\u2500Conv2d: 2-6                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-7                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500Conv2d: 2-8                       [1, 10, 32, 32]           910\n\u2502    \u2514\u2500ReLU: 2-9                         [1, 10, 32, 32]           --\n\u2502    \u2514\u2500MaxPool2d: 2-10                   [1, 10, 16, 16]           --\n\u251c\u2500Sequential: 1-3                        [1, 3]                    --\n\u2502    \u2514\u2500Flatten: 2-11                     [1, 2560]                 --\n\u2502    \u2514\u2500Linear: 2-12                      [1, 3]                    7,683\n==========================================================================================\nTotal params: 10,693\nTrainable params: 10,693\nNon-trainable params: 0\nTotal mult-adds (M): 6.75\n==========================================================================================\nInput size (MB): 0.05\nForward/backward pass size (MB): 0.82\nParams size (MB): 0.04\nEstimated Total Size (MB): 0.91\n==========================================================================================</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p><code>torchinfo.summary()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u4e3a\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5173\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u8bb8\u591a\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b <code>Total params</code>\uff0c\u200b\u5373\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u53c2\u6570\u200b\u603b\u6570\u200b\uff0c\u200b\u4ee5\u53ca\u200b <code>Estimated Total Size (MB)</code>\uff0c\u200b\u5373\u200b\u6a21\u578b\u200b\u7684\u200b\u4f30\u8ba1\u200b\u603b\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6570\u636e\u200b\u5728\u200b\u4ee5\u200b\u67d0\u4e2a\u200b <code>input_size</code> \u200b\u8f93\u5165\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7684\u200b\u53d8\u5316\u200b\u3002</p> <p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u548c\u200b\u6a21\u578b\u200b\u603b\u200b\u5927\u5c0f\u200b\u90fd\u200b\u5f88\u200b\u4f4e\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4ee5\u540e\u200b\u9700\u8981\u200b\u589e\u52a0\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u505a\u200b\u3002</p> In\u00a0[44]: Copied! <pre>def train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer):\n    # Put model in train mode\n    model.train()\n    \n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n    \n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item() \n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch \n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n</pre> def train_step(model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer):     # Put model in train mode     model.train()          # Setup train loss and train accuracy values     train_loss, train_acc = 0, 0          # Loop through data loader data batches     for batch, (X, y) in enumerate(dataloader):         # Send data to target device         X, y = X.to(device), y.to(device)          # 1. Forward pass         y_pred = model(X)          # 2. Calculate  and accumulate loss         loss = loss_fn(y_pred, y)         train_loss += loss.item()           # 3. Optimizer zero grad         optimizer.zero_grad()          # 4. Loss backward         loss.backward()          # 5. Optimizer step         optimizer.step()          # Calculate and accumulate accuracy metric across all batches         y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)         train_acc += (y_pred_class == y).sum().item()/len(y_pred)      # Adjust metrics to get average loss and accuracy per batch      train_loss = train_loss / len(dataloader)     train_acc = train_acc / len(dataloader)     return train_loss, train_acc <p>\u200b\u54c7\u200b\u54e6\u200b\uff01<code>train_step()</code> \u200b\u51fd\u6570\u200b\u5b8c\u6210\u200b\u4e86\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b <code>test_step()</code> \u200b\u51fd\u6570\u200b\u505a\u200b\u540c\u6837\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u4e3b\u8981\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b <code>test_step()</code> \u200b\u4e0d\u4f1a\u200b\u63a5\u6536\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e0d\u4f1a\u200b\u6267\u884c\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u786e\u4fdd\u200b\u5f00\u542f\u200b <code>torch.inference_mode()</code> \u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b\u6765\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[45]: Copied! <pre>def test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module):\n    # Put model in eval mode\n    model.eval() \n    \n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n    \n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n    \n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n            \n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n            \n    # Adjust metrics to get average loss and accuracy per batch \n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n</pre> def test_step(model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module):     # Put model in eval mode     model.eval()           # Setup test loss and test accuracy values     test_loss, test_acc = 0, 0          # Turn on inference context manager     with torch.inference_mode():         # Loop through DataLoader batches         for batch, (X, y) in enumerate(dataloader):             # Send data to target device             X, y = X.to(device), y.to(device)                  # 1. Forward pass             test_pred_logits = model(X)              # 2. Calculate and accumulate loss             loss = loss_fn(test_pred_logits, y)             test_loss += loss.item()                          # Calculate and accumulate accuracy             test_pred_labels = test_pred_logits.argmax(dim=1)             test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))                  # Adjust metrics to get average loss and accuracy per batch      test_loss = test_loss / len(dataloader)     test_acc = test_acc / len(dataloader)     return test_loss, test_acc <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> In\u00a0[46]: Copied! <pre>from tqdm.auto import tqdm\n\n# 1. Take in various parameters required for training and test steps\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),\n          epochs: int = 5):\n    \n    # 2. Create empty results dictionary\n    results = {\"train_loss\": [],\n        \"train_acc\": [],\n        \"test_loss\": [],\n        \"test_acc\": []\n    }\n    \n    # 3. Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer)\n        test_loss, test_acc = test_step(model=model,\n            dataloader=test_dataloader,\n            loss_fn=loss_fn)\n        \n        # 4. Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # 5. Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # 6. Return the filled results at the end of the epochs\n    return results\n</pre> from tqdm.auto import tqdm  # 1. Take in various parameters required for training and test steps def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),           epochs: int = 5):          # 2. Create empty results dictionary     results = {\"train_loss\": [],         \"train_acc\": [],         \"test_loss\": [],         \"test_acc\": []     }          # 3. Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer)         test_loss, test_acc = test_step(model=model,             dataloader=test_dataloader,             loss_fn=loss_fn)                  # 4. Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # 5. Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)      # 6. Return the filled results at the end of the epochs     return results In\u00a0[47]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Recreate an instance of TinyVGG\nmodel_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB) \n                  hidden_units=10, \n                  output_shape=len(train_data.classes)).to(device)\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_0 \nmodel_0_results = train(model=model_0, \n                        train_dataloader=train_dataloader_simple,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Recreate an instance of TinyVGG model_0 = TinyVGG(input_shape=3, # number of color channels (3 for RGB)                    hidden_units=10,                    output_shape=len(train_data.classes)).to(device)  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_0.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_0  model_0_results = train(model=model_0,                          train_dataloader=train_dataloader_simple,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1078 | train_acc: 0.2578 | test_loss: 1.1360 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0847 | train_acc: 0.4258 | test_loss: 1.1620 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.1157 | train_acc: 0.2930 | test_loss: 1.1697 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.0956 | train_acc: 0.4141 | test_loss: 1.1384 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.0985 | train_acc: 0.2930 | test_loss: 1.1426 | test_acc: 0.1979\nTotal training time: 4.935 seconds\n</pre> <p>\u200b\u55ef\u200b...</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u7cdf\u7cd5\u200b\u3002</p> <p>\u200b\u4f46\u200b\u76ee\u524d\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u7ee7\u7eed\u200b\u575a\u6301\u4e0b\u53bb\u200b\u3002</p> <p>\u200b\u6709\u200b\u54ea\u4e9b\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u6f5c\u5728\u5730\u200b\u6539\u8fdb\u200b\u5b83\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u67e5\u770b\u200b\u7b14\u8bb0\u672c\u200b 02 \u200b\u4e2d\u200b\u7684\u200b\u4ece\u200b\u6a21\u578b\u200b\u89d2\u5ea6\u200b\u6539\u8fdb\u200b\u6a21\u578b\u200b\u90e8\u5206\u200b\uff0c\u200b\u4e86\u89e3\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b TinyVGG \u200b\u6a21\u578b\u200b\u7684\u200b\u60f3\u6cd5\u200b\u3002</p> In\u00a0[48]: Copied! <pre># Check the model_0_results keys\nmodel_0_results.keys()\n</pre> # Check the model_0_results keys model_0_results.keys() Out[48]: <pre>dict_keys(['train_loss', 'train_acc', 'test_loss', 'test_acc'])</pre> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u63d0\u53d6\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u952e\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u56fe\u8868\u200b\u3002</p> In\u00a0[49]: Copied! <pre>def plot_loss_curves(results: Dict[str, List[float]]):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    \n    # Get the loss values of the results dictionary (training and test)\n    loss = results['train_loss']\n    test_loss = results['test_loss']\n\n    # Get the accuracy values of the results dictionary (training and test)\n    accuracy = results['train_acc']\n    test_accuracy = results['test_acc']\n\n    # Figure out how many epochs there were\n    epochs = range(len(results['train_loss']))\n\n    # Setup a plot \n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label='train_loss')\n    plt.plot(epochs, test_loss, label='test_loss')\n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label='train_accuracy')\n    plt.plot(epochs, test_accuracy, label='test_accuracy')\n    plt.title('Accuracy')\n    plt.xlabel('Epochs')\n    plt.legend();\n</pre> def plot_loss_curves(results: Dict[str, List[float]]):     \"\"\"Plots training curves of a results dictionary.      Args:         results (dict): dictionary containing list of values, e.g.             {\"train_loss\": [...],              \"train_acc\": [...],              \"test_loss\": [...],              \"test_acc\": [...]}     \"\"\"          # Get the loss values of the results dictionary (training and test)     loss = results['train_loss']     test_loss = results['test_loss']      # Get the accuracy values of the results dictionary (training and test)     accuracy = results['train_acc']     test_accuracy = results['test_acc']      # Figure out how many epochs there were     epochs = range(len(results['train_loss']))      # Setup a plot      plt.figure(figsize=(15, 7))      # Plot loss     plt.subplot(1, 2, 1)     plt.plot(epochs, loss, label='train_loss')     plt.plot(epochs, test_loss, label='test_loss')     plt.title('Loss')     plt.xlabel('Epochs')     plt.legend()      # Plot accuracy     plt.subplot(1, 2, 2)     plt.plot(epochs, accuracy, label='train_accuracy')     plt.plot(epochs, test_accuracy, label='test_accuracy')     plt.title('Accuracy')     plt.xlabel('Epochs')     plt.legend(); <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b <code>plot_loss_curves()</code> \u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[50]: Copied! <pre>plot_loss_curves(model_0_results)\n</pre> plot_loss_curves(model_0_results) <p>\u200b\u54c7\u200b\u54e6\u200b\u3002</p> <p>\u200b\u770b\u8d77\u6765\u200b\u4e8b\u60c5\u200b\u4e00\u56e2\u7cdf\u200b...</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u591a\u5c11\u200b\u9884\u6599\u5230\u200b\u4e86\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u671f\u95f4\u200b\u7684\u200b\u6253\u5370\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u5e76\u200b\u6ca1\u6709\u200b\u663e\u793a\u200b\u51fa\u592a\u591a\u200b\u5e0c\u671b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff0c\u200b\u770b\u770b\u200b\u5728\u200b\u66f4\u957f\u200b\u7684\u200b\u65f6\u95f4\u8de8\u5ea6\u200b\u4e0a\u200b\u7ed8\u5236\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\u3002</p> In\u00a0[51]: Copied! <pre># Create training transform with TrivialAugment\ntrain_transform_trivial_augment = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.TrivialAugmentWide(num_magnitude_bins=31),\n    transforms.ToTensor() \n])\n\n# Create testing transform (no data augmentation)\ntest_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n    transforms.ToTensor()\n])\n</pre> # Create training transform with TrivialAugment train_transform_trivial_augment = transforms.Compose([     transforms.Resize((64, 64)),     transforms.TrivialAugmentWide(num_magnitude_bins=31),     transforms.ToTensor()  ])  # Create testing transform (no data augmentation) test_transform = transforms.Compose([     transforms.Resize((64, 64)),     transforms.ToTensor() ]) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>Dataset</code>\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader()</code> \u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u3002</p> In\u00a0[52]: Copied! <pre># Turn image folders into Datasets\ntrain_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment)\ntest_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)\n\ntrain_data_augmented, test_data_simple\n</pre> # Turn image folders into Datasets train_data_augmented = datasets.ImageFolder(train_dir, transform=train_transform_trivial_augment) test_data_simple = datasets.ImageFolder(test_dir, transform=test_transform)  train_data_augmented, test_data_simple Out[52]: <pre>(Dataset ImageFolder\n     Number of datapoints: 225\n     Root location: data/pizza_steak_sushi/train\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n                ToTensor()\n            ),\n Dataset ImageFolder\n     Number of datapoints: 75\n     Root location: data/pizza_steak_sushi/test\n     StandardTransform\n Transform: Compose(\n                Resize(size=(64, 64), interpolation=bilinear, max_size=None, antialias=None)\n                ToTensor()\n            ))</pre> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b <code>DataLoader</code>\uff0c\u200b\u5176\u200b <code>batch_size</code> \u200b\u8bbe\u200b\u4e3a\u200b 32\uff0c\u200b\u5e76\u200b\u5c06\u200b <code>num_workers</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u673a\u5668\u200b\u4e0a\u200b\u53ef\u7528\u200b\u7684\u200b CPU \u200b\u6570\u91cf\u200b\uff08\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>os.cpu_count()</code> \u200b\u6765\u200b\u83b7\u53d6\u200b\u8fd9\u4e2a\u200b\u503c\u200b\uff09\u3002</p> In\u00a0[53]: Copied! <pre># Turn Datasets into DataLoader's\nimport os\nBATCH_SIZE = 32\nNUM_WORKERS = os.cpu_count()\n\ntorch.manual_seed(42)\ntrain_dataloader_augmented = DataLoader(train_data_augmented, \n                                        batch_size=BATCH_SIZE, \n                                        shuffle=True,\n                                        num_workers=NUM_WORKERS)\n\ntest_dataloader_simple = DataLoader(test_data_simple, \n                                    batch_size=BATCH_SIZE, \n                                    shuffle=False, \n                                    num_workers=NUM_WORKERS)\n\ntrain_dataloader_augmented, test_dataloader\n</pre> # Turn Datasets into DataLoader's import os BATCH_SIZE = 32 NUM_WORKERS = os.cpu_count()  torch.manual_seed(42) train_dataloader_augmented = DataLoader(train_data_augmented,                                          batch_size=BATCH_SIZE,                                          shuffle=True,                                         num_workers=NUM_WORKERS)  test_dataloader_simple = DataLoader(test_data_simple,                                      batch_size=BATCH_SIZE,                                      shuffle=False,                                      num_workers=NUM_WORKERS)  train_dataloader_augmented, test_dataloader Out[53]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f53c6d64040&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f53c0b9de50&gt;)</pre> In\u00a0[54]: Copied! <pre># Create model_1 and send it to the target device\ntorch.manual_seed(42)\nmodel_1 = TinyVGG(\n    input_shape=3,\n    hidden_units=10,\n    output_shape=len(train_data_augmented.classes)).to(device)\nmodel_1\n</pre> # Create model_1 and send it to the target device torch.manual_seed(42) model_1 = TinyVGG(     input_shape=3,     hidden_units=10,     output_shape=len(train_data_augmented.classes)).to(device) model_1 Out[54]: <pre>TinyVGG(\n  (conv_block_1): Sequential(\n    (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv_block_2): Sequential(\n    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU()\n    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU()\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (classifier): Sequential(\n    (0): Flatten(start_dim=1, end_dim=-1)\n    (1): Linear(in_features=2560, out_features=3, bias=True)\n  )\n)</pre> <p>\u200b\u6a21\u578b\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\uff01</p> <p>\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u7684\u200b\u51fd\u6570\u200b\uff08<code>train_step()</code>\uff09\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u7684\u200b\u51fd\u6570\u200b\uff08<code>test_step()</code>\uff09\uff0c\u200b\u4ee5\u53ca\u200b\u5c06\u200b\u5b83\u4eec\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\u7684\u200b<code>train()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u7528\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e0e\u200b<code>model_0</code>\u200b\u76f8\u540c\u200b\u7684\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u4ec5\u200b\u5728\u200b<code>train_dataloader</code>\u200b\u53c2\u6570\u200b\u4e0a\u200b\u6709\u6240\u4e0d\u540c\u200b\uff1a</p> <ul> <li>\u200b\u8bad\u7ec3\u200b 5 \u200b\u4e2a\u200b\u5468\u671f\u200b\u3002</li> <li>\u200b\u5728\u200b<code>train()</code>\u200b\u4e2d\u200b\u4f7f\u7528\u200b<code>train_dataloader=train_dataloader_augmented</code>\u200b\u4f5c\u4e3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b<code>torch.nn.CrossEntropyLoss()</code>\u200b\u4f5c\u4e3a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b<code>torch.optim.Adam()</code>\uff0c\u200b\u5b66\u4e60\u200b\u7387\u200b<code>lr=0.001</code>\u200b\u4f5c\u4e3a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</li> </ul> In\u00a0[55]: Copied! <pre># Set random seeds\ntorch.manual_seed(42) \ntorch.cuda.manual_seed(42)\n\n# Set number of epochs\nNUM_EPOCHS = 5\n\n# Setup loss function and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Train model_1\nmodel_1_results = train(model=model_1, \n                        train_dataloader=train_dataloader_augmented,\n                        test_dataloader=test_dataloader_simple,\n                        optimizer=optimizer,\n                        loss_fn=loss_fn, \n                        epochs=NUM_EPOCHS)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set random seeds torch.manual_seed(42)  torch.cuda.manual_seed(42)  # Set number of epochs NUM_EPOCHS = 5  # Setup loss function and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(params=model_1.parameters(), lr=0.001)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Train model_1 model_1_results = train(model=model_1,                          train_dataloader=train_dataloader_augmented,                         test_dataloader=test_dataloader_simple,                         optimizer=optimizer,                         loss_fn=loss_fn,                          epochs=NUM_EPOCHS)  # End the timer and print out how long it took end_time = timer() print(f\"Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.1074 | train_acc: 0.2500 | test_loss: 1.1058 | test_acc: 0.2604\nEpoch: 2 | train_loss: 1.0791 | train_acc: 0.4258 | test_loss: 1.1382 | test_acc: 0.2604\nEpoch: 3 | train_loss: 1.0803 | train_acc: 0.4258 | test_loss: 1.1685 | test_acc: 0.2604\nEpoch: 4 | train_loss: 1.1285 | train_acc: 0.3047 | test_loss: 1.1623 | test_acc: 0.2604\nEpoch: 5 | train_loss: 1.0880 | train_acc: 0.4258 | test_loss: 1.1472 | test_acc: 0.2604\nTotal training time: 4.924 seconds\n</pre> <p>\u200b\u55ef\u200b...</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u53c8\u200b\u4e0d\u200b\u592a\u200b\u7406\u60f3\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u5b83\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u3002</p> In\u00a0[56]: Copied! <pre>plot_loss_curves(model_1_results)\n</pre> plot_loss_curves(model_1_results) <p>\u200b\u54c7\u200b...</p> <p>\u200b\u8fd9\u4e9b\u200b\u770b\u8d77\u6765\u200b\u4e5f\u200b\u4e0d\u592a\u597d\u200b...</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b\u6b20\u200b\u62df\u5408\u200b\u8fd8\u662f\u200b\u8fc7\u200b\u62df\u5408\u200b\uff1f</p> <p>\u200b\u8fd8\u662f\u200b\u4e24\u8005\u200b\u90fd\u200b\u6709\u200b\uff1f</p> <p>\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5b83\u200b\u7684\u200b\u51c6\u786e\u6027\u200b\u66f4\u9ad8\u200b\uff0c\u200b\u635f\u5931\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u5bf9\u200b\u5427\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u54ea\u4e9b\u200b\u65b9\u6cd5\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u4e9b\u200b\u76ee\u6807\u200b\u5462\u200b\uff1f</p> In\u00a0[57]: Copied! <pre>import pandas as pd\nmodel_0_df = pd.DataFrame(model_0_results)\nmodel_1_df = pd.DataFrame(model_1_results)\nmodel_0_df\n</pre> import pandas as pd model_0_df = pd.DataFrame(model_0_results) model_1_df = pd.DataFrame(model_1_results) model_0_df Out[57]: train_loss train_acc test_loss test_acc 0 1.107833 0.257812 1.136041 0.260417 1 1.084713 0.425781 1.162014 0.197917 2 1.115697 0.292969 1.169704 0.197917 3 1.095564 0.414062 1.138373 0.197917 4 1.098520 0.292969 1.142631 0.197917 <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>matplotlib</code> \u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u7ed8\u56fe\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5c06\u200b <code>model_0</code> \u200b\u548c\u200b <code>model_1</code> \u200b\u7684\u200b\u7ed3\u679c\u200b\u4e00\u8d77\u200b\u53ef\u89c6\u5316\u200b\u3002</p> In\u00a0[58]: Copied! <pre># Setup a plot \nplt.figure(figsize=(15, 10))\n\n# Get number of epochs\nepochs = range(len(model_0_df))\n\n# Plot train loss\nplt.subplot(2, 2, 1)\nplt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\")\nplt.title(\"Train Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test loss\nplt.subplot(2, 2, 2)\nplt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\")\nplt.title(\"Test Loss\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot train accuracy\nplt.subplot(2, 2, 3)\nplt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\")\nplt.title(\"Train Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend()\n\n# Plot test accuracy\nplt.subplot(2, 2, 4)\nplt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\")\nplt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\")\nplt.title(\"Test Accuracy\")\nplt.xlabel(\"Epochs\")\nplt.legend();\n</pre> # Setup a plot  plt.figure(figsize=(15, 10))  # Get number of epochs epochs = range(len(model_0_df))  # Plot train loss plt.subplot(2, 2, 1) plt.plot(epochs, model_0_df[\"train_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_loss\"], label=\"Model 1\") plt.title(\"Train Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test loss plt.subplot(2, 2, 2) plt.plot(epochs, model_0_df[\"test_loss\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_loss\"], label=\"Model 1\") plt.title(\"Test Loss\") plt.xlabel(\"Epochs\") plt.legend()  # Plot train accuracy plt.subplot(2, 2, 3) plt.plot(epochs, model_0_df[\"train_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"train_acc\"], label=\"Model 1\") plt.title(\"Train Accuracy\") plt.xlabel(\"Epochs\") plt.legend()  # Plot test accuracy plt.subplot(2, 2, 4) plt.plot(epochs, model_0_df[\"test_acc\"], label=\"Model 0\") plt.plot(epochs, model_1_df[\"test_acc\"], label=\"Model 1\") plt.title(\"Test Accuracy\") plt.xlabel(\"Epochs\") plt.legend(); <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u90fd\u200b\u4e0d\u200b\u592a\u200b\u7406\u60f3\u200b\uff0c\u200b\u4e14\u200b\u6ce2\u52a8\u200b\u8f83\u5927\u200b\uff08\u200b\u6307\u6807\u200b\u6025\u5267\u200b\u4e0a\u5347\u200b\u548c\u200b\u4e0b\u964d\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u6784\u5efa\u200b\u4e86\u200b <code>model_2</code>\uff0c\u200b\u4e3a\u4e86\u200b\u5c1d\u8bd5\u200b\u63d0\u9ad8\u200b\u6027\u80fd\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8003\u8651\u200b\u4ee5\u4e0b\u200b\u51e0\u4e2a\u200b\u65b9\u9762\u200b\uff1a</p> <ol> <li><p>\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\uff1a</p> <ul> <li>\u200b\u68c0\u67e5\u6570\u636e\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u7f3a\u5931\u200b\u503c\u200b\u6216\u200b\u5f02\u5e38\u200b\u503c\u200b\uff0c\u200b\u5e76\u200b\u8fdb\u884c\u200b\u9002\u5f53\u200b\u7684\u200b\u5904\u7406\u200b\u3002</li> <li>\u200b\u6807\u51c6\u5316\u200b\u6216\u200b\u5f52\u4e00\u5316\u200b\u6570\u636e\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u6240\u6709\u200b\u7279\u5f81\u200b\u5728\u200b\u540c\u4e00\u200b\u5c3a\u5ea6\u200b\u4e0a\u200b\u3002</li> <li>\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6280\u672f\u200b\uff0c\u200b\u5c24\u5176\u200b\u662f\u200b\u5bf9\u4e8e\u200b\u56fe\u50cf\u200b\u6216\u200b\u6587\u672c\u200b\u6570\u636e\u200b\u3002</li> </ul> </li> <li><p>\u200b\u7279\u5f81\u200b\u5de5\u7a0b\u200b\uff1a</p> <ul> <li>\u200b\u63a2\u7d22\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7279\u5f81\u200b\u7ec4\u5408\u200b\u6216\u200b\u53d8\u6362\u200b\uff0c\u200b\u4ee5\u200b\u63d0\u53d6\u200b\u66f4\u200b\u6709\u200b\u4fe1\u606f\u91cf\u200b\u7684\u200b\u7279\u5f81\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u7279\u5f81\u9009\u62e9\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b\u76f8\u5173\u6027\u200b\u5206\u6790\u200b\u3001\u200b\u9012\u5f52\u200b\u7279\u5f81\u200b\u6d88\u9664\u200b\uff08RFE\uff09\u200b\u7b49\u200b\uff0c\u200b\u4ee5\u200b\u51cf\u5c11\u200b\u5197\u4f59\u200b\u7279\u5f81\u200b\u3002</li> </ul> </li> <li><p>\u200b\u6a21\u578b\u200b\u9009\u62e9\u200b\u4e0e\u200b\u8c03\u4f18\u200b\uff1a</p> <ul> <li>\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff0c\u200b\u5982\u200b\u589e\u52a0\u200b\u7f51\u7edc\u5c42\u200b\u6570\u200b\u3001\u200b\u6539\u53d8\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u7b49\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u4ea4\u53c9\u200b\u9a8c\u8bc1\u200b\u6765\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u7684\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\u3002</li> <li>\u200b\u5e94\u7528\u200b\u8d85\u200b\u53c2\u6570\u200b\u8c03\u4f18\u200b\u6280\u672f\u200b\uff0c\u200b\u5982\u200b\u7f51\u683c\u200b\u641c\u7d22\u200b\uff08Grid Search\uff09\u200b\u6216\u200b\u968f\u673a\u200b\u641c\u7d22\u200b\uff08Random Search\uff09\uff0c\u200b\u4ee5\u200b\u627e\u5230\u200b\u6700\u4f73\u200b\u53c2\u6570\u200b\u7ec4\u5408\u200b\u3002</li> </ul> </li> <li><p>\u200b\u6b63\u5219\u200b\u5316\u200b\uff1a</p> <ul> <li>\u200b\u5f15\u5165\u200b\u6b63\u5219\u200b\u5316\u9879\u200b\uff08\u200b\u5982\u200bL1\u3001L2\u200b\u6b63\u5219\u200b\u5316\u200b\uff09\uff0c\u200b\u4ee5\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002</li> <li>\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b dropout \u200b\u6216\u200b batch normalization \u200b\u7b49\u200b\u6280\u672f\u200b\u3002</li> </ul> </li> <li><p>\u200b\u96c6\u6210\u200b\u5b66\u4e60\u200b\uff1a</p> <ul> <li>\u200b\u8003\u8651\u200b\u4f7f\u7528\u200b\u96c6\u6210\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5982\u200b bagging\u3001boosting \u200b\u6216\u200b stacking\uff0c\u200b\u4ee5\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u7684\u200b\u7a33\u5b9a\u6027\u200b\u548c\u200b\u6027\u80fd\u200b\u3002</li> </ul> </li> <li><p>\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u6574\u200b\uff1a</p> <ul> <li>\u200b\u4f7f\u7528\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u7b56\u7565\u200b\uff0c\u200b\u5982\u200b\u4f59\u5f26\u200b\u9000\u706b\u200b\uff08Cosine Annealing\uff09\u200b\u6216\u200b\u5b66\u4e60\u200b\u7387\u200b\u8c03\u5ea6\u200b\u5668\u200b\uff08Learning Rate Scheduler\uff09\u3002</li> </ul> </li> <li><p>\u200b\u8bc4\u4f30\u200b\u4e0e\u200b\u76d1\u63a7\u200b\uff1a</p> <ul> <li>\u200b\u5b9a\u671f\u68c0\u67e5\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u6307\u6807\u200b\u53d8\u5316\u200b\uff0c\u200b\u786e\u4fdd\u200b\u6a21\u578b\u200b\u5728\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u5411\u200b\u4e0a\u200b\u4f18\u5316\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u53ef\u89c6\u5316\u200b\u5de5\u5177\u200b\uff0c\u200b\u5982\u200b TensorBoard\uff0c\u200b\u6765\u200b\u76d1\u63a7\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u548c\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u3002</li> </ul> </li> </ol> <p>\u200b\u901a\u8fc7\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7cfb\u7edf\u5730\u200b\u6539\u8fdb\u200b <code>model_2</code>\uff0c\u200b\u4ee5\u200b\u671f\u671b\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u6027\u80fd\u200b\u3002</p> In\u00a0[59]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\") <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> In\u00a0[60]: Copied! <pre>import torchvision\n\n# Read in custom image\ncustom_image_uint8 = torchvision.io.read_image(str(custom_image_path))\n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image_uint8}\\n\")\nprint(f\"Custom image shape: {custom_image_uint8.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image_uint8.dtype}\")\n</pre> import torchvision  # Read in custom image custom_image_uint8 = torchvision.io.read_image(str(custom_image_path))  # Print out image data print(f\"Custom image tensor:\\n{custom_image_uint8}\\n\") print(f\"Custom image shape: {custom_image_uint8.shape}\\n\") print(f\"Custom image dtype: {custom_image_uint8.dtype}\") <pre>Custom image tensor:\ntensor([[[154, 173, 181,  ...,  21,  18,  14],\n         [146, 165, 181,  ...,  21,  18,  15],\n         [124, 146, 172,  ...,  18,  17,  15],\n         ...,\n         [ 72,  59,  45,  ..., 152, 150, 148],\n         [ 64,  55,  41,  ..., 150, 147, 144],\n         [ 64,  60,  46,  ..., 149, 146, 143]],\n\n        [[171, 190, 193,  ...,  22,  19,  15],\n         [163, 182, 193,  ...,  22,  19,  16],\n         [141, 163, 184,  ...,  19,  18,  16],\n         ...,\n         [ 55,  42,  28,  ..., 107, 104, 103],\n         [ 47,  38,  24,  ..., 108, 104, 102],\n         [ 47,  43,  29,  ..., 107, 104, 101]],\n\n        [[119, 138, 147,  ...,  17,  14,  10],\n         [111, 130, 145,  ...,  17,  14,  11],\n         [ 87, 111, 136,  ...,  14,  13,  11],\n         ...,\n         [ 35,  22,   8,  ...,  52,  52,  48],\n         [ 27,  18,   4,  ...,  50,  49,  44],\n         [ 27,  23,   9,  ...,  49,  46,  43]]], dtype=torch.uint8)\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.uint8\n</pre> <p>\u200b\u5f88\u200b\u597d\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u662f\u200b\u4ee5\u200b\u5f20\u91cf\u200b\u683c\u5f0f\u200b\u5b58\u5728\u200b\u7684\u200b\uff0c\u200b\u4f46\u662f\u200b\u8fd9\u79cd\u200b\u56fe\u50cf\u683c\u5f0f\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u517c\u5bb9\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>custom_image</code> \u200b\u5f20\u91cf\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u662f\u200b <code>torch.uint8</code>\uff0c\u200b\u5176\u503c\u200b\u5728\u200b <code>[0, 255]</code> \u200b\u4e4b\u95f4\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u63a5\u53d7\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u662f\u200b <code>torch.float32</code>\uff0c\u200b\u5e76\u4e14\u200b\u503c\u200b\u5728\u200b <code>[0, 1]</code> \u200b\u4e4b\u95f4\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u76f8\u540c\u200b\u7684\u200b\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u6a21\u578b\u200b\u5c06\u4f1a\u200b\u62a5\u9519\u200b\u3002</p> In\u00a0[61]: Copied! <pre># Try to make a prediction on image in uint8 format (this will error)\nmodel_1.eval()\nwith torch.inference_mode():\n    model_1(custom_image_uint8.to(device))\n</pre> # Try to make a prediction on image in uint8 format (this will error) model_1.eval() with torch.inference_mode():     model_1(custom_image_uint8.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [61], in &lt;cell line: 3&gt;()\n      2 model_1.eval()\n      3 with torch.inference_mode():\n----&gt; 4     model_1(custom_image_uint8.to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [41], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:457, in Conv2d.forward(self, input)\n    456 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 457     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:453, in Conv2d._conv_forward(self, input, weight, bias)\n    449 if self.padding_mode != 'zeros':\n    450     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    451                     weight, bias, self.stride,\n    452                     _pair(0), self.dilation, self.groups)\n--&gt; 453 return F.conv2d(input, weight, bias, self.stride,\n    454                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (torch.cuda.ByteTensor) and weight type (torch.cuda.FloatTensor) should be the same</pre> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4f1a\u200b\u9047\u5230\u200b\u5982\u4e0b\u200b\u9519\u8bef\u200b\uff1a</p> <p><code>RuntimeError: \u200b\u8f93\u5165\u200b\u7c7b\u578b\u200b (torch.cuda.ByteTensor) \u200b\u548c\u200b\u6743\u91cd\u200b\u7c7b\u578b\u200b (torch.cuda.FloatTensor) \u200b\u5e94\u8be5\u200b\u76f8\u540c\u200b</code></p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08<code>torch.float32</code>\uff09\u200b\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> In\u00a0[62]: Copied! <pre># Load in custom image and convert the tensor values to float32\ncustom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)\n\n# Divide the image pixel values by 255 to get them between [0, 1]\ncustom_image = custom_image / 255. \n\n# Print out image data\nprint(f\"Custom image tensor:\\n{custom_image}\\n\")\nprint(f\"Custom image shape: {custom_image.shape}\\n\")\nprint(f\"Custom image dtype: {custom_image.dtype}\")\n</pre> # Load in custom image and convert the tensor values to float32 custom_image = torchvision.io.read_image(str(custom_image_path)).type(torch.float32)  # Divide the image pixel values by 255 to get them between [0, 1] custom_image = custom_image / 255.   # Print out image data print(f\"Custom image tensor:\\n{custom_image}\\n\") print(f\"Custom image shape: {custom_image.shape}\\n\") print(f\"Custom image dtype: {custom_image.dtype}\") <pre>Custom image tensor:\ntensor([[[0.6039, 0.6784, 0.7098,  ..., 0.0824, 0.0706, 0.0549],\n         [0.5725, 0.6471, 0.7098,  ..., 0.0824, 0.0706, 0.0588],\n         [0.4863, 0.5725, 0.6745,  ..., 0.0706, 0.0667, 0.0588],\n         ...,\n         [0.2824, 0.2314, 0.1765,  ..., 0.5961, 0.5882, 0.5804],\n         [0.2510, 0.2157, 0.1608,  ..., 0.5882, 0.5765, 0.5647],\n         [0.2510, 0.2353, 0.1804,  ..., 0.5843, 0.5725, 0.5608]],\n\n        [[0.6706, 0.7451, 0.7569,  ..., 0.0863, 0.0745, 0.0588],\n         [0.6392, 0.7137, 0.7569,  ..., 0.0863, 0.0745, 0.0627],\n         [0.5529, 0.6392, 0.7216,  ..., 0.0745, 0.0706, 0.0627],\n         ...,\n         [0.2157, 0.1647, 0.1098,  ..., 0.4196, 0.4078, 0.4039],\n         [0.1843, 0.1490, 0.0941,  ..., 0.4235, 0.4078, 0.4000],\n         [0.1843, 0.1686, 0.1137,  ..., 0.4196, 0.4078, 0.3961]],\n\n        [[0.4667, 0.5412, 0.5765,  ..., 0.0667, 0.0549, 0.0392],\n         [0.4353, 0.5098, 0.5686,  ..., 0.0667, 0.0549, 0.0431],\n         [0.3412, 0.4353, 0.5333,  ..., 0.0549, 0.0510, 0.0431],\n         ...,\n         [0.1373, 0.0863, 0.0314,  ..., 0.2039, 0.2039, 0.1882],\n         [0.1059, 0.0706, 0.0157,  ..., 0.1961, 0.1922, 0.1725],\n         [0.1059, 0.0902, 0.0353,  ..., 0.1922, 0.1804, 0.1686]]])\n\nCustom image shape: torch.Size([3, 4032, 3024])\n\nCustom image dtype: torch.float32\n</pre> In\u00a0[63]: Copied! <pre># Plot custom image\nplt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error\nplt.title(f\"Image shape: {custom_image.shape}\")\nplt.axis(False);\n</pre> # Plot custom image plt.imshow(custom_image.permute(1, 2, 0)) # need to permute image dimensions from CHW -&gt; HWC otherwise matplotlib will error plt.title(f\"Image shape: {custom_image.shape}\") plt.axis(False); <p>\u200b\u4e24\u4e2a\u200b\u5927\u62c7\u6307\u200b\uff01</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u4e00\u81f4\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4f7f\u7528\u200b <code>torchvision.transforms.Resize()</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u53d8\u6362\u200b\u7ba1\u9053\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> In\u00a0[64]: Copied! <pre># Create transform pipleine to resize image\ncustom_image_transform = transforms.Compose([\n    transforms.Resize((64, 64)),\n])\n\n# Transform target image\ncustom_image_transformed = custom_image_transform(custom_image)\n\n# Print out original shape and new shape\nprint(f\"Original shape: {custom_image.shape}\")\nprint(f\"New shape: {custom_image_transformed.shape}\")\n</pre> # Create transform pipleine to resize image custom_image_transform = transforms.Compose([     transforms.Resize((64, 64)), ])  # Transform target image custom_image_transformed = custom_image_transform(custom_image)  # Print out original shape and new shape print(f\"Original shape: {custom_image.shape}\") print(f\"New shape: {custom_image_transformed.shape}\") <pre>Original shape: torch.Size([3, 4032, 3024])\nNew shape: torch.Size([3, 64, 64])\n</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ec8\u4e8e\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5427\u200b\u3002</p> In\u00a0[65]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed)\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [65], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [41], in TinyVGG.forward(self, x)\n     39 def forward(self, x: torch.Tensor):\n---&gt; 40     x = self.conv_block_1(x)\n     41     # print(x.shape)\n     42     x = self.conv_block_2(x)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:457, in Conv2d.forward(self, input)\n    456 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 457     return self._conv_forward(input, self.weight, self.bias)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/conv.py:453, in Conv2d._conv_forward(self, input, weight, bias)\n    449 if self.padding_mode != 'zeros':\n    450     return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n    451                     weight, bias, self.stride,\n    452                     _pair(0), self.dilation, self.groups)\n--&gt; 453 return F.conv2d(input, weight, bias, self.stride,\n    454                 self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cpu and cuda:0! (when checking argument for argument weight in method wrapper___slow_conv2d_forward)</pre> <p>\u200b\u54e6\u200b\u5929\u200b\u54ea\u200b...</p> <p>\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u505a\u597d\u200b\u4e86\u200b\u51c6\u5907\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u548c\u200b\u6a21\u578b\u200b\u5206\u522b\u200b\u4f4d\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u4e8e\u662f\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b\u8fd9\u6837\u200b\u7684\u200b\u9519\u8bef\u200b\uff1a</p> <p><code>RuntimeError: \u200b\u9884\u671f\u200b\u6240\u6709\u200b\u5f20\u91cf\u200b\u90fd\u200b\u5728\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u4f46\u200b\u53d1\u73b0\u200b\u4e86\u200b\u81f3\u5c11\u200b\u4e24\u4e2a\u200b\u8bbe\u5907\u200b\uff0ccpu \u200b\u548c\u200b cuda:0\uff01\uff08\u200b\u5728\u200b\u68c0\u67e5\u200b\u65b9\u6cd5\u200b wrapper___slow_conv2d_forward \u200b\u7684\u200b\u53c2\u6570\u200b weight \u200b\u65f6\u200b\uff09</code></p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5c06\u200b <code>custom_image_transformed</code> \u200b\u653e\u7f6e\u200b\u5728\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u4e0a\u6765\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u3002</p> In\u00a0[66]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    custom_image_pred = model_1(custom_image_transformed.to(device))\n</pre> model_1.eval() with torch.inference_mode():     custom_image_pred = model_1(custom_image_transformed.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [66], in &lt;cell line: 2&gt;()\n      1 model_1.eval()\n      2 with torch.inference_mode():\n----&gt; 3     custom_image_pred = model_1(custom_image_transformed.to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nInput In [41], in TinyVGG.forward(self, x)\n     42 x = self.conv_block_2(x)\n     43 # print(x.shape)\n---&gt; 44 x = self.classifier(x)\n     45 # print(x.shape)\n     46 return x\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (10x256 and 2560x3)</pre> <p>\u200b\u73b0\u5728\u200b\u600e\u4e48\u529e\u200b\uff1f</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u4f1a\u200b\u8fd9\u6837\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u76f8\u540c\u200b\u4e86\u200b...</p> <p>\u200b\u54e6\u200b\uff0c\u200b\u7b49\u7b49\u200b...</p> <p>\u200b\u6211\u4eec\u200b\u5ffd\u7565\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u671f\u671b\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u5728\u200b\u5f00\u5934\u200b\u6709\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u7ef4\u5ea6\u200b\uff08<code>NCHW</code>\uff0c\u200b\u5176\u4e2d\u200b<code>N</code>\u200b\u662f\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff09\u3002</p> <p>\u200b\u53ea\u4e0d\u8fc7\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u53ea\u6709\u200b<code>CHW</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>torch.unsqueeze(dim=0)</code>\u200b\u6765\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u6700\u7ec8\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u544a\u8bc9\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5355\u4e2a\u200b\u56fe\u50cf\u200b\uff08\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u4e3a\u200b1\u200b\u7684\u200b\u56fe\u50cf\u200b\uff09\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[67]: Copied! <pre>model_1.eval()\nwith torch.inference_mode():\n    # Add an extra dimension to image\n    custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)\n    \n    # Print out different shapes\n    print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")\n    print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")\n    \n    # Make a prediction on image with an extra dimension\n    custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device))\n</pre> model_1.eval() with torch.inference_mode():     # Add an extra dimension to image     custom_image_transformed_with_batch_size = custom_image_transformed.unsqueeze(dim=0)          # Print out different shapes     print(f\"Custom image transformed shape: {custom_image_transformed.shape}\")     print(f\"Unsqueezed custom image shape: {custom_image_transformed_with_batch_size.shape}\")          # Make a prediction on image with an extra dimension     custom_image_pred = model_1(custom_image_transformed.unsqueeze(dim=0).to(device)) <pre>Custom image transformed shape: torch.Size([3, 64, 64])\nUnsqueezed custom image shape: torch.Size([1, 3, 64, 64])\n</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01\uff01\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6210\u529f\u200b\u4e86\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u7ecf\u5386\u200b\u7684\u200b\u662f\u200b\u4e09\u79cd\u200b\u7ecf\u5178\u200b\u4e14\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u548c\u200b PyTorch \u200b\u95ee\u9898\u200b\uff1a</p> <ol> <li>\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b - \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u671f\u671b\u200b <code>torch.float32</code>\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u539f\u59cb\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u4e3a\u200b <code>uint8</code>\u3002</li> <li>\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b - \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4f4d\u4e8e\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\uff08\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u662f\u200b GPU\uff09\u200b\u4e0a\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u6570\u636e\u200b\u5c1a\u672a\u200b\u79fb\u52a8\u200b\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</li> <li>\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b - \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u671f\u671b\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[N, C, H, W]</code> \u200b\u6216\u200b <code>[batch_size, color_channels, height, width]</code>\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[color_channels, height, width]</code>\u3002</li> </ol> <p>\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u9519\u8bef\u200b\u4e0d\u4ec5\u4ec5\u200b\u51fa\u73b0\u200b\u5728\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u51e0\u4e4e\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u5728\u200b\u4f60\u200b\u5904\u7406\u200b\u7684\u200b\u6bcf\u200b\u4e00\u79cd\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08\u200b\u6587\u672c\u200b\u3001\u200b\u97f3\u9891\u200b\u3001\u200b\u7ed3\u6784\u5316\u200b\u6570\u636e\u200b\uff09\u200b\u548c\u200b\u95ee\u9898\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</p> In\u00a0[68]: Copied! <pre>custom_image_pred\n</pre> custom_image_pred Out[68]: <pre>tensor([[ 0.1172,  0.0160, -0.1425]], device='cuda:0')</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u4ecd\u7136\u200b\u662f\u200b logit \u200b\u5f62\u5f0f\u200b\uff08\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\u79f0\u4e3a\u200b logits\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u4ece\u200b logits \u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff0c\u200b\u518d\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</p> In\u00a0[69]: Copied! <pre># Print out prediction logits\nprint(f\"Prediction logits: {custom_image_pred}\")\n\n# Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\ncustom_image_pred_probs = torch.softmax(custom_image_pred, dim=1)\nprint(f\"Prediction probabilities: {custom_image_pred_probs}\")\n\n# Convert prediction probabilities -&gt; prediction labels\ncustom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1)\nprint(f\"Prediction label: {custom_image_pred_label}\")\n</pre> # Print out prediction logits print(f\"Prediction logits: {custom_image_pred}\")  # Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification) custom_image_pred_probs = torch.softmax(custom_image_pred, dim=1) print(f\"Prediction probabilities: {custom_image_pred_probs}\")  # Convert prediction probabilities -&gt; prediction labels custom_image_pred_label = torch.argmax(custom_image_pred_probs, dim=1) print(f\"Prediction label: {custom_image_pred_label}\") <pre>Prediction logits: tensor([[ 0.1172,  0.0160, -0.1425]], device='cuda:0')\nPrediction probabilities: tensor([[0.3738, 0.3378, 0.2883]], device='cuda:0')\nPrediction label: tensor([0], device='cuda:0')\n</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5f53\u7136\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u4ecd\u7136\u200b\u662f\u200b\u7d22\u5f15\u200b/\u200b\u5f20\u91cf\u200b\u5f62\u5f0f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5728\u200b <code>class_names</code> \u200b\u5217\u8868\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u7d22\u5f15\u200b\u6765\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5b57\u7b26\u4e32\u200b\u7c7b\u540d\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[70]: Copied! <pre># Find the predicted label\ncustom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error\ncustom_image_pred_class\n</pre> # Find the predicted label custom_image_pred_class = class_names[custom_image_pred_label.cpu()] # put pred label to CPU, otherwise will error custom_image_pred_class Out[70]: <pre>'pizza'</pre> <p>\u200b\u54c7\u200b\u3002</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6a21\u578b\u200b\u5728\u200b\u9884\u6d4b\u200b\u4e0a\u200b\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u7684\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u8868\u73b0\u200b\u4e00\u76f4\u200b\u4e0d\u4f73\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u76ee\u524d\u200b\u5f62\u5f0f\u200b\u7684\u200b\u6a21\u578b\u200b\u65e0\u8bba\u200b\u7ed9\u51fa\u200b\u4ec0\u4e48\u200b\u56fe\u50cf\u200b\uff0c\u200b\u90fd\u200b\u4f1a\u200b\u9884\u6d4b\u200b\u201c\u200b\u62ab\u8428\u200b\u201d\u3001\u201c\u200b\u725b\u6392\u200b\u201d\u200b\u6216\u200b\u201c\u200b\u5bff\u53f8\u200b\u201d\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u8ba9\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u76f8\u5e94\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u68c0\u67e5\u200b <code>custom_image_pred_probs</code>\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u6743\u91cd\u200b\u51e0\u4e4e\u200b\u76f8\u540c\u200b\uff08\u200b\u6570\u503c\u200b\u76f8\u4f3c\u200b\uff09\u3002</p> In\u00a0[71]: Copied! <pre># The values of the prediction probabilities are quite similar\ncustom_image_pred_probs\n</pre> # The values of the prediction probabilities are quite similar custom_image_pred_probs Out[71]: <pre>tensor([[0.3738, 0.3378, 0.2883]], device='cuda:0')</pre> <p>\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u5982\u6b64\u200b\u63a5\u8fd1\u200b\u53ef\u80fd\u200b\u610f\u5473\u7740\u200b\u4ee5\u4e0b\u51e0\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u6a21\u578b\u200b\u8bd5\u56fe\u200b\u540c\u65f6\u200b\u9884\u6d4b\u200b\u6240\u6709\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\uff08\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u4e00\u5f20\u200b\u540c\u65f6\u200b\u5305\u542b\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u7247\u200b\uff09\u3002</li> <li>\u200b\u6a21\u578b\u200b\u5e76\u200b\u4e0d\u200b\u6e05\u695a\u200b\u5b83\u200b\u60f3\u8981\u200b\u9884\u6d4b\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53ea\u662f\u200b\u7ed9\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u5206\u914d\u200b\u4e86\u200b\u76f8\u4f3c\u200b\u7684\u200b\u503c\u200b\u3002</li> </ol> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6848\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u60c5\u51b5\u200b\u662f\u200b\u7b2c\u200b2\u200b\u70b9\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u4e0d\u4f73\u200b\uff0c\u200b\u57fa\u672c\u4e0a\u200b\u662f\u200b\u5728\u200b\u731c\u6d4b\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</p> In\u00a0[72]: Copied! <pre>def pred_and_plot_image(model: torch.nn.Module, \n                        image_path: str, \n                        class_names: List[str] = None, \n                        transform=None,\n                        device: torch.device = device):\n    \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"\n    \n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n    \n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255. \n    \n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n    \n    # 4. Make sure the model is on the target device\n    model.to(device)\n    \n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n    \n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n        \n    # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n    \n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else: \n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False);\n</pre> def pred_and_plot_image(model: torch.nn.Module,                          image_path: str,                          class_names: List[str] = None,                          transform=None,                         device: torch.device = device):     \"\"\"Makes a prediction on a target image and plots the image with its prediction.\"\"\"          # 1. Load in image and convert the tensor values to float32     target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)          # 2. Divide the image pixel values by 255 to get them between [0, 1]     target_image = target_image / 255.           # 3. Transform if necessary     if transform:         target_image = transform(target_image)          # 4. Make sure the model is on the target device     model.to(device)          # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():         # Add an extra dimension to the image         target_image = target_image.unsqueeze(dim=0)              # Make a prediction on image with an extra dimension and send it to the target device         target_image_pred = model(target_image.to(device))              # 6. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 7. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)          # 8. Plot the image alongside the prediction and prediction probability     plt.imshow(target_image.squeeze().permute(1, 2, 0)) # make sure it's the right size for matplotlib     if class_names:         title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     else:          title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"     plt.title(title)     plt.axis(False); <p>\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[73]: Copied! <pre># Pred on our custom image\npred_and_plot_image(model=model_1,\n                    image_path=custom_image_path,\n                    class_names=class_names,\n                    transform=custom_image_transform,\n                    device=device)\n</pre> # Pred on our custom image pred_and_plot_image(model=model_1,                     image_path=custom_image_path,                     class_names=class_names,                     transform=custom_image_transform,                     device=device) <p>\u200b\u518d\u6b21\u200b\u53cc\u51fb\u200b666\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4ec5\u4ec5\u200b\u662f\u200b\u731c\u6d4b\u200b\u5c31\u200b\u731c\u200b\u5bf9\u200b\u4e86\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u5176\u4ed6\u200b\u56fe\u50cf\u200b\uff0c\u200b\u60c5\u51b5\u200b\u53ef\u80fd\u200b\u5c31\u200b\u4e0d\u200b\u603b\u662f\u200b\u8fd9\u6837\u200b\u4e86\u200b...</p> <p>\u200b\u8fd9\u5f20\u200b\u56fe\u7247\u200b\u4e5f\u200b\u6709\u4e9b\u200b\u50cf\u7d20\u200b\u5316\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>custom_image_transform</code> \u200b\u5c06\u200b\u5176\u200b\u8c03\u6574\u200b\u4e3a\u200b <code>[64, 64]</code>\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u5c1d\u8bd5\u200b\u7528\u200b\u4f60\u200b\u81ea\u5df1\u200b\u62cd\u6444\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#04-pytorch","title":"04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u4e0a\u200b\u4e00\u7bc7\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\uff0c\u200b\u7b14\u8bb0\u672c\u200b 03\uff0c\u200b\u6211\u4eec\u200b\u63a2\u8ba8\u200b\u4e86\u200b\u5982\u4f55\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u57fa\u4e8e\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\uff08FashionMNIST\uff09\u200b\u6784\u5efa\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6240\u200b\u91c7\u53d6\u200b\u7684\u200b\u6b65\u9aa4\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u95ee\u9898\u200b\u4e2d\u200b\u90fd\u200b\u662f\u200b\u76f8\u4f3c\u200b\u7684\u200b\u3002</p> <p>\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5c06\u200b\u6570\u636e\u200b\u96c6\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u6570\u503c\u200b\uff0c\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff08\u200b\u6216\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\uff09\u200b\u4ee5\u200b\u53d1\u73b0\u200b\u90a3\u4e9b\u200b\u53ef\u200b\u7528\u4e8e\u200b\u9884\u6d4b\u200b\u7684\u200b\u6570\u503c\u200b\u6a21\u5f0f\u200b\u3002</p> <p>PyTorch \u200b\u62e5\u6709\u200b\u8bb8\u591a\u200b\u7528\u4e8e\u200b\u5404\u79cd\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\uff1f\u00b6","text":"<p>\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u662f\u200b\u4e0e\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u7279\u5b9a\u200b\u95ee\u9898\u200b\u76f8\u5173\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u5408\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u53ef\u4ee5\u200b\u7531\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\u7ec4\u6210\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u50cf\u200bNutrify\u200b\u8fd9\u6837\u200b\u7684\u200b\u98df\u7269\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u5e94\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u53ef\u80fd\u200b\u662f\u200b\u98df\u7269\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8bd5\u56fe\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5206\u7c7b\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u7684\u200b\u6587\u672c\u200b\u8bc4\u8bba\u200b\u662f\u200b\u6b63\u9762\u200b\u8fd8\u662f\u200b\u8d1f\u9762\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u53ef\u80fd\u200b\u662f\u200b\u73b0\u6709\u200b\u7684\u200b\u5ba2\u6237\u200b\u8bc4\u8bba\u200b\u53ca\u5176\u200b\u8bc4\u5206\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8bd5\u56fe\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u58f0\u97f3\u200b\u5206\u7c7b\u200b\u5e94\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u53ef\u80fd\u200b\u662f\u200b\u5e26\u6709\u200b\u6837\u672c\u200b\u6807\u7b7e\u200b\u7684\u200b\u58f0\u97f3\u200b\u6837\u672c\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8bd5\u56fe\u200b\u4e3a\u200b\u5728\u200b\u6211\u4eec\u200b\u7f51\u7ad9\u200b\u4e0a\u200b\u8d2d\u7269\u200b\u7684\u200b\u5ba2\u6237\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u63a8\u8350\u200b\u7cfb\u7edf\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u53ef\u80fd\u200b\u662f\u200b\u5176\u4ed6\u4eba\u200b\u8d2d\u4e70\u200b\u8fc7\u200b\u7684\u200b\u4ea7\u54c1\u200b\u793a\u4f8b\u200b\u3002</p> <p>PyTorch\u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u73b0\u6709\u200b\u7684\u200b\u51fd\u6570\u200b\u6765\u200b\u52a0\u8f7d\u200b<code>TorchVision</code>\u3001<code>TorchText</code>\u3001<code>TorchAudio</code>\u200b\u548c\u200b<code>TorchRec</code>\u200b\u9886\u57df\u200b\u5e93\u4e2d\u200b\u7684\u200b\u5404\u79cd\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6709\u65f6\u200b\u8fd9\u4e9b\u200b\u73b0\u6709\u200b\u7684\u200b\u51fd\u6570\u200b\u53ef\u80fd\u200b\u4e0d\u591f\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u603b\u662f\u200b\u53ef\u4ee5\u200b\u5b50\u200b\u7c7b\u5316\u200b<code>torch.utils.data.Dataset</code>\u200b\u5e76\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u7684\u200b\u559c\u597d\u200b\u8fdb\u884c\u200b\u5b9a\u5236\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/","title":"\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5e94\u7528\u200b\u5728\u200bnotebook 01\u200b\u548c\u200bnotebook 02\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200bPyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u6765\u200b\u89e3\u51b3\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u4e0d\u4f1a\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5185\u7f6e\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u800c\u662f\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u52a0\u8f7d\u200b\u8fdb\u6765\u200b\uff0c\u200b\u7136\u540e\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u9884\u6d4b\u200b\u3002</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>torchvision.datasets</code>\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b<code>Dataset</code>\u200b\u7c7b\u6765\u200b\u52a0\u8f7d\u200b\u98df\u7269\u200b\u56fe\u50cf\u200b\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200bPyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e0c\u671b\u200b\u80fd\u591f\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. \u200b\u5bfc\u5165\u200bPyTorch\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u52a0\u8f7d\u200bPyTorch\uff0c\u200b\u7136\u540e\u200b\u9075\u5faa\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u4e0e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u3002 1. \u200b\u83b7\u53d6\u6570\u636e\u200b \u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5305\u542b\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u3002 2. \u200b\u4e0e\u200b\u6570\u636e\u200b\u878d\u4e3a\u4e00\u4f53\u200b\uff08\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\uff09 \u200b\u5728\u200b\u4efb\u4f55\u200b\u65b0\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\u5f00\u59cb\u200b\u65f6\u200b\uff0c\u200b\u4e86\u89e3\u200b\u4f60\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u81f3\u5173\u91cd\u8981\u200b\u3002\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u91c7\u53d6\u200b\u4e00\u4e9b\u200b\u6b65\u9aa4\u200b\u6765\u200b\u5f04\u6e05\u695a\u200b\u6211\u4eec\u200b\u62e5\u6709\u200b\u7684\u200b\u6570\u636e\u200b\u3002 3. \u200b\u8f6c\u6362\u200b\u6570\u636e\u200b \u200b\u901a\u5e38\u200b\uff0c\u200b\u4f60\u200b\u83b7\u5f97\u200b\u7684\u200b\u6570\u636e\u200b\u4e0d\u4f1a\u200b100%\u200b\u51c6\u5907\u200b\u597d\u200b\u7528\u4e8e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u5c06\u200b\u770b\u770b\u200b\u4e00\u4e9b\u200b\u53ef\u4ee5\u200b\u91c7\u53d6\u200b\u7684\u200b\u6b65\u9aa4\u200b\u6765\u200b\u8f6c\u6362\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u4f7f\u200b\u5b83\u4eec\u200b\u51c6\u5907\u200b\u597d\u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u3002 4. \u200b\u4f7f\u7528\u200b<code>ImageFolder</code>\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\uff08\u200b\u9009\u9879\u200b1\uff09 PyTorch\u200b\u6709\u200b\u8bb8\u591a\u200b\u5185\u7f6e\u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u529f\u80fd\u200b\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200b\u5e38\u89c1\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\u3002<code>ImageFolder</code>\u200b\u5728\u200b\u56fe\u50cf\u200b\u5904\u4e8e\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\u65f6\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002 5. \u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b<code>Dataset</code>\u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b \u200b\u5982\u679c\u200bPyTorch\u200b\u6ca1\u6709\u200b\u5185\u7f6e\u200b\u7684\u200b\u51fd\u6570\u200b\u6765\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u600e\u4e48\u529e\u200b\uff1f\u200b\u8fd9\u65f6\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u6784\u5efa\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b<code>torch.utils.data.Dataset</code>\u200b\u5b50\u7c7b\u200b\u3002 6. \u200b\u5176\u4ed6\u200b\u5f62\u5f0f\u200b\u7684\u200b\u8f6c\u6362\u200b\uff08\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff09 \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u662f\u200b\u6269\u5c55\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u591a\u6837\u6027\u200b\u7684\u200b\u5e38\u89c1\u200b\u6280\u672f\u200b\u3002\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u63a2\u7d22\u200b<code>torchvision</code>\u200b\u7684\u200b\u4e00\u4e9b\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u529f\u80fd\u200b\u3002 7. \u200b\u6a21\u578b\u200b0\uff1a\u200b\u4e0d\u5e26\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bTinyVGG \u200b\u5230\u200b\u8fd9\u200b\u4e00\u6b65\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u51c6\u5907\u200b\u597d\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u6570\u636e\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u51fd\u6570\u200b\u3002 8. \u200b\u63a2\u7d22\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b \u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u662f\u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b/\u200b\u6539\u8fdb\u200b\u968f\u200b\u65f6\u95f4\u200b\u53d8\u5316\u200b\u7684\u200b\u597d\u200b\u65b9\u6cd5\u200b\u3002\u200b\u5b83\u4eec\u200b\u4e5f\u200b\u662f\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u6b20\u200b\u62df\u5408\u200b\u6216\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u597d\u200b\u65b9\u6cd5\u200b\u3002 9. \u200b\u6a21\u578b\u200b1\uff1a\u200b\u5e26\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bTinyVGG \u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c1d\u8bd5\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u4e0d\u5e26\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4e00\u4e2a\u200b\u5e26\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u6a21\u578b\u200b\u3002 10. \u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u6bd4\u8f83\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u770b\u770b\u200b\u54ea\u4e2a\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\uff0c\u200b\u5e76\u200b\u8ba8\u8bba\u200b\u4e00\u4e9b\u200b\u63d0\u9ad8\u200b\u6027\u80fd\u200b\u7684\u200b\u9009\u9879\u200b\u3002 11. \u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u5728\u200b\u4e00\u4e2a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u3002\u200b\u5728\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ecb\u7ecd\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u73b0\u6709\u200b\u6570\u636e\u200b\u96c6\u200b\u4e4b\u5916\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002"},{"location":"04_pytorch_custom_datasets/","title":"\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u5b58\u653e\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub Discussions \u200b\u9875\u9762\u200b \u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u6587\u6863\u200b \u200b\u548c\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#0-pytorch","title":"0. \u200b\u5bfc\u5165\u200bPyTorch\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u00b6","text":""},{"location":"04_pytorch_custom_datasets/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u5c31\u200b\u50cf\u200b\u4efb\u4f55\u200b\u4f18\u79c0\u200b\u7684\u200b\u70f9\u996a\u200b\u8282\u76ee\u200b\u4e00\u6837\u200b\uff0c\u200b\u5df2\u7ecf\u200b\u4e3a\u200b\u6211\u4eec\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u5c0f\u200b\u89c4\u6a21\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u76ee\u524d\u200b\u4e0d\u200b\u6253\u7b97\u200b\u8bad\u7ec3\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u6216\u200b\u4f7f\u7528\u200b\u6700\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8fed\u4ee3\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u4ece\u5c0f\u200b\u89c4\u6a21\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5148\u200b\u8ba9\u200b\u67d0\u4e9b\u200b\u529f\u80fd\u200b\u8fd0\u884c\u200b\u8d77\u6765\u200b\uff0c\u200b\u5fc5\u8981\u200b\u65f6\u200b\u518d\u200b\u589e\u52a0\u200b\u89c4\u6a21\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u7684\u200b\u4e00\u4e2a\u200b\u5b50\u96c6\u200b\u3002</p> <p>Food101 \u200b\u662f\u200b\u4e00\u4e2a\u200b\u6d41\u884c\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u57fa\u51c6\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5305\u542b\u200b\u4e86\u200b 101 \u200b\u79cd\u200b\u4e0d\u540c\u200b\u98df\u7269\u200b\u7684\u200b 1000 \u200b\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u603b\u5171\u200b 101,000 \u200b\u5f20\u200b\u56fe\u7247\u200b\uff0875,750 \u200b\u5f20\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\uff0c25,250 \u200b\u5f20\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\uff09\u3002</p> <p>\u200b\u4f60\u200b\u80fd\u200b\u60f3\u5230\u200b 101 \u200b\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u98df\u7269\u200b\u5417\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u80fd\u200b\u60f3\u5230\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5206\u7c7b\u200b 101 \u200b\u79cd\u200b\u98df\u7269\u200b\u7684\u200b\u8ba1\u7b97\u673a\u7a0b\u5e8f\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u200b\u53ef\u4ee5\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b notebook 03 \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200b PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u4f1a\u200b\u4ece\u200b 101 \u200b\u79cd\u200b\u98df\u7269\u200b\u7c7b\u522b\u200b\u5f00\u59cb\u200b\uff0c\u200b\u800c\u662f\u200b\u4ece\u200b 3 \u200b\u79cd\u200b\u5f00\u59cb\u200b\uff1a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u4f1a\u200b\u4ece\u200b\u6bcf\u7c7b\u200b 1,000 \u200b\u5f20\u200b\u56fe\u7247\u200b\u5f00\u59cb\u200b\uff0c\u200b\u800c\u662f\u200b\u4ece\u200b\u968f\u673a\u200b 10% \u200b\u5f00\u59cb\u200b\uff08\u200b\u4ece\u5c0f\u200b\u89c4\u6a21\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5fc5\u8981\u200b\u65f6\u200b\u518d\u200b\u589e\u52a0\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u67e5\u770b\u200b\u6570\u636e\u200b\u7684\u200b\u6765\u6e90\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u539f\u59cb\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u8bba\u6587\u200b\u7f51\u7ad9\u200b\u3002</li> <li><code>torchvision.datasets.Food101</code> - \u200b\u6211\u200b\u4e3a\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u6570\u636e\u200b\u7248\u672c\u200b\u3002</li> <li><code>extras/04_custom_data_creation.ipynb</code> - \u200b\u6211\u200b\u7528\u4e8e\u200b\u683c\u5f0f\u5316\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u4ee5\u200b\u4f9b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4f7f\u7528\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u3002</li> <li><code>data/pizza_steak_sushi.zip</code> - \u200b\u4ece\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u63d0\u53d6\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b\u7684\u200b\u538b\u7f29\u5305\u200b\uff0c\u200b\u7531\u200b\u4e0a\u8ff0\u200b\u7b14\u8bb0\u672c\u200b\u521b\u5efa\u200b\u3002</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ece\u200b GitHub \u200b\u4e0b\u8f7d\u200b\u683c\u5f0f\u5316\u200b\u540e\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u5373\u5c06\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u5df2\u7ecf\u200b\u9884\u5148\u200b\u683c\u5f0f\u5316\u200b\u4e3a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200b\u5f62\u5f0f\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u4e3a\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u4efb\u4f55\u200b\u95ee\u9898\u200b\u683c\u5f0f\u5316\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u8fd9\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u7684\u200b\u5e38\u89c4\u200b\u505a\u6cd5\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#2","title":"2. \u200b\u6df1\u5165\u200b\u7406\u89e3\u200b\u6570\u636e\u200b\uff08\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\uff09\u00b6","text":"<p>\u200b\u6570\u636e\u200b\u96c6\u5df2\u200b\u4e0b\u8f7d\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u6df1\u5165\u200b\u7406\u89e3\u200b\u5b83\u200b\u7684\u200b\u65f6\u5019\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u6b63\u5982\u200b\u4e9a\u4f2f\u62c9\u7f55\u200b\u00b7\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u6240\u8bf4\u200b...</p> <p>\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u81f3\u5173\u91cd\u8981\u200b\u3002\u200b\u5728\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6df1\u5165\u200b\u7406\u89e3\u200b\u6570\u636e\u200b\u3002\u200b\u95ee\u200b\u81ea\u5df1\u200b\uff1a\u200b\u6211\u200b\u5728\u200b\u8fd9\u91cc\u200b\u8bd5\u56fe\u200b\u505a\u200b\u4ec0\u4e48\u200b\uff1f\u200b\u6765\u6e90\u200b\uff1a@mrdbourke Twitter\u3002</p> <p>\u200b\u4ec0\u4e48\u200b\u662f\u200b\u68c0\u67e5\u6570\u636e\u200b\u5e76\u200b\u6df1\u5165\u200b\u7406\u89e3\u200b\u5b83\u200b\uff1f</p> <p>\u200b\u5728\u200b\u5f00\u59cb\u200b\u4e00\u4e2a\u200b\u9879\u76ee\u200b\u6216\u200b\u6784\u5efa\u200b\u4efb\u4f55\u200b\u7c7b\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\uff0c\u200b\u4e86\u89e3\u200b\u4f60\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\u5305\u542b\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u5206\u522b\u200b\u5b58\u50a8\u200b\u5728\u200b\u4ee5\u200b\u7279\u5b9a\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u547d\u540d\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6240\u6709\u200b <code>pizza</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\u90fd\u200b\u5305\u542b\u200b\u5728\u200b <code>pizza/</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u8fd9\u79cd\u200b\u683c\u5f0f\u200b\u5728\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b\u4e2d\u200b\u90fd\u200b\u5f88\u200b\u6d41\u884c\u200b\uff0c\u200b\u5305\u62ec\u200b ImageNet\uff08\u200b\u6700\u200b\u6d41\u884c\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u57fa\u51c6\u200b\u6570\u636e\u200b\u96c6\u200b\u4e4b\u4e00\u200b\uff09\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e0b\u9762\u200b\u770b\u5230\u200b\u5b58\u50a8\u200b\u683c\u5f0f\u200b\u7684\u200b\u793a\u4f8b\u200b\uff0c\u200b\u56fe\u50cf\u200b\u7f16\u53f7\u200b\u662f\u200b\u4efb\u610f\u200b\u7684\u200b\u3002</p> <pre><code>pizza_steak_sushi/ &lt;- \u200b\u603b\u4f53\u200b\u6570\u636e\u200b\u96c6\u200b\u6587\u4ef6\u5939\u200b\n    train/ &lt;- \u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\n        pizza/ &lt;- \u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u4f5c\u4e3a\u200b\u6587\u4ef6\u5939\u200b\u540d\u79f0\u200b\n            image01.jpeg\n            image02.jpeg\n            ...\n        steak/\n            image24.jpeg\n            image25.jpeg\n            ...\n        sushi/\n            image37.jpeg\n            ...\n    test/ &lt;- \u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\n        pizza/\n            image101.jpeg\n            image102.jpeg\n            ...\n        steak/\n            image154.jpeg\n            image155.jpeg\n            ...\n        sushi/\n            image167.jpeg\n            ...\n</code></pre> <p>\u200b\u76ee\u6807\u200b\u662f\u200b\u5c06\u200b\u8fd9\u79cd\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b\u7ed3\u6784\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u53ef\u200b\u4e0e\u200b PyTorch \u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\u4f1a\u200b\u6839\u636e\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u800c\u200b\u6709\u6240\u4e0d\u540c\u200b\u3002\u200b\u4f46\u200b\u524d\u63d0\u200b\u4ecd\u7136\u200b\u662f\u200b\u4e00\u6837\u200b\u7684\u200b\uff1a\u200b\u6df1\u5165\u200b\u7406\u89e3\u200b\u6570\u636e\u200b\uff0c\u200b\u7136\u540e\u200b\u627e\u5230\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e0e\u200b PyTorch \u200b\u517c\u5bb9\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u7684\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u6765\u200b\u904d\u5386\u200b\u6bcf\u4e2a\u200b\u5b50\u76ee\u5f55\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u6587\u4ef6\u200b\u6570\u91cf\u200b\uff0c\u200b\u4ece\u800c\u200b\u68c0\u67e5\u6570\u636e\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Python \u200b\u5185\u7f6e\u200b\u7684\u200b <code>os.walk()</code>\u3002</p>"},{"location":"04_pytorch_custom_datasets/#21","title":"2.1 \u200b\u53ef\u89c6\u5316\u200b\u56fe\u50cf\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u76ee\u5f55\u200b\u7ed3\u6784\u200b\u7684\u200b\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u672c\u7740\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u7cbe\u795e\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u8fdb\u884c\u200b \u200b\u53ef\u89c6\u5316\u200b\u3001\u200b\u53ef\u89c6\u5316\u200b\u3001\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\u6765\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b <code>pathlib.Path.glob()</code> \u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u4ee5\u200b <code>.jpg</code> \u200b\u7ed3\u5c3e\u200b\u7684\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>random.choice()</code> \u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>pathlib.Path.parent.stem</code> \u200b\u83b7\u53d6\u200b\u56fe\u50cf\u200b\u7684\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u3002</li> <li>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>PIL.Image.open()</code>\uff08PIL \u200b\u4ee3\u8868\u200b Python \u200b\u56fe\u50cf\u200b\u5e93\u200b\uff09\u200b\u6253\u5f00\u200b\u968f\u673a\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u3002</li> <li>\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u663e\u793a\u200b\u56fe\u50cf\u200b\u5e76\u6253\u5370\u200b\u4e00\u4e9b\u200b\u5143\u200b\u6570\u636e\u200b\u3002</li> </ol>"},{"location":"04_pytorch_custom_datasets/#3","title":"3. \u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u200b\u5c06\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5230\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u5e94\u8be5\u200b\u600e\u4e48\u200b\u505a\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5728\u200b\u4f7f\u7528\u200b PyTorch \u200b\u5904\u7406\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\uff1a</p> <ol> <li>\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff08\u200b\u56fe\u50cf\u200b\u7684\u200b\u6570\u503c\u200b\u8868\u793a\u200b\uff09\u3002</li> <li>\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>torch.utils.data.Dataset</code>\uff0c\u200b\u8fdb\u800c\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>torch.utils.data.DataLoader</code>\uff0c\u200b\u6211\u4eec\u200b\u7b80\u79f0\u200b\u8fd9\u4e9b\u200b\u4e3a\u200b <code>Dataset</code> \u200b\u548c\u200b <code>DataLoader</code>\u3002</li> </ol> <p>\u200b\u6839\u636e\u200b\u6240\u200b\u5904\u7406\u200b\u7684\u200b\u95ee\u9898\u200b\u7c7b\u578b\u200b\uff0cPyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u79cd\u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002</p> \u200b\u95ee\u9898\u200b\u9886\u57df\u200b \u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u51fd\u6570\u200b \u200b\u89c6\u89c9\u200b <code>torchvision.datasets</code> \u200b\u97f3\u9891\u200b <code>torchaudio.datasets</code> \u200b\u6587\u672c\u200b <code>torchtext.datasets</code> \u200b\u63a8\u8350\u200b\u7cfb\u7edf\u200b <code>torchrec.datasets</code> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torchvision.datasets</code> \u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torchvision.transforms</code> \u200b\u6765\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bfc\u5165\u200b\u4e00\u4e9b\u200b\u57fa\u7840\u200b\u5e93\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#31-torchvisiontransforms","title":"3.1 \u200b\u4f7f\u7528\u200b <code>torchvision.transforms</code> \u200b\u8f6c\u6362\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u62e5\u6709\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u4f7f\u7528\u200b PyTorch \u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchvision.transforms</code> \u200b\u6a21\u5757\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p><code>torchvision.transforms</code> \u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u683c\u5f0f\u5316\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u751a\u81f3\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff08\u200b\u901a\u8fc7\u200b\u6539\u53d8\u200b\u6570\u636e\u200b\u4f7f\u200b\u6a21\u578b\u200b\u66f4\u200b\u96be\u4ee5\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u719f\u6089\u200b <code>torchvision.transforms</code>\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u7cfb\u5217\u200b\u8f6c\u6362\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b <code>transforms.Resize()</code> \u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\uff08\u200b\u4ece\u200b\u5927\u7ea6\u200b 512x512 \u200b\u8c03\u6574\u200b\u4e3a\u200b 64x64\uff0c\u200b\u4e0e\u200b CNN Explainer \u200b\u7f51\u7ad9\u200b \u200b\u4e0a\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>transforms.RandomHorizontalFlip()</code> \u200b\u968f\u673a\u200b\u6c34\u5e73\u200b\u7ffb\u8f6c\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\uff08\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u4e00\u79cd\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4f1a\u200b\u4eba\u4e3a\u200b\u5730\u200b\u6539\u53d8\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>transforms.ToTensor()</code> \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u4ece\u200b PIL \u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b PyTorch \u200b\u5f20\u91cf\u200b\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchvision.transforms.Compose()</code> \u200b\u5c06\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#4-1-imagefolder","title":"4. \u200b\u9009\u9879\u200b1\uff1a\u200b\u4f7f\u7528\u200b <code>ImageFolder</code> \u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u8f6c\u6362\u6210\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u4e0e\u200b PyTorch \u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b\u7684\u200b <code>Dataset</code> \u200b\u4e86\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u91c7\u7528\u200b\u6807\u51c6\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchvision.datasets.ImageFolder</code> \u200b\u7c7b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u76ee\u5f55\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u4ee5\u53ca\u200b\u4e00\u7cfb\u5217\u200b\u5e0c\u671b\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u6267\u884c\u200b\u7684\u200b\u53d8\u6362\u200b\u4f20\u9012\u200b\u7ed9\u200b\u5b83\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u6587\u4ef6\u5939\u200b <code>train_dir</code> \u200b\u548c\u200b <code>test_dir</code> \u200b\u4e0a\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u4f20\u9012\u200b <code>transform=data_transform</code> \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#41-dataloader","title":"4.1 \u200b\u5c06\u200b\u52a0\u8f7d\u200b\u7684\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u56fe\u50cf\u200b\u4f5c\u4e3a\u200b PyTorch \u200b\u7684\u200b <code>Dataset</code>\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b <code>Dataset</code> \u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code> \u200b\u4f7f\u200b\u5b83\u4eec\u200b\u53ef\u200b\u8fed\u4ee3\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u6837\u672c\u200b\u548c\u200b\u76ee\u6807\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5173\u7cfb\u200b\uff08\u200b\u7279\u5f81\u200b\u548c\u200b\u6807\u7b7e\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u7b80\u5355\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>batch_size=1</code> \u200b\u548c\u200b <code>num_workers=1</code>\u3002</p> <p>\u200b\u4ec0\u4e48\u200b\u662f\u200b <code>num_workers</code>\uff1f</p> <p>\u200b\u597d\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u5b83\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u5c06\u200b\u521b\u5efa\u200b\u591a\u5c11\u200b\u4e2a\u5b50\u200b\u8fdb\u7a0b\u200b\u6765\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u7406\u89e3\u200b\uff0c<code>num_workers</code> \u200b\u8bbe\u7f6e\u200b\u7684\u200b\u503c\u200b\u8d8a\u200b\u9ad8\u200b\uff0cPyTorch \u200b\u5c06\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u6765\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u200b\u4e2a\u4eba\u200b\u901a\u5e38\u200b\u5c06\u200b\u5176\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u6211\u200b\u673a\u5668\u200b\u4e0a\u200b\u7684\u200b CPU \u200b\u603b\u6570\u200b\uff0c\u200b\u901a\u8fc7\u200b Python \u200b\u7684\u200b <code>os.cpu_count()</code>\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u786e\u4fdd\u200b <code>DataLoader</code> \u200b\u5c3d\u53ef\u80fd\u200b\u591a\u5730\u200b\u62db\u52df\u200b\u6838\u5fc3\u200b\u6765\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b PyTorch \u200b\u6587\u6863\u200b \u200b\u4e2d\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b <code>torch.utils.data.DataLoader</code> \u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#5-2-dataset","title":"5. \u200b\u9009\u9879\u200b2\uff1a\u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b <code>Dataset</code> \u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u50cf\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u8fd9\u6837\u200b\u7684\u200b\u9884\u200b\u6784\u5efa\u200b <code>Dataset</code> \u200b\u521b\u5efa\u200b\u5668\u200b\u4e0d\u200b\u5b58\u5728\u200b\u600e\u4e48\u529e\u200b\uff1f</p> <p>\u200b\u6216\u8005\u200b\u9488\u5bf9\u200b\u4f60\u200b\u7684\u200b\u7279\u5b9a\u200b\u95ee\u9898\u200b\u6ca1\u6709\u200b\u8fd9\u6837\u200b\u7684\u200b\u521b\u5efa\u200b\u5668\u200b\u600e\u4e48\u529e\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u81ea\u5df1\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u3002</p> <p>\u200b\u4f46\u662f\u200b\u7b49\u7b49\u200b\uff0c\u200b\u521b\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u6709\u200b\u54ea\u4e9b\u200b\u5229\u5f0a\u200b\u5462\u200b\uff1f</p> \u200b\u521b\u5efa\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u7684\u200b\u4f18\u70b9\u200b \u200b\u521b\u5efa\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u7684\u200b\u7f3a\u70b9\u200b \u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u4efb\u4f55\u200b\u6570\u636e\u6e90\u200b\u521b\u5efa\u200b <code>Dataset</code>\u3002 \u200b\u867d\u7136\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u6570\u636e\u6e90\u200b\u521b\u5efa\u200b <code>Dataset</code>\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u200b\u610f\u5473\u7740\u200b\u5b83\u4f1a\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\u3002 \u200b\u4e0d\u200b\u53d7\u9650\u4e8e\u200b PyTorch \u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b <code>Dataset</code> \u200b\u51fd\u6570\u200b\u3002 \u200b\u4f7f\u7528\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u901a\u5e38\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u7f16\u5199\u200b\u66f4\u200b\u591a\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bb9\u6613\u200b\u51fa\u9519\u200b\u6216\u200b\u6027\u80fd\u200b\u95ee\u9898\u200b\u3002 <p>\u200b\u4e3a\u4e86\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u7ee7\u627f\u200b <code>torch.utils.data.Dataset</code>\uff08PyTorch \u200b\u4e2d\u200b\u6240\u6709\u200b <code>Dataset</code> \u200b\u7684\u200b\u57fa\u7c7b\u200b\uff09\u200b\u6765\u200b\u590d\u5236\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5bfc\u5165\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6a21\u5757\u200b\uff1a</p> <ul> <li>Python \u200b\u7684\u200b <code>os</code> \u200b\u6a21\u5757\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u76ee\u5f55\u200b\uff08\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u5b58\u50a8\u200b\u5728\u200b\u76ee\u5f55\u200b\u4e2d\u200b\uff09\u3002</li> <li>Python \u200b\u7684\u200b <code>pathlib</code> \u200b\u6a21\u5757\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u90fd\u200b\u6709\u200b\u552f\u4e00\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\uff09\u3002</li> <li><code>torch</code> \u200b\u7528\u4e8e\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u64cd\u4f5c\u200b\u3002</li> <li>PIL \u200b\u7684\u200b <code>Image</code> \u200b\u7c7b\u200b\u7528\u4e8e\u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\u3002</li> <li><code>torch.utils.data.Dataset</code> \u200b\u7528\u4e8e\u200b\u7ee7\u627f\u200b\u5e76\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code>\u3002</li> <li><code>torchvision.transforms</code> \u200b\u7528\u4e8e\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</li> <li>Python \u200b\u7684\u200b <code>typing</code> \u200b\u6a21\u5757\u200b\u4e2d\u200b\u7684\u200b\u5404\u79cd\u7c7b\u578b\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4e3a\u200b\u4ee3\u7801\u200b\u6dfb\u52a0\u200b\u7c7b\u578b\u200b\u63d0\u793a\u200b\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u81ea\u5b9a\u4e49\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u3002\u200b\u524d\u63d0\u200b\u662f\u200b\uff1a\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u4ee5\u200b\u4f60\u200b\u5e0c\u671b\u200b\u7684\u200b\u683c\u5f0f\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#51","title":"5.1 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u7c7b\u540d\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u6765\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u80fd\u591f\u200b\u5728\u200b\u7ed9\u5b9a\u200b\u76ee\u5f55\u200b\u8def\u5f84\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7c7b\u540d\u200b\u5217\u8868\u200b\u548c\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u7c7b\u540d\u200b\u53ca\u5176\u200b\u7d22\u5f15\u200b\u7684\u200b\u5b57\u5178\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b <code>os.scandir()</code> \u200b\u904d\u5386\u200b\u76ee\u6807\u76ee\u5f55\u200b\u4ee5\u200b\u83b7\u53d6\u200b\u7c7b\u540d\u200b\uff08\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u76ee\u5f55\u200b\u5e94\u200b\u91c7\u7528\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\uff09\u3002</li> <li>\u200b\u5982\u679c\u200b\u627e\u200b\u4e0d\u5230\u200b\u7c7b\u540d\u200b\uff0c\u200b\u5219\u200b\u629b\u51fa\u200b\u4e00\u4e2a\u200b\u9519\u8bef\u200b\uff08\u200b\u5982\u679c\u200b\u53d1\u751f\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff0c\u200b\u53ef\u80fd\u200b\u662f\u200b\u76ee\u5f55\u200b\u7ed3\u6784\u200b\u6709\u200b\u95ee\u9898\u200b\uff09\u3002</li> <li>\u200b\u5c06\u200b\u7c7b\u540d\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6570\u503c\u200b\u6807\u7b7e\u200b\u7684\u200b\u5b57\u5178\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7c7b\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u6807\u7b7e\u200b\u3002</li> </ol> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u5b8c\u6574\u200b\u51fd\u6570\u200b\u4e4b\u524d\u200b\uff0c\u200b\u5148\u200b\u6765\u770b\u200b\u4e00\u4e2a\u200b\u5c0f\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u4e86\u89e3\u200b\u6b65\u9aa4\u200b1\u200b\u7684\u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#52-dataset-imagefolder","title":"5.2 \u200b\u521b\u5efa\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u4ee5\u200b\u590d\u5236\u200b <code>ImageFolder</code>\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u51c6\u5907\u200b\u597d\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u4ee5\u200b\u590d\u5236\u200b <code>torchvision.datasets.ImageFolder()</code> \u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u6b64\u5916\u200b\uff0c\u200b\u5b83\u200b\u8fd8\u200b\u4f1a\u200b\u63ed\u793a\u200b\u4e00\u4e9b\u200b\u521b\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u8fd9\u4f1a\u200b\u662f\u200b\u4e00\u6bb5\u200b\u76f8\u5f53\u200b\u591a\u200b\u7684\u200b\u4ee3\u7801\u200b...\u200b\u4f46\u200b\u6211\u4eec\u200b\u5b8c\u5168\u200b\u53ef\u4ee5\u200b\u5e94\u5bf9\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9010\u6b65\u200b\u5206\u89e3\u200b\uff1a</p> <ol> <li>\u200b\u7ee7\u627f\u200b <code>torch.utils.data.Dataset</code> \u200b\u7c7b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>targ_dir</code> \u200b\u53c2\u6570\u200b\uff08\u200b\u76ee\u6807\u200b\u6570\u636e\u200b\u76ee\u5f55\u200b\uff09\u200b\u548c\u200b <code>transform</code> \u200b\u53c2\u6570\u200b\uff08\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u9700\u8981\u200b\u65f6\u200b\u5bf9\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\uff09\u200b\u521d\u59cb\u5316\u200b\u6211\u4eec\u200b\u7684\u200b\u5b50\u7c7b\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u51e0\u4e2a\u200b\u5c5e\u6027\u200b\uff0c\u200b\u5305\u62ec\u200b <code>paths</code>\uff08\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u7684\u200b\u8def\u5f84\u200b\uff09\u3001<code>transform</code>\uff08\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u4f7f\u7528\u200b\u7684\u200b\u8f6c\u6362\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u662f\u200b <code>None</code>\uff09\u3001<code>classes</code> \u200b\u548c\u200b <code>class_to_idx</code>\uff08\u200b\u4ece\u200b\u6211\u4eec\u200b\u7684\u200b <code>find_classes()</code> \u200b\u51fd\u6570\u200b\u4e2d\u200b\u83b7\u53d6\u200b\uff09\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ece\u6587\u4ef6\u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\u5e76\u200b\u8fd4\u56de\u200b\u5b83\u4eec\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>PIL</code> \u200b\u6216\u200b <code>torchvision.io</code>\uff08\u200b\u7528\u4e8e\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u7684\u200b\u8f93\u5165\u200b/\u200b\u8f93\u51fa\u200b\uff09\u3002</li> <li>\u200b\u91cd\u5199\u200b <code>torch.utils.data.Dataset</code> \u200b\u7684\u200b <code>__len__</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4ee5\u200b\u8fd4\u56de\u200b <code>Dataset</code> \u200b\u4e2d\u200b\u7684\u200b\u6837\u672c\u200b\u6570\u91cf\u200b\uff0c\u200b\u8fd9\u662f\u200b\u63a8\u8350\u200b\u7684\u200b\u4f46\u200b\u4e0d\u662f\u200b\u5fc5\u9700\u200b\u7684\u200b\u3002\u200b\u8fd9\u6837\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b <code>len(Dataset)</code>\u3002</li> <li>\u200b\u91cd\u5199\u200b <code>torch.utils.data.Dataset</code> \u200b\u7684\u200b <code>__getitem__</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4ee5\u200b\u4ece\u200b <code>Dataset</code> \u200b\u4e2d\u200b\u8fd4\u56de\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\uff0c\u200b\u8fd9\u662f\u200b\u5fc5\u9700\u200b\u7684\u200b\u3002</li> </ol> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u5427\u200b\uff01</p>"},{"location":"04_pytorch_custom_datasets/#53","title":"5.3 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u663e\u793a\u200b\u968f\u673a\u200b\u56fe\u50cf\u200b\u7684\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u77e5\u9053\u200b\u73b0\u5728\u200b\u662f\u200b\u4ec0\u4e48\u200b\u65f6\u5019\u200b\u4e86\u200b\uff01</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u6234\u4e0a\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u5e3d\u5b50\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>display_random_images()</code> \u200b\u7684\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u5728\u200b <code>Dataset</code> \u200b\u4e2d\u200b\u53ef\u89c6\u5316\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b <code>Dataset</code> \u200b\u548c\u200b\u4e00\u4e9b\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200b <code>classes</code>\uff08\u200b\u76ee\u6807\u200b\u7c7b\u522b\u200b\u7684\u200b\u540d\u79f0\u200b\uff09\u3001\u200b\u8981\u200b\u663e\u793a\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b <code>n</code> \u200b\u548c\u200b\u4e00\u4e2a\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u3002</li> <li>\u200b\u4e3a\u4e86\u200b\u9632\u6b62\u200b\u663e\u793a\u200b\u5931\u63a7\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b <code>n</code> \u200b\u9650\u5236\u200b\u5728\u200b 10 \u200b\u5f20\u200b\u56fe\u50cf\u200b\u4ee5\u5185\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u7ed8\u56fe\u200b\u7684\u200b\u53ef\u91cd\u590d\u6027\u200b\uff08\u200b\u5982\u679c\u200b\u8bbe\u7f6e\u200b\u4e86\u200b <code>seed</code>\uff09\u3002</li> <li>\u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u968f\u673a\u6837\u672c\u200b\u7d22\u5f15\u200b\u5217\u8868\u200b\uff08\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>random.sample()</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u7ed8\u56fe\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b <code>matplotlib</code> \u200b\u7ed8\u56fe\u200b\u73af\u5883\u200b\u3002</li> <li>\u200b\u904d\u5386\u200b\u5728\u200b\u6b65\u9aa4\u200b 4 \u200b\u4e2d\u200b\u627e\u5230\u200b\u7684\u200b\u968f\u673a\u6837\u672c\u200b\u7d22\u5f15\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>matplotlib</code> \u200b\u7ed8\u5236\u200b\u5b83\u4eec\u200b\u3002</li> <li>\u200b\u786e\u4fdd\u200b\u6837\u672c\u200b\u56fe\u50cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>HWC</code>\uff08\u200b\u9ad8\u5ea6\u200b\uff0c\u200b\u5bbd\u5ea6\u200b\uff0c\u200b\u989c\u8272\u200b\u901a\u9053\u200b\uff09\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7ed8\u5236\u200b\u5b83\u4eec\u200b\u3002</li> </ol>"},{"location":"04_pytorch_custom_datasets/#54-dataloader","title":"5.4 \u200b\u5c06\u200b\u81ea\u5b9a\u4e49\u200b\u52a0\u8f7d\u200b\u7684\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u901a\u8fc7\u200b <code>ImageFolderCustom</code> \u200b\u7c7b\u200b\u5c06\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>Dataset</code> \u200b\u7684\u200b\u65b9\u6cd5\u200b\uff08\u200b\u5373\u5c06\u200b\u7279\u5f81\u200b\u6620\u5c04\u200b\u5230\u200b\u6807\u7b7e\u200b\u6216\u200b <code>X</code> \u200b\u6620\u5c04\u200b\u5230\u200b <code>y</code>\uff09\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5c06\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b <code>Dataset</code> \u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code> \u200b\u5462\u200b\uff1f</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u731c\u200b\u662f\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader()</code>\uff0c\u200b\u90a3\u200b\u5c31\u200b\u5bf9\u200b\u4e86\u200b\uff01</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b <code>Dataset</code> \u200b\u7ee7\u627f\u200b\u81ea\u200b <code>torch.utils.data.Dataset</code>\uff0c\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5c06\u200b\u5b83\u4eec\u200b\u4e0e\u200b <code>torch.utils.data.DataLoader()</code> \u200b\u4e00\u8d77\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u521b\u5efa\u200b\u7684\u200b <code>Dataset</code>\u3002</p>"},{"location":"04_pytorch_custom_datasets/#6","title":"6. \u200b\u5176\u4ed6\u200b\u5f62\u5f0f\u200b\u7684\u200b\u53d8\u6362\u200b\uff08\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff09\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u5b9e\u9645\u4e0a\u200b\u8fd8\u6709\u200b\u5f88\u591a\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>torchvision.transforms</code> \u200b\u6587\u6863\u200b\u4e2d\u200b\u67e5\u770b\u200b\u6240\u6709\u200b\u53d8\u6362\u200b\u3002</p> <p>\u200b\u53d8\u6362\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u4ee5\u200b\u67d0\u79cd\u200b\u65b9\u5f0f\u200b\u6539\u53d8\u200b\u4f60\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff08\u200b\u5982\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u6240\u200b\u89c1\u200b\uff09\u3002</p> <p>\u200b\u6216\u8005\u200b\u662f\u200b\u88c1\u526a\u200b\u56fe\u50cf\u200b\u3001\u200b\u968f\u673a\u200b\u64e6\u9664\u200b\u90e8\u5206\u200b\u533a\u57df\u200b\u6216\u200b\u968f\u673a\u200b\u65cb\u8f6c\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u7c7b\u200b\u53d8\u6362\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u662f\u200b\u901a\u8fc7\u200b\u6539\u53d8\u200b\u6570\u636e\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6765\u200b\u4eba\u5de5\u200b\u589e\u52a0\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u7684\u200b\u591a\u6837\u6027\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u79cd\u200b\u4eba\u5de5\u200b\u6539\u53d8\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u6709\u671b\u200b\u4f7f\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u66f4\u597d\u200b\u7684\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\uff08\u200b\u5b83\u200b\u5b66\u5230\u200b\u7684\u200b\u6a21\u5f0f\u200b\u5bf9\u200b\u672a\u6765\u200b\u7684\u200b\u672a\u200b\u89c1\u200b\u793a\u4f8b\u200b\u66f4\u52a0\u200b\u9c81\u68d2\u200b\uff09\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b PyTorch \u200b\u7684\u200b \u200b\u53d8\u6362\u200b\u793a\u4f8b\u200b\u56fe\u89e3\u200b\u4e2d\u200b\u770b\u5230\u200b\u8bb8\u591a\u200b\u4f7f\u7528\u200b <code>torchvision.transforms</code> \u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u4e0d\u540c\u200b\u793a\u4f8b\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8ba9\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u90fd\u200b\u662f\u200b\u5173\u4e8e\u200b\u5229\u7528\u200b\u968f\u673a\u6027\u200b\u7684\u200b\u529b\u91cf\u200b\uff0c\u200b\u7814\u7a76\u200b\u8868\u660e\u200b\u968f\u673a\u200b\u53d8\u6362\u200b\uff08\u200b\u5982\u200b <code>transforms.RandAugment()</code> \u200b\u548c\u200b <code>transforms.TrivialAugmentWide()</code>\uff09\u200b\u901a\u5e38\u200b\u6bd4\u200b\u624b\u5de5\u200b\u6311\u9009\u200b\u7684\u200b\u53d8\u6362\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\u3002</p> <p>TrivialAugment \u200b\u80cc\u540e\u200b\u7684\u200b\u60f3\u6cd5\u200b...\u200b\u55ef\u200b\uff0c\u200b\u5f88\u200b\u7b80\u5355\u200b\u3002</p> <p>\u200b\u4f60\u200b\u6709\u200b\u4e00\u7ec4\u200b\u53d8\u6362\u200b\uff0c\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u5176\u4e2d\u200b\u4e00\u4e9b\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u7ed9\u5b9a\u200b\u8303\u56f4\u200b\u5185\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u53d8\u6362\u200b\u7684\u200b\u5f3a\u5ea6\u200b\uff08\u200b\u5f3a\u5ea6\u200b\u8d8a\u9ad8\u200b\u610f\u5473\u7740\u200b\u8d8a\u200b\u5f3a\u70c8\u200b\uff09\u3002</p> <p>PyTorch \u200b\u56e2\u961f\u200b\u751a\u81f3\u200b\u4f7f\u7528\u200b TrivialAugment \u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4ed6\u4eec\u200b\u6700\u65b0\u200b\u7684\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</p> <p></p> <p>TrivialAugment \u200b\u662f\u200b\u6700\u8fd1\u200b\u5bf9\u200b\u5404\u79cd\u200b PyTorch \u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6700\u200b\u5148\u8fdb\u200b\u8bad\u7ec3\u200b\u5347\u7ea7\u200b\u7684\u200b\u6210\u5206\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u81ea\u5df1\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5728\u200b <code>transforms.TrivialAugmentWide()</code> \u200b\u4e2d\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u4e3b\u8981\u53c2\u6570\u200b\u662f\u200b <code>num_magnitude_bins=31</code>\u3002</p> <p>\u200b\u5b83\u200b\u5b9a\u4e49\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5f3a\u5ea6\u200b\u503c\u200b\u5c06\u200b\u9009\u62e9\u200b\u591a\u5927\u200b\u7684\u200b\u8303\u56f4\u200b\u6765\u200b\u5e94\u7528\u200b\u67d0\u4e2a\u200b\u53d8\u6362\u200b\uff0c<code>0</code> \u200b\u8868\u793a\u200b\u6ca1\u6709\u200b\u8303\u56f4\u200b\uff0c<code>31</code> \u200b\u8868\u793a\u200b\u6700\u5927\u200b\u8303\u56f4\u200b\uff08\u200b\u6700\u9ad8\u200b\u5f3a\u5ea6\u200b\u7684\u200b\u9ad8\u200b\u6982\u7387\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b <code>transforms.TrivialAugmentWide()</code> \u200b\u5408\u5e76\u200b\u5230\u200b <code>transforms.Compose()</code> \u200b\u4e2d\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#7-0tinyvgg","title":"7. \u200b\u6a21\u578b\u200b0\uff1a\u200b\u65e0\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bTinyVGG\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u5982\u4f55\u200b\u5c06\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7ecf\u8fc7\u200b\u53d8\u6362\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u80fd\u591f\u200b\u5206\u7c7b\u200b\u51fa\u200b\u56fe\u50cf\u200b\u662f\u5426\u200b\u4e3a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u53d8\u6362\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4ec5\u200b\u5c06\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u8c03\u6574\u200b\u4e3a\u200b <code>(64, 64)</code> \u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#71-0","title":"7.1 \u200b\u4e3a\u200b\u6a21\u578b\u200b0\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u00b6","text":""},{"location":"04_pytorch_custom_datasets/#72-tinyvgg","title":"7.2 \u200b\u521b\u5efa\u200b TinyVGG \u200b\u6a21\u578b\u200b\u7c7b\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b 03\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u4e86\u200b\u6765\u81ea\u200bCNN Explainer \u200b\u7f51\u7ad9\u200b\u7684\u200b TinyVGG \u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5f69\u8272\u56fe\u50cf\u200b\u800c\u200b\u4e0d\u662f\u200b\u7070\u5ea6\u200b\u56fe\u50cf\u200b\uff08\u200b\u5bf9\u4e8e\u200b RGB \u200b\u50cf\u7d20\u200b\uff0c<code>in_channels=3</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>in_channels=1</code>\uff09\u3002</p>"},{"location":"04_pytorch_custom_datasets/#73","title":"7.3 \u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\uff08\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\uff09\u00b6","text":"<p>\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\u7684\u200b\u4e00\u4e2a\u200b\u597d\u200b\u65b9\u6cd5\u200b\u662f\u200b\u9488\u5bf9\u200b\u5355\u4e2a\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u6d4b\u8bd5\u200b\u6211\u4eec\u200b\u4e0d\u540c\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7684\u200b\u4fbf\u6377\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u8981\u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\uff1a</p> <ol> <li>\u200b\u4ece\u200b <code>DataLoader</code> \u200b\u4e2d\u200b\u83b7\u53d6\u200b\u4e00\u6279\u200b\u56fe\u50cf\u200b\u548c\u200b\u6807\u7b7e\u200b\u3002</li> <li>\u200b\u4ece\u200b\u6279\u6b21\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>unsqueeze()</code> \u200b\u65b9\u6cd5\u200b\u5c06\u200b\u5176\u200b\u6269\u5c55\u200b\u4e3a\u200b\u5177\u6709\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b <code>1</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\uff08\u200b\u4ee5\u4fbf\u200b\u5176\u200b\u5f62\u72b6\u200b\u9002\u5408\u200b\u6a21\u578b\u200b\uff09\u3002</li> <li>\u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\uff08\u200b\u786e\u4fdd\u200b\u5c06\u200b\u56fe\u50cf\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b <code>device</code>\uff09\u3002</li> <li>\u200b\u6253\u5370\u200b\u51fa\u200b\u53d1\u751f\u200b\u7684\u200b\u60c5\u51b5\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torch.softmax()</code> \u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b logits \u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u591a\u7c7b\u200b\u6570\u636e\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>torch.argmax()</code> \u200b\u5c06\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</li> </ol>"},{"location":"04_pytorch_custom_datasets/#74-torchinfo","title":"7.4 \u200b\u4f7f\u7528\u200b <code>torchinfo</code> \u200b\u4e86\u89e3\u200b\u6a21\u578b\u200b\u4e2d\u200b\u6570\u636e\u200b\u7684\u200b\u5f62\u72b6\u200b\u00b6","text":"<p>\u200b\u901a\u8fc7\u200b <code>print(model)</code> \u200b\u6253\u5370\u200b\u51fa\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5927\u81f4\u200b\u4e86\u89e3\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u6253\u5370\u200b\u51fa\u200b\u6570\u636e\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6709\u7528\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4f7f\u7528\u200b <code>torchinfo</code> \u200b\u6765\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p><code>torchinfo</code> \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b <code>summary()</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8be5\u200b\u65b9\u6cd5\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\u548c\u200b\u4e00\u4e2a\u200b <code>input_shape</code>\uff0c\u200b\u5e76\u200b\u8fd4\u56de\u200b\u5f20\u91cf\u200b\u5728\u200b\u6a21\u578b\u200b\u4e2d\u200b\u6d41\u52a8\u200b\u65f6\u200b\u7684\u200b\u53d8\u5316\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b <code>torchinfo</code>\u3002</p>"},{"location":"04_pytorch_custom_datasets/#75","title":"7.5 \u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u6570\u636e\u200b\uff0c\u200b\u4e5f\u200b\u6784\u5efa\u200b\u4e86\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u91cd\u590d\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u51fd\u6570\u200b\u5316\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e09\u4e2a\u200b\u51fd\u6570\u200b\uff1a</p> <ol> <li><code>train_step()</code> - \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b <code>DataLoader</code>\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b <code>DataLoader</code> \u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</li> <li><code>test_step()</code> - \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b <code>DataLoader</code> \u200b\u548c\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b <code>DataLoader</code> \u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u3002</li> <li><code>train()</code> - \u200b\u6267\u884c\u200b 1 \u200b\u548c\u200b 2\uff0c\u200b\u4e00\u8d77\u200b\u8fdb\u884c\u200b\u7ed9\u5b9a\u200b\u6570\u91cf\u200b\u7684\u200b epoch\uff0c\u200b\u5e76\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u7ed3\u679c\u200b\u5b57\u5178\u200b\u3002</li> </ol> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u5728\u200b notebook 01 \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200b PyTorch \u200b\u4f18\u5316\u200b\u5faa\u73af\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4ee5\u53ca\u200b \u200b\u975e\u5b98\u65b9\u200b\u7684\u200b PyTorch \u200b\u4f18\u5316\u200b\u5faa\u73af\u200b\u6b4c\u66f2\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b notebook 03 \u200b\u4e2d\u200b\u6784\u5efa\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u6784\u5efa\u200b <code>train_step()</code> \u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200b <code>DataLoader</code> \u200b\u4e2d\u200b\u5904\u7406\u200b\u6279\u6b21\u200b\u6570\u636e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u7d2f\u79ef\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u548c\u200b\u51c6\u786e\u5ea6\u200b\u503c\u200b\uff08\u200b\u901a\u8fc7\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u6279\u6b21\u200b\u7d2f\u52a0\u200b\u5b83\u4eec\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6700\u540e\u200b\u8c03\u6574\u200b\u5b83\u4eec\u200b\u540e\u200b\u518d\u200b\u8fd4\u56de\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#76-train-train_step-test_step","title":"7.6 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u7ec4\u5408\u200b <code>train_step()</code> \u200b\u548c\u200b <code>test_step()</code>\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u5c06\u200b <code>train_step()</code> \u200b\u548c\u200b <code>test_step()</code> \u200b\u51fd\u6570\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u6253\u5305\u200b\u5728\u200b\u4e00\u4e2a\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u8bc4\u4f30\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b <code>DataLoader</code>\u3001\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4ee5\u53ca\u200b\u6bcf\u4e2a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6b65\u9aa4\u200b\u8981\u200b\u6267\u884c\u200b\u7684\u200b\u5468\u671f\u200b\u6570\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7a7a\u200b\u7684\u200b results \u200b\u5b57\u5178\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b <code>train_loss</code>\u3001<code>train_acc</code>\u3001<code>test_loss</code> \u200b\u548c\u200b <code>test_acc</code> \u200b\u503c\u200b\uff08\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u586b\u5145\u200b\u8fd9\u4e2a\u200b\u5b57\u5178\u200b\uff09\u3002</li> <li>\u200b\u5bf9\u200b\u4e00\u5b9a\u200b\u6570\u91cf\u200b\u7684\u200b\u5468\u671f\u200b\u5faa\u73af\u200b\u6267\u884c\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6b65\u9aa4\u200b\u51fd\u6570\u200b\u3002</li> <li>\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u5468\u671f\u200b\u7ed3\u675f\u200b\u65f6\u200b\u6253\u5370\u200b\u51fa\u200b\u5f53\u524d\u200b\u7684\u200b\u8fdb\u5c55\u200b\u60c5\u51b5\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6bcf\u4e2a\u200b\u5468\u671f\u200b\u7684\u200b\u66f4\u65b0\u200b\u6307\u6807\u200b\u66f4\u65b0\u200b\u7a7a\u200b\u7684\u200b results \u200b\u5b57\u5178\u200b\u3002</li> <li>\u200b\u8fd4\u56de\u200b\u586b\u5145\u200b\u597d\u200b\u7684\u200b results \u200b\u5b57\u5178\u200b\u3002</li> </ol> <p>\u200b\u4e3a\u4e86\u200b\u8ddf\u8e2a\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u7ecf\u5386\u200b\u4e86\u200b\u591a\u5c11\u200b\u4e2a\u200b\u5468\u671f\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b <code>tqdm.auto</code> \u200b\u5bfc\u5165\u200b <code>tqdm</code>\uff08<code>tqdm</code> \u200b\u662f\u200b Python \u200b\u4e2d\u200b\u6700\u200b\u6d41\u884c\u200b\u7684\u200b\u8fdb\u5ea6\u6761\u200b\u5e93\u200b\u4e4b\u4e00\u200b\uff0c<code>tqdm.auto</code> \u200b\u4f1a\u200b\u81ea\u52a8\u200b\u51b3\u5b9a\u200b\u6700\u200b\u9002\u5408\u200b\u4f60\u200b\u7684\u200b\u8ba1\u7b97\u73af\u5883\u200b\u7684\u200b\u8fdb\u5ea6\u6761\u200b\u7c7b\u578b\u200b\uff0c\u200b\u4f8b\u5982\u200b Jupyter Notebook \u200b\u4e0e\u200b Python \u200b\u811a\u672c\u200b\uff09\u3002</p>"},{"location":"04_pytorch_custom_datasets/#77-0","title":"7.7 \u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b0\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u6240\u6709\u200b\u9700\u8981\u200b\u7684\u200b\u6210\u5206\u200b\u6765\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b <code>TinyVGG</code> \u200b\u6a21\u578b\u200b\u3001<code>DataLoader</code> \u200b\u548c\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u80fd\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u533a\u5206\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b <code>model_0</code>\uff08\u200b\u6211\u4eec\u200b\u4e0d\u200b\u9700\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u5b8c\u6574\u6027\u200b\u6211\u4eec\u200b\u4f1a\u200b\u8fd9\u6837\u200b\u505a\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u8c03\u7528\u200b\u6211\u4eec\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u5e76\u200b\u4f20\u5165\u200b\u5fc5\u8981\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u5feb\u901f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b 5 \u200b\u4e2a\u200b\u5468\u671f\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u589e\u52a0\u200b\u8fd9\u4e2a\u200b\u6570\u5b57\u200b\uff09\u3002</p> <p>\u200b\u81f3\u4e8e\u200b \u200b\u4f18\u5316\u200b\u5668\u200b \u200b\u548c\u200b \u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.nn.CrossEntropyLoss()</code>\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\uff09\u200b\u548c\u200b <code>torch.optim.Adam()</code>\uff0c\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b <code>1e-3</code>\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u67e5\u770b\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bfc\u5165\u200b Python \u200b\u7684\u200b <code>timeit.default_timer()</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u8ba1\u7b97\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#78-0","title":"7.8 \u200b\u7ed8\u5236\u200b\u6a21\u578b\u200b0\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u4ece\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b<code>model_0</code>\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u6765\u770b\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u8868\u73b0\u200b\u4f3c\u4e4e\u200b\u5e76\u200b\u4e0d\u200b\u7406\u60f3\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7ed8\u5236\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u8fdb\u4e00\u6b65\u200b\u8bc4\u4f30\u200b\u5b83\u200b\u3002</p> <p>\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u5c55\u793a\u200b\u4e86\u200b\u6a21\u578b\u200b\u968f\u200b\u65f6\u95f4\u200b\u53d8\u5316\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u5b83\u4eec\u200b\u662f\u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u5728\u200b\u4e0d\u540c\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u4f8b\u5982\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff09\u200b\u4e0a\u200b\u8868\u73b0\u200b\u7684\u200b\u4e00\u79cd\u200b\u6781\u597d\u200b\u65b9\u5f0f\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u7ed8\u5236\u200b<code>model_0_results</code>\u200b\u5b57\u5178\u200b\u4e2d\u200b\u7684\u200b\u503c\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#8","title":"8. \u200b\u7406\u60f3\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u5e94\u8be5\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\uff1f\u00b6","text":"<p>\u200b\u89c2\u5bdf\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u662f\u200b\u5224\u65ad\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u597d\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u8fc7\u200b\u62df\u5408\u200b\u6a21\u578b\u200b\u662f\u200b\u6307\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b\uff08\u200b\u901a\u5e38\u200b\u6709\u200b\u8f83\u5927\u200b\u5dee\u8ddd\u200b\uff09\u200b\u4f18\u4e8e\u200b\u9a8c\u8bc1\u200b/\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u8bad\u7ec3\u200b\u635f\u5931\u200b\u8fdc\u200b\u4f4e\u4e8e\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u5c31\u662f\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\u5b66\u4e60\u200b\u5f97\u200b\u8fc7\u4e8e\u200b\u900f\u5f7b\u200b\uff0c\u200b\u800c\u200b\u8fd9\u4e9b\u200b\u6a21\u5f0f\u200b\u5e76\u200b\u6ca1\u6709\u200b\u6cdb\u5316\u200b\u5230\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u53e6\u4e00\u65b9\u9762\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u6ca1\u6709\u200b\u4f60\u200b\u671f\u671b\u200b\u7684\u200b\u90a3\u4e48\u200b\u4f4e\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6b20\u200b\u62df\u5408\u200b\u3002</p> <p>\u200b\u7406\u60f3\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u662f\u200b\u5b83\u4eec\u200b\u5f7c\u6b64\u200b\u7d27\u5bc6\u200b\u5bf9\u9f50\u200b\u3002</p> <p></p> <p>\u200b\u5de6\u200b\uff1a\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u6ca1\u6709\u200b\u4f60\u200b\u671f\u671b\u200b\u7684\u200b\u90a3\u4e48\u200b\u4f4e\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6b20\u200b\u62df\u5408\u200b\u3002\u200b\u4e2d\u95f4\u200b\uff1a\u200b\u5f53\u200b\u4f60\u200b\u7684\u200b\u6d4b\u8bd5\u200b/\u200b\u9a8c\u8bc1\u200b\u635f\u5931\u200b\u9ad8\u4e8e\u200b\u8bad\u7ec3\u200b\u635f\u5931\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002\u200b\u53f3\u200b\uff1a\u200b\u7406\u60f3\u200b\u7684\u200b\u60c5\u666f\u200b\u662f\u200b\u4f60\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u968f\u200b\u65f6\u95f4\u200b\u5bf9\u9f50\u200b\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u6cdb\u5316\u200b\u5f97\u200b\u5f88\u200b\u597d\u200b\u3002\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u7ec4\u5408\u200b\u548c\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u7684\u200b\u4e0d\u540c\u200b\u60c5\u51b5\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b\u8c37\u6b4c\u200b\u7684\u200b\u89e3\u91ca\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u6307\u5357\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#81","title":"8.1 \u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u8fc7\u200b\u62df\u5408\u200b\u00b6","text":"<p>\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u4e3b\u8981\u200b\u95ee\u9898\u200b\u662f\u200b\u6a21\u578b\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u62df\u5408\u200b\u8fc7\u4e8e\u200b\u7cbe\u786e\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f60\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b\u4e00\u4e9b\u200b\u6280\u672f\u200b\u6765\u200b\u201c\u200b\u7ea6\u675f\u200b\u201d\u200b\u5b83\u200b\u3002</p> <p>\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u5e38\u89c1\u200b\u6280\u672f\u200b\u79f0\u4e3a\u200b\u6b63\u5219\u200b\u5316\u200b\u3002</p> <p>\u200b\u6211\u200b\u559c\u6b22\u200b\u5c06\u200b\u5176\u200b\u89c6\u4e3a\u200b\u201c\u200b\u4f7f\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u66f4\u200b\u89c4\u8303\u200b\u201d\uff0c\u200b\u5373\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u66f4\u200b\u591a\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u51e0\u79cd\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> \u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b \u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f \u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b \u200b\u62e5\u6709\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\u4f7f\u200b\u6a21\u578b\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u673a\u4f1a\u200b\u5b66\u4e60\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6a21\u5f0f\u200b\u53ef\u80fd\u200b\u66f4\u200b\u9002\u7528\u200b\u4e8e\u200b\u65b0\u200b\u6837\u672c\u200b\u3002 \u200b\u7b80\u5316\u200b\u6a21\u578b\u200b \u200b\u5982\u679c\u200b\u5f53\u524d\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u8fc7\u200b\u62df\u5408\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u200b\u53ef\u80fd\u200b\u8fc7\u4e8e\u200b\u590d\u6742\u200b\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u8fc7\u4e8e\u200b\u7cbe\u786e\u200b\u5730\u200b\u5b66\u4e60\u200b\u4e86\u200b\u6570\u636e\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u65e0\u6cd5\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u6cdb\u5316\u200b\u5230\u200b\u672a\u200b\u89c1\u200b\u8fc7\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u7b80\u5316\u200b\u6a21\u578b\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u662f\u200b\u901a\u8fc7\u200b\u51cf\u5c11\u200b\u5c42\u6570\u200b\u6216\u200b\u51cf\u5c11\u200b\u6bcf\u5c42\u200b\u4e2d\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u91cf\u200b\u3002 \u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u901a\u8fc7\u200b\u64cd\u7eb5\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u4f7f\u200b\u6a21\u578b\u200b\u66f4\u200b\u96be\u4ee5\u200b\u5b66\u4e60\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u4eba\u4e3a\u200b\u5730\u200b\u589e\u52a0\u200b\u4e86\u200b\u6570\u636e\u200b\u7684\u200b\u591a\u6837\u6027\u200b\u3002\u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5b66\u4e60\u200b\u589e\u5f3a\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u200b\u53ef\u80fd\u200b\u66f4\u597d\u200b\u5730\u200b\u6cdb\u5316\u200b\u5230\u200b\u672a\u200b\u89c1\u200b\u8fc7\u200b\u7684\u200b\u6570\u636e\u200b\u3002 \u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u5229\u7528\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5b66\u5230\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\uff09\u200b\u4f5c\u4e3a\u200b\u81ea\u5df1\u200b\u4efb\u52a1\u200b\u7684\u200b\u57fa\u7840\u200b\u3002\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e00\u4e2a\u200b\u5728\u200b\u5927\u200b\u8303\u56f4\u200b\u56fe\u50cf\u200b\u4e0a\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u7a0d\u5fae\u200b\u8c03\u6574\u200b\u4f7f\u200b\u5176\u200b\u66f4\u200b\u9002\u5408\u200b\u98df\u54c1\u200b\u56fe\u50cf\u200b\u3002 \u200b\u4f7f\u7528\u200b dropout \u200b\u5c42\u200b Dropout \u200b\u5c42\u200b\u968f\u673a\u200b\u79fb\u9664\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u9690\u85cf\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8fde\u63a5\u200b\uff0c\u200b\u6709\u6548\u200b\u5730\u200b\u7b80\u5316\u200b\u6a21\u578b\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u4f7f\u200b\u5269\u4f59\u200b\u8fde\u63a5\u200b\u66f4\u597d\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b <code>torch.nn.Dropout()</code>\u3002 \u200b\u4f7f\u7528\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b \u200b\u8fd9\u91cc\u200b\u7684\u200b\u60f3\u6cd5\u200b\u662f\u200b\u968f\u7740\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u9010\u6e10\u200b\u964d\u4f4e\u200b\u5b66\u4e60\u200b\u7387\u200b\u3002\u200b\u8fd9\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4f38\u624b\u200b\u53bb\u200b\u62ff\u200b\u6c99\u53d1\u200b\u540e\u9762\u200b\u7684\u200b\u786c\u5e01\u200b\u3002\u200b\u4f60\u200b\u8d8a\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6b65\u5b50\u200b\u5c31\u200b\u8d8a\u200b\u5c0f\u200b\u3002\u200b\u5b66\u4e60\u200b\u7387\u200b\u4e5f\u200b\u662f\u200b\u5982\u6b64\u200b\uff0c\u200b\u8d8a\u200b\u63a5\u8fd1\u200b\u6536\u655b\u200b\uff0c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u6743\u91cd\u200b\u66f4\u65b0\u200b\u8d8a\u5c0f\u200b\u3002 \u200b\u4f7f\u7528\u200b\u65e9\u200b\u505c\u6cd5\u200b \u200b\u65e9\u200b\u505c\u6cd5\u200b\u5728\u200b\u6a21\u578b\u200b\u5f00\u59cb\u200b\u8fc7\u200b\u62df\u5408\u200b\u4e4b\u524d\u200b\u505c\u6b62\u200b\u8bad\u7ec3\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u5728\u200b\u8fc7\u53bb\u200b10\u200b\u4e2a\u200b\u5468\u671f\u200b\u5185\u200b\u6ca1\u6709\u200b\u51cf\u5c11\u200b\uff08\u200b\u8fd9\u4e2a\u200b\u6570\u5b57\u200b\u662f\u200b\u4efb\u610f\u200b\u7684\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u5728\u200b\u6b64\u200b\u505c\u6b62\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u635f\u5931\u200b\u6700\u4f4e\u200b\u7684\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\uff0810\u200b\u4e2a\u200b\u5468\u671f\u200b\u4e4b\u524d\u200b\uff09\u3002 <p>\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u5904\u7406\u200b\u8fc7\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u4e00\u4e9b\u200b\u4e3b\u8981\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u968f\u7740\u200b\u4f60\u200b\u5f00\u59cb\u200b\u6784\u5efa\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u6df1\u5ea6\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u7531\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5728\u200b\u6570\u636e\u200b\u4e2d\u200b\u5b66\u4e60\u200b\u6a21\u5f0f\u200b\u7684\u200b\u80fd\u529b\u200b\u975e\u5e38\u200b\u5f3a\u200b\uff0c\u200b\u5904\u7406\u200b\u8fc7\u200b\u62df\u5408\u200b\u662f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e3b\u8981\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#82","title":"8.2 \u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u6b20\u200b\u62df\u5408\u200b\u00b6","text":"<p>\u200b\u5f53\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6b20\u200b\u62df\u5408\u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b\u9884\u6d4b\u200b\u80fd\u529b\u200b\u8f83\u5dee\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6b20\u200b\u62df\u5408\u200b\u7684\u200b\u6a21\u578b\u200b\u65e0\u6cd5\u200b\u5c06\u200b\u635f\u5931\u200b\u503c\u200b\u964d\u4f4e\u200b\u5230\u200b\u671f\u671b\u200b\u7684\u200b\u6c34\u5e73\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u89c2\u5bdf\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u6211\u200b\u8ba4\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>TinyVGG</code> \u200b\u6a21\u578b\u200b <code>model_0</code> \u200b\u6b20\u200b\u62df\u5408\u200b\u4e86\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u5904\u7406\u200b\u6b20\u200b\u62df\u5408\u200b\u7684\u200b\u4e3b\u8981\u200b\u601d\u8def\u200b\u662f\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u80fd\u529b\u200b\u3002</p> <p>\u200b\u6709\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\u53ef\u4ee5\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> \u200b\u9632\u6b62\u200b\u6b20\u200b\u62df\u5408\u200b\u7684\u200b\u65b9\u6cd5\u200b \u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f \u200b\u589e\u52a0\u200b\u6a21\u578b\u200b\u7684\u200b\u5c42\u6570\u200b/\u200b\u5355\u5143\u200b\u6570\u200b \u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u6b20\u200b\u62df\u5408\u200b\uff0c\u200b\u53ef\u80fd\u200b\u662f\u56e0\u4e3a\u200b\u5b83\u200b\u6ca1\u6709\u200b\u8db3\u591f\u200b\u7684\u200b\u5bb9\u91cf\u200b\u6765\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6a21\u5f0f\u200b/\u200b\u6743\u91cd\u200b/\u200b\u8868\u793a\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002\u200b\u589e\u52a0\u200b\u9690\u85cf\u200b\u5c42\u200b/\u200b\u5355\u5143\u200b\u6570\u662f\u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u80fd\u529b\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u3002 \u200b\u8c03\u6574\u200b\u5b66\u4e60\u200b\u7387\u200b \u200b\u4e5f\u8bb8\u200b\u6a21\u578b\u200b\u7684\u200b\u521d\u59cb\u200b\u5b66\u4e60\u200b\u7387\u592a\u9ad8\u200b\uff0c\u200b\u6bcf\u6b21\u200b\u8fed\u4ee3\u200b\u66f4\u65b0\u200b\u6743\u91cd\u200b\u8fc7\u591a\u200b\uff0c\u200b\u53cd\u800c\u200b\u6ca1\u6709\u200b\u5b66\u5230\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u964d\u4f4e\u200b\u5b66\u4e60\u200b\u7387\u200b\u5e76\u200b\u89c2\u5bdf\u200b\u6548\u679c\u200b\u3002 \u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u80fd\u591f\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u548c\u200b\u6b20\u200b\u62df\u5408\u200b\u3002\u200b\u5b83\u200b\u6d89\u53ca\u200b\u4f7f\u7528\u200b\u5148\u524d\u200b\u5de5\u4f5c\u200b\u6a21\u578b\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8c03\u6574\u200b\u5230\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u4e0a\u200b\u3002 \u200b\u5ef6\u957f\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b \u200b\u6709\u65f6\u5019\u200b\u6a21\u578b\u200b\u53ea\u662f\u200b\u9700\u8981\u200b\u66f4\u200b\u591a\u200b\u65f6\u95f4\u200b\u6765\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u7684\u200b\u8868\u793a\u200b\u3002\u200b\u5982\u679c\u200b\u5728\u200b\u5c0f\u89c4\u6a21\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u53d1\u73b0\u200b\u6a21\u578b\u200b\u6ca1\u6709\u200b\u5b66\u5230\u200b\u4efb\u4f55\u200b\u4e1c\u897f\u200b\uff0c\u200b\u53ef\u80fd\u200b\u8ba9\u200b\u5b83\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u591a\u200b\u8fed\u4ee3\u200b\u4f1a\u200b\u63d0\u9ad8\u200b\u6027\u80fd\u200b\u3002 \u200b\u51cf\u5c11\u200b\u6b63\u5219\u200b\u5316\u200b \u200b\u4e5f\u8bb8\u200b\u6a21\u578b\u200b\u6b20\u200b\u62df\u5408\u200b\u662f\u56e0\u4e3a\u200b\u4f60\u200b\u8bd5\u56fe\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u8fc7\u5ea6\u200b\u3002\u200b\u51cf\u5c11\u200b\u6b63\u5219\u200b\u5316\u200b\u6280\u672f\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6a21\u578b\u200b\u66f4\u597d\u200b\u5730\u200b\u62df\u5408\u200b\u6570\u636e\u200b\u3002"},{"location":"04_pytorch_custom_datasets/#83","title":"8.3 \u200b\u8fc7\u200b\u62df\u5408\u200b\u4e0e\u200b\u6b20\u200b\u62df\u5408\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5e73\u8861\u200b\u00b6","text":"<p>\u200b\u4e0a\u8ff0\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u4e0d\u662f\u200b\u4e07\u80fd\u200b\u7684\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5e76\u200b\u4e0d\u200b\u603b\u662f\u200b\u6709\u6548\u200b\u3002</p> <p>\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u548c\u200b\u6b20\u200b\u62df\u5408\u200b\u53ef\u80fd\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u4e2d\u200b\u6700\u200b\u6d3b\u8dc3\u200b\u7684\u200b\u9886\u57df\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u6bcf\u4e2a\u200b\u4eba\u200b\u90fd\u200b\u5e0c\u671b\u200b\u4ed6\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u62df\u5408\u200b\u5f97\u200b\u66f4\u597d\u200b\uff08\u200b\u51cf\u5c11\u200b\u6b20\u200b\u62df\u5408\u200b\uff09\uff0c\u200b\u4f46\u200b\u53c8\u200b\u4e0d\u80fd\u200b\u597d\u200b\u5230\u200b\u65e0\u6cd5\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u6cdb\u5316\u200b\u5e76\u200b\u5728\u200b\u73b0\u5b9e\u200b\u4e16\u754c\u200b\u4e2d\u200b\u8868\u73b0\u200b\uff08\u200b\u51cf\u5c11\u200b\u8fc7\u200b\u62df\u5408\u200b\uff09\u3002</p> <p>\u200b\u8fc7\u200b\u62df\u5408\u200b\u548c\u200b\u6b20\u200b\u62df\u5408\u200b\u4e4b\u95f4\u200b\u6709\u200b\u4e00\u6761\u200b\u5fae\u5999\u200b\u7684\u200b\u754c\u9650\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u8fc7\u591a\u200b\u7684\u200b\u4efb\u4f55\u200b\u4e00\u4e2a\u200b\u90fd\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u3002</p> <p>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u53ef\u80fd\u200b\u662f\u200b\u5904\u7406\u200b\u4f60\u200b\u81ea\u5df1\u200b\u95ee\u9898\u200b\u4e2d\u8fc7\u200b\u62df\u5408\u200b\u548c\u200b\u6b20\u200b\u62df\u5408\u200b\u7684\u200b\u6700\u200b\u5f3a\u5927\u200b\u6280\u672f\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u4e0e\u5176\u200b\u624b\u5de5\u200b\u8bbe\u8ba1\u200b\u4e0d\u540c\u200b\u7684\u200b\u8fc7\u200b\u62df\u5408\u200b\u548c\u200b\u6b20\u200b\u62df\u5408\u200b\u6280\u672f\u200b\uff0c\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u4f7f\u200b\u4f60\u200b\u80fd\u591f\u200b\u91c7\u7528\u200b\u4e00\u4e2a\u200b\u5df2\u7ecf\u200b\u5728\u200b\u7c7b\u4f3c\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u7a7a\u95f4\u200b\u4e2d\u200b\u6709\u6548\u200b\u5de5\u4f5c\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u6bd4\u5982\u200b\u6765\u81ea\u200bpaperswithcode.com/sota\u200b\u6216\u200bHugging Face \u200b\u6a21\u578b\u200b\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5e94\u7528\u200b\u4e8e\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u540e\u9762\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u770b\u5230\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u5f3a\u5927\u200b\u4e4b\u200b\u5904\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#9-1tinyvgg","title":"9. \u200b\u6a21\u578b\u200b1\uff1a\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bTinyVGG\u00b6","text":"<p>\u200b\u662f\u200b\u65f6\u5019\u200b\u5c1d\u8bd5\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8fd9\u6b21\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u5e76\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6765\u200b\u770b\u770b\u200b\u662f\u5426\u200b\u80fd\u4ee5\u200b\u4efb\u4f55\u200b\u65b9\u5f0f\u200b\u6539\u5584\u200b\u6211\u4eec\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ec4\u5408\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u53d8\u6362\u200b\uff0c\u200b\u5305\u62ec\u200b<code>transforms.TrivialAugmentWide()</code>\uff0c\u200b\u4ee5\u53ca\u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u5e76\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u6d4b\u8bd5\u200b\u53d8\u6362\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u505a\u200b\u540c\u6837\u200b\u7684\u200b\u4e8b\u60c5\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u200b\u5305\u62ec\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#91","title":"9.1 \u200b\u521b\u5efa\u200b\u5305\u542b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u53d8\u6362\u200b\u00b6","text":""},{"location":"04_pytorch_custom_datasets/#92-dataset-dataloader","title":"9.2 \u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200b <code>Dataset</code> \u200b\u548c\u200b <code>DataLoader</code>\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u786e\u4fdd\u200b\u8bad\u7ec3\u200b <code>Dataset</code> \u200b\u4f7f\u7528\u200b <code>train_transform_trivial_augment</code>\uff0c\u200b\u800c\u200b\u6d4b\u8bd5\u200b <code>Dataset</code> \u200b\u4f7f\u7528\u200b <code>test_transform</code>\u3002</p>"},{"location":"04_pytorch_custom_datasets/#93-1","title":"9.3 \u200b\u6784\u5efa\u200b\u5e76\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b1\u00b6","text":"<p>\u200b\u6570\u636e\u200b\u5df2\u200b\u52a0\u8f7d\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6784\u5efa\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b <code>model_1</code>\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91cd\u7528\u200b\u4e4b\u524d\u200b\u7684\u200b <code>TinyVGG</code> \u200b\u7c7b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f1a\u200b\u786e\u4fdd\u200b\u5c06\u200b\u5176\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#94-1","title":"9.4 \u200b\u7ed8\u5236\u200b\u6a21\u578b\u200b1\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b <code>model_1</code> \u200b\u7684\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5728\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>model_1_results</code> \u200b\u7684\u200b\u7ed3\u679c\u200b\u5b57\u5178\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>plot_loss_curves()</code> \u200b\u6765\u200b\u7ed8\u5236\u200b\u5b83\u4eec\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#10","title":"10. \u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u7cdf\u7cd5\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecd\u7136\u200b\u53ef\u4ee5\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u6765\u200b\u6bd4\u8f83\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u8f6c\u6362\u200b\u4e3a\u200b pandas DataFrame\u3002</p>"},{"location":"04_pytorch_custom_datasets/#11","title":"11. \u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u5728\u200b\u4e00\u4e2a\u200b\u7279\u5b9a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5728\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5bf9\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u52a0\u8f7d\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u7136\u540e\u200b\u4ee5\u200b\u4e00\u79cd\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u7c7b\u578b\u200b\u76f8\u5339\u914d\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5e76\u200b\u786e\u4fdd\u200b\u5b83\u200b\u5728\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6a21\u578b\u200b\u4e4b\u524d\u200b\u5177\u6709\u200b\u6b63\u786e\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200b\u4e00\u5f20\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u56fe\u50cf\u200b\u662f\u5426\u200b\u5305\u542b\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u4e00\u5f20\u200b\u6211\u200b\u7238\u7238\u200b\u5bf9\u200b\u4e00\u5f20\u200b\u5927\u200b\u62ab\u8428\u200b\u7ad6\u8d77\u200b\u4e24\u4e2a\u200b\u5927\u62c7\u6307\u200b\u7684\u200b\u7167\u7247\u200b\uff0c\u200b\u6765\u81ea\u200b\u300a\u200b\u4f7f\u7528\u200b PyTorch \u200b\u8fdb\u884c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u300bGitHub \u200b\u4ed3\u5e93\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>requests</code> \u200b\u6a21\u5757\u200b\u4e0b\u8f7d\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5de6\u4fa7\u200b\u83dc\u5355\u200b -&gt; \u200b\u6587\u4ef6\u200b -&gt; \u200b\u4e0a\u200b\u4f20\u5230\u200b\u4f1a\u8bdd\u200b\u5b58\u50a8\u200b\u6765\u200b\u4e0a\u4f20\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u5230\u200b\u5f53\u524d\u200b\u4f1a\u8bdd\u200b\u3002\u200b\u4e0d\u8fc7\u200b\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u7684\u200b Google Colab \u200b\u4f1a\u8bdd\u200b\u7ed3\u675f\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u5f20\u200b\u56fe\u50cf\u200b\u4f1a\u200b\u88ab\u200b\u5220\u9664\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#111-pytorch","title":"11.1 \u200b\u4f7f\u7528\u200b PyTorch \u200b\u52a0\u8f7d\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u00b6","text":"<p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u4e00\u5f20\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8def\u5f84\u200b\u4e3a\u200b <code>data/04-pizza-dad.jpeg</code>\u3002</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u52a0\u8f7d\u200b\u5b83\u200b\u4e86\u200b\u3002</p> <p>PyTorch \u200b\u7684\u200b <code>torchvision</code> \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u79cd\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\uff08\u200b\u7b80\u79f0\u200b\u201cIO\u201d\u200b\u6216\u200b\u201cio\u201d\uff09\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8bfb\u53d6\u200b\u548c\u200b\u5199\u5165\u200b\u56fe\u50cf\u200b\u548c\u200b\u89c6\u9891\u200b\uff0c\u200b\u5177\u4f53\u200b\u53ef\u200b\u5728\u200b <code>torchvision.io</code> \u200b\u4e2d\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u52a0\u8f7d\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torchvision.io.read_image()</code>\u3002</p> <p>\u200b\u6b64\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u8bfb\u53d6\u200b JPEG \u200b\u6216\u200b PNG \u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e09\u7ef4\u200b\u7684\u200b RGB \u200b\u6216\u200b\u7070\u5ea6\u200b <code>torch.Tensor</code>\uff0c\u200b\u5176\u503c\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e3a\u200b <code>uint8</code>\uff0c\u200b\u8303\u56f4\u200b\u5728\u200b <code>[0, 255]</code> \u200b\u4e4b\u95f4\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u770b\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#112-pytorch","title":"11.2 \u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200bPyTorch\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u00b6","text":"<p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u73b0\u5728\u200b\u548c\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u683c\u5f0f\u200b\u4e00\u81f4\u200b\u4e86\u200b\u3002</p> <p>\u200b\u9664\u4e86\u200b\u4e00\u4e2a\u200b\u4e8b\u60c5\u200b...</p> <p>\u200b\u90a3\u200b\u5c31\u662f\u200b<code>\u200b\u5f62\u72b6\u200b</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>[3, 64, 64]</code>\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u5f53\u524d\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b<code>[3, 4032, 3024]</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u786e\u4fdd\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6709\u6ca1\u6709\u200b\u4ec0\u4e48\u200b<code>torchvision.transforms</code>\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\uff1f</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u56de\u7b54\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b<code>matplotlib</code>\u200b\u7ed8\u5236\u200b\u56fe\u50cf\u200b\uff0c\u200b\u786e\u4fdd\u200b\u5b83\u200b\u770b\u8d77\u6765\u200b\u6ca1\u200b\u95ee\u9898\u200b\u3002\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u7ef4\u5ea6\u200b\u4ece\u200b<code>CHW</code>\u200b\u8f6c\u6362\u200b\u4e3a\u200b<code>HWC</code>\u200b\u4ee5\u200b\u9002\u5e94\u200b<code>matplotlib</code>\u200b\u7684\u200b\u8981\u6c42\u200b\u3002</p>"},{"location":"04_pytorch_custom_datasets/#113","title":"11.3 \u200b\u6574\u5408\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u9884\u6d4b\u200b\uff1a\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u6bcf\u6b21\u200b\u60f3\u8981\u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u91cd\u590d\u200b\u4e0a\u8ff0\u200b\u6240\u6709\u200b\u6b65\u9aa4\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u7e41\u7410\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\u6574\u5408\u200b\u5230\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u5730\u200b\u53cd\u590d\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\uff1a</p> <ol> <li>\u200b\u63a5\u6536\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6a21\u578b\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6b63\u786e\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08<code>torch.float32</code>\uff09\u3002</li> <li>\u200b\u786e\u4fdd\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u50cf\u7d20\u200b\u503c\u200b\u5728\u200b <code>[0, 1]</code> \u200b\u8303\u56f4\u200b\u5185\u200b\u3002</li> <li>\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u8f6c\u6362\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u3002</li> <li>\u200b\u786e\u4fdd\u200b\u6a21\u578b\u200b\u4f4d\u4e8e\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08\u200b\u786e\u4fdd\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u6b63\u786e\u200b\u4e14\u200b\u4e0e\u200b\u6a21\u578b\u200b\u4f4d\u4e8e\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff09\u3002</li> <li>\u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\u5bf9\u6570\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</li> <li>\u200b\u5c06\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</li> <li>\u200b\u7ed8\u5236\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u4ee5\u53ca\u200b\u6a21\u578b\u200b\u9884\u6d4b\u200b\u548c\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</li> </ol> <p>\u200b\u867d\u7136\u200b\u6b65\u9aa4\u200b\u4e0d\u5c11\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u80fd\u200b\u505a\u5230\u200b\uff01</p>"},{"location":"04_pytorch_custom_datasets/","title":"\u4e3b\u8981\u200b\u6536\u83b7\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u672c\u200b\u6a21\u5757\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u6db5\u76d6\u200b\u4e86\u200b\u76f8\u5f53\u200b\u591a\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b\u51e0\u4e2a\u200b\u8981\u70b9\u200b\u6765\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\u3002</p> <ul> <li>PyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5185\u7f6e\u200b\u51fd\u6570\u200b\u6765\u200b\u5904\u7406\u200b\u5404\u79cd\u200b\u6570\u636e\u200b\uff0c\u200b\u4ece\u200b\u89c6\u89c9\u200b\u5230\u200b\u6587\u672c\u200b\u3001\u200b\u97f3\u9891\u200b\u518d\u200b\u5230\u200b\u63a8\u8350\u200b\u7cfb\u7edf\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b PyTorch \u200b\u7684\u200b\u5185\u7f6e\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u51fd\u6570\u200b\u4e0d\u200b\u7b26\u5408\u200b\u4f60\u200b\u7684\u200b\u9700\u6c42\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u901a\u8fc7\u200b\u7ee7\u627f\u200b <code>torch.utils.data.Dataset</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u3002</li> <li>PyTorch \u200b\u4e2d\u200b\u7684\u200b <code>torch.utils.data.DataLoader</code> \u200b\u5e2e\u52a9\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b <code>Dataset</code> \u200b\u8f6c\u6362\u200b\u4e3a\u200b\u53ef\u200b\u8fed\u4ee3\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5bf9\u8c61\u200b\u5728\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\u65f6\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u3002</li> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u5f88\u5927\u200b\u4e00\u90e8\u5206\u200b\u5de5\u4f5c\u200b\u662f\u200b\u5904\u7406\u200b \u200b\u8fc7\u200b\u62df\u5408\u200b \u200b\u548c\u200b \u200b\u6b20\u200b\u62df\u5408\u200b \u200b\u4e4b\u95f4\u200b\u7684\u200b\u5e73\u8861\u200b\uff08\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u4e0d\u540c\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u7ec3\u4e60\u200b\u662f\u200b\u8fdb\u4e00\u6b65\u200b\u7814\u7a76\u200b\u5e76\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u6280\u672f\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u662f\u200b\u53ef\u80fd\u200b\u7684\u200b\uff0c\u200b\u53ea\u8981\u200b\u4f60\u200b\u5c06\u200b\u6570\u636e\u683c\u5f0f\u200b\u5316\u4e3a\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u76f8\u4f3c\u200b\u7684\u200b\u683c\u5f0f\u200b\u3002\u200b\u786e\u4fdd\u200b\u4f60\u200b\u6ce8\u610f\u200b\u4e09\u4e2a\u200b\u4e3b\u8981\u200b\u7684\u200b PyTorch \u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9519\u8bef\u200b\uff1a<ol> <li>\u200b\u9519\u8bef\u200b\u7684\u200b\u7c7b\u578b\u200b - \u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u671f\u671b\u200b <code>torch.float32</code>\uff0c\u200b\u800c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b <code>torch.uint8</code>\u3002</li> <li>\u200b\u9519\u8bef\u200b\u7684\u200b\u6570\u636e\u200b\u5f62\u72b6\u200b - \u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u671f\u671b\u200b <code>[batch_size, color_channels, height, width]</code>\uff0c\u200b\u800c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b <code>[color_channels, height, width]</code>\u3002</li> <li>\u200b\u9519\u8bef\u200b\u7684\u200b\u8bbe\u5907\u200b - \u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff0c\u200b\u800c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\u3002</li> </ol> </li> </ul>"},{"location":"04_pytorch_custom_datasets/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u96c6\u4e2d\u200b\u5728\u200b\u7ec3\u4e60\u200b\u4e0a\u8ff0\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u7ae0\u8282\u200b\u6216\u200b\u9075\u5faa\u200b\u6240\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>04\u200b\u7ae0\u8282\u200b\u7684\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b</li> <li>04\u200b\u7ae0\u8282\u200b\u7684\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5728\u200b\u505a\u200b\u7ec3\u4e60\u200b\u4e4b\u524d\u200b\u4e0d\u8981\u200b\u770b\u200b\u8fd9\u4e2a\u200b\uff09</li> </ul> <ol> <li>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u4e0d\u4f73\u200b\uff08\u200b\u4e0d\u200b\u9002\u5408\u200b\u6570\u636e\u200b\uff09\u3002\u200b\u9632\u6b62\u200b\u6b20\u200b\u62df\u5408\u200b\u7684\u200b3\u200b\u79cd\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\u200b\u5199\u4e0b\u200b\u5b83\u4eec\u200b\u5e76\u200b\u5206\u522b\u200b\u7528\u200b\u4e00\u53e5\u200b\u8bdd\u200b\u89e3\u91ca\u200b\u3002</li> <li>\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b1\u30012\u30013\u200b\u548c\u200b4\u200b\u8282\u4e2d\u200b\u6784\u5efa\u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u51fd\u6570\u200b\u3002\u200b\u4f60\u200b\u5e94\u8be5\u200b\u51c6\u5907\u200b\u597d\u200b\u53ef\u7528\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b<code>DataLoader</code>\u3002</li> <li>\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b7\u200b\u8282\u4e2d\u200b\u6784\u5efa\u200b\u7684\u200b<code>model_0</code>\u3002</li> <li>\u200b\u4e3a\u200b<code>model_0</code>\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u51fd\u6570\u200b\u3002</li> <li>\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u4f60\u200b\u5728\u200b\u7ec3\u4e60\u200b3\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b5\u300120\u200b\u548c\u200b50\u200b\u4e2a\u200b\u5468\u671f\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\uff1f<ul> <li>\u200b\u4f7f\u7528\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b0.001\u200b\u7684\u200b<code>torch.optim.Adam()</code>\u200b\u4f5c\u4e3a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</li> </ul> </li> <li>\u200b\u5c06\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u91cf\u200b\u589e\u52a0\u4e00\u500d\u200b\uff0c\u200b\u5e76\u200b\u8bad\u7ec3\u200b20\u200b\u4e2a\u200b\u5468\u671f\u200b\uff0c\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\uff1f</li> <li>\u200b\u5c06\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u91cf\u200b\u589e\u52a0\u4e00\u500d\u200b\uff0c\u200b\u5e76\u200b\u8bad\u7ec3\u200b20\u200b\u4e2a\u200b\u5468\u671f\u200b\uff0c\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\uff1f<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u521b\u5efa\u200b\u7b14\u8bb0\u672c\u200b\u6765\u200b\u6269\u5c55\u200b\u4f60\u200b\u7684\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</li> <li>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200bGitHub\u200b\u4e0a\u200b\u627e\u5230\u200b\u5df2\u7ecf\u200b\u683c\u5f0f\u5316\u200b\u7684\u200b\u53cc\u500d\u200b\u6570\u636e\u200b\uff0820%\u200b\u800c\u200b\u4e0d\u662f\u200b10%\u200b\u5b50\u96c6\u200b\uff09\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u50cf\u200b\u7ec3\u4e60\u200b2\u200b\u4e2d\u200b\u90a3\u6837\u200b\u7f16\u5199\u200b\u4e0b\u8f7d\u200b\u4ee3\u7801\u200b\u6765\u200b\u5c06\u200b\u5176\u200b\u5bfc\u5165\u5230\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u3002</li> </ul> </li> <li>\u200b\u5bf9\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u62ab\u8428\u200b/\u200b\u725b\u6392\u200b/\u200b\u5bff\u53f8\u200b\u7684\u200b\u5b9a\u5236\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08\u200b\u4f60\u200b\u751a\u81f3\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u4e92\u8054\u7f51\u200b\u4e0a\u200b\u4e0b\u8f7d\u200b\u4e00\u5f20\u200b\uff09\u200b\u5e76\u200b\u5206\u4eab\u200b\u4f60\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002<ul> <li>\u200b\u4f60\u200b\u5728\u200b\u7ec3\u4e60\u200b7\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u6b63\u786e\u200b\uff1f</li> <li>\u200b\u5982\u679c\u200b\u4e0d\u662f\u200b\uff0c\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u53ef\u4ee5\u200b\u505a\u4e9b\u200b\u4ec0\u4e48\u200b\u6765\u200b\u6539\u8fdb\u200b\u5b83\u200b\uff1f</li> </ul> </li> </ol>"},{"location":"04_pytorch_custom_datasets/","title":"\u989d\u5916\u200b\u8bfe\u7a0b\u200b\u00b6","text":"<ul> <li>\u200b\u901a\u8fc7\u200bPyTorch\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u6559\u7a0b\u200b\u7b14\u8bb0\u672c\u200b\u6765\u200b\u7ec3\u4e60\u200b\u4f60\u200b\u5bf9\u200bPyTorch <code>Dataset</code>\u200b\u548c\u200b<code>DataLoader</code>\u200b\u7684\u200b\u7406\u89e3\u200b\u3002</li> <li>\u200b\u82b1\u200b10\u200b\u5206\u949f\u200b\u9605\u8bfb\u200bPyTorch <code>torchvision.transforms</code>\u200b\u6587\u6863\u200b\u3002<ul> <li>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u53d8\u6362\u200b\u793a\u4f8b\u200b\u6559\u7a0b\u200b\u4e2d\u200b\u770b\u5230\u200b\u53d8\u6362\u200b\u7684\u200b\u6f14\u793a\u200b\u3002</li> </ul> </li> <li>\u200b\u82b1\u200b10\u200b\u5206\u949f\u200b\u9605\u8bfb\u200bPyTorch\u200b\u7684\u200b<code>torchvision.datasets</code>\u200b\u6587\u6863\u200b\u3002<ul> <li>\u200b\u6709\u200b\u54ea\u4e9b\u200b\u6570\u636e\u200b\u96c6\u8ba9\u200b\u4f60\u200b\u5370\u8c61\u200b\u6df1\u523b\u200b\uff1f</li> <li>\u200b\u4f60\u200b\u5982\u4f55\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\uff1f</li> </ul> </li> <li>TorchData\u200b\u76ee\u524d\u200b\u5904\u4e8e\u200b\u6d4b\u8bd5\u9636\u6bb5\u200b\uff08\u200b\u622a\u81f3\u200b2022\u200b\u5e74\u200b4\u200b\u6708\u200b\uff09\uff0c\u200b\u5b83\u200b\u5c06\u200b\u662f\u200b\u672a\u6765\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u4f46\u200b\u73b0\u5728\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u4e86\u89e3\u200b\u5b83\u200b\u3002</li> <li>\u200b\u4e3a\u4e86\u200b\u52a0\u901f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u91c7\u53d6\u200b\u4e00\u4e9b\u200b\u6280\u5de7\u200b\u6765\u200b\u6539\u8fdb\u200b\u8ba1\u7b97\u200b\u3001\u200b\u5185\u5b58\u200b\u548c\u200b\u5f00\u9500\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u9605\u8bfb\u200bHorace He\u200b\u7684\u200b\u6587\u7ae0\u200bMaking Deep Learning Go Brrrr From First Principles\u3002</li> </ul>"},{"location":"05_pytorch_going_modular/","title":"05. PyTorch \u200b\u6a21\u5757\u5316","text":"<p>View Source Code | View Slides</p>"},{"location":"05_pytorch_going_modular/#05-pytorch","title":"05. PyTorch \u200b\u6a21\u5757\u5316","text":"<p>\u200b\u672c\u200b\u8282\u200b\u56de\u7b54\u200b\u4e86\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\uff1a\u201c\u200b\u5982\u4f55\u200b\u5c06\u200b\u6211\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4ee3\u7801\u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u811a\u672c\u200b\uff1f\u201d</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b notebook 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u4e2d\u200b\u6700\u200b\u6709\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\u5355\u5143\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7cfb\u5217\u200b Python \u200b\u811a\u672c\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u5230\u200b\u540d\u4e3a\u200b <code>going_modular</code> \u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#_1","title":"\u4ec0\u4e48\u200b\u662f\u200b\u6a21\u5757\u5316\u200b\uff1f","text":"<p>\u200b\u6a21\u5757\u5316\u200b\u6d89\u53ca\u200b\u5c06\u200b\u7b14\u8bb0\u672c\u200b\u4ee3\u7801\u200b\uff08\u200b\u6765\u81ea\u200b Jupyter Notebook \u200b\u6216\u200b Google Colab \u200b\u7b14\u8bb0\u672c\u200b\uff09\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7cfb\u5217\u200b\u63d0\u4f9b\u200b\u7c7b\u4f3c\u200b\u529f\u80fd\u200b\u7684\u200b Python \u200b\u811a\u672c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u7b14\u8bb0\u672c\u200b\u4ee3\u7801\u200b\u4ece\u200b\u4e00\u7cfb\u5217\u200b\u5355\u5143\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4ee5\u4e0b\u200b Python \u200b\u6587\u4ef6\u200b\uff1a</p> <ul> <li><code>data_setup.py</code> - \u200b\u7528\u4e8e\u200b\u51c6\u5907\u200b\u548c\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\uff08\u200b\u5982\u679c\u200b\u9700\u8981\u200b\uff09\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002</li> <li><code>engine.py</code> - \u200b\u5305\u542b\u200b\u5404\u79cd\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002</li> <li><code>model_builder.py</code> \u200b\u6216\u200b <code>model.py</code> - \u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002</li> <li><code>train.py</code> - \u200b\u5229\u7528\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u6587\u4ef6\u200b\u5e76\u200b\u8bad\u7ec3\u200b\u76ee\u6807\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002</li> <li><code>utils.py</code> - \u200b\u4e13\u95e8\u200b\u7528\u4e8e\u200b\u6709\u7528\u200b\u5de5\u5177\u200b\u51fd\u6570\u200b\u7684\u200b\u6587\u4ef6\u200b\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0a\u8ff0\u200b\u6587\u4ef6\u200b\u7684\u200b\u547d\u540d\u200b\u548c\u200b\u5e03\u5c40\u200b\u5c06\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u7684\u200b\u4f7f\u7528\u200b\u573a\u666f\u200b\u548c\u200b\u4ee3\u7801\u200b\u9700\u6c42\u200b\u3002Python \u200b\u811a\u672c\u200b\u4e0e\u200b\u5355\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u5355\u5143\u200b\u4e00\u6837\u200b\u901a\u7528\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\uff0c\u200b\u60a8\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u4e3a\u200b\u4efb\u4f55\u200b\u7c7b\u578b\u200b\u7684\u200b\u529f\u80fd\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u811a\u672c\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#_2","title":"\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u6a21\u5757\u5316\u200b\uff1f","text":"<p>\u200b\u7b14\u8bb0\u672c\u200b\u975e\u5e38\u9002\u5408\u200b\u8fed\u4ee3\u200b\u5730\u200b\u63a2\u7d22\u200b\u548c\u200b\u5feb\u901f\u200b\u8fd0\u884c\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u66f4\u200b\u5927\u89c4\u6a21\u200b\u7684\u200b\u9879\u76ee\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b Python \u200b\u811a\u672c\u200b\u66f4\u5177\u200b\u53ef\u91cd\u590d\u6027\u200b\u4e14\u200b\u66f4\u200b\u6613\u4e8e\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u5c3d\u7ba1\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u6709\u200b\u4e89\u8bae\u200b\u7684\u200b\u8bdd\u9898\u200b\uff0c\u200b\u4f46\u200b\u50cf\u200b Netflix \u200b\u8fd9\u6837\u200b\u7684\u200b\u516c\u53f8\u200b\u5df2\u7ecf\u200b\u5c55\u793a\u200b\u4e86\u200b\u4ed6\u4eec\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b\u7b14\u8bb0\u672c\u200b\u8fdb\u884c\u200b\u751f\u4ea7\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u751f\u4ea7\u200b\u4ee3\u7801\u200b \u200b\u662f\u200b\u8fd0\u884c\u200b\u4ee5\u5411\u200b\u67d0\u4eba\u200b\u6216\u200b\u67d0\u7269\u200b\u63d0\u4f9b\u200b\u670d\u52a1\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5728\u7ebf\u200b\u8fd0\u884c\u200b\u7684\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\uff0c\u200b\u5176\u4ed6\u4eba\u200b\u53ef\u4ee5\u200b\u8bbf\u95ee\u200b\u548c\u200b\u4f7f\u7528\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8fd0\u884c\u200b\u8be5\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u7684\u200b\u4ee3\u7801\u200b\u5c31\u200b\u88ab\u200b\u89c6\u4e3a\u200b \u200b\u751f\u4ea7\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u50cf\u200b fast.ai \u200b\u7684\u200b <code>nb-dev</code>\uff08\u200b\u7b14\u8bb0\u672c\u200b\u5f00\u53d1\u200b\u7684\u200b\u7b80\u79f0\u200b\uff09\u200b\u8fd9\u6837\u200b\u7684\u200b\u5e93\u200b\uff0c\u200b\u4f7f\u200b\u60a8\u200b\u80fd\u591f\u200b\u4f7f\u7528\u200b Jupyter Notebooks \u200b\u7f16\u5199\u200b\u6574\u4e2a\u200b Python \u200b\u5e93\u200b\uff08\u200b\u5305\u62ec\u200b\u6587\u6863\u200b\uff09\u3002</p>"},{"location":"05_pytorch_going_modular/#python","title":"\u7b14\u8bb0\u672c\u200b\u4e0e\u200bPython\u200b\u811a\u672c\u200b\u7684\u200b\u4f18\u7f3a\u70b9","text":"<p>\u200b\u53cc\u65b9\u200b\u90fd\u200b\u6709\u200b\u5404\u81ea\u200b\u7684\u200b\u7406\u7531\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8fd9\u4e2a\u200b\u5217\u8868\u200b\u603b\u7ed3\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u4e3b\u8981\u200b\u8bae\u9898\u200b\u3002</p> \u200b\u4f18\u70b9\u200b \u200b\u7f3a\u70b9\u200b \u200b\u7b14\u8bb0\u672c\u200b \u200b\u6613\u4e8e\u200b\u5b9e\u9a8c\u200b/\u200b\u5165\u95e8\u200b \u200b\u7248\u672c\u63a7\u5236\u200b\u53ef\u80fd\u200b\u5f88\u200b\u56f0\u96be\u200b \u200b\u6613\u4e8e\u200b\u5206\u4eab\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5206\u4eab\u200b\u4e00\u4e2a\u200bGoogle Colab\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u94fe\u63a5\u200b\uff09 \u200b\u96be\u4ee5\u200b\u4ec5\u200b\u4f7f\u7528\u200b\u7279\u5b9a\u200b\u90e8\u5206\u200b \u200b\u975e\u5e38\u200b\u76f4\u89c2\u200b \u200b\u6587\u672c\u200b\u548c\u200b\u56fe\u5f62\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u59a8\u788d\u200b\u4ee3\u7801\u200b \u200b\u4f18\u70b9\u200b \u200b\u7f3a\u70b9\u200b Python\u200b\u811a\u672c\u200b \u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4ee3\u7801\u200b\u6253\u5305\u200b\u5728\u200b\u4e00\u8d77\u200b\uff08\u200b\u907f\u514d\u200b\u5728\u200b\u4e0d\u540c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u91cd\u590d\u200b\u7f16\u5199\u200b\u76f8\u4f3c\u200b\u4ee3\u7801\u200b\uff09 \u200b\u5b9e\u9a8c\u200b\u4e0d\u591f\u200b\u76f4\u89c2\u200b\uff08\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u8fd0\u884c\u200b\u6574\u4e2a\u200b\u811a\u672c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5355\u4e2a\u200b\u5355\u5143\u200b\uff09 \u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bgit\u200b\u8fdb\u884c\u200b\u7248\u672c\u63a7\u5236\u200b \u200b\u8bb8\u591a\u200b\u5f00\u6e90\u200b\u9879\u76ee\u200b\u4f7f\u7528\u200b\u811a\u672c\u200b \u200b\u5927\u578b\u9879\u76ee\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e91\u200b\u670d\u52a1\u5546\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff08\u200b\u5bf9\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u652f\u6301\u200b\u4e0d\u5982\u200b\u811a\u672c\u200b\uff09"},{"location":"05_pytorch_going_modular/#_3","title":"\u6211\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b","text":"<p>\u200b\u6211\u200b\u901a\u5e38\u200b\u5728\u200bJupyter/Google Colab\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u5f00\u59cb\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5feb\u901f\u200b\u5b9e\u9a8c\u200b\u548c\u200b\u53ef\u89c6\u5316\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u5f53\u200b\u6211\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6210\u679c\u200b\u540e\u200b\uff0c\u200b\u6211\u4f1a\u200b\u5c06\u200b\u6700\u200b\u6709\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\u7247\u6bb5\u200b\u79fb\u200b\u5230\u200bPython\u200b\u811a\u672c\u200b\u4e2d\u200b\u3002</p> <p></p> <p>\u200b\u7f16\u5199\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u6709\u200b\u8bb8\u591a\u200b\u53ef\u80fd\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002\u200b\u6709\u4e9b\u200b\u4eba\u200b\u559c\u6b22\u200b\u4ece\u200b\u811a\u672c\u200b\u5f00\u59cb\u200b\uff0c\u200b\u800c\u200b\u53e6\u200b\u4e00\u4e9b\u200b\u4eba\u200b\uff08\u200b\u50cf\u200b\u6211\u200b\u4e00\u6837\u200b\uff09\u200b\u66f4\u200b\u559c\u6b22\u200b\u4ece\u200b\u7b14\u8bb0\u672c\u200b\u5f00\u59cb\u200b\uff0c\u200b\u7a0d\u540e\u200b\u518d\u200b\u8f6c\u5230\u200b\u811a\u672c\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#pytorch","title":"\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\u7684\u200bPyTorch","text":"<p>\u200b\u5728\u200b\u4f60\u200b\u63a2\u7d22\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u8bb8\u591a\u200b\u57fa\u4e8e\u200bPyTorch\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\u7684\u200b\u4ee3\u7801\u200b\u5e93\u200b\u90fd\u200b\u6709\u200b\u5982\u4f55\u200b\u4ee5\u200bPython\u200b\u811a\u672c\u200b\u5f62\u5f0f\u200b\u8fd0\u884c\u200bPyTorch\u200b\u4ee3\u7801\u200b\u7684\u200b\u8bf4\u660e\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u88ab\u200b\u6307\u793a\u200b\u5728\u200b\u7ec8\u7aef\u200b/\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u5982\u4e0b\u200b\u4ee3\u7801\u200b\u6765\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1a</p> <p><pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre> </p> <p>\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u5e26\u6709\u200b\u5404\u79cd\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u7684\u200bPyTorch <code>train.py</code>\u200b\u811a\u672c\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c<code>train.py</code>\u200b\u662f\u200b\u76ee\u6807\u200bPython\u200b\u811a\u672c\u200b\uff0c\u200b\u5b83\u200b\u53ef\u80fd\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200bPyTorch\u200b\u6a21\u578b\u200b\u7684\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u800c\u200b<code>--model</code>\u3001<code>--batch_size</code>\u3001<code>--lr</code>\u200b\u548c\u200b<code>--num_epochs</code>\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u53c2\u6570\u200b\u6807\u5fd7\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b\u4efb\u4f55\u200b\u4f60\u200b\u559c\u6b22\u200b\u7684\u200b\u503c\u200b\uff0c\u200b\u5982\u679c\u200b\u5b83\u4eec\u200b\u4e0e\u200b<code>train.py</code>\u200b\u517c\u5bb9\u200b\uff0c\u200b\u5b83\u4eec\u200b\u5c31\u200b\u4f1a\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u5426\u5219\u200b\u5c31\u200b\u4f1a\u200b\u62a5\u9519\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u60f3\u200b\u7528\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b32\u3001\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b0.001\u200b\u7684\u200b\u53c2\u6570\u200b\u8bad\u7ec3\u200b\u7b14\u8bb0\u672c\u200b04\u200b\u4e2d\u200b\u7684\u200bTinyVGG\u200b\u6a21\u578b\u200b10\u200b\u4e2a\u200b\u5468\u671f\u200b\uff1a</p> <pre><code>python train.py --model tinyvgg --batch_size 32 --lr 0.001 --num_epochs 10\n</code></pre> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4f60\u200b\u7684\u200b<code>train.py</code>\u200b\u811a\u672c\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u4efb\u610f\u200b\u6570\u91cf\u200b\u7684\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u6807\u5fd7\u200b\uff0c\u200b\u4ee5\u200b\u6ee1\u8db3\u200b\u4f60\u200b\u7684\u200b\u9700\u6c42\u200b\u3002</p> <p>PyTorch\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200b\u4e2d\u200b\u8bad\u7ec3\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u4e5f\u200b\u4f7f\u7528\u200b\u4e86\u200b\u8fd9\u79cd\u200b\u98ce\u683c\u200b\u3002</p> <p></p> <p>\u200b\u4f7f\u7528\u200b8\u200b\u4e2a\u200bGPU\u200b\u8bad\u7ec3\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u7684\u200bPyTorch\u200b\u547d\u4ee4\u884c\u200b\u8bad\u7ec3\u200b\u811a\u672c\u200b\u914d\u65b9\u200b\u3002 \u200b\u6765\u6e90\u200b\uff1aPyTorch\u200b\u535a\u5ba2\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#_4","title":"\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9","text":"<p>\u200b\u672c\u8282\u200b\u7684\u200b\u4e3b\u8981\u200b\u6982\u5ff5\u200b\u662f\u200b\uff1a\u200b\u5c06\u200b\u5b9e\u7528\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4ee3\u7801\u200b\u5355\u5143\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u53ef\u200b\u91cd\u590d\u4f7f\u7528\u200b\u7684\u200bPython\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u505a\u200b\u53ef\u4ee5\u200b\u907f\u514d\u200b\u6211\u4eec\u200b\u4e00\u904d\u200b\u53c8\u200b\u4e00\u904d\u200b\u5730\u200b\u7f16\u5199\u200b\u76f8\u540c\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u672c\u8282\u200b\u6709\u200b\u4e24\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\uff1a</p> <ol> <li>05. \u200b\u6a21\u5757\u5316\u200b\uff1a\u200b\u7b2c\u200b1\u200b\u90e8\u5206\u200b\uff08\u200b\u5355\u5143\u200b\u6a21\u5f0f\u200b\uff09 - \u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u4ee5\u200b\u4f20\u7edf\u200b\u7684\u200bJupyter Notebook/Google Colab\u200b\u7b14\u8bb0\u672c\u200b\u8fd0\u884c\u200b\uff0c\u200b\u662f\u200b\u7b14\u8bb0\u672c\u200b04\u200b\u7684\u200b\u6d53\u7f29\u200b\u7248\u672c\u200b\u3002</li> <li>05. \u200b\u6a21\u5757\u5316\u200b\uff1a\u200b\u7b2c\u200b2\u200b\u90e8\u5206\u200b\uff08\u200b\u811a\u672c\u200b\u6a21\u5f0f\u200b\uff09 - \u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u4e0e\u200b\u7b2c\u200b1\u200b\u4e2a\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u589e\u52a0\u200b\u4e86\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u4e3b\u8981\u200b\u90e8\u5206\u200b\u8f6c\u6362\u200b\u4e3a\u200bPython\u200b\u811a\u672c\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u4f8b\u5982\u200b<code>data_setup.py</code>\u200b\u548c\u200b<code>train.py</code>\u3002</li> </ol> <p>\u200b\u672c\u200b\u6587\u6863\u200b\u4e2d\u200b\u7684\u200b\u6587\u672c\u200b\u91cd\u70b9\u200b\u4ecb\u7ecd\u200b\u4ee3\u7801\u200b\u5355\u5143\u200b05. \u200b\u6a21\u5757\u5316\u200b\uff1a\u200b\u7b2c\u200b2\u200b\u90e8\u5206\u200b\uff08\u200b\u811a\u672c\u200b\u6a21\u5f0f\u200b\uff09\uff0c\u200b\u5373\u200b\u9876\u90e8\u200b\u5e26\u6709\u200b<code>%%writefile ...</code>\u200b\u7684\u200b\u5355\u5143\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#_5","title":"\u4e3a\u4ec0\u4e48\u200b\u5206\u4e24\u200b\u90e8\u5206\u200b\uff1f","text":"<p>\u200b\u56e0\u4e3a\u200b\u6709\u65f6\u200b\u5b66\u4e60\u200b\u67d0\u4ef6\u4e8b\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u662f\u200b\u770b\u200b\u5b83\u200b\u4e0e\u200b\u522b\u7684\u200b\u4e8b\u6709\u4f55\u200b\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5e76\u6392\u200b\u8fd0\u884c\u200b\u6bcf\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u770b\u5230\u200b\u5b83\u4eec\u200b\u7684\u200b\u4e0d\u540c\u4e4b\u5904\u200b\uff0c\u200b\u8fd9\u200b\u5c31\u662f\u200b\u5173\u952e\u200b\u7684\u200b\u5b66\u4e60\u200b\u70b9\u200b\u3002</p> <p></p> <p>\u200b\u5e76\u6392\u200b\u8fd0\u884c\u200b\u7b2c\u200b05\u200b\u8282\u200b\u7684\u200b\u4e24\u672c\u200b\u7b14\u8bb0\u672c\u200b\u3002\u200b\u4f60\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u811a\u672c\u200b\u6a21\u5f0f\u200b\u7b14\u8bb0\u672c\u200b\u6709\u200b\u989d\u5916\u200b\u7684\u200b\u4ee3\u7801\u200b\u5355\u5143\u200b\uff0c\u200b\u5c06\u200b\u5355\u5143\u200b\u6a21\u5f0f\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u4ee3\u7801\u200b\u8f6c\u6362\u200b\u4e3a\u200bPython\u200b\u811a\u672c\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#_6","title":"\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807","text":"<p>\u200b\u901a\u8fc7\u200b\u672c\u8282\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u8fbe\u5230\u200b\u4ee5\u4e0b\u200b\u4e24\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u7684\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b04\uff08Food Vision Mini\uff09\u200b\u4e2d\u200b\u6784\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a<code>python train.py</code>\u3002</li> <li>\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u91cd\u590d\u4f7f\u7528\u200b\u7684\u200bPython\u200b\u811a\u672c\u200b\u76ee\u5f55\u200b\u7ed3\u6784\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff1a</li> </ol> <pre><code>going_modular/\n\u251c\u2500\u2500 going_modular/\n\u2502   \u251c\u2500\u2500 data_setup.py\n\u2502   \u251c\u2500\u2500 engine.py\n\u2502   \u251c\u2500\u2500 model_builder.py\n\u2502   \u251c\u2500\u2500 train.py\n\u2502   \u2514\u2500\u2500 utils.py\n\u251c\u2500\u2500 models/\n\u2502   \u251c\u2500\u2500 05_going_modular_cell_mode_tinyvgg_model.pth\n\u2502   \u2514\u2500\u2500 05_going_modular_script_mode_tinyvgg_model.pth\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 pizza_steak_sushi/\n        \u251c\u2500\u2500 train/\n        \u2502   \u251c\u2500\u2500 pizza/\n        \u2502   \u2502   \u251c\u2500\u2500 image01.jpeg\n        \u2502   \u2502   \u2514\u2500\u2500 ...\n        \u2502   \u251c\u2500\u2500 steak/\n        \u2502   \u2514\u2500\u2500 sushi/\n        \u2514\u2500\u2500 test/\n            \u251c\u2500\u2500 pizza/\n            \u251c\u2500\u2500 steak/\n            \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"05_pytorch_going_modular/#_7","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u200b\u6587\u6863\u200b\u5b57\u7b26\u4e32\u200b - \u200b\u7f16\u5199\u200b\u53ef\u200b\u590d\u73b0\u200b\u4e14\u200b\u6613\u4e8e\u200b\u7406\u89e3\u200b\u7684\u200b\u4ee3\u7801\u200b\u81f3\u5173\u91cd\u8981\u200b\u3002\u200b\u9274\u4e8e\u200b\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u7f16\u5199\u200b\u811a\u672c\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u51fd\u6570\u200b/\u200b\u7c7b\u65f6\u200b\u90fd\u200b\u9075\u5faa\u200b\u4e86\u200b Google \u200b\u7684\u200b Python \u200b\u6587\u6863\u200b\u5b57\u7b26\u4e32\u200b\u98ce\u683c\u200b\u3002</li> <li>\u200b\u811a\u672c\u200b\u9876\u90e8\u200b\u5bfc\u5165\u200b\u6a21\u5757\u200b - \u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u8981\u200b\u521b\u5efa\u200b\u7684\u200b\u6240\u6709\u200b Python \u200b\u811a\u672c\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u72ec\u7acb\u200b\u7684\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6240\u6709\u200b\u811a\u672c\u200b\u90fd\u200b\u9700\u8981\u200b\u5728\u200b\u5176\u200b\u5f00\u5934\u200b\u5bfc\u5165\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6a21\u5757\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff1a</li> </ul> <pre><code># Import modules required for train.py\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n</code></pre>"},{"location":"05_pytorch_going_modular/#_8","title":"\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1f","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u6750\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub Discussions \u200b\u9875\u9762\u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u6587\u6863\u200b\u548c\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\u7684\u200b PyTorch \u200b\u76f8\u5173\u200b\u8d44\u6e90\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#0-vs","title":"0. \u200b\u5355\u5143\u200b\u6a21\u5f0f\u200b vs. \u200b\u811a\u672c\u200b\u6a21\u5f0f","text":"<p>\u200b\u5355\u5143\u200b\u6a21\u5f0f\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b 05. \u200b\u6a21\u5757\u5316\u200b\u7b2c\u200b1\u200b\u90e8\u5206\u200b\uff08\u200b\u5355\u5143\u200b\u6a21\u5f0f\u200b\uff09\uff0c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6b63\u5e38\u200b\u8fd0\u884c\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5355\u5143\u683c\u200b\u8981\u4e48\u200b\u662f\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8981\u4e48\u200b\u662f\u200b Markdown\u3002</p> <p>\u200b\u811a\u672c\u200b\u6a21\u5f0f\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b 05. \u200b\u6a21\u5757\u5316\u200b\u7b2c\u200b2\u200b\u90e8\u5206\u200b\uff08\u200b\u811a\u672c\u200b\u6a21\u5f0f\u200b\uff09\uff0c\u200b\u4e0e\u200b\u5355\u5143\u200b\u6a21\u5f0f\u200b\u7b14\u8bb0\u672c\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u4f46\u200b\u8bb8\u591a\u200b\u4ee3\u7801\u200b\u5355\u5143\u683c\u200b\u53ef\u80fd\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u811a\u672c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u4e0d\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u7b14\u8bb0\u672c\u200b\u521b\u5efa\u200b Python \u200b\u811a\u672c\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u901a\u8fc7\u200b VS Code \u200b\u7b49\u200b\u96c6\u6210\u200b\u5f00\u53d1\u200b\u73af\u5883\u200b\uff08IDE\uff09\u200b\u521b\u5efa\u200b\u5b83\u4eec\u200b\u3002\u200b\u5c06\u200b\u811a\u672c\u200b\u6a21\u5f0f\u200b\u7b14\u8bb0\u672c\u200b\u4f5c\u4e3a\u200b\u672c\u8282\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u53ea\u662f\u200b\u4e3a\u4e86\u200b\u6f14\u793a\u200b\u4ece\u200b\u7b14\u8bb0\u672c\u200b\u5230\u200b Python \u200b\u811a\u672c\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u3002</p>"},{"location":"05_pytorch_going_modular/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e","text":"<p>\u200b\u5728\u200b\u6bcf\u4e2a\u200b 05 \u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u83b7\u53d6\u6570\u636e\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e0e\u200b \u200b\u7b14\u8bb0\u672c\u200b 04 \u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b Python \u200b\u7684\u200b <code>requests</code> \u200b\u6a21\u5757\u200b\u5411\u200b GitHub \u200b\u53d1\u51fa\u8bf7\u6c42\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u4e00\u4e2a\u200b <code>.zip</code> \u200b\u6587\u4ef6\u200b\u5e76\u200b\u89e3\u538b\u200b\u3002</p> <pre><code>import os\nimport requests\nimport zipfile\nfrom pathlib import Path\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n\n# Download pizza, steak, sushi data\nwith open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n    request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n    print(\"Downloading pizza, steak, sushi data...\")\n    f.write(request.content)\n\n# Unzip pizza, steak, sushi data\nwith zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n    print(\"Unzipping pizza, steak, sushi data...\") \n    zip_ref.extractall(image_path)\n\n# Remove zip file\nos.remove(data_path / \"pizza_steak_sushi.zip\")\n</code></pre> <p>\u200b\u8fd9\u6837\u200b\u5c31\u200b\u4f1a\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>data</code> \u200b\u7684\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>pizza_steak_sushi</code> \u200b\u7684\u200b\u76ee\u5f55\u200b\uff0c\u200b\u91cc\u9762\u200b\u6709\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u683c\u5f0f\u200b\u4e3a\u200b\u6807\u51c6\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\u3002</p> <pre><code>data/\n\u2514\u2500\u2500 pizza_steak_sushi/\n    \u251c\u2500\u2500 train/\n    \u2502   \u251c\u2500\u2500 pizza/\n    \u2502   \u2502   \u251c\u2500\u2500 train_image01.jpeg\n    \u2502   \u2502   \u251c\u2500\u2500 test_image02.jpeg\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u251c\u2500\u2500 steak/\n    \u2502   \u2502   \u2514\u2500\u2500 ...\n    \u2502   \u2514\u2500\u2500 sushi/\n    \u2502       \u2514\u2500\u2500 ...\n    \u2514\u2500\u2500 test/\n        \u251c\u2500\u2500 pizza/\n        \u2502   \u251c\u2500\u2500 test_image01.jpeg\n        \u2502   \u2514\u2500\u2500 test_image02.jpeg\n        \u251c\u2500\u2500 steak/\n        \u2514\u2500\u2500 sushi/\n</code></pre>"},{"location":"05_pytorch_going_modular/#2-data_setuppy","title":"2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b (<code>data_setup.py</code>)","text":"<p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u83b7\u53d6\u6570\u636e\u200b\u540e\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b PyTorch \u200b\u7684\u200b <code>Dataset</code> \u200b\u548c\u200b <code>DataLoader</code>\uff08\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u6709\u7528\u200b\u7684\u200b <code>Dataset</code> \u200b\u548c\u200b <code>DataLoader</code> \u200b\u521b\u5efa\u200b\u4ee3\u7801\u200b\u5c01\u88c5\u200b\u6210\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>create_dataloaders()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5e76\u200b\u901a\u8fc7\u200b <code>%%writefile going_modular/data_setup.py</code> \u200b\u5c06\u200b\u5176\u200b\u5199\u5165\u200b\u6587\u4ef6\u200b\u3002</p> data_setup.py<pre><code>%%writefile going_modular/data_setup.py\n\"\"\"\nContains functionality for creating PyTorch DataLoaders for \nimage classification data.\n\"\"\"\nimport os\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str, \n    test_dir: str, \n    transform: transforms.Compose, \n    batch_size: int, \n    num_workers: int=NUM_WORKERS\n):\n  \"\"\"Creates training and testing DataLoaders.\n\n  Takes in a training directory and testing directory path and turns\n  them into PyTorch Datasets and then into PyTorch DataLoaders.\n\n  Args:\n    train_dir: Path to training directory.\n    test_dir: Path to testing directory.\n    transform: torchvision transforms to perform on training and testing data.\n    batch_size: Number of samples per batch in each of the DataLoaders.\n    num_workers: An integer for number of workers per DataLoader.\n\n  Returns:\n    A tuple of (train_dataloader, test_dataloader, class_names).\n    Where class_names is a list of the target classes.\n    Example usage:\n      train_dataloader, test_dataloader, class_names = \\\n        = create_dataloaders(train_dir=path/to/train_dir,\n                             test_dir=path/to/test_dir,\n                             transform=some_transform,\n                             batch_size=32,\n                             num_workers=4)\n  \"\"\"\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False, # don't need to shuffle test data\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names\n</code></pre> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u521b\u5efa\u200b<code>DataLoader</code>\uff0c\u200b\u73b0\u5728\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u8fd9\u6837\u200b\u4f7f\u7528\u200b<code>data_setup.py</code>\u200b\u4e2d\u200b\u7684\u200b\u51fd\u6570\u200b\uff1a</p> <pre><code># Import data_setup.py\nfrom going_modular import data_setup\n\n# Create train/test dataloader and get class names as a list\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(...)\n</code></pre>"},{"location":"05_pytorch_going_modular/#3-model_builderpy","title":"3. \u200b\u6784\u5efa\u200b\u6a21\u578b\u200b (<code>model_builder.py</code>)","text":"<p>\u200b\u5728\u200b\u8fc7\u53bb\u200b\u7684\u200b\u51e0\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u7b14\u8bb0\u672c\u200b03\u200b\u548c\u200b\u7b14\u8bb0\u672c\u200b04\uff09\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u591a\u6b21\u200b\u6784\u5efa\u200b\u4e86\u200bTinyVGG\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5c06\u200b\u6a21\u578b\u200b\u653e\u5165\u200b\u5176\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53cd\u590d\u200b\u91cd\u7528\u200b\u662f\u200b\u5f88\u200b\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b<code>TinyVGG()</code>\u200b\u6a21\u578b\u200b\u7c7b\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b\u811a\u672c\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u884c\u200b<code>%%writefile going_modular/model_builder.py</code>\uff1a</p> model_builder.py<pre><code>%%writefile going_modular/model_builder.py\n\"\"\"\nContains PyTorch model code to instantiate a TinyVGG model.\n\"\"\"\nimport torch\nfrom torch import nn \n\nclass TinyVGG(nn.Module):\n  \"\"\"Creates the TinyVGG architecture.\n\n  Replicates the TinyVGG architecture from the CNN explainer website in PyTorch.\n  See the original architecture here: https://poloclub.github.io/cnn-explainer/\n\n  Args:\n    input_shape: An integer indicating number of input channels.\n    hidden_units: An integer indicating number of hidden units between layers.\n    output_shape: An integer indicating number of output units.\n  \"\"\"\n  def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -&gt; None:\n      super().__init__()\n      self.conv_block_1 = nn.Sequential(\n          nn.Conv2d(in_channels=input_shape, \n                    out_channels=hidden_units, \n                    kernel_size=3, \n                    stride=1, \n                    padding=0),  \n          nn.ReLU(),\n          nn.Conv2d(in_channels=hidden_units, \n                    out_channels=hidden_units,\n                    kernel_size=3,\n                    stride=1,\n                    padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(kernel_size=2,\n                        stride=2)\n      )\n      self.conv_block_2 = nn.Sequential(\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=0),\n          nn.ReLU(),\n          nn.MaxPool2d(2)\n      )\n      self.classifier = nn.Sequential(\n          nn.Flatten(),\n          # Where did this in_features shape come from? \n          # It's because each layer of our network compresses and changes the shape of our inputs data.\n          nn.Linear(in_features=hidden_units*13*13,\n                    out_features=output_shape)\n      )\n\n  def forward(self, x: torch.Tensor):\n      x = self.conv_block_1(x)\n      x = self.conv_block_2(x)\n      x = self.classifier(x)\n      return x\n      # return self.classifier(self.conv_block_2(self.conv_block_1(x))) # &lt;- leverage the benefits of operator fusion\n</code></pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u65b9\u6cd5\u200b\u5bfc\u5165\u200b TinyVGG \u200b\u6a21\u578b\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6bcf\u6b21\u200b\u90fd\u200b\u4ece\u5934\u5f00\u59cb\u200b\u7f16\u5199\u200b TinyVGG \u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>import torch\n# Import model_builder.py\nfrom going_modular import model_builder\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Instantiate an instance of the model from the \"model_builder.py\" script\ntorch.manual_seed(42)\nmodel = model_builder.TinyVGG(input_shape=3,\n                              hidden_units=10, \n                              output_shape=len(class_names)).to(device)\n</code></pre>"},{"location":"05_pytorch_going_modular/#4-train_step-test_step-train","title":"4. \u200b\u521b\u5efa\u200b <code>train_step()</code> \u200b\u548c\u200b <code>test_step()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u7528\u200b <code>train()</code> \u200b\u7ec4\u5408\u200b\u5b83\u4eec","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b notebook 04 \u200b\u4e2d\u200b\u7f16\u5199\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\uff1a</p> <ol> <li><code>train_step()</code> - \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b <code>DataLoader</code>\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b <code>DataLoader</code> \u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</li> <li><code>test_step()</code> - \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b <code>DataLoader</code> \u200b\u548c\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b <code>DataLoader</code> \u200b\u4e0a\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u3002</li> <li><code>train()</code> - \u200b\u9488\u5bf9\u200b\u7ed9\u5b9a\u200b\u7684\u200b epoch \u200b\u6570\u200b\uff0c\u200b\u6267\u884c\u200b 1 \u200b\u548c\u200b 2\uff0c\u200b\u5e76\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u7ed3\u679c\u200b\u5b57\u5178\u200b\u3002</li> </ol> <p>\u200b\u7531\u4e8e\u200b\u8fd9\u4e9b\u200b\u5c06\u200b\u662f\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u7684\u200b \u200b\u5f15\u64ce\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5168\u90e8\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>engine.py</code> \u200b\u7684\u200b Python \u200b\u811a\u672c\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>%%writefile going_modular/engine.py</code> \u200b\u547d\u4ee4\u200b\uff1a</p> engine.py<pre><code>%%writefile going_modular/engine.py\n\"\"\"\n\u200b\u5305\u542b\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u51fd\u6570\u200b\u3002\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\u7684\u200b\u4e00\u4e2a\u200b epoch\u3002\n\n  \u200b\u5c06\u200b\u76ee\u6807\u200b PyTorch \u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u7136\u540e\u200b\n  \u200b\u6267\u884c\u200b\u6240\u6709\u200b\u5fc5\u9700\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6b65\u9aa4\u200b\uff08\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u3001\u200b\u635f\u5931\u200b\u8ba1\u7b97\u200b\u3001\u200b\u4f18\u5316\u200b\u5668\u200b\u6b65\u9aa4\u200b\uff09\u3002\n\n  Args:\n    model: \u200b\u8981\u200b\u8bad\u7ec3\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u3002\n    dataloader: \u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u7684\u200b DataLoader \u200b\u5b9e\u4f8b\u200b\u3002\n    loss_fn: \u200b\u8981\u200b\u6700\u5c0f\u5316\u200b\u7684\u200b PyTorch \u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\n    optimizer: \u200b\u5e2e\u52a9\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b PyTorch \u200b\u4f18\u5316\u200b\u5668\u200b\u3002\n    device: \u200b\u8ba1\u7b97\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\uff08\u200b\u4f8b\u5982\u200b \"cuda\" \u200b\u6216\u200b \"cpu\"\uff09\u3002\n\n  Returns:\n    \u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200b\u635f\u5931\u200b\u548c\u200b\u8bad\u7ec3\u200b\u51c6\u786e\u5ea6\u200b\u6307\u6807\u200b\u7684\u200b\u5143\u7ec4\u200b\u3002\n    \u200b\u5f62\u5f0f\u200b\u4e3a\u200b (train_loss, train_accuracy)\u3002\u200b\u4f8b\u5982\u200b\uff1a\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  for batch, (X, y) in enumerate(dataloader):\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device) -&gt; Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Turn on inference context manager\n  with torch.inference_mode():\n      # Loop through DataLoader batches\n      for batch, (X, y) in enumerate(dataloader):\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n  \"\"\"\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\u3002\n\n  \u200b\u901a\u8fc7\u200b train_step() \u200b\u548c\u200b test_step() \u200b\u51fd\u6570\u200b\u5bf9\u200b\u76ee\u6807\u200b PyTorch \u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u82e5\u5e72\u200b\u8f6e\u6b21\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\uff0c\n\u200b\u5728\u200b\u540c\u4e00\u4e2a\u200b\u8f6e\u6b21\u200b\u5faa\u73af\u200b\u4e2d\u200b\u5b8c\u6210\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u3002\n\n  \u200b\u5728\u200b\u6574\u4e2a\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u8ba1\u7b97\u200b\u3001\u200b\u6253\u5370\u200b\u5e76\u200b\u5b58\u50a8\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u3002\n\n  Args:\n    model: \u200b\u9700\u8981\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u3002\n    train_dataloader: \u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u7684\u200b DataLoader \u200b\u5b9e\u4f8b\u200b\u3002\n    test_dataloader: \u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u6d4b\u8bd5\u200b\u7684\u200b DataLoader \u200b\u5b9e\u4f8b\u200b\u3002\n    optimizer: \u200b\u5e2e\u52a9\u200b\u6700\u5c0f\u5316\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b PyTorch \u200b\u4f18\u5316\u200b\u5668\u200b\u3002\n    loss_fn: \u200b\u7528\u4e8e\u200b\u8ba1\u7b97\u200b\u4e24\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u635f\u5931\u200b\u7684\u200b PyTorch \u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002\n    epochs: \u200b\u8868\u793a\u200b\u8bad\u7ec3\u200b\u8f6e\u6b21\u200b\u7684\u200b\u6574\u6570\u200b\u3002\n    device: \u200b\u8ba1\u7b97\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\uff08\u200b\u4f8b\u5982\u200b \"cuda\" \u200b\u6216\u200b \"cpu\"\uff09\u3002\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs)):\n      train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n      test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n\n  # Return the filled results at the end of the epochs\n  return results\n</code></pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b <code>engine.py</code> \u200b\u811a\u672c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u4ece\u4e2d\u200b\u5bfc\u5165\u200b\u51fd\u6570\u200b\uff1a</p> <pre><code># Import engine.py\nfrom going_modular import engine\n\n# Use train() by calling it from engine.py\nengine.train(...)\n</code></pre>"},{"location":"05_pytorch_going_modular/#5-utilspy","title":"5. \u200b\u521b\u5efa\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u51fd\u6570\u200b\uff08<code>utils.py</code>\uff09","text":"<p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6216\u200b\u8bad\u7ec3\u200b\u540e\u200b\uff0c\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u591a\u6b21\u200b\u7f16\u5199\u200b\u4e86\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u51fd\u6570\u200b\u5e76\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u662f\u200b\u5408\u7406\u200b\u7684\u200b\u3002</p> <p>\u200b\u5c06\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u5b58\u50a8\u200b\u5728\u200b\u540d\u4e3a\u200b <code>utils.py</code> \u200b\u7684\u200b\u6587\u4ef6\u200b\u4e2d\u662f\u200b\u4e00\u79cd\u200b\u5e38\u89c1\u200b\u505a\u6cd5\u200b\uff08utilities \u200b\u7684\u200b\u7f29\u5199\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b <code>save_model()</code> \u200b\u51fd\u6570\u200b\u4fdd\u5b58\u200b\u5230\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>utils.py</code> \u200b\u7684\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b\u547d\u4ee4\u200b <code>%%writefile going_modular/utils.py</code>\uff1a</p> utils.py<pre><code>%%writefile going_modular/utils.py\n\"\"\"\nContains various utility functions for PyTorch model training and saving.\n\"\"\"\nimport torch\nfrom pathlib import Path\n\ndef save_model(model: torch.nn.Module,\n               target_dir: str,\n               model_name: str):\n  \"\"\"Saves a PyTorch model to a target directory.\n\n  Args:\n    model: A target PyTorch model to save.\n    target_dir: A directory for saving the model to.\n    model_name: A filename for the saved model. Should include\n      either \".pth\" or \".pt\" as the file extension.\n\n  Example usage:\n    save_model(model=model_0,\n               target_dir=\"models\",\n               model_name=\"05_going_modular_tingvgg_model.pth\")\n  \"\"\"\n  # Create target directory\n  target_dir_path = Path(target_dir)\n  target_dir_path.mkdir(parents=True,\n                        exist_ok=True)\n\n  # Create model save path\n  assert model_name.endswith(\".pth\") or model_name.endswith(\".pt\"), \"model_name should end with '.pt' or '.pth'\"\n  model_save_path = target_dir_path / model_name\n\n  # Save the model state_dict()\n  print(f\"[INFO] Saving model to: {model_save_path}\")\n  torch.save(obj=model.state_dict(),\n             f=model_save_path)\n</code></pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u200b\u4f7f\u7528\u200b <code>save_model()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u91cd\u65b0\u200b\u7f16\u5199\u200b\u4e00\u904d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5bfc\u5165\u200b\u5b83\u200b\u5e76\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u4f7f\u7528\u200b\uff1a</p> <pre><code># Import utils.py\nfrom going_modular import utils\n\n# Save a model to file\nsave_model(model=...\n           target_dir=...,\n           model_name=...)\n</code></pre>"},{"location":"05_pytorch_going_modular/#6-trainpy","title":"6. \u200b\u8bad\u7ec3\u200b\u3001\u200b\u8bc4\u4f30\u200b\u5e76\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\uff08<code>train.py</code>\uff09","text":"<p>\u200b\u5982\u524d\u6240\u8ff0\u200b\uff0c\u200b\u4f60\u200b\u7ecf\u5e38\u200b\u4f1a\u200b\u9047\u5230\u200b\u5c06\u200b\u6240\u6709\u200b\u529f\u80fd\u200b\u6574\u5408\u200b\u5728\u200b\u4e00\u4e2a\u200b <code>train.py</code> \u200b\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b PyTorch \u200b\u4ed3\u5e93\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u6587\u4ef6\u200b\u672c\u8d28\u200b\u4e0a\u200b\u662f\u200b\u5728\u200b\u8bf4\u200b\u201c\u200b\u4f7f\u7528\u200b\u4efb\u4f55\u200b\u53ef\u7528\u200b\u6570\u636e\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u201d\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b <code>train.py</code> \u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ed3\u5408\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u5176\u4ed6\u200b Python \u200b\u811a\u672c\u200b\u7684\u200b\u6240\u6709\u200b\u529f\u80fd\u200b\uff0c\u200b\u5e76\u7528\u200b\u5b83\u200b\u6765\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u6765\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>python train.py\n</code></pre> <p>\u200b\u4e3a\u4e86\u200b\u521b\u5efa\u200b <code>train.py</code>\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6309\u7167\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u8fdb\u884c\u200b\uff1a</p> <ol> <li>\u200b\u5bfc\u5165\u200b\u5404\u79cd\u200b\u4f9d\u8d56\u200b\u9879\u200b\uff0c\u200b\u5373\u200b <code>torch</code>\u3001<code>os</code>\u3001<code>torchvision.transforms</code> \u200b\u4ee5\u53ca\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u811a\u672c\u200b\uff0c\u200b\u5305\u62ec\u200b <code>data_setup</code>\u3001<code>engine</code>\u3001<code>model_builder</code>\u3001<code>utils</code>\u3002</li> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7531\u4e8e\u200b <code>train.py</code> \u200b\u5c06\u200b\u4f4d\u4e8e\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\u5185\u90e8\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>import ...</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>from going_modular import ...</code> \u200b\u6765\u200b\u5bfc\u5165\u200b\u5176\u4ed6\u200b\u6a21\u5757\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u5404\u79cd\u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3001\u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b\u3001\u200b\u5b66\u4e60\u200b\u7387\u200b\u548c\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u91cf\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u672a\u6765\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b Python \u200b\u7684\u200b <code>argparse</code> \u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\uff09\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u5fc5\u8981\u200b\u7684\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>data_setup.py</code> \u200b\u521b\u5efa\u200b DataLoader\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>model_builder.py</code> \u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>engine.py</code> \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>utils.py</code> \u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b\u5355\u5143\u683c\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b <code>%%writefile going_modular/train.py</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u6587\u4ef6\u200b:</p> train.py<pre><code>%%writefile going_modular/train.py\n\"\"\"\nTrains a PyTorch image classification model using device-agnostic code.\n\"\"\"\n\nimport os\nimport torch\nimport data_setup, engine, model_builder, utils\n\nfrom torchvision import transforms\n\n# Setup hyperparameters\nNUM_EPOCHS = 5\nBATCH_SIZE = 32\nHIDDEN_UNITS = 10\nLEARNING_RATE = 0.001\n\n# Setup directories\ntrain_dir = \"data/pizza_steak_sushi/train\"\ntest_dir = \"data/pizza_steak_sushi/test\"\n\n# Setup target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Create transforms\ndata_transform = transforms.Compose([\n  transforms.Resize((64, 64)),\n  transforms.ToTensor()\n])\n\n# Create DataLoaders with help from data_setup.py\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=data_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create model with help from model_builder.py\nmodel = model_builder.TinyVGG(\n    input_shape=3,\n    hidden_units=HIDDEN_UNITS,\n    output_shape=len(class_names)\n).to(device)\n\n# Set loss and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Start training with help from engine.py\nengine.train(model=model,\n             train_dataloader=train_dataloader,\n             test_dataloader=test_dataloader,\n             loss_fn=loss_fn,\n             optimizer=optimizer,\n             epochs=NUM_EPOCHS,\n             device=device)\n\n# Save the model with help from utils.py\nutils.save_model(model=model,\n                 target_dir=\"models\",\n                 model_name=\"05_going_modular_script_mode_tinyvgg_model.pth\")\n</code></pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5728\u200b\u547d\u4ee4\u884c\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u6765\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b PyTorch \u200b\u6a21\u578b\u200b\uff1a</p> <pre><code>python train.py\n</code></pre> <p>\u200b\u8fd9\u6837\u200b\u505a\u200b\u5c06\u200b\u5229\u7528\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u4ee3\u7801\u200b\u811a\u672c\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u613f\u610f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b <code>train.py</code> \u200b\u6587\u4ef6\u200b\uff0c\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>argparse</code> \u200b\u6a21\u5757\u200b\u6765\u200b\u5904\u7406\u200b\u53c2\u6570\u200b\u6807\u5fd7\u200b\u8f93\u5165\u200b\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u5141\u8bb8\u200b\u6211\u4eec\u200b\u63d0\u4f9b\u200b\u4e0d\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u4e4b\u524d\u200b\u8ba8\u8bba\u200b\u7684\u200b\u90a3\u6837\u200b\uff1a</p> <pre><code>python train.py --model MODEL_NAME --batch_size BATCH_SIZE --lr LEARNING_RATE --num_epochs NUM_EPOCHS\n</code></pre>"},{"location":"05_pytorch_going_modular/#_9","title":"\u7ec3\u4e60","text":"<p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>05 \u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b</li> <li>05 \u200b\u7ec3\u4e60\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b<ul> <li>YouTube \u200b\u4e0a\u200b\u7684\u200b 05 \u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\u5b9e\u65f6\u200b\u7f16\u7801\u200b\u6f14\u793a\u200b</li> </ul> </li> </ul> <p>\u200b\u7ec3\u4e60\u200b\uff1a</p> <ol> <li>\u200b\u5c06\u200b\u83b7\u53d6\u6570\u636e\u200b\u7684\u200b\u4ee3\u7801\u200b\uff08\u200b\u6765\u81ea\u200b\u4e0a\u9762\u200b\u7684\u200b\u7b2c\u200b 1 \u200b\u8282\u200b \u200b\u83b7\u53d6\u6570\u636e\u200b\uff09\u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u811a\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>get_data.py</code>\u3002<ul> <li>\u200b\u5f53\u200b\u4f60\u200b\u8fd0\u884c\u200b\u811a\u672c\u200b <code>python get_data.py</code> \u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u5e94\u8be5\u200b\u68c0\u67e5\u6570\u636e\u200b\u662f\u5426\u200b\u5df2\u7ecf\u200b\u5b58\u5728\u200b\u5e76\u200b\u8df3\u200b\u8fc7\u200b\u4e0b\u8f7d\u200b\uff08\u200b\u5982\u679c\u200b\u5b58\u5728\u200b\uff09\u3002</li> <li>\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u4e0b\u8f7d\u200b\u6210\u529f\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u4ece\u200b <code>data</code> \u200b\u76ee\u5f55\u200b\u8bbf\u95ee\u200b <code>pizza_steak_sushi</code> \u200b\u56fe\u50cf\u200b\u3002</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>argparse</code> \u200b\u6a21\u5757\u200b \u200b\u6765\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u53d1\u9001\u200b <code>train.py</code> \u200b\u81ea\u5b9a\u4e49\u200b\u8d85\u200b\u53c2\u6570\u503c\u200b\u3002<ul> <li>\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u53c2\u6570\u200b\uff1a<ul> <li>\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b</li> <li>\u200b\u5b66\u4e60\u200b\u7387\u200b</li> <li>\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b</li> <li>\u200b\u8bad\u7ec3\u200b\u7684\u200b\u5468\u671f\u200b\u6570\u200b</li> <li>TinyVGG \u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u200b</li> </ul> </li> <li>\u200b\u4fdd\u6301\u200b\u6bcf\u4e2a\u200b\u53c2\u6570\u200b\u7684\u200b\u9ed8\u8ba4\u503c\u200b\u4e3a\u200b\u5176\u200b\u5f53\u524d\u200b\u503c\u200b\uff08\u200b\u5982\u200b\u7b14\u8bb0\u672c\u200b 05 \u200b\u4e2d\u200b\u6240\u793a\u200b\uff09\u3002</li> <li>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u8fd0\u884c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u6765\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b 0.003 \u200b\u4e14\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b 64 \u200b\u7684\u200b TinyVGG \u200b\u6a21\u578b\u200b\uff0c\u200b\u8bad\u7ec3\u200b 20 \u200b\u4e2a\u200b\u5468\u671f\u200b\uff1a<code>python train.py --learning_rate 0.003 --batch_size 64 --num_epochs 20</code>\u3002</li> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7531\u4e8e\u200b <code>train.py</code> \u200b\u5229\u7528\u200b\u4e86\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u5176\u4ed6\u200b\u811a\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>model_builder.py</code>\u3001<code>utils.py</code> \u200b\u548c\u200b <code>engine.py</code>\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u5b83\u4eec\u200b\u4e5f\u200b\u53ef\u7528\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b GitHub \u200b\u4e0a\u200b\u7684\u200b <code>going_modular</code> \u200b\u6587\u4ef6\u5939\u200b \u200b\u4e2d\u200b\u627e\u5230\u200b\u8fd9\u4e9b\u200b\u811a\u672c\u200b\u3002</li> </ul> </li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u9884\u6d4b\u200b\u811a\u672c\u200b\uff08\u200b\u4f8b\u5982\u200b <code>predict.py</code>\uff09\uff0c\u200b\u4f7f\u7528\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u7ed9\u5b9a\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u7684\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002<ul> <li>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u8fd0\u884c\u200b\u547d\u4ee4\u200b <code>python predict.py some_image.jpeg</code>\uff0c\u200b\u5e76\u200b\u8ba9\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8fd4\u56de\u200b\u5176\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</li> <li>\u200b\u8981\u200b\u67e5\u770b\u200b\u793a\u4f8b\u200b\u9884\u6d4b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200b\u7b14\u8bb0\u672c\u200b 04 \u200b\u4e2d\u200b\u7684\u200b \u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u90e8\u5206\u200b\u3002</li> <li>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u8fd8\u200b\u9700\u8981\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u6765\u200b\u52a0\u8f7d\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> </ul> </li> </ol>"},{"location":"05_pytorch_going_modular/#_10","title":"\u989d\u5916\u200b\u8bfe\u7a0b","text":"<ul> <li>\u200b\u8981\u200b\u4e86\u89e3\u200b\u6709\u5173\u200b\u6784\u5efa\u200b Python \u200b\u9879\u76ee\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200b Real Python \u200b\u7684\u200b\u6307\u5357\u200b Python \u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u5e03\u5c40\u200b\u3002</li> <li>\u200b\u8981\u200b\u4e86\u89e3\u200b\u6709\u5173\u200b\u6837\u5f0f\u200b\u5316\u200b PyTorch \u200b\u4ee3\u7801\u200b\u7684\u200b\u60f3\u6cd5\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200b Igor Susmelj \u200b\u7684\u200b PyTorch \u200b\u6837\u5f0f\u200b\u6307\u5357\u200b\uff08\u200b\u672c\u7ae0\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u6837\u5f0f\u200b\u57fa\u4e8e\u200b\u6b64\u200b\u6307\u5357\u200b + \u200b\u5404\u79cd\u200b\u7c7b\u4f3c\u200b\u7684\u200b PyTorch \u200b\u4ed3\u5e93\u200b\uff09\u3002</li> <li>\u200b\u8981\u200b\u67e5\u770b\u200b\u7531\u200b PyTorch \u200b\u56e2\u961f\u200b\u7f16\u5199\u200b\u7684\u200b <code>train.py</code> \u200b\u811a\u672c\u200b\u548c\u200b\u5176\u4ed6\u200b\u5404\u79cd\u200b PyTorch \u200b\u811a\u672c\u200b\uff0c\u200b\u4ee5\u200b\u8bad\u7ec3\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200b\u4ed6\u4eec\u200b\u7684\u200b GitHub \u200b\u4e0a\u200b\u7684\u200b <code>classification</code> \u200b\u4ed3\u5e93\u200b\u3002</li> </ul>"},{"location":"06_pytorch_transfer_learning/","title":"06. PyTorch\u200b\u8fc1\u79fb\u200b\u5b66\u4e60","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u4f60\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u5f00\u542f\u200b GPU\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u901a\u8fc7\u200b <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code> \u200b\u6765\u200b\u5f00\u542f\u200b\u4e00\u4e2a\u200b GPU \u200b\u4e86\u200b\u3002</p> In\u00a0[3]: Copied! <pre># Setup device agnostic code\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> # Setup device agnostic code device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\n# Setup path to data folder\ndata_path = Path(\"data/\")\nimage_path = data_path / \"pizza_steak_sushi\"\n\n# If the image folder doesn't exist, download it and prepare it... \nif image_path.is_dir():\n    print(f\"{image_path} directory exists.\")\nelse:\n    print(f\"Did not find {image_path} directory, creating one...\")\n    image_path.mkdir(parents=True, exist_ok=True)\n    \n    # Download pizza, steak, sushi data\n    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n        print(\"Downloading pizza, steak, sushi data...\")\n        f.write(request.content)\n\n    # Unzip pizza, steak, sushi data\n    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n        print(\"Unzipping pizza, steak, sushi data...\") \n        zip_ref.extractall(image_path)\n\n    # Remove .zip file\n    os.remove(data_path / \"pizza_steak_sushi.zip\")\n</pre> import os import zipfile  from pathlib import Path  import requests  # Setup path to data folder data_path = Path(\"data/\") image_path = data_path / \"pizza_steak_sushi\"  # If the image folder doesn't exist, download it and prepare it...  if image_path.is_dir():     print(f\"{image_path} directory exists.\") else:     print(f\"Did not find {image_path} directory, creating one...\")     image_path.mkdir(parents=True, exist_ok=True)          # Download pizza, steak, sushi data     with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:         request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")         print(\"Downloading pizza, steak, sushi data...\")         f.write(request.content)      # Unzip pizza, steak, sushi data     with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:         print(\"Unzipping pizza, steak, sushi data...\")          zip_ref.extractall(image_path)      # Remove .zip file     os.remove(data_path / \"pizza_steak_sushi.zip\") <pre>data/pizza_steak_sushi directory exists.\n</pre> <p>\u200b\u5f88\u200b\u597d\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e4b\u524d\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5373\u200b\u4e00\u7cfb\u5217\u200b\u4ee5\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\u5448\u73b0\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u6307\u5411\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\u7684\u200b\u8def\u5f84\u200b\u3002</p> In\u00a0[5]: Copied! <pre># Setup Dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup Dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[6]: Copied! <pre># Create a transforms pipeline manually (required for torchvision &lt; 0.13)\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n    transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1 \n    transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n                         std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n])\n</pre> # Create a transforms pipeline manually (required for torchvision &lt; 0.13) manual_transforms = transforms.Compose([     transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)     transforms.ToTensor(), # 2. Turn image values to between 0 &amp; 1      transforms.Normalize(mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)                          std=[0.229, 0.224, 0.225]) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel), ]) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\u53d8\u6362\u200b\u6765\u200b\u51c6\u5907\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b05. PyTorch Going Modular Part 2\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>data_setup.py</code>\u200b\u811a\u672c\u200b\u4e2d\u200b\u7684\u200b<code>create_dataloaders</code>\u200b\u51fd\u6570\u200b\u6765\u200b\u521b\u5efa\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b<code>batch_size=32</code>\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6bcf\u6b21\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b32\u200b\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b\u5c0f\u200b\u6279\u6b21\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u521b\u5efa\u200b\u7684\u200b\u53d8\u6362\u200b\u7ba1\u9053\u200b\u6765\u200b\u53d8\u6362\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b<code>transform=manual_transforms</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u5305\u542b\u200b\u4e86\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u53d8\u6362\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u4f7f\u7528\u200b\u8fd9\u79cd\u200b\u98ce\u683c\u200b\u7684\u200b\u8d44\u6e90\u200b\u3002\u200b\u540c\u6837\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u7531\u4e8e\u200b\u8fd9\u4e9b\u200b\u53d8\u6362\u200b\u662f\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4e5f\u200b\u662f\u200b\u65e0\u9650\u200b\u53ef\u200b\u5b9a\u5236\u200b\u7684\u200b\u3002\u200b\u6240\u4ee5\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u5728\u200b\u4f60\u200b\u7684\u200b\u53d8\u6362\u200b\u7ba1\u9053\u200b\u4e2d\u200b\u5305\u542b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u6280\u672f\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u505a\u5230\u200b\u3002</p> In\u00a0[7]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=manual_transforms, # resize, convert images to between 0 &amp; 1 and normalize them                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a3a60&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa9429a37c0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Get a set of pretrained model weights\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\nweights\n</pre> # Get a set of pretrained model weights weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet weights Out[8]: <pre>EfficientNet_B0_Weights.IMAGENET1K_V1</pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8981\u200b\u8bbf\u95ee\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b <code>weights</code> \u200b\u76f8\u5173\u8054\u200b\u7684\u200b\u53d8\u6362\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>transforms()</code> \u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u5728\u200b\u8bf4\u200b\uff1a\u201c\u200b\u83b7\u53d6\u200b\u7528\u4e8e\u200b\u5728\u200b ImageNet \u200b\u4e0a\u200b\u8bad\u7ec3\u200b <code>EfficientNet_B0_Weights</code> \u200b\u7684\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\u3002\u201d</p> In\u00a0[9]: Copied! <pre># Get the transforms used to create our pretrained weights\nauto_transforms = weights.transforms()\nauto_transforms\n</pre> # Get the transforms used to create our pretrained weights auto_transforms = weights.transforms() auto_transforms Out[9]: <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)</pre> <p>\u200b\u6ce8\u610f\u200b<code>auto_transforms</code>\u200b\u4e0e\u200b<code>manual_transforms</code>\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u552f\u4e00\u200b\u7684\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b<code>auto_transforms</code>\u200b\u662f\u200b\u968f\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u4e00\u8d77\u200b\u63d0\u4f9b\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b<code>manual_transforms</code>\u3002</p> <p>\u200b\u901a\u8fc7\u200b<code>weights.transforms()</code>\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u7684\u200b\u597d\u5904\u200b\u5728\u4e8e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u786e\u4fdd\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u7684\u200b\u7f3a\u70b9\u200b\u662f\u200b\u7f3a\u4e4f\u200b\u5b9a\u5236\u200b\u6027\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u50cf\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>auto_transforms</code>\u200b\u901a\u8fc7\u200b<code>create_dataloaders()</code>\u200b\u521b\u5efa\u200bDataLoader\u3002</p> In\u00a0[10]: Copied! <pre># Create training and testing DataLoaders as well as get a list of class names\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                               test_dir=test_dir,\n                                                                               transform=auto_transforms, # perform same data transforms on our own data as the pretrained model\n                                                                               batch_size=32) # set mini-batch size to 32\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Create training and testing DataLoaders as well as get a list of class names train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                test_dir=test_dir,                                                                                transform=auto_transforms, # perform same data transforms on our own data as the pretrained model                                                                                batch_size=32) # set mini-batch size to 32  train_dataloader, test_dataloader, class_names Out[10]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951460&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7fa942951550&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[11]: Copied! <pre># OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13)\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)\n\n# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights \nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n#model # uncomment to output (it's very long)\n</pre> # OLD: Setup the model with pretrained weights and send it to the target device (this was prior to torchvision v0.13) # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD method (with pretrained=True)  # NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights  model = torchvision.models.efficientnet_b0(weights=weights).to(device)  #model # uncomment to output (it's very long) <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5728\u200b <code>torchvision</code> \u200b\u7684\u200b\u65e9\u671f\u200b\u7248\u672c\u200b\u4e2d\u200b\uff0c\u200b\u60a8\u200b\u4f1a\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1a</p> <p><code>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)</code></p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u200b\u4f7f\u7528\u200b <code>torchvision</code> v0.13+ \u200b\u8fd0\u884c\u200b\u6b64\u200b\u4ee3\u7801\u200b\u65f6\u200b\uff0c\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u5982\u4e0b\u200b\u9519\u8bef\u200b\uff1a</p> <p><code>UserWarning: \u200b\u53c2\u6570\u200b 'pretrained' \u200b\u81ea\u200b 0.13 \u200b\u7248\u672c\u200b\u8d77\u200b\u5df2\u5f03\u200b\u7528\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5728\u200b 0.15 \u200b\u7248\u672c\u200b\u4e2d\u200b\u79fb\u9664\u200b\uff0c\u200b\u8bf7\u200b\u6539\u7528\u200b 'weights'\u3002</code></p> <p>\u200b\u4ee5\u53ca\u200b...</p> <p><code>UserWarning: \u200b\u81ea\u200b 0.13 \u200b\u7248\u672c\u200b\u8d77\u200b\uff0c\u200b\u9664\u4e86\u200b\u6743\u91cd\u200b\u679a\u4e3e\u200b\u6216\u200b None \u200b\u4e4b\u5916\u200b\u7684\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\u5df2\u5f03\u200b\u7528\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5728\u200b 0.15 \u200b\u7248\u672c\u200b\u4e2d\u200b\u79fb\u9664\u200b\u3002\u200b\u5f53\u524d\u200b\u884c\u4e3a\u200b\u7b49\u540c\u4e8e\u200b\u4f20\u9012\u200b weights=EfficientNet_B0_Weights.IMAGENET1K_V1\u3002\u200b\u60a8\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b weights=EfficientNet_B0_Weights.DEFAULT \u200b\u6765\u200b\u83b7\u53d6\u200b\u6700\u65b0\u200b\u7684\u200b\u6743\u91cd\u200b\u3002</code></p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6253\u5370\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f1a\u200b\u5f97\u5230\u200b\u7c7b\u4f3c\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u5185\u5bb9\u200b\uff1a</p> <p>\u200b\u975e\u5e38\u200b\u975e\u5e38\u200b\u591a\u200b\u7684\u200b\u5c42\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u597d\u5904\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u5229\u7528\u200b\u4e16\u754c\u200b\u4e0a\u200b\u4e00\u4e9b\u200b\u6700\u200b\u4f18\u79c0\u200b\u7684\u200b\u5de5\u7a0b\u5e08\u200b\u5df2\u7ecf\u200b\u7cbe\u5fc3\u8bbe\u8ba1\u200b\u7684\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5e94\u7528\u200b\u4e8e\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>efficientnet_b0</code> \u200b\u4e3b\u8981\u200b\u5305\u542b\u200b\u4e09\u4e2a\u200b\u90e8\u5206\u200b\uff1a</p> <ol> <li><code>features</code> - \u200b\u4e00\u7cfb\u5217\u200b\u5377\u79ef\u200b\u5c42\u200b\u548c\u200b\u5176\u4ed6\u200b\u5404\u79cd\u200b\u6fc0\u6d3b\u200b\u5c42\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b66\u4e60\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u7684\u200b\u57fa\u7ebf\u200b\u8868\u793a\u200b\uff08\u200b\u8fd9\u4e2a\u200b\u57fa\u7ebf\u200b\u8868\u793a\u200b/\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u7279\u5f81\u200b\u6216\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff0c\"\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\u5b66\u4e60\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0d\u540c\u200b\u7279\u5f81\u200b\"\uff09\u3002</li> <li><code>avgpool</code> - \u200b\u53d6\u200b <code>features</code> \u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7279\u5f81\u5411\u91cf\u200b\u3002</li> <li><code>classifier</code> - \u200b\u5c06\u200b\u7279\u5f81\u5411\u91cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e0e\u200b\u6240\u200b\u9700\u200b\u8f93\u51fa\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\u76f8\u540c\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5411\u91cf\u200b\uff08\u200b\u7531\u4e8e\u200b <code>efficientnet_b0</code> \u200b\u662f\u200b\u5728\u200b ImageNet \u200b\u4e0a\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u800c\u200b ImageNet \u200b\u6709\u200b 1000 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b <code>out_features=1000</code>\uff09\u3002</li> </ol> In\u00a0[12]: Copied! <pre># Print a summary using torchinfo (uncomment for actual output)\nsummary(model=model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # Print a summary using torchinfo (uncomment for actual output) summary(model=model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"         # col_names=[\"input_size\"], # uncomment for smaller output         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] )  Out[12]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 1000]           --                   True\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   True\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   864                  True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   64                   True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   1,448                True\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     6,004                True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     10,710               True\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     15,350               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     31,290               True\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     37,130               True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     102,900              True\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    126,004              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    208,572              True\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      262,492              True\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      587,952              True\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   True\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      717,232              True\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   True\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     409,600              True\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     2,560                True\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 1000]           --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 1000]           1,281,000            True\n============================================================================================================================================\nTotal params: 5,288,548\nTrainable params: 5,288,548\nNon-trainable params: 0\nTotal mult-adds (G): 12.35\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.35\nParams size (MB): 21.15\nEstimated Total Size (MB): 3492.77\n============================================================================================================================================</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u8fd9\u200b\u53ef\u200b\u771f\u662f\u200b\u4e2a\u200b\u5927\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u4ece\u200b\u6c47\u603b\u200b\u8f93\u51fa\u200b\u6765\u770b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u968f\u7740\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u901a\u8fc7\u200b\u6a21\u578b\u200b\uff0c\u200b\u5404\u79cd\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7684\u200b\u53d8\u5316\u200b\u3002</p> <p>\u200b\u800c\u4e14\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u603b\u200b\u53c2\u6570\u200b\uff08\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\uff09\u200b\u6765\u200b\u8bc6\u522b\u200b\u6211\u4eec\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u540c\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u4f5c\u4e3a\u200b\u53c2\u8003\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b TinyVGG \u200b\u6709\u200b 8,083 \u200b\u4e2a\u200b\u53c2\u6570\u200b\uff0c\u200b\u800c\u200b <code>efficientnet_b0</code> \u200b\u6709\u200b 5,288,548 \u200b\u4e2a\u200b\u53c2\u6570\u200b\uff0c\u200b\u589e\u52a0\u200b\u4e86\u200b\u7ea6\u200b 654 \u200b\u500d\u200b\uff01</p> <p>\u200b\u4f60\u200b\u89c9\u5f97\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6027\u80fd\u200b\u4f1a\u200b\u66f4\u597d\u200b\u5417\u200b\uff1f</p> In\u00a0[13]: Copied! <pre># Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False for param in model.features.parameters():     param.requires_grad = False <p>\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5c42\u200b\u5df2\u200b\u51bb\u7ed3\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u8c03\u6574\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\u6216\u200b <code>classifier</code> \u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u76ee\u524d\u200b\u6211\u4eec\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b <code>out_features=1000</code>\uff0c\u200b\u56e0\u4e3a\u200b ImageNet \u200b\u6709\u200b 1000 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e76\u200b\u6ca1\u6709\u200b 1000 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u6709\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\uff1a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u4e00\u7cfb\u5217\u200b\u65b0\u5c42\u200b\u6765\u200b\u6539\u53d8\u200b\u6a21\u578b\u200b\u7684\u200b <code>classifier</code> \u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u5f53\u524d\u200b\u7684\u200b <code>classifier</code> \u200b\u7531\u200b\u4ee5\u4e0b\u200b\u90e8\u5206\u200b\u7ec4\u6210\u200b\uff1a</p> <pre><code>(classifier): Sequential(\n    (0): Dropout(p=0.2, inplace=True)\n    (1): Linear(in_features=1280, out_features=1000, bias=True)\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4fdd\u6301\u200b <code>Dropout</code> \u200b\u5c42\u200b\u4e0d\u53d8\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torch.nn.Dropout(p=0.2, inplace=True)</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a Dropout \u200b\u5c42\u200b \u200b\u4ee5\u200b\u6982\u7387\u200b <code>p</code> \u200b\u968f\u673a\u200b\u79fb\u9664\u200b\u4e24\u5c42\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8fde\u63a5\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b <code>p=0.2</code>\uff0c\u200b\u6bcf\u6b21\u200b\u4f20\u9012\u200b\u65f6\u4f1a\u200b\u968f\u673a\u200b\u79fb\u9664\u200b 20% \u200b\u7684\u200b\u8fde\u63a5\u200b\u3002\u200b\u8fd9\u79cd\u200b\u505a\u6cd5\u200b\u65e8\u5728\u200b\u901a\u8fc7\u200b\u786e\u4fdd\u200b\u5269\u4f59\u200b\u8fde\u63a5\u5b66\u4e60\u200b\u7279\u5f81\u200b\u4ee5\u200b\u8865\u507f\u200b\u5176\u4ed6\u200b\u8fde\u63a5\u200b\u7684\u200b\u79fb\u9664\u200b\uff08\u200b\u5e0c\u671b\u200b\u8fd9\u4e9b\u200b\u5269\u4f59\u200b\u7279\u5f81\u200b\u66f4\u200b\u901a\u7528\u200b\uff09\u200b\u6765\u200b\u5e2e\u52a9\u200b\u6b63\u5219\u200b\u5316\u200b\uff08\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\uff09\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4fdd\u6301\u200b <code>Linear</code> \u200b\u8f93\u51fa\u200b\u5c42\u200b\u7684\u200b <code>in_features=1280</code>\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5c06\u200b <code>out_features</code> \u200b\u503c\u200b\u66f4\u200b\u6539\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>class_names</code> \u200b\u7684\u200b\u957f\u5ea6\u200b\uff08<code>len(['pizza', 'steak', 'sushi']) = 3</code>\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u65b0\u200b <code>classifier</code> \u200b\u5c42\u5e94\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b <code>model</code> \u200b\u4f4d\u4e8e\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p> In\u00a0[14]: Copied! <pre># Set the manual seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Get the length of class_names (one output unit for each class)\noutput_shape = len(class_names)\n\n# Recreate the classifier layer and seed it to the target device\nmodel.classifier = torch.nn.Sequential(\n    torch.nn.Dropout(p=0.2, inplace=True), \n    torch.nn.Linear(in_features=1280, \n                    out_features=output_shape, # same number of output units as our number of classes\n                    bias=True)).to(device)\n</pre> # Set the manual seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Get the length of class_names (one output unit for each class) output_shape = len(class_names)  # Recreate the classifier layer and seed it to the target device model.classifier = torch.nn.Sequential(     torch.nn.Dropout(p=0.2, inplace=True),      torch.nn.Linear(in_features=1280,                      out_features=output_shape, # same number of output units as our number of classes                     bias=True)).to(device) <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u8f93\u51fa\u200b\u5c42\u200b\u5df2\u200b\u66f4\u65b0\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u518d\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u770b\u200b\u6709\u200b\u54ea\u4e9b\u200b\u53d8\u5316\u200b\u3002</p> In\u00a0[15]: Copied! <pre># # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\nsummary(model, \n        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n        verbose=0,\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"]\n)\n</pre> # # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output) summary(model,          input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)         verbose=0,         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],         col_width=20,         row_settings=[\"var_names\"] ) Out[15]: <pre>============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nEfficientNet (EfficientNet)                                  [32, 3, 224, 224]    [32, 3]              --                   Partial\n\u251c\u2500Sequential (features)                                      [32, 3, 224, 224]    [32, 1280, 7, 7]     --                   False\n\u2502    \u2514\u2500Conv2dNormActivation (0)                              [32, 3, 224, 224]    [32, 32, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 3, 224, 224]    [32, 32, 112, 112]   (864)                False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 32, 112, 112]   [32, 32, 112, 112]   (64)                 False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 32, 112, 112]   [32, 32, 112, 112]   --                   --\n\u2502    \u2514\u2500Sequential (1)                                        [32, 32, 112, 112]   [32, 16, 112, 112]   --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 32, 112, 112]   [32, 16, 112, 112]   (1,448)              False\n\u2502    \u2514\u2500Sequential (2)                                        [32, 16, 112, 112]   [32, 24, 56, 56]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 16, 112, 112]   [32, 24, 56, 56]     (6,004)              False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 24, 56, 56]     [32, 24, 56, 56]     (10,710)             False\n\u2502    \u2514\u2500Sequential (3)                                        [32, 24, 56, 56]     [32, 40, 28, 28]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 24, 56, 56]     [32, 40, 28, 28]     (15,350)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 40, 28, 28]     [32, 40, 28, 28]     (31,290)             False\n\u2502    \u2514\u2500Sequential (4)                                        [32, 40, 28, 28]     [32, 80, 14, 14]     --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 40, 28, 28]     [32, 80, 14, 14]     (37,130)             False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 80, 14, 14]     [32, 80, 14, 14]     (102,900)            False\n\u2502    \u2514\u2500Sequential (5)                                        [32, 80, 14, 14]     [32, 112, 14, 14]    --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 80, 14, 14]     [32, 112, 14, 14]    (126,004)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 112, 14, 14]    [32, 112, 14, 14]    (208,572)            False\n\u2502    \u2514\u2500Sequential (6)                                        [32, 112, 14, 14]    [32, 192, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 112, 14, 14]    [32, 192, 7, 7]      (262,492)            False\n\u2502    \u2502    \u2514\u2500MBConv (1)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (2)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2502    \u2514\u2500MBConv (3)                                       [32, 192, 7, 7]      [32, 192, 7, 7]      (587,952)            False\n\u2502    \u2514\u2500Sequential (7)                                        [32, 192, 7, 7]      [32, 320, 7, 7]      --                   False\n\u2502    \u2502    \u2514\u2500MBConv (0)                                       [32, 192, 7, 7]      [32, 320, 7, 7]      (717,232)            False\n\u2502    \u2514\u2500Conv2dNormActivation (8)                              [32, 320, 7, 7]      [32, 1280, 7, 7]     --                   False\n\u2502    \u2502    \u2514\u2500Conv2d (0)                                       [32, 320, 7, 7]      [32, 1280, 7, 7]     (409,600)            False\n\u2502    \u2502    \u2514\u2500BatchNorm2d (1)                                  [32, 1280, 7, 7]     [32, 1280, 7, 7]     (2,560)              False\n\u2502    \u2502    \u2514\u2500SiLU (2)                                         [32, 1280, 7, 7]     [32, 1280, 7, 7]     --                   --\n\u251c\u2500AdaptiveAvgPool2d (avgpool)                                [32, 1280, 7, 7]     [32, 1280, 1, 1]     --                   --\n\u251c\u2500Sequential (classifier)                                    [32, 1280]           [32, 3]              --                   True\n\u2502    \u2514\u2500Dropout (0)                                           [32, 1280]           [32, 1280]           --                   --\n\u2502    \u2514\u2500Linear (1)                                            [32, 1280]           [32, 3]              3,843                True\n============================================================================================================================================\nTotal params: 4,011,391\nTrainable params: 3,843\nNon-trainable params: 4,007,548\nTotal mult-adds (G): 12.31\n============================================================================================================================================\nInput size (MB): 19.27\nForward/backward pass size (MB): 3452.09\nParams size (MB): 16.05\nEstimated Total Size (MB): 3487.41\n============================================================================================================================================</pre> <p>\u200b\u563f\u563f\u200b\uff01\u200b\u8fd9\u91cc\u200b\u6709\u200b\u5f88\u591a\u200b\u53d8\u5316\u200b\u5462\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9010\u4e00\u200b\u6765\u770b\u200b\uff1a</p> <ul> <li>\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u5217\u200b - \u200b\u4f60\u200b\u4f1a\u200b\u770b\u5230\u200b\u8bb8\u591a\u200b\u57fa\u7840\u200b\u5c42\u200b\uff08\u200b\u4f4d\u4e8e\u200b <code>features</code> \u200b\u90e8\u5206\u200b\u7684\u200b\u5c42\u200b\uff09\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u503c\u4e3a\u200b <code>False</code>\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u4e86\u200b\u5b83\u4eec\u200b\u7684\u200b <code>requires_grad=False</code> \u200b\u5c5e\u6027\u200b\u3002\u200b\u9664\u975e\u200b\u6211\u4eec\u200b\u6539\u53d8\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5426\u5219\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u5728\u200b\u672a\u6765\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4e2d\u200b\u4e0d\u4f1a\u200b\u88ab\u200b\u66f4\u65b0\u200b\u3002</li> <li><code>classifier</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b - \u200b\u6a21\u578b\u200b\u7684\u200b <code>classifier</code> \u200b\u90e8\u5206\u200b\u73b0\u5728\u200b\u7684\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u503c\u4e3a\u200b <code>[32, 3]</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>[32, 1000]</code>\u3002\u200b\u5b83\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u503c\u200b\u4e5f\u200b\u662f\u200b <code>True</code>\u3002\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u7684\u200b\u53c2\u6570\u200b\u5c06\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u88ab\u200b\u66f4\u65b0\u200b\u3002\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>features</code> \u200b\u90e8\u5206\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>classifier</code> \u200b\u90e8\u5206\u200b\u63d0\u4f9b\u200b\u56fe\u50cf\u200b\u7684\u200b\u57fa\u7840\u200b\u8868\u793a\u200b\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u7684\u200b <code>classifier</code> \u200b\u5c42\u200b\u5c06\u200b\u5b66\u4e60\u200b\u5982\u4f55\u200b\u5c06\u200b\u8fd9\u79cd\u200b\u57fa\u7840\u200b\u8868\u793a\u200b\u4e0e\u200b\u6211\u4eec\u200b\u9762\u4e34\u200b\u7684\u200b\u95ee\u9898\u200b\u5bf9\u9f50\u200b\u3002</li> <li>\u200b\u66f4\u5c11\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b - \u200b\u4e4b\u524d\u200b\u6709\u200b 5,288,548 \u200b\u4e2a\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\u3002\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u51bb\u7ed3\u200b\u4e86\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u5c42\u200b\uff0c\u200b\u53ea\u200b\u4fdd\u7559\u200b\u4e86\u200b <code>classifier</code> \u200b\u4e3a\u200b\u53ef\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u73b0\u5728\u200b\u53ea\u6709\u200b 3,843 \u200b\u4e2a\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\uff08\u200b\u751a\u81f3\u200b\u6bd4\u200b\u6211\u4eec\u200b\u7684\u200b TinyVGG \u200b\u6a21\u578b\u200b\u8fd8\u8981\u200b\u5c11\u200b\uff09\u3002\u200b\u867d\u7136\u200b\u8fd8\u6709\u200b 4,007,548 \u200b\u4e2a\u200b\u4e0d\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u5c06\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u57fa\u7840\u200b\u8868\u793a\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u8f93\u5165\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b <code>classifier</code> \u200b\u5c42\u200b\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u548c\u200b\u65f6\u95f4\u200b\u5c31\u200b\u8d8a\u957f\u200b\u3002\u200b\u51bb\u7ed3\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\u5e76\u200b\u51cf\u5c11\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5e94\u8be5\u200b\u80fd\u200b\u5f88\u5feb\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u3002\u200b\u8fd9\u662f\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e00\u5927\u200b\u597d\u5904\u200b\uff0c\u200b\u5229\u7528\u200b\u5df2\u7ecf\u200b\u5728\u200b\u7c7b\u4f3c\u200b\u95ee\u9898\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u8fc7\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u5df2\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u7a0d\u5fae\u200b\u8c03\u6574\u200b\u8f93\u51fa\u200b\u4ee5\u200b\u9002\u5e94\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> In\u00a0[16]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u4e3a\u4e86\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b05. PyTorch Going Modular \u200b\u7b2c\u200b 04 \u200b\u8282\u4e2d\u200b\u5b9a\u4e49\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p><code>train()</code> \u200b\u51fd\u6570\u200b\u4f4d\u4e8e\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\u4e0b\u200b\u7684\u200b <code>engine.py</code> \u200b\u811a\u672c\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b 5 \u200b\u4e2a\u200b\u5468\u671f\u200b\u9700\u8981\u200b\u591a\u957f\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u53ea\u200b\u8bad\u7ec3\u200b <code>classifier</code> \u200b\u53c2\u6570\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u5176\u4ed6\u200b\u53c2\u6570\u200b\u5df2\u7ecf\u200b\u88ab\u200b\u51bb\u7ed3\u200b\u3002</p> In\u00a0[17]: Copied! <pre># Set the random seeds\ntorch.manual_seed(42)\ntorch.cuda.manual_seed(42)\n\n# Start the timer\nfrom timeit import default_timer as timer \nstart_time = timer()\n\n# Setup training and save the results\nresults = engine.train(model=model,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=5,\n                       device=device)\n\n# End the timer and print out how long it took\nend_time = timer()\nprint(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")\n</pre> # Set the random seeds torch.manual_seed(42) torch.cuda.manual_seed(42)  # Start the timer from timeit import default_timer as timer  start_time = timer()  # Setup training and save the results results = engine.train(model=model,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=5,                        device=device)  # End the timer and print out how long it took end_time = timer() print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8717 | train_acc: 0.7773 | test_loss: 0.7912 | test_acc: 0.8153\nEpoch: 3 | train_loss: 0.7648 | train_acc: 0.7930 | test_loss: 0.7463 | test_acc: 0.8561\nEpoch: 4 | train_loss: 0.7108 | train_acc: 0.7539 | test_loss: 0.6372 | test_acc: 0.8655\nEpoch: 5 | train_loss: 0.6254 | train_acc: 0.7852 | test_loss: 0.6260 | test_acc: 0.8561\n[INFO] Total training time: 8.977 seconds\n</pre> <p>\u200b\u54c7\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\u76f8\u5f53\u200b\u5feb\u200b\uff08\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u672c\u5730\u200b\u673a\u5668\u200b\u4e0a\u200b\u4f7f\u7528\u200bNVIDIA TITAN RTX GPU\u200b\u5927\u7ea6\u200b5\u200b\u79d2\u200b\uff0c\u200b\u5728\u200bGoogle Colab\u200b\u4e0a\u200b\u4f7f\u7528\u200bNVIDIA P100 GPU\u200b\u5927\u7ea6\u200b15\u200b\u79d2\u200b\uff09\u3002</p> <p>\u200b\u800c\u4e14\u200b\u770b\u8d77\u6765\u200b\u5b83\u200b\u5b8c\u5168\u200b\u8d85\u8d8a\u200b\u4e86\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u7684\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\uff01</p> <p>\u200b\u4f7f\u7528\u200b<code>efficientnet_b0</code>\u200b\u4f5c\u4e3a\u200b\u9aa8\u5e72\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fbe\u5230\u200b\u4e86\u200b\u8fd1\u200b85%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u662f\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bTinyVGG\u200b\u6240\u200b\u80fd\u200b\u8fbe\u5230\u200b\u7684\u200b\u4e24\u500d\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200b\u6211\u4eec\u200b\u7528\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\u6765\u8bf4\u200b\uff0c\u200b\u8fd9\u200b\u5df2\u7ecf\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u4e86\u200b\u3002</p> In\u00a0[18]: Copied! <pre># Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it\ntry:\n    from helper_functions import plot_loss_curves\nexcept:\n    print(\"[INFO] Couldn't find helper_functions.py, downloading...\")\n    with open(\"helper_functions.py\", \"wb\") as f:\n        import requests\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n        f.write(request.content)\n    from helper_functions import plot_loss_curves\n\n# Plot the loss curves of our model\nplot_loss_curves(results)\n</pre> # Get the plot_loss_curves() function from helper_functions.py, download the file if we don't have it try:     from helper_functions import plot_loss_curves except:     print(\"[INFO] Couldn't find helper_functions.py, downloading...\")     with open(\"helper_functions.py\", \"wb\") as f:         import requests         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")         f.write(request.content)     from helper_functions import plot_loss_curves  # Plot the loss curves of our model plot_loss_curves(results) <p>\u200b\u8fd9\u4e9b\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u770b\u8d77\u6765\u200b\u975e\u5e38\u200b\u68d2\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u4e24\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\uff09\u200b\u7684\u200b\u635f\u5931\u200b\u90fd\u200b\u5728\u200b\u671d\u7740\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u5411\u200b\u53d1\u5c55\u200b\u3002</p> <p>\u200b\u51c6\u786e\u7387\u200b\u503c\u200b\u4e5f\u200b\u662f\u200b\u4e00\u6837\u200b\uff0c\u200b\u5448\u4e0a\u5347\u200b\u8d8b\u52bf\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c55\u793a\u200b\u4e86\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u5f3a\u5927\u200b\u4e4b\u200b\u5904\u200b\u3002\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u901a\u5e38\u200b\u80fd\u200b\u5728\u200b\u8f83\u200b\u77ed\u65f6\u95f4\u200b\u5185\u7528\u200b\u5c11\u91cf\u200b\u6570\u636e\u200b\u53d6\u5f97\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6211\u200b\u5728\u200b\u60f3\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5c1d\u8bd5\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\u5730\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u6216\u8005\u200b\u589e\u52a0\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\uff0c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u5462\u200b\uff1f</p> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u89c2\u5bdf\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u770b\u8d77\u6765\u200b\u662f\u200b\u8fc7\u200b\u62df\u5408\u200b\u3001\u200b\u6b20\u200b\u62df\u5408\u200b\uff0c\u200b\u8fd8\u662f\u200b\u4e24\u8005\u200b\u90fd\u200b\u4e0d\u662f\u200b\uff1f\u200b\u63d0\u793a\u200b\uff1a\u200b\u67e5\u770b\u200b\u7b14\u8bb0\u672c\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b8\u200b\u90e8\u5206\u200b. \u200b\u7406\u60f3\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u5e94\u8be5\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\uff1f \u200b\u83b7\u53d6\u200b\u601d\u8def\u200b\u3002</p> In\u00a0[19]: Copied! <pre>from typing import List, Tuple\n\nfrom PIL import Image\n\n# 1. Take in a trained model, class names, image path, image size, a transform and target device\ndef pred_and_plot_image(model: torch.nn.Module,\n                        image_path: str, \n                        class_names: List[str],\n                        image_size: Tuple[int, int] = (224, 224),\n                        transform: torchvision.transforms = None,\n                        device: torch.device=device):\n    \n    \n    # 2. Open image\n    img = Image.open(image_path)\n\n    # 3. Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose([\n            transforms.Resize(image_size),\n            transforms.ToTensor(),\n            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225]),\n        ])\n\n    ### Predict on image ### \n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n      transformed_image = image_transform(img).unsqueeze(dim=0)\n\n      # 7. Make a prediction on image with an extra dimension and send it to the target device\n      target_image_pred = model(transformed_image.to(device))\n\n    # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 9. Convert prediction probabilities -&gt; prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 10. Plot image with predicted label and probability \n    plt.figure()\n    plt.imshow(img)\n    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n    plt.axis(False);\n</pre> from typing import List, Tuple  from PIL import Image  # 1. Take in a trained model, class names, image path, image size, a transform and target device def pred_and_plot_image(model: torch.nn.Module,                         image_path: str,                          class_names: List[str],                         image_size: Tuple[int, int] = (224, 224),                         transform: torchvision.transforms = None,                         device: torch.device=device):               # 2. Open image     img = Image.open(image_path)      # 3. Create transformation for image (if one doesn't exist)     if transform is not None:         image_transform = transform     else:         image_transform = transforms.Compose([             transforms.Resize(image_size),             transforms.ToTensor(),             transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225]),         ])      ### Predict on image ###       # 4. Make sure the model is on the target device     model.to(device)      # 5. Turn on model evaluation mode and inference mode     model.eval()     with torch.inference_mode():       # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])       transformed_image = image_transform(img).unsqueeze(dim=0)        # 7. Make a prediction on image with an extra dimension and send it to the target device       target_image_pred = model(transformed_image.to(device))      # 8. Convert logits -&gt; prediction probabilities (using torch.softmax() for multi-class classification)     target_image_pred_probs = torch.softmax(target_image_pred, dim=1)      # 9. Convert prediction probabilities -&gt; prediction labels     target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)      # 10. Plot image with predicted label and probability      plt.figure()     plt.imshow(img)     plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")     plt.axis(False); <p>\u200b\u591a\u4e48\u200b\u6f02\u4eae\u200b\u7684\u200b\u51fd\u6570\u200b\u554a\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u51e0\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>list(Path(test_dir).glob(\"*/*.jpg\"))</code> \u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u6d4b\u8bd5\u200b\u56fe\u7247\u200b\u7684\u200b\u8def\u5f84\u200b\uff0c<code>glob()</code> \u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u7684\u200b\u661f\u53f7\u200b\u8868\u793a\u200b\u201c\u200b\u4efb\u4f55\u200b\u5339\u914d\u200b\u6b64\u200b\u6a21\u5f0f\u200b\u7684\u200b\u6587\u4ef6\u200b\u201d\uff0c\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u4efb\u4f55\u200b\u4ee5\u200b <code>.jpg</code> \u200b\u7ed3\u5c3e\u200b\u7684\u200b\u6587\u4ef6\u200b\uff08\u200b\u6211\u4eec\u200b\u6240\u6709\u200b\u7684\u200b\u56fe\u7247\u200b\uff09\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>random.sample(population, k)</code> \u200b\u4ece\u200b\u8fd9\u4e9b\u200b\u8def\u5f84\u200b\u4e2d\u200b\u968f\u673a\u200b\u62bd\u53d6\u200b\u4e00\u4e9b\u200b\u6837\u672c\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>population</code> \u200b\u662f\u200b\u8981\u200b\u62bd\u6837\u200b\u7684\u200b\u5e8f\u5217\u200b\uff0c<code>k</code> \u200b\u662f\u200b\u8981\u200b\u62bd\u53d6\u200b\u7684\u200b\u6837\u672c\u200b\u6570\u91cf\u200b\u3002</p> In\u00a0[20]: Copied! <pre># Get a random list of image paths from test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data \ntest_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n\n# Make predictions on and plot the images\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=model, \n                        image_path=image_path,\n                        class_names=class_names,\n                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n                        image_size=(224, 224))\n</pre> # Get a random list of image paths from test set import random num_images_to_plot = 3 test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data  test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths                                        k=num_images_to_plot) # randomly select 'k' image paths to pred and plot  # Make predictions on and plot the images for image_path in test_image_path_sample:     pred_and_plot_image(model=model,                          image_path=image_path,                         class_names=class_names,                         # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights                         image_size=(224, 224)) <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u770b\u8d77\u6765\u200b\u6bd4\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u7684\u200bTinyVGG\u200b\u6a21\u578b\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u9884\u6d4b\u200b\u8981\u200b\u597d\u5f97\u591a\u200b\u3002</p> In\u00a0[21]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = data_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = data_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>\u200b\u53cc\u51fb\u200b\u70b9\u8d5e\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u53c8\u200b\u4e00\u6b21\u200b\u9884\u6d4b\u200b\u5bf9\u200b\u4e86\u200b\uff01</p> <p>\u200b\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u9884\u6d4b\u200b\u7684\u200b\u6982\u7387\u200b\u6bd4\u200b TinyVGG \u200b\u6a21\u578b\u200b\uff08<code>0.373</code>\uff09\u200b\u5728\u200b04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b 11.3 \u200b\u8282\u4e2d\u200b\u7684\u200b\u8981\u200b\u9ad8\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u8868\u660e\u200b\u6211\u4eec\u200b\u7684\u200b <code>efficientnet_b0</code> \u200b\u6a21\u578b\u200b\u5bf9\u200b\u5176\u200b\u9884\u6d4b\u200b\u66f4\u52a0\u200b\u81ea\u4fe1\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b TinyVGG \u200b\u6a21\u578b\u200b\u53ea\u662f\u200b\u52c9\u5f3a\u200b\u731c\u200b\u5bf9\u200b\u800c\u5df2\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#06-pytorch","title":"06. PyTorch\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4f7f\u7528\u200b\u4e86\u200b<code>torchvision</code>\u200b\u7684\u200b\u65b0\u200b\u591a\u200b\u6743\u91cd\u200b\u652f\u6301\u200bAPI\uff08\u200b\u5728\u200b<code>torchvision</code> v0.13+\u200b\u4e2d\u200b\u53ef\u7528\u200b\uff09\u3002</p> <p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u624b\u52a8\u200b\u6784\u5efa\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5b83\u4eec\u200b\u7684\u200b\u6027\u80fd\u200b\u4e00\u76f4\u200b\u4e0d\u4f73\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u60f3\u200b\uff0c\u200b\u662f\u5426\u200b\u5df2\u7ecf\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u9488\u5bf9\u200b\u6211\u4eec\u200b\u95ee\u9898\u200b\u7684\u200b\u6027\u80fd\u200b\u826f\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff1f</p> <p>\u200b\u800c\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\uff0c\u200b\u7b54\u6848\u200b\u901a\u5e38\u200b\u662f\u200b\u80af\u5b9a\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b\u4e00\u79cd\u200b\u5f3a\u5927\u200b\u7684\u200b\u6280\u672f\u200b\u2014\u2014\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u6765\u200b\u4e86\u89e3\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff1f\u00b6","text":"<p>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u4f7f\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u91c7\u7528\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u4ece\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u4e2d\u5b66\u200b\u5230\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u6743\u91cd\u200b\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5e94\u7528\u200b\u4e8e\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u4ece\u200b ImageNet\uff08\u200b\u5305\u542b\u200b\u6570\u767e\u4e07\u200b\u5f20\u200b\u4e0d\u540c\u200b\u7269\u4f53\u200b\u7684\u200b\u56fe\u50cf\u200b\uff09\u200b\u7b49\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u5b66\u5230\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u7528\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u4e00\u4e2a\u200b \u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\uff08\u200b\u4e00\u4e2a\u200b\u901a\u8fc7\u200b\u5927\u91cf\u200b\u6587\u672c\u200b\u5b66\u4e60\u200b\u8bed\u8a00\u200b\u8868\u793a\u200b\u7684\u200b\u6a21\u578b\u200b\uff09\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u4f5c\u4e3a\u200b\u5206\u7c7b\u200b\u4e0d\u540c\u200b\u6587\u672c\u200b\u6837\u672c\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u3002</p> <p>\u200b\u524d\u63d0\u200b\u662f\u200b\uff1a\u200b\u627e\u5230\u200b\u4e00\u4e2a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5e94\u7528\u200b\u4e8e\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u548c\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff08NLP\uff09\u200b\u4e2d\u200b\u7684\u200b\u5e94\u7528\u200b\u793a\u4f8b\u200b\u3002\u200b\u5728\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u5728\u200b ImageNet \u200b\u4e0a\u200b\u7684\u200b\u6570\u767e\u4e07\u200b\u5f20\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u5b66\u4e60\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u7136\u540e\u200b\u5229\u7528\u200b\u8fd9\u4e9b\u200b\u6a21\u5f0f\u200b\u63a8\u65ad\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u95ee\u9898\u200b\u3002\u200b\u5bf9\u4e8e\u200b NLP\uff0c\u200b\u4e00\u4e2a\u200b\u8bed\u8a00\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u901a\u8fc7\u200b\u9605\u8bfb\u200b\u6240\u6709\u200b\u7ef4\u57fa\u767e\u79d1\u200b\uff08\u200b\u751a\u81f3\u200b\u66f4\u200b\u591a\u200b\uff09\u200b\u6765\u200b\u5b66\u4e60\u200b\u8bed\u8a00\u200b\u7ed3\u6784\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u77e5\u8bc6\u200b\u5e94\u7528\u200b\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/","title":"\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff1f\u00b6","text":"<p>\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u6709\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u597d\u5904\u200b\uff1a</p> <ol> <li>\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u4e00\u4e2a\u200b\u5df2\u7ecf\u200b\u5728\u200b\u7c7b\u4f3c\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u4e0a\u200b\u8bc1\u660e\u200b\u6709\u6548\u200b\u7684\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\uff08\u200b\u901a\u5e38\u200b\u662f\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\uff09\u3002</li> <li>\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u4e00\u4e2a\u200b\u5df2\u7ecf\u200b\u5728\u200b\u7c7b\u4f3c\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u5b66\u4e60\u200b\u4e86\u200b\u6a21\u5f0f\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u8fd9\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u8f83\u5c11\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5b9e\u73b0\u200b\u51fa\u8272\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200bFoodVision Mini\u200b\u95ee\u9898\u200b\u6d4b\u8bd5\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u91c7\u7528\u200b\u4e00\u4e2a\u200b\u5728\u200bImageNet\u200b\u4e0a\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5c1d\u8bd5\u200b\u5229\u7528\u200b\u5176\u200b\u5e95\u5c42\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b\u8868\u793a\u200b\u6765\u200b\u5bf9\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u3002</p> <p>\u200b\u7814\u7a76\u200b\u4e0e\u200b\u5b9e\u8df5\u200b\u90fd\u200b\u652f\u6301\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u6700\u8fd1\u200b\u4e00\u7bc7\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u53d1\u73b0\u200b\u5efa\u8bae\u200b\u4ece\u4e1a\u8005\u200b\u5c3d\u53ef\u80fd\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u3002</p> <p></p> <p>\u200b\u4e00\u9879\u200b\u4ece\u200b\u4ece\u4e1a\u8005\u200b\u89d2\u5ea6\u200b\u7814\u7a76\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u54ea\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u7814\u7a76\u200b\u53d1\u73b0\u200b\uff0c\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u5728\u200b\u6210\u672c\u200b\u548c\u200b\u65f6\u95f4\u200b\u65b9\u9762\u200b\u66f4\u4e3a\u200b\u6709\u76ca\u200b\u3002\u200b\u6765\u6e90\u200b\uff1a \u200b\u5982\u4f55\u200b\u8bad\u7ec3\u200b\u4f60\u200b\u7684\u200bViT\uff1f\u200b\u6570\u636e\u200b\u3001\u200b\u589e\u5f3a\u200b\u548c\u200b\u6b63\u5219\u200b\u5316\u5728\u200b\u89c6\u89c9\u200b\u53d8\u6362\u5668\u200b\u4e2d\u200b\u7684\u200b\u5e94\u7528\u200b \u200b\u8bba\u6587\u200b\u7b2c\u200b6\u200b\u8282\u200b\uff08\u200b\u7ed3\u8bba\u200b\uff09\u3002</p> <p>Jeremy Howard\uff08fastai \u200b\u521b\u59cb\u4eba\u200b\uff09\u200b\u4e5f\u200b\u662f\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u575a\u5b9a\u200b\u652f\u6301\u8005\u200b\u3002</p> <p>\u200b\u771f\u6b63\u200b\u4ea7\u751f\u200b\u5f71\u54cd\u200b\u7684\u200b\u4e8b\u60c5\u200b\uff08\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff09\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u80fd\u200b\u5728\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u4e0a\u200b\u505a\u200b\u5f97\u200b\u66f4\u597d\u200b\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6539\u53d8\u200b\u4e16\u754c\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002\u200b\u7a81\u7136\u200b\u4e4b\u95f4\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u4eba\u200b\u53ef\u4ee5\u200b\u7528\u200b\u66f4\u200b\u5c11\u200b\u7684\u200b\u8d44\u6e90\u200b\u548c\u200b\u66f4\u200b\u5c11\u200b\u7684\u200b\u6570\u636e\u200b\u505a\u51fa\u200b\u4e16\u754c\u7ea7\u200b\u7684\u200b\u5de5\u4f5c\u200b\u3002 \u2014 Jeremy Howard\u200b\u5728\u200bLex Fridman\u200b\u64ad\u5ba2\u200b\u4e0a\u200b\u7684\u200b\u8bb2\u8bdd\u200b</p>"},{"location":"06_pytorch_transfer_learning/","title":"\u5728\u200b\u54ea\u91cc\u200b\u627e\u5230\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u4e16\u754c\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4ee4\u4eba\u60ca\u53f9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p> <p>\u200b\u5982\u6b64\u200b\u4ee4\u4eba\u60ca\u53f9\u200b\uff0c\u200b\u4ee5\u81f3\u4e8e\u200b\u4e16\u754c\u5404\u5730\u200b\u7684\u200b\u8bb8\u591a\u200b\u4eba\u200b\u90fd\u200b\u5206\u4eab\u200b\u4ed6\u4eec\u200b\u7684\u200b\u5de5\u4f5c\u200b\u3002</p> <p>\u200b\u901a\u5e38\u200b\uff0c\u200b\u6700\u200b\u5148\u8fdb\u200b\u7814\u7a76\u200b\u7684\u200b\u4ee3\u7801\u200b\u548c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200b\u53d1\u5e03\u200b\u540e\u200b\u7684\u200b\u51e0\u5929\u200b\u5185\u200b\u5c31\u200b\u4f1a\u200b\u88ab\u200b\u516c\u5f00\u200b\u3002</p> <p>\u200b\u800c\u4e14\u200b\uff0c\u200b\u6709\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> \u200b\u4f4d\u7f6e\u200b \u200b\u90a3\u91cc\u200b\u6709\u200b\u4ec0\u4e48\u200b\uff1f \u200b\u94fe\u63a5\u200b PyTorch \u200b\u9886\u57df\u200b\u5e93\u200b \u200b\u6bcf\u4e2a\u200b PyTorch \u200b\u9886\u57df\u200b\u5e93\u200b\uff08\u200b\u5982\u200b <code>torchvision</code>\u3001<code>torchtext</code>\uff09\u200b\u90fd\u200b\u5e26\u6709\u200b\u67d0\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u4f7f\u7528\u200b\u3002 <code>torchvision.models</code>, <code>torchtext.models</code>, <code>torchaudio.models</code>, <code>torchrec.models</code> HuggingFace Hub \u200b\u6765\u81ea\u200b\u4e16\u754c\u5404\u5730\u200b\u7ec4\u7ec7\u200b\u7684\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u9886\u57df\u200b\uff08\u200b\u89c6\u89c9\u200b\u3001\u200b\u6587\u672c\u200b\u3001\u200b\u97f3\u9891\u200b\u7b49\u200b\uff09\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7cfb\u5217\u200b\u3002\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002 https://huggingface.co/models, https://huggingface.co/datasets <code>timm</code>\uff08PyTorch \u200b\u56fe\u50cf\u200b\u6a21\u578b\u200b\uff09\u200b\u5e93\u200b \u200b\u51e0\u4e4e\u200b\u6240\u6709\u200b\u6700\u65b0\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u4ee5\u53ca\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u6709\u7528\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u529f\u80fd\u200b\uff0c\u200b\u90fd\u200b\u4ee5\u200b PyTorch \u200b\u4ee3\u7801\u200b\u7684\u200b\u5f62\u5f0f\u200b\u63d0\u4f9b\u200b\u3002 https://github.com/rwightman/pytorch-image-models Paperswithcode \u200b\u4e00\u7cfb\u5217\u200b\u9644\u6709\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u7684\u200b\u6700\u65b0\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bba\u6587\u200b\u3002\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u627e\u5230\u200b\u4e0d\u540c\u200b\u4efb\u52a1\u200b\u4e0a\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\u7684\u200b\u57fa\u51c6\u200b\u3002 https://paperswithcode.com/ <p>\u200b\u6709\u200b\u4e86\u200b\u4e0a\u8ff0\u200b\u9ad8\u8d28\u91cf\u200b\u8d44\u6e90\u200b\uff0c\u200b\u5728\u200b\u5f00\u59cb\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\u65f6\u200b\uff0c\u200b\u8be2\u95ee\u200b\u201c\u200b\u6211\u200b\u7684\u200b\u95ee\u9898\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1f\u201d\u200b\u5e94\u8be5\u200b\u6210\u4e3a\u200b\u4e00\u79cd\u200b\u5e38\u89c1\u200b\u7684\u200b\u505a\u6cd5\u200b\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u82b1\u200b5\u200b\u5206\u949f\u200b\u6d4f\u89c8\u200b <code>torchvision.models</code> \u200b\u548c\u200b HuggingFace Hub Models \u200b\u9875\u9762\u200b\uff0c\u200b\u4f60\u200b\u53d1\u73b0\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff1f\uff08\u200b\u8fd9\u91cc\u200b\u6ca1\u6709\u200b\u6b63\u786e\u200b\u7b54\u6848\u200b\uff0c\u200b\u53ea\u662f\u200b\u4e3a\u4e86\u200b\u7ec3\u4e60\u200b\u63a2\u7d22\u200b\uff09</p>"},{"location":"06_pytorch_transfer_learning/","title":"\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u5b9a\u5236\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u95ee\u9898\u200b\u4e0a\u200b\u5de5\u4f5c\u200b\uff08\u200b\u5e76\u200b\u6709\u671b\u200b\u6539\u8fdb\u200b\uff09\u3002</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b \u200b\u5728\u200b\u524d\u200b\u51e0\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u7f16\u5199\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u6709\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u5b83\u200b\u5e76\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002 1. \u200b\u83b7\u53d6\u6570\u636e\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u83b7\u53d6\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4ee5\u200b\u5c1d\u8bd5\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002 2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b \u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5728\u200b\u7b2c\u200b05\u200b\u7ae0\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b <code>data_setup.py</code> \u200b\u811a\u672c\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002 3. \u200b\u83b7\u53d6\u200b\u5e76\u200b\u5b9a\u5236\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u4e0b\u8f7d\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5b9a\u5236\u200b\u4e3a\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002 4. \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u65b0\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5728\u200b\u6211\u4eec\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u524d\u200b\u4e00\u7ae0\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\u3002 5. \u200b\u901a\u8fc7\u200b\u7ed8\u5236\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b \u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff1f\u200b\u5b83\u200b\u662f\u200b\u8fc7\u200b\u62df\u5408\u200b\u8fd8\u662f\u200b\u6b20\u200b\u62df\u5408\u200b\uff1f 6. \u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b \u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u8bc4\u4f30\u200b\u6307\u6807\u200b\u662f\u200b\u4e00\u200b\u56de\u4e8b\u200b\uff0c\u200b\u4f46\u200b\u67e5\u770b\u200b\u5176\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u4e0a\u200b\u7684\u200b\u9884\u6d4b\u200b\u662f\u200b\u53e6\u4e00\u56de\u4e8b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01"},{"location":"06_pytorch_transfer_learning/","title":"\u5982\u4f55\u200b\u83b7\u53d6\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200bGitHub\u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200bGitHub\u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200bPyTorch\u200b\u6587\u6863\u200b\u548c\u200bPyTorch\u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u5173\u4e8e\u200bPyTorch\u200b\u6240\u6709\u200b\u4e8b\u9879\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#0","title":"0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9996\u5148\u200b\u5bfc\u5165\u200b/\u200b\u4e0b\u8f7d\u200b\u672c\u8282\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6a21\u5757\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8282\u7701\u200b\u7f16\u5199\u200b\u989d\u5916\u200b\u4ee3\u7801\u200b\u7684\u200b\u5de5\u4f5c\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5229\u7528\u200b\u4e0a\u200b\u4e00\u8282\u200b 05. PyTorch Going Modular \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u4e00\u4e9b\u200b Python \u200b\u811a\u672c\u200b\uff08\u200b\u5982\u200b <code>data_setup.py</code> \u200b\u548c\u200b <code>engine.py</code>\uff09\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>pytorch-deep-learning</code> \u200b\u4ed3\u5e93\u200b\u4e0b\u8f7d\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u4e0b\u8f7d\u200b\u7684\u8bdd\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u83b7\u53d6\u200b <code>torchinfo</code> \u200b\u5305\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u5b89\u88c5\u200b\uff09\u3002</p> <p><code>torchinfo</code> \u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u4ee5\u200b\u53ef\u89c6\u5316\u200b\u65b9\u5f0f\u200b\u5c55\u793a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u622a\u81f3\u200b 2022 \u200b\u5e74\u200b 6 \u200b\u6708\u200b\uff0c\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4f7f\u7528\u200b <code>torch</code> \u200b\u548c\u200b <code>torchvision</code> \u200b\u7684\u200b nightly \u200b\u7248\u672c\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u9700\u8981\u200b <code>torchvision</code> v0.13+ \u200b\u6765\u200b\u4f7f\u7528\u200b\u66f4\u65b0\u200b\u7684\u200b\u591a\u200b\u6743\u91cd\u200b API\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u5b89\u88c5\u200b\u8fd9\u4e9b\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u4e86\u89e3\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u5c1d\u8bd5\u200b\u7684\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u7528\u4e8e\u200bFoodVision Mini\u200b\u7684\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ece\u200b\u8bfe\u7a0b\u200bGitHub\u200b\u4e0b\u8f7d\u200b<code>pizza_steak_sushi.zip</code>\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u7136\u540e\u200b\u89e3\u538b\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u786e\u4fdd\u200b\u5982\u679c\u200b\u5df2\u7ecf\u200b\u62e5\u6709\u200b\u6570\u636e\u200b\uff0c\u200b\u5b83\u200b\u4e0d\u4f1a\u200b\u91cd\u65b0\u200b\u4e0b\u8f7d\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#2","title":"2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e0b\u8f7d\u200b\u4e86\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u90e8\u5206\u200b\u521b\u5efa\u200b\u7684\u200b <code>data_setup.py</code> \u200b\u811a\u672c\u200b\u6765\u200b\u51c6\u5907\u200b\u548c\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5148\u200b\u51c6\u5907\u200b\u7279\u5b9a\u200b\u7684\u200b\u56fe\u50cf\u200b\u53d8\u6362\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#21-torchvisionmodels","title":"2.1 \u200b\u4e3a\u200b <code>torchvision.models</code> \u200b\u521b\u5efa\u200b\u53d8\u6362\u200b\uff08\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\uff09\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u622a\u81f3\u200b <code>torchvision</code> v0.13+\uff0c\u200b\u5173\u4e8e\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b <code>torchvision.models</code> \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u53d8\u6362\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6709\u6240\u200b\u66f4\u65b0\u200b\u3002\u200b\u6211\u200b\u5c06\u200b\u4e4b\u524d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u79f0\u4e3a\u200b\u201c\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u201d\uff0c\u200b\u65b0\u200b\u7684\u200b\u65b9\u6cd5\u200b\u79f0\u4e3a\u200b\u201c\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\u201d\u3002\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u5c06\u200b\u5c55\u793a\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u5f53\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u8f93\u5165\u200b\u5230\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u4e0e\u200b\u6a21\u578b\u200b\u539f\u59cb\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u65b9\u5f0f\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u5728\u200b <code>torchvision</code> v0.13+ \u200b\u4e4b\u524d\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4e3a\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\u53d8\u6362\u200b\uff0c\u200b\u6587\u6863\u200b\u6307\u51fa\u200b\uff1a</p> <p>\u200b\u6240\u6709\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u671f\u671b\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u4ee5\u200b\u76f8\u540c\u200b\u65b9\u5f0f\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u5373\u200b\u5f62\u72b6\u200b\u4e3a\u200b (3 x H x W) \u200b\u7684\u200b 3 \u200b\u901a\u9053\u200b RGB \u200b\u56fe\u50cf\u200b\u7684\u200b\u5c0f\u200b\u6279\u6b21\u200b\uff0c\u200b\u5176\u4e2d\u200b H \u200b\u548c\u200b W \u200b\u81f3\u5c11\u200b\u4e3a\u200b 224\u3002</p> <p>\u200b\u56fe\u50cf\u200b\u5fc5\u987b\u200b\u52a0\u8f7d\u200b\u5230\u200b <code>[0, 1]</code> \u200b\u8303\u56f4\u200b\u5185\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>mean = [0.485, 0.456, 0.406]</code> \u200b\u548c\u200b <code>std = [0.229, 0.224, 0.225]</code> \u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u53d8\u6362\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\uff1a</p> <pre><code>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</code></pre> <p>\u200b\u597d\u6d88\u606f\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7ec4\u5408\u200b\u4ee5\u4e0b\u200b\u53d8\u6362\u200b\u6765\u200b\u5b9e\u73b0\u200b\u4e0a\u8ff0\u200b\u64cd\u4f5c\u200b\uff1a</p> \u200b\u53d8\u6362\u200b\u7f16\u53f7\u200b \u200b\u6240\u200b\u9700\u200b\u53d8\u6362\u200b \u200b\u6267\u884c\u200b\u53d8\u6362\u200b\u7684\u200b\u4ee3\u7801\u200b 1 \u200b\u5927\u5c0f\u200b\u4e3a\u200b <code>[batch_size, 3, height, width]</code> \u200b\u7684\u200b\u5c0f\u200b\u6279\u6b21\u200b\uff0c\u200b\u5176\u4e2d\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u81f3\u5c11\u200b\u4e3a\u200b 224x224\u3002 \u200b\u4f7f\u7528\u200b <code>torchvision.transforms.Resize()</code> \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8c03\u6574\u200b\u4e3a\u200b <code>[3, 224, 224]</code>\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader()</code> \u200b\u521b\u5efa\u200b\u56fe\u50cf\u200b\u6279\u6b21\u200b\u3002 2 \u200b\u503c\u200b\u5728\u200b 0 \u200b\u5230\u200b 1 \u200b\u4e4b\u95f4\u200b\u3002 \u200b\u4f7f\u7528\u200b <code>torchvision.transforms.ToTensor()</code> 3 \u200b\u5747\u503c\u200b\u4e3a\u200b <code>[0.485, 0.456, 0.406]</code>\uff08\u200b\u6bcf\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u7684\u200b\u503c\u200b\uff09\u3002 \u200b\u4f7f\u7528\u200b <code>torchvision.transforms.Normalize(mean=...)</code> \u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u7684\u200b\u5747\u503c\u200b\u3002 4 \u200b\u6807\u51c6\u5dee\u200b\u4e3a\u200b <code>[0.229, 0.224, 0.225]</code>\uff08\u200b\u6bcf\u4e2a\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u7684\u200b\u503c\u200b\uff09\u3002 \u200b\u4f7f\u7528\u200b <code>torchvision.transforms.Normalize(std=...)</code> \u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u7684\u200b\u6807\u51c6\u5dee\u200b\u3002 <p>\u200b\u6ce8\u610f\u200b\uff1a ^<code>torchvision.models</code> \u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e9b\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u5177\u6709\u200b\u4e0d\u540c\u4e8e\u200b <code>[3, 224, 224]</code> \u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e9b\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u63a5\u53d7\u200b <code>[3, 240, 240]</code> \u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u5bf9\u4e8e\u200b\u7279\u5b9a\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b\u6587\u6863\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u5747\u503c\u200b\u548c\u200b\u6807\u51c6\u200b\u5dee\u503c\u200b\u6765\u81ea\u200b\u54ea\u91cc\u200b\uff1f\u200b\u4e3a\u4ec0\u4e48\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1f</p> <p>\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u662f\u4ece\u200b\u6570\u636e\u200b\u4e2d\u200b\u8ba1\u7b97\u200b\u5f97\u51fa\u200b\u7684\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u662f\u4ece\u200b ImageNet \u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u5bf9\u200b\u4e00\u7ec4\u200b\u56fe\u50cf\u200b\u8ba1\u7b97\u200b\u5f97\u51fa\u200b\u7684\u200b\u5747\u503c\u200b\u548c\u200b\u6807\u51c6\u5dee\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e5f\u200b\u4e0d\u662f\u200b\u5fc5\u987b\u200b\u8fd9\u6837\u200b\u505a\u200b\u3002\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u901a\u5e38\u200b\u80fd\u591f\u200b\u81ea\u884c\u200b\u8ba1\u7b97\u200b\u51fa\u200b\u5408\u9002\u200b\u7684\u200b\u6570\u636e\u5206\u5e03\u200b\uff08\u200b\u5b83\u4eec\u200b\u4f1a\u200b\u81ea\u884c\u200b\u8ba1\u7b97\u200b\u5747\u503c\u200b\u548c\u200b\u6807\u51c6\u5dee\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff09\uff0c\u200b\u4f46\u200b\u5728\u200b\u5f00\u59cb\u200b\u65f6\u200b\u8bbe\u7f6e\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u53ef\u4ee5\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u7684\u200b\u7f51\u7edc\u200b\u66f4\u5feb\u200b\u5730\u200b\u8fbe\u5230\u200b\u66f4\u597d\u200b\u7684\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ec4\u5408\u200b\u4e00\u7cfb\u5217\u200b <code>torchvision.transforms</code> \u200b\u6765\u200b\u6267\u884c\u200b\u4e0a\u8ff0\u200b\u6b65\u9aa4\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#22-torchvisionmodels","title":"2.2 \u200b\u4e3a\u200b <code>torchvision.models</code> \u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\uff08\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\uff09\u00b6","text":"<p>\u200b\u5982\u524d\u6240\u8ff0\u200b\uff0c\u200b\u5f53\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u8f93\u5165\u200b\u5230\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u4e0e\u200b\u6a21\u578b\u200b\u539f\u59cb\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4ee5\u200b\u76f8\u540c\u200b\u65b9\u5f0f\u200b\u51c6\u5907\u200b\u3002</p> <p>\u200b\u4e0a\u9762\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4e86\u200b\u5982\u4f55\u200b\u624b\u52a8\u200b\u4e3a\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4ece\u200b <code>torchvision</code> v0.13+ \u200b\u5f00\u59cb\u200b\uff0c\u200b\u589e\u52a0\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u5f53\u200b\u4f60\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u8bbe\u7f6e\u200b\u6a21\u578b\u200b\u5e76\u200b\u9009\u62e9\u200b\u4f60\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b\u65f6\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\uff1a</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n</pre> <p>\u200b\u5176\u4e2d\u200b\uff0c</p> <ul> <li><code>EfficientNet_B0_Weights</code> \u200b\u662f\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u4f7f\u7528\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u6743\u91cd\u200b\uff08<code>torchvision.models</code> \u200b\u4e2d\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u9009\u9879\u200b\uff09\u3002</li> <li><code>DEFAULT</code> \u200b\u8868\u793a\u200b\u6700\u4f73\u200b\u53ef\u7528\u200b\u6743\u91cd\u200b\uff08\u200b\u5728\u200b ImageNet \u200b\u4e2d\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6839\u636e\u200b\u4f60\u200b\u9009\u62e9\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u770b\u5230\u200b\u5176\u4ed6\u200b\u9009\u9879\u200b\uff0c\u200b\u5982\u200b <code>IMAGENET_V1</code> \u200b\u548c\u200b <code>IMAGENET_V2</code>\uff0c\u200b\u901a\u5e38\u200b\u7248\u672c\u53f7\u200b\u8d8a\u9ad8\u8d8a\u200b\u597d\u200b\u3002\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u83b7\u5f97\u6700\u4f73\u200b\u53ef\u7528\u200b\u6743\u91cd\u200b\uff0c<code>DEFAULT</code> \u200b\u662f\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u9009\u9879\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u53c2\u9605\u200b <code>torchvision.models</code> \u200b\u6587\u6863\u200b\u3002</li> </ul> </li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u770b\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#3","title":"3. \u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6709\u8da3\u200b\u7684\u200b\u90e8\u5206\u200b\u6765\u200b\u4e86\u200b\uff01</p> <p>\u200b\u5728\u200b\u8fc7\u53bb\u200b\u7684\u200b\u51e0\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u4ece\u5934\u5f00\u59cb\u200b\u6784\u5efa\u200b PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u8fd9\u662f\u200b\u4e00\u9879\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u6280\u80fd\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5e76\u200b\u6ca1\u6709\u200b\u8868\u73b0\u200b\u5f97\u200b\u5982\u200b\u6211\u4eec\u200b\u6240\u200b\u613f\u200b\u3002</p> <p>\u200b\u8fd9\u65f6\u5019\u200b\u5c31\u200b\u8f6e\u200b\u5230\u200b \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u767b\u573a\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u6574\u4e2a\u200b\u601d\u60f3\u200b\u662f\u200b \u200b\u91c7\u7528\u200b\u5728\u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u4f60\u200b\u76f8\u4f3c\u200b\u7684\u200b\u95ee\u9898\u200b\u7a7a\u95f4\u200b\u4e0a\u200b\u5df2\u7ecf\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u6839\u636e\u200b\u4f60\u200b\u7684\u200b\u7528\u4f8b\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u5b9a\u5236\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\uff08\u200b\u4f7f\u7528\u200b FoodVision Mini \u200b\u8fdb\u884c\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u627e\u5230\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u63a2\u7d22\u200b\u6587\u6863\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u8bb8\u591a\u200b\u5e38\u89c1\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u67b6\u6784\u200b\u9aa8\u5e72\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff1a</p> \u200b\u67b6\u6784\u200b\u9aa8\u5e72\u200b \u200b\u4ee3\u7801\u200b ResNet <code>torchvision.models.resnet18()</code>, <code>torchvision.models.resnet50()</code>... VGG\uff08\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u6211\u4eec\u200b\u7528\u4e8e\u200b TinyVGG \u200b\u7684\u200b\u6a21\u578b\u200b\uff09 <code>torchvision.models.vgg16()</code> EfficientNet <code>torchvision.models.efficientnet_b0()</code>, <code>torchvision.models.efficientnet_b1()</code>... VisionTransformer\uff08ViT\uff09 <code>torchvision.models.vit_b_16()</code>, <code>torchvision.models.vit_b_32()</code>... ConvNeXt <code>torchvision.models.convnext_tiny()</code>,  <code>torchvision.models.convnext_small()</code>... <code>torchvision.models</code> \u200b\u4e2d\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u53ef\u7528\u200b\u6a21\u578b\u200b <code>torchvision.models...</code>"},{"location":"06_pytorch_transfer_learning/#31","title":"3.1 \u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u54ea\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1f\u00b6","text":"<p>\u200b\u8fd9\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u6216\u200b\u4f60\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\u8bbe\u5907\u200b\u3002</p> <p>\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b\u4e2d\u200b\u7684\u200b\u6570\u5b57\u200b\u8d8a\u5927\u200b\uff08\u200b\u4f8b\u5982\u200b <code>efficientnet_b0()</code> -&gt; <code>efficientnet_b1()</code> -&gt; <code>efficientnet_b7()</code>\uff09\u200b\u610f\u5473\u7740\u200b\u6027\u80fd\u200b\u66f4\u597d\u200b\uff0c\u200b\u4f46\u200b\u6a21\u578b\u200b\u4e5f\u200b\u66f4\u200b\u5927\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba4\u4e3a\u200b\u6027\u80fd\u200b\u66f4\u597d\u200b\u603b\u662f\u200b\u66f4\u597d\u200b\uff0c\u200b\u5bf9\u200b\u5427\u200b\uff1f</p> <p>\u200b\u8fd9\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u4e00\u4e9b\u200b\u6027\u80fd\u200b\u66f4\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u4e8e\u200b\u67d0\u4e9b\u200b\u8bbe\u5907\u200b\u6765\u8bf4\u200b\u592a\u5927\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u4f60\u200b\u60f3\u200b\u5728\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u6709\u9650\u200b\u7684\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4f60\u200b\u4f1a\u200b\u5bfb\u627e\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u65e0\u9650\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\uff0c\u200b\u6b63\u5982\u200bThe Bitter Lesson\u200b\u6240\u8ff0\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9009\u62e9\u200b\u4f60\u200b\u80fd\u200b\u627e\u5230\u200b\u7684\u200b\u6700\u5927\u200b\u3001\u200b\u6700\u200b\u6d88\u8017\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u7406\u89e3\u200b\u8fd9\u79cd\u200b\u6027\u80fd\u200b\u4e0e\u200b\u901f\u5ea6\u200b\u4e0e\u200b\u5927\u5c0f\u200b\u7684\u200b\u6743\u8861\u200b\u4f1a\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u548c\u200b\u5b9e\u8df5\u200b\u800c\u200b\u6765\u200b\u3002</p> <p>\u200b\u5bf9\u200b\u6211\u200b\u6765\u8bf4\u200b\uff0c\u200b\u6211\u200b\u53d1\u73b0\u200b <code>efficientnet_bX</code> \u200b\u6a21\u578b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e0d\u9519\u200b\u7684\u200b\u5e73\u8861\u70b9\u200b\u3002</p> <p>\u200b\u622a\u81f3\u200b2022\u200b\u5e74\u200b5\u200b\u6708\u200b\uff0cNutrify\uff08\u200b\u6211\u200b\u6b63\u5728\u200b\u5f00\u53d1\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9a71\u52a8\u200b\u7684\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\uff09\u200b\u7531\u200b <code>efficientnet_b0</code> \u200b\u9a71\u52a8\u200b\u3002</p> <p>Comma.ai\uff08\u200b\u4e00\u5bb6\u200b\u5236\u4f5c\u200b\u5f00\u6e90\u200b\u81ea\u52a8\u200b\u9a7e\u9a76\u200b\u6c7d\u8f66\u200b\u8f6f\u4ef6\u200b\u7684\u200b\u516c\u53f8\u200b\uff09\u200b\u4f7f\u7528\u200b <code>efficientnet_b2</code> \u200b\u6765\u200b\u5b66\u4e60\u200b\u9053\u8def\u200b\u7684\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>efficientnet_bX</code>\uff0c\u200b\u4f46\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u4e0d\u8981\u200b\u8fc7\u4e8e\u200b\u4f9d\u8d56\u200b\u4efb\u4f55\u200b\u4e00\u79cd\u200b\u67b6\u6784\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u968f\u7740\u200b\u65b0\u200b\u7814\u7a76\u200b\u7684\u200b\u53d1\u5e03\u200b\uff0c\u200b\u5b83\u4eec\u200b\u603b\u662f\u200b\u5728\u200b\u53d8\u5316\u200b\u3002\u200b\u6700\u597d\u200b\u8fdb\u884c\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u770b\u770b\u200b\u54ea\u200b\u79cd\u200b\u65b9\u6cd5\u200b\u5bf9\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u6709\u6548\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#32","title":"3.2 \u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u662f\u200b <code>torchvision.models.efficientnet_b0()</code>\u3002</p> <p>\u200b\u8be5\u200b\u67b6\u6784\u200b\u6765\u81ea\u200b\u8bba\u6587\u200b EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5373\u5c06\u200b\u521b\u5efa\u200b\u7684\u200b\u793a\u4f8b\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u6765\u81ea\u200b <code>torchvision.models</code> \u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b <code>EfficientNet_B0</code> \u200b\u6a21\u578b\u200b\uff0c\u200b\u5176\u200b\u8f93\u51fa\u200b\u5c42\u200b\u5df2\u200b\u8c03\u6574\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5206\u7c7b\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u76f8\u540c\u200b\u7684\u200b\u4ee3\u7801\u200b\u6765\u200b\u8bbe\u7f6e\u200b <code>EfficientNet_B0</code> \u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ImageNet \u200b\u6743\u91cd\u200b\u3002</p> <pre>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = ImageNet \u200b\u7684\u200b\u6700\u4f73\u200b\u53ef\u7528\u200b\u6743\u91cd\u200b\n</pre> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u8be5\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u5728\u200b\u6570\u767e\u4e07\u200b\u5f20\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5177\u6709\u200b\u826f\u597d\u200b\u7684\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u57fa\u7840\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u8be5\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b PyTorch \u200b\u7248\u672c\u200b\u80fd\u591f\u200b\u5728\u200b ImageNet \u200b\u7684\u200b 1000 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\u4e2d\u200b\u8fbe\u5230\u200b\u7ea6\u200b 77.7% \u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u4f1a\u200b\u5c06\u200b\u5176\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#33-torchinfosummary","title":"3.3 \u200b\u4f7f\u7528\u200b <code>torchinfo.summary()</code> \u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u7684\u200b\u6982\u8981\u200b\u4fe1\u606f\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u66f4\u200b\u6df1\u5165\u200b\u5730\u200b\u4e86\u89e3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torchinfo</code> \u200b\u7684\u200b <code>summary()</code> \u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f20\u5165\u200b\u4ee5\u4e0b\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li><code>model</code> - \u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u83b7\u53d6\u200b\u6982\u8981\u200b\u4fe1\u606f\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> <li><code>input_size</code> - \u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6a21\u578b\u200b\u7684\u200b\u6570\u636e\u200b\u5f62\u72b6\u200b\uff0c\u200b\u5bf9\u4e8e\u200b <code>efficientnet_b0</code> \u200b\u6a21\u578b\u200b\uff0c\u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\u662f\u200b <code>(batch_size, 3, 224, 224)</code>\uff0c\u200b\u5c3d\u7ba1\u200b \u200b\u5176\u4ed6\u200b <code>efficientnet_bX</code> \u200b\u53d8\u4f53\u200b\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b\u8f93\u5165\u200b\u5927\u5c0f\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bb8\u591a\u200b\u73b0\u4ee3\u200b\u6a21\u578b\u200b\u7531\u4e8e\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>torch.nn.AdaptiveAvgPool2d()</code> \u200b\u5c42\u200b\uff0c\u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u5c42\u4f1a\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u81ea\u200b\u9002\u5e94\u200b\u5730\u200b\u8c03\u6574\u200b\u7ed9\u5b9a\u200b\u8f93\u5165\u200b\u7684\u200b <code>output_size</code>\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5411\u200b <code>summary()</code> \u200b\u6216\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u4f20\u9012\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</li> </ul> </li> <li><code>col_names</code> - \u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u770b\u5230\u200b\u7684\u200b\u5173\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u5404\u79cd\u200b\u4fe1\u606f\u200b\u5217\u200b\u3002</li> <li><code>col_width</code> - \u200b\u6982\u8981\u200b\u4fe1\u606f\u200b\u4e2d\u200b\u5404\u5217\u200b\u7684\u200b\u5bbd\u5ea6\u200b\u3002</li> <li><code>row_settings</code> - \u200b\u884c\u4e2d\u200b\u663e\u793a\u200b\u7684\u200b\u7279\u5f81\u200b\u3002</li> </ul>"},{"location":"06_pytorch_transfer_learning/#34","title":"3.4 \u200b\u51bb\u7ed3\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b\u5e76\u200b\u8c03\u6574\u200b\u8f93\u51fa\u200b\u5c42\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b\u9700\u6c42\u200b\u00b6","text":"<p>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u901a\u5e38\u200b\u662f\u200b\u8fd9\u6837\u200b\u7684\u200b\uff1a\u200b\u51bb\u7ed3\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u4e00\u4e9b\u200b\u57fa\u7840\u200b\u5c42\u200b\uff08\u200b\u901a\u5e38\u200b\u662f\u200b <code>features</code> \u200b\u90e8\u5206\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u8c03\u6574\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u5934\u90e8\u200b/\u200b\u5206\u7c7b\u5668\u200b\u5c42\u200b\uff09\u200b\u4ee5\u200b\u9002\u5e94\u200b\u4f60\u200b\u7684\u200b\u9700\u6c42\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u66f4\u6539\u200b\u8f93\u51fa\u200b\u5c42\u4ee5\u200b\u9002\u5e94\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u6765\u200b\u5b9a\u5236\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002\u200b\u539f\u59cb\u200b\u7684\u200b <code>torchvision.models.efficientnet_b0()</code> \u200b\u5e26\u6709\u200b <code>out_features=1000</code>\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u5b83\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b ImageNet \u200b\u4e2d\u6709\u200b 1000 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5373\u200b\u5bf9\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b <code>out_features=3</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u51bb\u7ed3\u200b <code>efficientnet_b0</code> \u200b\u6a21\u578b\u200b\u4e2d\u200b <code>features</code> \u200b\u90e8\u5206\u200b\u7684\u200b\u6240\u6709\u200b\u5c42\u200b/\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u51bb\u7ed3\u200b\u5c42\u200b\u610f\u5473\u7740\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4fdd\u6301\u200b\u5b83\u4eec\u200b\u4e0d\u53d8\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u6709\u9884\u200b\u8bad\u7ec3\u200b\u5c42\u200b\uff0c\u200b\u51bb\u7ed3\u200b\u5b83\u4eec\u200b\u5c31\u662f\u8bf4\u200b\uff1a\u201c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4e0d\u8981\u200b\u6539\u53d8\u200b\u8fd9\u4e9b\u200b\u5c42\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u4fdd\u6301\u200b\u5b83\u4eec\u200b\u539f\u6765\u200b\u7684\u200b\u6837\u5b50\u200b\u3002\u201d \u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4fdd\u6301\u200b\u6a21\u578b\u200b\u4ece\u200b ImageNet \u200b\u5b66\u5230\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b/\u200b\u6a21\u5f0f\u200b\u4f5c\u4e3a\u200b\u4e3b\u5e72\u200b\uff0c\u200b\u7136\u540e\u200b\u53ea\u200b\u6539\u53d8\u200b\u8f93\u51fa\u200b\u5c42\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b\u5c5e\u6027\u200b <code>requires_grad=False</code> \u200b\u6765\u200b\u51bb\u7ed3\u200b <code>features</code> \u200b\u90e8\u5206\u200b\u7684\u200b\u6240\u6709\u200b\u5c42\u200b/\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b <code>requires_grad=False</code> \u200b\u7684\u200b\u53c2\u6570\u200b\uff0cPyTorch \u200b\u4e0d\u4f1a\u200b\u8ddf\u8e2a\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u4e0d\u4f1a\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u88ab\u200b\u6211\u4eec\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u6539\u53d8\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u5177\u6709\u200b <code>requires_grad=False</code> \u200b\u7684\u200b\u53c2\u6570\u200b\u662f\u200b\u201c\u200b\u4e0d\u53ef\u200b\u8bad\u7ec3\u200b\u201d\u200b\u7684\u200b\u6216\u200b\u201c\u200b\u51bb\u7ed3\u200b\u201d\u200b\u5728\u200b\u539f\u5730\u200b\u7684\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#4","title":"4. \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u534a\u200b\u51bb\u7ed3\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6dfb\u52a0\u200b\u4e86\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b <code>classifier</code>\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u5b9e\u9645\u6548\u679c\u200b\u5427\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4ecd\u200b\u5728\u200b\u8fdb\u884c\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>nn.CrossEntropyLoss()</code> \u200b\u4f5c\u4e3a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u4f7f\u7528\u200b <code>torch.optim.Adam()</code> \u200b\u4f5c\u4e3a\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u8bbe\u200b\u4e3a\u200b <code>lr=0.001</code>\u3002</p>"},{"location":"06_pytorch_transfer_learning/#5","title":"5. \u200b\u901a\u8fc7\u200b\u7ed8\u5236\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u770b\u8d77\u6765\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ed8\u5236\u200b\u5176\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u770b\u770b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u968f\u200b\u65f6\u95f4\u200b\u7684\u200b\u53d8\u5316\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b7.8\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>plot_loss_curves()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u7ed8\u5236\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u3002</p> <p>\u200b\u8be5\u200b\u51fd\u6570\u200b\u5b58\u50a8\u200b\u5728\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u4e2d\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5c06\u200b\u5c1d\u8bd5\u200b\u5bfc\u5165\u200b\u5b83\u200b\u5e76\u200b\u5728\u200b\u6ca1\u6709\u200b\u8be5\u200b\u811a\u672c\u200b\u65f6\u200b\u4e0b\u8f7d\u200b\u5b83\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#6","title":"6. \u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u5b9a\u91cf\u200b\u8bc4\u4f30\u200b\u4e0a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\uff0c\u200b\u4f46\u200b\u5b9a\u6027\u200b\u8bc4\u4f30\u200b\u5982\u4f55\u200b\u5462\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u4e2d\u200b\u7684\u200b\u56fe\u50cf\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u672a\u200b\u88ab\u200b\u4f7f\u7528\u200b\uff09\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u7ed8\u5236\u200b\u7ed3\u679c\u200b\u6765\u200b\u4e00\u63a2\u200b\u7a76\u7adf\u200b\u3002</p> <p>\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8bb0\u4f4f\u200b\u7684\u200b\u4e00\u70b9\u200b\u662f\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u6a21\u578b\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5fc5\u987b\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u4f7f\u7528\u200b\u7684\u200b\u56fe\u50cf\u683c\u5f0f\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u56fe\u50cf\u200b\u5177\u6709\u200b\uff1a</p> <ul> <li>\u200b\u76f8\u540c\u200b\u7684\u200b\u5f62\u72b6\u200b - \u200b\u5982\u679c\u200b\u56fe\u50cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u9047\u5230\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u3002</li> <li>\u200b\u76f8\u540c\u200b\u7684\u200b\u7c7b\u578b\u200b - \u200b\u5982\u679c\u200b\u56fe\u50cf\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e0d\u540c\u200b\uff08\u200b\u4f8b\u5982\u200b <code>torch.int8</code> \u200b\u4e0e\u200b <code>torch.float32</code>\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u9047\u5230\u200b\u7c7b\u578b\u200b\u9519\u8bef\u200b\u3002</li> <li>\u200b\u76f8\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b - \u200b\u5982\u679c\u200b\u56fe\u50cf\u200b\u4f4d\u4e8e\u200b\u4e0e\u200b\u6a21\u578b\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u9047\u5230\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\u3002</li> <li>\u200b\u76f8\u540c\u200b\u7684\u200b\u53d8\u6362\u200b - \u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b\u7ecf\u8fc7\u200b\u7279\u5b9a\u200b\u65b9\u5f0f\u200b\u53d8\u6362\u200b\uff08\u200b\u4f8b\u5982\u200b\u4f7f\u7528\u200b\u7279\u5b9a\u200b\u5747\u503c\u200b\u548c\u200b\u6807\u51c6\u5dee\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\uff09\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u5bf9\u4ee5\u200b\u4e0d\u540c\u200b\u65b9\u5f0f\u200b\u53d8\u6362\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0d\u200b\u51c6\u786e\u200b\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e9b\u200b\u8981\u6c42\u200b\u9002\u7528\u200b\u4e8e\u200b\u6240\u6709\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u8bd5\u56fe\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002\u200b\u4f60\u200b\u60f3\u8981\u200b\u9884\u6d4b\u200b\u7684\u200b\u6570\u636e\u200b\u5e94\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u6570\u636e\u683c\u5f0f\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u5207\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b <code>pred_and_plot_image()</code> \u200b\u6765\u200b\uff1a</p> <ol> <li>\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u4e2a\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u5217\u8868\u200b\u3001\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u3001\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u3001\u200b\u53d8\u6362\u200b\u548c\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>PIL.Image.open()</code> \u200b\u6253\u5f00\u200b\u56fe\u50cf\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u53d8\u6362\u200b\uff08\u200b\u9ed8\u8ba4\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u521b\u5efa\u200b\u7684\u200b <code>manual_transforms</code>\uff0c\u200b\u6216\u8005\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>weights.transforms()</code> \u200b\u751f\u6210\u200b\u7684\u200b\u53d8\u6362\u200b\uff09\u3002</li> <li>\u200b\u786e\u4fdd\u200b\u6a21\u578b\u200b\u4f4d\u4e8e\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>model.eval()</code> \u200b\u5f00\u542f\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u6a21\u5f0f\u200b\uff08\u200b\u8fd9\u200b\u5c06\u200b\u5173\u95ed\u200b <code>nn.Dropout()</code> \u200b\u7b49\u5c42\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5b83\u4eec\u200b\u4e0d\u200b\u7528\u4e8e\u200b\u63a8\u7406\u200b\uff09\u200b\u548c\u200b\u63a8\u7406\u200b\u6a21\u5f0f\u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6b65\u9aa4\u200b 3 \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u53d8\u6362\u200b\u5bf9\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torch.unsqueeze(dim=0)</code> \u200b\u6dfb\u52a0\u200b\u989d\u5916\u200b\u7684\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u5177\u6709\u200b\u5f62\u72b6\u200b <code>[batch_size, color_channels, height, width]</code>\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u786e\u4fdd\u200b\u56fe\u50cf\u200b\u4f4d\u4e8e\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u5c06\u200b\u56fe\u50cf\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torch.softmax()</code> \u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\u5bf9\u6570\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torch.argmax()</code> \u200b\u5c06\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>matplotlib</code> \u200b\u7ed8\u5236\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u6807\u9898\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u6b65\u9aa4\u200b 9 \u200b\u4e2d\u200b\u7684\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u548c\u200b\u6b65\u9aa4\u200b 8 \u200b\u4e2d\u200b\u7684\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</li> </ol> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u4e0e\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u7ae0\u8282\u200b 11.3 \u200b\u4e2d\u200b\u7684\u200b <code>pred_and_plot_image()</code> \u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u4f46\u200b\u8fdb\u884c\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u8c03\u6574\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/#61","title":"6.1 \u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u6570\u636e\u200b\u4e0a\u200b\u5b9a\u6027\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u5462\u200b\uff1f</p> <p>\u200b\u8fd9\u624d\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u771f\u6b63\u200b\u6709\u8da3\u200b\u7684\u200b\u5730\u65b9\u200b\uff01</p> <p>\u200b\u5bf9\u200b\u4e0d\u200b\u5c5e\u4e8e\u200b\u4efb\u4f55\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u6216\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u7684\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5728\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u6d4b\u8bd5\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bfc\u5165\u200b\u7ecf\u5178\u200b\u7684\u200b <code>pizza-dad.jpeg</code> \u200b\u56fe\u50cf\u200b\uff08\u200b\u4e00\u5f20\u200b\u6211\u200b\u7238\u7238\u200b\u5403\u200b\u62ab\u8428\u200b\u7684\u200b\u7167\u7247\u200b\uff09\u3002</p> <p>\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u200b\u4f20\u9012\u200b\u7ed9\u200b\u4e0a\u9762\u200b\u521b\u5efa\u200b\u7684\u200b <code>pred_and_plot_image()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p>"},{"location":"06_pytorch_transfer_learning/","title":"\u4e3b\u8981\u200b\u6536\u83b7\u200b\u00b6","text":"<ul> <li>\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u901a\u5e38\u200b\u5141\u8bb8\u200b\u4f60\u200b\u7528\u200b\u76f8\u5bf9\u200b\u8f83\u5c11\u200b\u7684\u200b\u6570\u636e\u200b\u83b7\u5f97\u200b\u826f\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</li> <li>\u200b\u4e86\u89e3\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u529b\u91cf\u200b\u540e\u200b\uff0c\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u95ee\u9898\u200b\u7684\u200b\u5f00\u59cb\u200b\u8be2\u95ee\u200b\u81ea\u5df1\u200b\uff1a\u201c\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u89e3\u51b3\u200b\u6211\u200b\u7684\u200b\u95ee\u9898\u200b\uff1f\u201d\u200b\u662f\u200b\u4e00\u4e2a\u200b\u597d\u200b\u4e3b\u610f\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u786e\u4fdd\u200b\u4f60\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u6570\u636e\u683c\u5f0f\u200b/\u200b\u9884\u5904\u7406\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u539f\u59cb\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e00\u81f4\u200b\uff0c\u200b\u5426\u5219\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u6027\u80fd\u200b\u4e0b\u964d\u200b\u3002</li> <li>\u200b\u540c\u6837\u200b\uff0c\u200b\u5728\u200b\u9884\u6d4b\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u65f6\u200b\uff0c\u200b\u786e\u4fdd\u200b\u4f60\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u6570\u636e\u683c\u5f0f\u200b\u4e0e\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u683c\u5f0f\u200b\u76f8\u540c\u200b\u3002</li> <li>\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u591a\u4e2a\u200b\u5730\u65b9\u200b\u627e\u5230\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u62ec\u200bPyTorch\u200b\u9886\u57df\u200b\u5e93\u200b\u3001HuggingFace Hub\u200b\u4ee5\u53ca\u200b<code>timm</code>\uff08PyTorch\u200b\u56fe\u50cf\u200b\u6a21\u578b\u200b\uff09\u200b\u7b49\u5e93\u200b\u3002</li> </ul>"},{"location":"06_pytorch_transfer_learning/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u7ec3\u4e60\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u6216\u200b\u9075\u5faa\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u7b2c\u200b06\u200b\u8bb2\u200b\u7684\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b</li> <li>\u200b\u7b2c\u200b06\u200b\u8bb2\u200b\u7684\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5728\u200b\u67e5\u770b\u200b\u8fd9\u4e2a\u200b\u4e4b\u524d\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\uff09<ul> <li>\u200b\u5728\u200bYouTube\u200b\u4e0a\u200b\u89c2\u770b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7684\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b\uff08\u200b\u5305\u62ec\u200b\u6240\u6709\u200b\u9519\u8bef\u200b\uff09</li> </ul> </li> </ul> <ol> <li>\u200b\u5bf9\u200b\u6574\u4e2a\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5e76\u200b\u7ed8\u5236\u200b\u4e00\u4e2a\u200b\u6df7\u6dc6\u200b\u77e9\u9635\u200b\uff0c\u200b\u6bd4\u8f83\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u771f\u5b9e\u200b\u6807\u7b7e\u200b\u3002\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b03. PyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7b2c\u200b10\u200b\u8282\u200b\u83b7\u53d6\u200b\u601d\u8def\u200b\u3002</li> <li>\u200b\u83b7\u53d6\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u9884\u6d4b\u200b\u201c\u200b\u6700\u200b\u9519\u8bef\u200b\u201d\u200b\u7684\u200b\u6837\u672c\u200b\uff0c\u200b\u5e76\u200b\u7ed8\u5236\u200b5\u200b\u4e2a\u200b\u201c\u200b\u6700\u200b\u9519\u8bef\u200b\u201d\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1a<ul> <li>\u200b\u5bf9\u200b\u6574\u4e2a\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5b58\u50a8\u200b\u6807\u7b7e\u200b\u548c\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3002</li> <li>\u200b\u6309\u200b\u9519\u8bef\u200b\u9884\u6d4b\u200b\u548c\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u964d\u5e8f\u200b\u6392\u5e8f\u200b\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u7ed9\u200b\u4f60\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u6700\u9ad8\u200b\u7684\u200b\u9519\u8bef\u200b\u9884\u6d4b\u200b\uff0c\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5c31\u662f\u200b\u201c\u200b\u6700\u200b\u9519\u8bef\u200b\u201d\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002</li> <li>\u200b\u7ed8\u5236\u200b\u524d\u200b5\u200b\u4e2a\u200b\u201c\u200b\u6700\u200b\u9519\u8bef\u200b\u201d\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u6a21\u578b\u200b\u4e3a\u4ec0\u4e48\u200b\u4f1a\u200b\u51fa\u9519\u200b\uff1f</li> </ul> </li> <li>\u200b\u5bf9\u200b\u81ea\u5df1\u200b\u62cd\u6444\u200b\u7684\u200b\u62ab\u8428\u200b/\u200b\u725b\u6392\u200b/\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u2014\u2014\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff1f\u200b\u5982\u679c\u200b\u5bf9\u975e\u200b\u62ab\u8428\u200b/\u200b\u725b\u6392\u200b/\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</li> <li>\u200b\u5c06\u200b\u4e0a\u8ff0\u200b\u7b2c\u200b4\u200b\u8282\u200b\u7684\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff0810\u200b\u4e2a\u200b\u5468\u671f\u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\uff09\uff0c\u200b\u6027\u80fd\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\uff1f</li> <li>\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\u8bad\u7ec3\u200b\u4e0a\u8ff0\u200b\u7b2c\u200b4\u200b\u8282\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4ece\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u9009\u53d6\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u3002<ul> <li>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200bGitHub\u200b\u4e0a\u200b\u627e\u5230\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u200b\u3002\u200b\u5b83\u200b\u662f\u200b\u901a\u8fc7\u200b\u7b14\u8bb0\u672c\u200b<code>extras/04_custom_data_creation.ipynb</code>\u200b\u521b\u5efa\u200b\u7684\u200b\u3002</li> </ul> </li> <li>\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u4e0a\u200b\u4f7f\u7528\u200b<code>torchvision.models</code>\u200b\u4e2d\u200b\u7684\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff1f<ul> <li>\u200b\u4f60\u200b\u9700\u8981\u200b\u8c03\u6574\u200b\u5206\u7c7b\u5668\u200b\u5c42\u200b\u7684\u200b\u5927\u5c0f\u200b\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</li> <li>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u60f3\u200b\u5c1d\u8bd5\u200b\u4e00\u4e2a\u200b\u6bd4\u200bB0\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200bEfficientNet\uff0c\u200b\u6bd4\u5982\u200b<code>torchvision.models.efficientnet_b2()</code>\uff1f</li> </ul> </li> </ol>"},{"location":"06_pytorch_transfer_learning/","title":"\u989d\u5916\u200b\u8bfe\u7a0b\u200b\u00b6","text":"<ul> <li>\u200b\u67e5\u627e\u200b\u4ec0\u4e48\u200b\u662f\u200b\u201c\u200b\u6a21\u578b\u200b\u5fae\u8c03\u200b\u201d\uff0c\u200b\u5e76\u82b1\u200b30\u200b\u5206\u949f\u200b\u7814\u7a76\u200b\u4f7f\u7528\u200bPyTorch\u200b\u8fdb\u884c\u200b\u4e0d\u540c\u200b\u65b9\u6cd5\u200b\u7684\u200b\u5fae\u8c03\u200b\u3002\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u6539\u53d8\u200b\u4ee3\u7801\u200b\u6765\u200b\u8fdb\u884c\u200b\u5fae\u8c03\u200b\uff1f\u200b\u63d0\u793a\u200b\uff1a\u200b\u5fae\u8c03\u200b\u901a\u5e38\u200b\u5728\u200b\u4f60\u200b\u6709\u200b\u5927\u91cf\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u65f6\u200b\u6548\u679c\u200b\u6700\u597d\u200b\uff0c\u200b\u800c\u200b\u7279\u5f81\u63d0\u53d6\u200b\u901a\u5e38\u200b\u5728\u200b\u4f60\u200b\u6709\u200b\u8f83\u200b\u5c11\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u65f6\u200b\u6548\u679c\u200b\u66f4\u597d\u200b\u3002</li> <li>\u200b\u67e5\u770b\u200b\u65b0\u200b\u7684\u200b/\u200b\u5373\u5c06\u200b\u63a8\u51fa\u200b\u7684\u200bPyTorch\u200b\u591a\u200b\u6743\u91cd\u200bAPI\uff08\u200b\u5728\u200b\u64b0\u5199\u200b\u672c\u6587\u200b\u65f6\u200b\u4ecd\u200b\u5904\u4e8e\u200b\u6d4b\u8bd5\u9636\u6bb5\u200b\uff0c2022\u200b\u5e74\u200b5\u200b\u6708\u200b\uff09\uff0c\u200b\u8fd9\u200b\u662f\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u65b0\u200b\u65b9\u6cd5\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u4ee3\u7801\u200b\u9700\u8981\u200b\u8fdb\u884c\u200b\u54ea\u4e9b\u200b\u66f4\u6539\u200b\u624d\u80fd\u200b\u4f7f\u7528\u200b\u65b0\u200bAPI\uff1f</li> <li>\u200b\u5c1d\u8bd5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u9488\u5bf9\u200b\u4e24\u7c7b\u200b\u56fe\u50cf\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6536\u96c6\u200b10\u200b\u5f20\u200b\u4f60\u200b\u548c\u200b\u4f60\u200b\u670b\u53cb\u200b\u7684\u200b\u72d7\u200b\u7684\u200b\u7167\u7247\u200b\uff0c\u200b\u5e76\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6765\u200b\u5206\u7c7b\u200b\u8fd9\u200b\u4e24\u53ea\u200b\u72d7\u200b\u3002\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u65e2\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8be5\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u3002</li> </ul>"},{"location":"07_pytorch_experiment_tracking/","title":"07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220620+cu113\ntorchvision version: 0.14.0.dev20220620+cu113\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u5728\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u5355\u5143\u683c\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u91cd\u542f\u200b\u8fd0\u884c\u200b\u65f6\u200b\u3002\u200b\u91cd\u542f\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u8fd0\u884c\u200b\u8be5\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u5e76\u200b\u9a8c\u8bc1\u200b\u4f60\u200b\u5df2\u200b\u5b89\u88c5\u200b\u6b63\u786e\u200b\u7248\u672c\u200b\u7684\u200b <code>torch</code>\uff080.12+\uff09\u200b\u548c\u200b <code>torchvision</code>\uff080.13+\uff09\u3002</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u4e0e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u5f00\u542f\u200b GPU\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u901a\u8fc7\u200b <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code> \u200b\u6765\u200b\u5f00\u542f\u200b\u4e00\u4e2a\u200b GPU \u200b\u4e86\u200b\u3002</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Set seeds\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n</pre> # Set seeds def set_seeds(seed: int=42):     \"\"\"Sets random sets for torch operations.      Args:         seed (int, optional): Random seed to set. Defaults to 42.     \"\"\"     # Set the seed for general torch operations     torch.manual_seed(seed)     # Set the seed for CUDA torch operations (ones that happen on the GPU)     torch.cuda.manual_seed(seed) In\u00a0[5]: Copied! <pre>import os\nimport zipfile\n\nfrom pathlib import Path\n\nimport requests\n\ndef download_data(source: str, \n                  destination: str,\n                  remove_source: bool = True) -&gt; Path:\n    \"\"\"Downloads a zipped dataset from source and unzips to destination.\n\n    Args:\n        source (str): A link to a zipped file containing data.\n        destination (str): A target directory to unzip data to.\n        remove_source (bool): Whether to remove the source after downloading and extracting.\n    \n    Returns:\n        pathlib.Path to downloaded data.\n    \n    Example usage:\n        download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                      destination=\"pizza_steak_sushi\")\n    \"\"\"\n    # Setup path to data folder\n    data_path = Path(\"data/\")\n    image_path = data_path / destination\n\n    # If the image folder doesn't exist, download it and prepare it... \n    if image_path.is_dir():\n        print(f\"[INFO] {image_path} directory exists, skipping download.\")\n    else:\n        print(f\"[INFO] Did not find {image_path} directory, creating one...\")\n        image_path.mkdir(parents=True, exist_ok=True)\n        \n        # Download pizza, steak, sushi data\n        target_file = Path(source).name\n        with open(data_path / target_file, \"wb\") as f:\n            request = requests.get(source)\n            print(f\"[INFO] Downloading {target_file} from {source}...\")\n            f.write(request.content)\n\n        # Unzip pizza, steak, sushi data\n        with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:\n            print(f\"[INFO] Unzipping {target_file} data...\") \n            zip_ref.extractall(image_path)\n\n        # Remove .zip file\n        if remove_source:\n            os.remove(data_path / target_file)\n    \n    return image_path\n\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> import os import zipfile  from pathlib import Path  import requests  def download_data(source: str,                    destination: str,                   remove_source: bool = True) -&gt; Path:     \"\"\"Downloads a zipped dataset from source and unzips to destination.      Args:         source (str): A link to a zipped file containing data.         destination (str): A target directory to unzip data to.         remove_source (bool): Whether to remove the source after downloading and extracting.          Returns:         pathlib.Path to downloaded data.          Example usage:         download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                       destination=\"pizza_steak_sushi\")     \"\"\"     # Setup path to data folder     data_path = Path(\"data/\")     image_path = data_path / destination      # If the image folder doesn't exist, download it and prepare it...      if image_path.is_dir():         print(f\"[INFO] {image_path} directory exists, skipping download.\")     else:         print(f\"[INFO] Did not find {image_path} directory, creating one...\")         image_path.mkdir(parents=True, exist_ok=True)                  # Download pizza, steak, sushi data         target_file = Path(source).name         with open(data_path / target_file, \"wb\") as f:             request = requests.get(source)             print(f\"[INFO] Downloading {target_file} from {source}...\")             f.write(request.content)          # Unzip pizza, steak, sushi data         with zipfile.ZipFile(data_path / target_file, \"r\") as zip_ref:             print(f\"[INFO] Unzipping {target_file} data...\")              zip_ref.extractall(image_path)          # Remove .zip file         if remove_source:             os.remove(data_path / target_file)          return image_path  image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[5]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u6807\u51c6\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u683c\u5f0f\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b\u3002</p> In\u00a0[6]: Copied! <pre># Setup directories\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \nprint(f\"Manually created transforms: {manual_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup directories train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])            print(f\"Manually created transforms: {manual_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n)\n</pre> Out[6]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d218e0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d216a0&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[7]: Copied! <pre># Setup dirs\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n\n# Setup pretrained weights (plenty of these available in torchvision.models)\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n\n# Get transforms from weights (these are the transforms that were used to obtain the weights)\nautomatic_transforms = weights.transforms() \nprint(f\"Automatically created transforms: {automatic_transforms}\")\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=automatic_transforms, # use automatic created transforms\n    batch_size=32\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = image_path / \"train\" test_dir = image_path / \"test\"  # Setup pretrained weights (plenty of these available in torchvision.models) weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT  # Get transforms from weights (these are the transforms that were used to obtain the weights) automatic_transforms = weights.transforms()  print(f\"Automatically created transforms: {automatic_transforms}\")  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=automatic_transforms, # use automatic created transforms     batch_size=32 )  train_dataloader, test_dataloader, class_names <pre>Automatically created transforms: ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n</pre> Out[7]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d213a0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7febf1d21490&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[8]: Copied! <pre># Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions.\n# model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD \n\n# Download the pretrained weights for EfficientNet_B0\nweights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"\n\n# Setup the model with the pretrained weights and send it to the target device\nmodel = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n# View the output of the model\n# model\n</pre> # Note: This is how a pretrained model would be created in torchvision &gt; 0.13, it will be deprecated in future versions. # model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # OLD   # Download the pretrained weights for EfficientNet_B0 weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # NEW in torchvision 0.13, \"DEFAULT\" means \"best weights available\"  # Setup the model with the pretrained weights and send it to the target device model = torchvision.models.efficientnet_b0(weights=weights).to(device)  # View the output of the model # model <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u5c06\u200b\u5176\u200b\u8f6c\u53d8\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u51bb\u7ed3\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u4ece\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u63d0\u53d6\u200b\u7279\u5f81\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u5c06\u200b\u6539\u53d8\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\uff08\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff09\u200b\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\uff08\u200b\u6211\u4eec\u200b\u6709\u200b3\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff1a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u521b\u5efa\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u6982\u5ff5\u200b\uff08\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6240\u200b\u505a\u200b\u7684\u200b\uff09\u200b\u5728\u200b06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b3.2\u200b\u8282\u200b\uff1a\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e2d\u6709\u200b\u66f4\u200b\u6df1\u5165\u200b\u7684\u200b\u63a2\u8ba8\u200b\u3002</p> In\u00a0[9]: Copied! <pre># Freeze all base layers by setting requires_grad attribute to False\nfor param in model.features.parameters():\n    param.requires_grad = False\n    \n# Since we're creating a new layer with random weights (torch.nn.Linear), \n# let's set the seeds\nset_seeds() \n\n# Update the classifier head to suit our problem\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2, inplace=True),\n    nn.Linear(in_features=1280, \n              out_features=len(class_names),\n              bias=True).to(device))\n</pre> # Freeze all base layers by setting requires_grad attribute to False for param in model.features.parameters():     param.requires_grad = False      # Since we're creating a new layer with random weights (torch.nn.Linear),  # let's set the seeds set_seeds()   # Update the classifier head to suit our problem model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2, inplace=True),     nn.Linear(in_features=1280,                out_features=len(class_names),               bias=True).to(device)) <p>\u200b\u57fa\u7840\u200b\u5c42\u200b\u5df2\u200b\u51bb\u7ed3\u200b\uff0c\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u5df2\u200b\u66f4\u6539\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torchinfo.summary()</code> \u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u7684\u200b\u6458\u8981\u200b\u3002</p> In\u00a0[10]: Copied! <pre>from torchinfo import summary\n\n# # Get a summary of the model (uncomment for full output)\n# summary(model, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n#         verbose=0,\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Get a summary of the model (uncomment for full output) # summary(model,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width) #         verbose=0, #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p><code>torchinfo.summary()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5c55\u793a\u200b\u4e86\u200b\u6211\u4eec\u200b\u7684\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b EffNetB0 \u200b\u6a21\u578b\u200b\uff0c\u200b\u6ce8\u610f\u200b\u57fa\u672c\u200b\u5c42\u200b\u662f\u200b\u5982\u4f55\u200b\u88ab\u200b\u51bb\u7ed3\u200b\uff08\u200b\u4e0d\u53ef\u200b\u8bad\u7ec3\u200b\uff09\u200b\u7684\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5c42\u200b\u662f\u200b\u5982\u4f55\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u5b9a\u5236\u200b\u7684\u200b\u3002</p> In\u00a0[11]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[12]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\n\n# Create a writer with all default settings\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter  # Create a writer with all default settings writer = SummaryWriter() <p>\u200b\u73b0\u5728\u200b\u8981\u200b\u4f7f\u7528\u200b\u8fd9\u4e2a\u200b\u5199\u5165\u200b\u5668\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff0c\u200b\u6216\u8005\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\u5728\u200b05. PyTorch Going Modular \u200b\u7b2c\u200b4\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u73b0\u6709\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b\u540e\u8005\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>engine.py</code> \u200b\u83b7\u53d6\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u200b\u8c03\u6574\u200b\u5b83\u200b\u4ee5\u200b\u4f7f\u7528\u200b <code>writer</code>\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6dfb\u52a0\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u8bb0\u5f55\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u53ca\u200b\u51c6\u786e\u5ea6\u200b\u503c\u200b\u7684\u200b\u529f\u80fd\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>writer.add_scalars(main_tag, tag_scalar_dict)</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>main_tag</code>\uff08\u200b\u5b57\u7b26\u4e32\u200b\uff09- \u200b\u88ab\u200b\u8ddf\u8e2a\u200b\u6807\u91cf\u200b\u7684\u200b\u540d\u79f0\u200b\uff08\u200b\u4f8b\u5982\u200b \"Accuracy\"\uff09</li> <li><code>tag_scalar_dict</code>\uff08\u200b\u5b57\u5178\u200b\uff09- \u200b\u88ab\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u503c\u200b\u7684\u200b\u5b57\u5178\u200b\uff08\u200b\u4f8b\u5982\u200b <code>{\"train_loss\": 0.3454}</code>\uff09<ul> <li> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8be5\u200b\u65b9\u6cd5\u200b\u540d\u4e3a\u200b <code>add_scalars()</code>\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u635f\u5931\u200b\u548c\u200b\u51c6\u786e\u5ea6\u200b\u503c\u200b\u901a\u5e38\u200b\u662f\u200b\u6807\u91cf\u200b\uff08\u200b\u5355\u4e2a\u200b\u503c\u200b\uff09\u3002</p> </li> </ul> </li> </ul> <p>\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u5b8c\u6210\u200b\u4e86\u200b\u503c\u200b\u7684\u200b\u8ddf\u8e2a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8c03\u7528\u200b <code>writer.close()</code> \u200b\u6765\u200b\u544a\u8bc9\u200b <code>writer</code> \u200b\u505c\u6b62\u200b\u5bfb\u627e\u200b\u8981\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u503c\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5f00\u59cb\u200b\u4fee\u6539\u200b <code>train()</code>\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u4ece\u200b <code>engine.py</code> \u200b\u5bfc\u5165\u200b <code>train_step()</code> \u200b\u548c\u200b <code>test_step()</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4ee3\u7801\u200b\u7684\u200b\u4efb\u4f55\u200b\u5730\u65b9\u200b\u8ddf\u8e2a\u200b\u6709\u5173\u200b\u6a21\u578b\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002\u200b\u4f46\u200b\u5b9e\u9a8c\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff08\u200b\u5728\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u5185\u90e8\u200b\uff09\u200b\u8fdb\u884c\u200b\u8ddf\u8e2a\u200b\u3002</p> <p><code>torch.utils.tensorboard.SummaryWriter()</code> \u200b\u7c7b\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u6a21\u578b\u200b/\u200b\u6570\u636e\u200b\u7684\u200b\u4e0d\u540c\u200b\u65b9\u9762\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>add_graph()</code> \u200b\u7528\u4e8e\u200b\u8ddf\u8e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u8ba1\u7b97\u200b\u56fe\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u9009\u9879\u200b\uff0c\u200b\u67e5\u770b\u200b <code>SummaryWriter()</code> \u200b\u6587\u6863\u200b\u3002</p> In\u00a0[13]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\nfrom going_modular.going_modular.engine import train_step, test_step\n\n# Import train() function from: \n# https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      \n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                           dataloader=train_dataloader,\n                                           loss_fn=loss_fn,\n                                           optimizer=optimizer,\n                                           device=device)\n        test_loss, test_acc = test_step(model=model,\n                                        dataloader=test_dataloader,\n                                        loss_fn=loss_fn,\n                                        device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        ### New: Experiment tracking ###\n        # Add loss results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n\n        # Add accuracy results to SummaryWriter\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n        \n        # Track the PyTorch model architecture\n        writer.add_graph(model=model, \n                         # Pass in an example input\n                         input_to_model=torch.randn(32, 3, 224, 224).to(device))\n    \n    # Close the writer\n    writer.close()\n    \n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  from going_modular.going_modular.engine import train_step, test_step  # Import train() function from:  # https://github.com/mrdbourke/pytorch-deep-learning/blob/main/going_modular/going_modular/engine.py def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").            Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                            dataloader=train_dataloader,                                            loss_fn=loss_fn,                                            optimizer=optimizer,                                            device=device)         test_loss, test_acc = test_step(model=model,                                         dataloader=test_dataloader,                                         loss_fn=loss_fn,                                         device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          ### New: Experiment tracking ###         # Add loss results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)          # Add accuracy results to SummaryWriter         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)                  # Track the PyTorch model architecture         writer.add_graph(model=model,                           # Pass in an example input                          input_to_model=torch.randn(32, 3, 224, 224).to(device))          # Close the writer     writer.close()          ### End new ###      # Return the filled results at the end of the epochs     return results <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u73b0\u5df2\u200b\u66f4\u65b0\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>SummaryWriter()</code> \u200b\u5b9e\u4f8b\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u200b\u8fd0\u884c\u200b 5 \u200b\u4e2a\u200b\u5468\u671f\u200b\u5982\u4f55\u200b\uff1f</p> In\u00a0[14]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated to use writer\nset_seeds()\nresults = train(model=model,\n                train_dataloader=train_dataloader,\n                test_dataloader=test_dataloader,\n                optimizer=optimizer,\n                loss_fn=loss_fn,\n                epochs=5,\n                device=device)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated to use writer set_seeds() results = train(model=model,                 train_dataloader=train_dataloader,                 test_dataloader=test_dataloader,                 optimizer=optimizer,                 loss_fn=loss_fn,                 epochs=5,                 device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0924 | train_acc: 0.3984 | test_loss: 0.9133 | test_acc: 0.5398\nEpoch: 2 | train_loss: 0.8975 | train_acc: 0.6562 | test_loss: 0.7838 | test_acc: 0.8561\nEpoch: 3 | train_loss: 0.8037 | train_acc: 0.7461 | test_loss: 0.6723 | test_acc: 0.8864\nEpoch: 4 | train_loss: 0.6769 | train_acc: 0.8516 | test_loss: 0.6698 | test_acc: 0.8049\nEpoch: 5 | train_loss: 0.7065 | train_acc: 0.7188 | test_loss: 0.6746 | test_acc: 0.7737\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u8fd9\u91cc\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b06. PyTorch\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u5f97\u5230\u200b\u7684\u200b\u7ed3\u679c\u200b\u7565\u6709\u4e0d\u540c\u200b\u3002\u200b\u8fd9\u79cd\u200b\u5dee\u5f02\u200b\u6765\u81ea\u200b\u4e8e\u200b\u4f7f\u7528\u200b <code>engine.train()</code> \u200b\u548c\u200b\u6211\u4eec\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u3002\u200b\u4f60\u200b\u80fd\u200b\u731c\u200b\u5230\u200b\u4e3a\u4ec0\u4e48\u200b\u5417\u200b\uff1fPyTorch\u200b\u5173\u4e8e\u200b\u968f\u673a\u6027\u200b\u7684\u200b\u6587\u6863\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u6240\u200b\u5e2e\u52a9\u200b\u3002</p> <p>\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e0e\u200b06. PyTorch\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b4\u200b\u8282\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u76f8\u4f3c\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u540c\u4e4b\u5904\u200b\u5728\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b <code>writer</code> \u200b\u5b9e\u4f8b\u200b\u5df2\u7ecf\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b <code>runs/</code> \u200b\u76ee\u5f55\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4fdd\u5b58\u200b\u4f4d\u7f6e\u200b\u53ef\u80fd\u200b\u770b\u8d77\u6765\u200b\u50cf\u200b\uff1a</p> <pre><code>runs/Jun21_00-46-03_daniels_macbook_pro\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\u9ed8\u8ba4\u200b\u683c\u5f0f\u200b\u662f\u200b <code>runs/CURRENT_DATETIME_HOSTNAME</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u67e5\u770b\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4f46\u200b\u4f5c\u4e3a\u200b\u63d0\u9192\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u662f\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u4e2d\u200b\u8ddf\u8e2a\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> In\u00a0[15]: Copied! <pre># Check out the model results\nresults\n</pre> # Check out the model results results Out[15]: <pre>{'train_loss': [1.0923754647374153,\n  0.8974628075957298,\n  0.803724929690361,\n  0.6769256368279457,\n  0.7064960040152073],\n 'train_acc': [0.3984375, 0.65625, 0.74609375, 0.8515625, 0.71875],\n 'test_loss': [0.9132757981618246,\n  0.7837507526079813,\n  0.6722926497459412,\n  0.6698453426361084,\n  0.6746167540550232],\n 'test_acc': [0.5397727272727273,\n  0.8560606060606061,\n  0.8863636363636364,\n  0.8049242424242425,\n  0.7736742424242425]}</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u683c\u5f0f\u200b\u5316\u4e3a\u200b\u4e00\u4e2a\u200b\u7f8e\u89c2\u200b\u7684\u200b\u56fe\u8868\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u80fd\u200b\u60f3\u8c61\u200b\u8981\u200b\u8ddf\u8e2a\u200b\u8fd9\u4e48\u200b\u591a\u200b\u5b57\u5178\u200b\u5417\u200b\uff1f</p> <p>\u200b\u80af\u5b9a\u200b\u6709\u200b\u66f4\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b...</p> In\u00a0[16]: Copied! <pre># Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Example code to run in Jupyter or Google Colab Notebook (uncomment to try it out) # %load_ext tensorboard # %tensorboard --logdir runs <p>\u200b\u5982\u679c\u200b\u4e00\u5207\u200b\u64cd\u4f5c\u200b\u6b63\u786e\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u770b\u5230\u200b\u7c7b\u4f3c\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u5185\u5bb9\u200b\uff1a</p> <p>\u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u67e5\u770b\u200b\u5355\u4e2a\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u548c\u200b\u635f\u5931\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6709\u5173\u200b\u5728\u200b\u7b14\u8bb0\u672c\u200b\u6216\u200b\u5176\u4ed6\u200b\u4f4d\u7f6e\u200b\u8fd0\u884c\u200b TensorBoard \u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> <ul> <li>TensorFlow \u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u4f7f\u7528\u200b TensorBoard \u200b\u6307\u5357\u200b</li> <li>\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b TensorBoard.dev\uff08\u200b\u6709\u52a9\u4e8e\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b TensorBoard \u200b\u65e5\u5fd7\u200b\u4e0a\u200b\u4f20\u5230\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5206\u4eab\u200b\u7684\u200b\u94fe\u63a5\u200b\uff09</li> </ul> In\u00a0[17]: Copied! <pre>def create_writer(experiment_name: str, \n                  model_name: str, \n                  extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():\n    \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.\n\n    log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.\n\n    Where timestamp is the current date in YYYY-MM-DD format.\n\n    Args:\n        experiment_name (str): Name of experiment.\n        model_name (str): Name of model.\n        extra (str, optional): Anything extra to add to the directory. Defaults to None.\n\n    Returns:\n        torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.\n\n    Example usage:\n        # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"\n        writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb2\",\n                               extra=\"5_epochs\")\n        # The above is the same as:\n        writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")\n    \"\"\"\n    from datetime import datetime\n    import os\n\n    # Get timestamp of current date (all experiments on certain day live in same folder)\n    timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format\n\n    if extra:\n        # Create log directory path\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n    else:\n        log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n        \n    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n    return SummaryWriter(log_dir=log_dir)\n</pre> def create_writer(experiment_name: str,                    model_name: str,                    extra: str=None) -&gt; torch.utils.tensorboard.writer.SummaryWriter():     \"\"\"Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.      log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.      Where timestamp is the current date in YYYY-MM-DD format.      Args:         experiment_name (str): Name of experiment.         model_name (str): Name of model.         extra (str, optional): Anything extra to add to the directory. Defaults to None.      Returns:         torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.      Example usage:         # Create a writer saving to \"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\"         writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb2\",                                extra=\"5_epochs\")         # The above is the same as:         writer = SummaryWriter(log_dir=\"runs/2022-06-04/data_10_percent/effnetb2/5_epochs/\")     \"\"\"     from datetime import datetime     import os      # Get timestamp of current date (all experiments on certain day live in same folder)     timestamp = datetime.now().strftime(\"%Y-%m-%d\") # returns current date in YYYY-MM-DD format      if extra:         # Create log directory path         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)     else:         log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)              print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")     return SummaryWriter(log_dir=log_dir) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b <code>create_writer()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u8bd5\u8bd5\u200b\u5b83\u200b\u7684\u200b\u6548\u679c\u200b\u3002</p> In\u00a0[18]: Copied! <pre># Create an example writer\nexample_writer = create_writer(experiment_name=\"data_10_percent\",\n                               model_name=\"effnetb0\",\n                               extra=\"5_epochs\")\n</pre> # Create an example writer example_writer = create_writer(experiment_name=\"data_10_percent\",                                model_name=\"effnetb0\",                                extra=\"5_epochs\") <pre>[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u8bb0\u5f55\u200b\u548c\u200b\u8ffd\u6eaf\u200b\u5404\u79cd\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> In\u00a0[19]: Copied! <pre>from typing import Dict, List\nfrom tqdm.auto import tqdm\n\n# Add writer parameter to train()\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device, \n          writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer\n          ) -&gt; Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Stores metrics to specified writer log_dir if present.\n\n    Args:\n      model: A PyTorch model to be trained and tested.\n      train_dataloader: A DataLoader instance for the model to be trained on.\n      test_dataloader: A DataLoader instance for the model to be tested on.\n      optimizer: A PyTorch optimizer to help minimize the loss function.\n      loss_fn: A PyTorch loss function to calculate loss on both datasets.\n      epochs: An integer indicating how many epochs to train for.\n      device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n      writer: A SummaryWriter() instance to log model results to.\n\n    Returns:\n      A dictionary of training and testing loss as well as training and\n      testing accuracy metrics. Each metric has a value in a list for \n      each epoch.\n      In the form: {train_loss: [...],\n                train_acc: [...],\n                test_loss: [...],\n                test_acc: [...]} \n      For example if training for epochs=2: \n              {train_loss: [2.0616, 1.0537],\n                train_acc: [0.3945, 0.3945],\n                test_loss: [1.2641, 1.5706],\n                test_acc: [0.3400, 0.2973]} \n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n\n        ### New: Use the writer parameter to track experiments ###\n        # See if there's a writer, if so, log to it\n        if writer:\n            # Add results to SummaryWriter\n            writer.add_scalars(main_tag=\"Loss\", \n                               tag_scalar_dict={\"train_loss\": train_loss,\n                                                \"test_loss\": test_loss},\n                               global_step=epoch)\n            writer.add_scalars(main_tag=\"Accuracy\", \n                               tag_scalar_dict={\"train_acc\": train_acc,\n                                                \"test_acc\": test_acc}, \n                               global_step=epoch)\n\n            # Close the writer\n            writer.close()\n        else:\n            pass\n    ### End new ###\n\n    # Return the filled results at the end of the epochs\n    return results\n</pre> from typing import Dict, List from tqdm.auto import tqdm  # Add writer parameter to train() def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,            writer: torch.utils.tensorboard.writer.SummaryWriter # new parameter to take in a writer           ) -&gt; Dict[str, List]:     \"\"\"Trains and tests a PyTorch model.      Passes a target PyTorch models through train_step() and test_step()     functions for a number of epochs, training and testing the model     in the same epoch loop.      Calculates, prints and stores evaluation metrics throughout.      Stores metrics to specified writer log_dir if present.      Args:       model: A PyTorch model to be trained and tested.       train_dataloader: A DataLoader instance for the model to be trained on.       test_dataloader: A DataLoader instance for the model to be tested on.       optimizer: A PyTorch optimizer to help minimize the loss function.       loss_fn: A PyTorch loss function to calculate loss on both datasets.       epochs: An integer indicating how many epochs to train for.       device: A target device to compute on (e.g. \"cuda\" or \"cpu\").       writer: A SummaryWriter() instance to log model results to.      Returns:       A dictionary of training and testing loss as well as training and       testing accuracy metrics. Each metric has a value in a list for        each epoch.       In the form: {train_loss: [...],                 train_acc: [...],                 test_loss: [...],                 test_acc: [...]}        For example if training for epochs=2:                {train_loss: [2.0616, 1.0537],                 train_acc: [0.3945, 0.3945],                 test_loss: [1.2641, 1.5706],                 test_acc: [0.3400, 0.2973]}      \"\"\"     # Create empty results dictionary     results = {\"train_loss\": [],                \"train_acc\": [],                \"test_loss\": [],                \"test_acc\": []     }      # Loop through training and testing steps for a number of epochs     for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(model=model,                                           dataloader=train_dataloader,                                           loss_fn=loss_fn,                                           optimizer=optimizer,                                           device=device)         test_loss, test_acc = test_step(model=model,           dataloader=test_dataloader,           loss_fn=loss_fn,           device=device)          # Print out what's happening         print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f}\"         )          # Update results dictionary         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)           ### New: Use the writer parameter to track experiments ###         # See if there's a writer, if so, log to it         if writer:             # Add results to SummaryWriter             writer.add_scalars(main_tag=\"Loss\",                                 tag_scalar_dict={\"train_loss\": train_loss,                                                 \"test_loss\": test_loss},                                global_step=epoch)             writer.add_scalars(main_tag=\"Accuracy\",                                 tag_scalar_dict={\"train_acc\": train_acc,                                                 \"test_acc\": test_acc},                                 global_step=epoch)              # Close the writer             writer.close()         else:             pass     ### End new ###      # Return the filled results at the end of the epochs     return results In\u00a0[20]: Copied! <pre># Download 10 percent and 20 percent training data (if necessary)\ndata_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                                     destination=\"pizza_steak_sushi\")\n\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n</pre> # Download 10 percent and 20 percent training data (if necessary) data_10_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                                      destination=\"pizza_steak_sushi\")  data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\") <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n</pre> <p>\u200b\u6570\u636e\u200b\u5df2\u200b\u4e0b\u8f7d\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u7528\u4e8e\u200b\u4e0d\u540c\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u6570\u636e\u6587\u4ef6\u200b\u8def\u5f84\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u76ee\u5f55\u200b\u8def\u5f84\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6240\u6709\u200b\u5b9e\u9a8c\u200b\u90fd\u200b\u5c06\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u5373\u200b pizza, steak, sushi 10% \u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\uff09\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\u8def\u5f84\u200b\u3002</p> In\u00a0[21]: Copied! <pre># Setup training directory paths\ntrain_dir_10_percent = data_10_percent_path / \"train\"\ntrain_dir_20_percent = data_20_percent_path / \"train\"\n\n# Setup testing directory paths (note: use the same test dataset for both to compare the results)\ntest_dir = data_10_percent_path / \"test\"\n\n# Check the directories\nprint(f\"Training directory 10%: {train_dir_10_percent}\")\nprint(f\"Training directory 20%: {train_dir_20_percent}\")\nprint(f\"Testing directory: {test_dir}\")\n</pre> # Setup training directory paths train_dir_10_percent = data_10_percent_path / \"train\" train_dir_20_percent = data_20_percent_path / \"train\"  # Setup testing directory paths (note: use the same test dataset for both to compare the results) test_dir = data_10_percent_path / \"test\"  # Check the directories print(f\"Training directory 10%: {train_dir_10_percent}\") print(f\"Training directory 20%: {train_dir_20_percent}\") print(f\"Testing directory: {test_dir}\") <pre>Training directory 10%: data/pizza_steak_sushi/train\nTraining directory 20%: data/pizza_steak_sushi_20_percent/train\nTesting directory: data/pizza_steak_sushi/test\n</pre> In\u00a0[22]: Copied! <pre>from torchvision import transforms\n\n# Create a transform to normalize data distribution to be inline with ImageNet\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]\n                                 std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]\n\n# Compose transforms into a pipeline\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)), # 1. Resize the images\n    transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1\n    normalize # 3. Normalize the images so their distributions match the ImageNet dataset \n])\n</pre> from torchvision import transforms  # Create a transform to normalize data distribution to be inline with ImageNet normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], # values per colour channel [red, green, blue]                                  std=[0.229, 0.224, 0.225]) # values per colour channel [red, green, blue]  # Compose transforms into a pipeline simple_transform = transforms.Compose([     transforms.Resize((224, 224)), # 1. Resize the images     transforms.ToTensor(), # 2. Turn the images into tensors with values between 0 &amp; 1     normalize # 3. Normalize the images so their distributions match the ImageNet dataset  ]) <p>\u200b\u8f6c\u6362\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u5728\u200b05. PyTorch Going Modular \u200b\u7b2c\u200b2\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>data_setup.py</code>\u200b\u6a21\u5757\u200b\u4e2d\u200b\u7684\u200b<code>create_dataloaders()</code>\u200b\u51fd\u6570\u200b\u6765\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200bDataLoader\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b32\u200b\u6765\u200b\u521b\u5efa\u200bDataLoader\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u6240\u6709\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b<code>test_dataloader</code>\uff08\u200b\u4ee5\u200b\u4fdd\u6301\u200b\u6bd4\u8f83\u200b\u7684\u200b\u4e00\u81f4\u6027\u200b\uff09\u3002</p> In\u00a0[23]: Copied! <pre>BATCH_SIZE = 32\n\n# Create 10% training and test DataLoaders\ntrain_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,\n    test_dir=test_dir, \n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Create 20% training and test data DataLoders\ntrain_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=BATCH_SIZE\n)\n\n# Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments)\nprint(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\")\nprint(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\")\nprint(f\"Number of classes: {len(class_names)}, class names: {class_names}\")\n</pre> BATCH_SIZE = 32  # Create 10% training and test DataLoaders train_dataloader_10_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_10_percent,     test_dir=test_dir,      transform=simple_transform,     batch_size=BATCH_SIZE )  # Create 20% training and test data DataLoders train_dataloader_20_percent, test_dataloader, class_names = data_setup.create_dataloaders(train_dir=train_dir_20_percent,     test_dir=test_dir,     transform=simple_transform,     batch_size=BATCH_SIZE )  # Find the number of samples/batches per dataloader (using the same test_dataloader for both experiments) print(f\"Number of batches of size {BATCH_SIZE} in 10 percent training data: {len(train_dataloader_10_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in 20 percent training data: {len(train_dataloader_20_percent)}\") print(f\"Number of batches of size {BATCH_SIZE} in testing data: {len(train_dataloader_10_percent)} (all experiments will use the same test set)\") print(f\"Number of classes: {len(class_names)}, class names: {class_names}\") <pre>Number of batches of size 32 in 10 percent training data: 8\nNumber of batches of size 32 in 20 percent training data: 15\nNumber of batches of size 32 in testing data: 8 (all experiments will use the same test set)\nNumber of classes: 3, class names: ['pizza', 'steak', 'sushi']\n</pre> In\u00a0[24]: Copied! <pre>import torchvision\nfrom torchinfo import summary\n\n# 1. Create an instance of EffNetB2 with pretrained weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)\n\n# # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# ) \n\n# 3. Get the number of in_features of the EfficientNetB2 classifier layer\nprint(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\")\n</pre> import torchvision from torchinfo import summary  # 1. Create an instance of EffNetB2 with pretrained weights effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT # \"DEFAULT\" means best available weights effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights)  # # 2. Get a summary of standard EffNetB2 from torchvision.models (uncomment for full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )   # 3. Get the number of in_features of the EfficientNetB2 classifier layer print(f\"Number of in_features to final layer of EfficientNetB2: {len(effnetb2.classifier.state_dict()['1.weight'][0])}\") <pre>Number of in_features to final layer of EfficientNetB2: 1408\n</pre> <p>EffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u6458\u8981\u200b\uff0c\u200b\u6240\u6709\u200b\u5c42\u200b\u672a\u200b\u51bb\u7ed3\u200b\uff08\u200b\u53ef\u200b\u8bad\u7ec3\u200b\uff09\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200bImageNet\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u77e5\u9053\u200bEffNetB2\u200b\u6a21\u578b\u200b\u6240\u200b\u9700\u200b\u7684\u200b<code>in_features</code>\u200b\u6570\u91cf\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u51e0\u4e2a\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB0\u200b\u548c\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u80fd\u591f\u200b\uff1a</p> <ol> <li>\u200b\u4ece\u200b<code>torchvision.models</code>\u200b\u83b7\u53d6\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b</li> <li>\u200b\u51bb\u7ed3\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\uff08\u200b\u8bbe\u7f6e\u200b<code>requires_grad=False</code>\uff09</li> <li>\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff08\u200b\u6211\u4eec\u200b\u4e0d\u200b\u9700\u8981\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u8fdb\u884c\u200b\u4e00\u7cfb\u5217\u200b\u5b9e\u9a8c\u200b\u5e76\u200b\u5728\u200b\u521d\u59cb\u5316\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u968f\u673a\u200b\u6743\u91cd\u200b\u7684\u200b\u65b0\u5c42\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6bcf\u6b21\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u968f\u673a\u6027\u200b\u76f8\u4f3c\u200b\uff09</li> <li>\u200b\u66f4\u6539\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\uff08\u200b\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff09</li> <li>\u200b\u7ed9\u200b\u6a21\u578b\u200b\u547d\u540d\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cEffNetB0\u200b\u4e3a\u200b\"effnetb0\"\uff09</li> </ol> In\u00a0[25]: Copied! <pre>import torchvision\nfrom torch import nn\n\n# Get num out features (one for each class pizza, steak, sushi)\nOUT_FEATURES = len(class_names)\n\n# Create an EffNetB0 feature extractor\ndef create_effnetb0():\n    # 1. Get the base mdoel with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b0(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.2),\n        nn.Linear(in_features=1280, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb0\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n\n# Create an EffNetB2 feature extractor\ndef create_effnetb2():\n    # 1. Get the base model with pretrained weights and send to target device\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    model = torchvision.models.efficientnet_b2(weights=weights).to(device)\n\n    # 2. Freeze the base model layers\n    for param in model.features.parameters():\n        param.requires_grad = False\n\n    # 3. Set the seeds\n    set_seeds()\n\n    # 4. Change the classifier head\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3),\n        nn.Linear(in_features=1408, out_features=OUT_FEATURES)\n    ).to(device)\n\n    # 5. Give the model a name\n    model.name = \"effnetb2\"\n    print(f\"[INFO] Created new {model.name} model.\")\n    return model\n</pre> import torchvision from torch import nn  # Get num out features (one for each class pizza, steak, sushi) OUT_FEATURES = len(class_names)  # Create an EffNetB0 feature extractor def create_effnetb0():     # 1. Get the base mdoel with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT     model = torchvision.models.efficientnet_b0(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.2),         nn.Linear(in_features=1280, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb0\"     print(f\"[INFO] Created new {model.name} model.\")     return model  # Create an EffNetB2 feature extractor def create_effnetb2():     # 1. Get the base model with pretrained weights and send to target device     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     model = torchvision.models.efficientnet_b2(weights=weights).to(device)      # 2. Freeze the base model layers     for param in model.features.parameters():         param.requires_grad = False      # 3. Set the seeds     set_seeds()      # 4. Change the classifier head     model.classifier = nn.Sequential(         nn.Dropout(p=0.3),         nn.Linear(in_features=1408, out_features=OUT_FEATURES)     ).to(device)      # 5. Give the model a name     model.name = \"effnetb2\"     print(f\"[INFO] Created new {model.name} model.\")     return model <p>\u200b\u8fd9\u4e9b\u200b\u51fd\u6570\u200b\u770b\u8d77\u6765\u200b\u5f88\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200bEffNetB0\u200b\u548c\u200bEffNetB2\u200b\u7684\u200b\u5b9e\u4f8b\u200b\u5e76\u200b\u67e5\u770b\u200b\u5b83\u4eec\u200b\u7684\u200b<code>summary()</code>\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u5b83\u4eec\u200b\u3002</p> In\u00a0[26]: Copied! <pre>effnetb0 = create_effnetb0() \n\n# Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output)\n# summary(model=effnetb0, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> effnetb0 = create_effnetb0()   # Get an output summary of the layers in our EffNetB0 feature extractor model (uncomment to view full output) # summary(model=effnetb0,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb0 model.\n</pre> <p>\u200b\u5e26\u6709\u200b\u51bb\u7ed3\u200b\u57fa\u7840\u200b\u5c42\u200b\uff08\u200b\u4e0d\u53ef\u200b\u8bad\u7ec3\u200b\uff09\u200b\u548c\u200b\u66f4\u65b0\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u7684\u200bEffNetB0\u200b\u6a21\u578b\u200b\u6458\u8981\u200b\uff08\u200b\u9002\u7528\u200b\u4e8e\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff09\u3002</p> In\u00a0[27]: Copied! <pre>effnetb2 = create_effnetb2()\n\n# Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output)\n# summary(model=effnetb2, \n#         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> effnetb2 = create_effnetb2()  # Get an output summary of the layers in our EffNetB2 feature extractor model (uncomment to view full output) # summary(model=effnetb2,  #         input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # )  <pre>[INFO] Created new effnetb2 model.\n</pre> <p>EffNetB2\u200b\u6a21\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u6458\u8981\u200b\uff0c\u200b\u57fa\u7840\u200b\u5c42\u200b\u51bb\u7ed3\u200b\uff08\u200b\u4e0d\u53ef\u200b\u8bad\u7ec3\u200b\uff09\uff0c\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\u66f4\u65b0\u200b\uff08\u200b\u9002\u7528\u200b\u4e8e\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff09\u3002</p> <p>\u200b\u4ece\u200b\u603b\u7ed3\u200b\u7684\u200b\u8f93\u51fa\u200b\u7ed3\u679c\u200b\u6765\u770b\u200b\uff0cEffNetB2 \u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u7684\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u51e0\u4e4e\u200b\u662f\u200b EffNetB0 \u200b\u7684\u200b\u4e24\u500d\u200b\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u603b\u200b\u53c2\u6570\u200b\uff08\u200b\u51bb\u7ed3\u200b/\u200b\u66f4\u6539\u200b\u5934\u90e8\u200b\u524d\u200b\uff09 \u200b\u603b\u200b\u53c2\u6570\u200b\uff08\u200b\u51bb\u7ed3\u200b/\u200b\u66f4\u6539\u200b\u5934\u90e8\u200b\u540e\u200b\uff09 \u200b\u53ef\u200b\u8bad\u7ec3\u200b\u603b\u200b\u53c2\u6570\u200b\uff08\u200b\u51bb\u7ed3\u200b/\u200b\u66f4\u6539\u200b\u5934\u90e8\u200b\u540e\u200b\uff09 EfficientNetB0 5,288,548 4,011,391 3,843 EfficientNetB2 9,109,994 7,705,221 4,227 <p>\u200b\u8fd9\u200b\u4f7f\u5f97\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u7684\u200b\u9aa8\u5e72\u200b\u7f51\u7edc\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u673a\u4f1a\u200b\u5f62\u6210\u200b\u5bf9\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u7684\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\uff08\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\uff09\u200b\u5dee\u5f02\u200b\u5e76\u4e0d\u5927\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u989d\u5916\u200b\u7684\u200b\u53c2\u6570\u200b\u4f1a\u200b\u5e26\u6765\u200b\u66f4\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u62ed\u76ee\u4ee5\u5f85\u200b...</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u672c\u7740\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7cbe\u795e\u200b\uff0c\u200b\u4f60\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u7684\u200b\u4efb\u4f55\u200b\u6a21\u578b\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e00\u6837\u200b\u3002\u200b\u6211\u4ec5\u200b\u9009\u62e9\u200b\u4e86\u200b EffNetB0 \u200b\u548c\u200b EffNetB2 \u200b\u4f5c\u4e3a\u200b\u793a\u4f8b\u200b\u3002\u200b\u4e5f\u8bb8\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u52a0\u5165\u200b\u7c7b\u4f3c\u200b <code>torchvision.models.convnext_tiny()</code> \u200b\u6216\u200b <code>torchvision.models.convnext_small()</code> \u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[28]: Copied! <pre># 1. Create epochs list\nnum_epochs = [5, 10]\n\n# 2. Create models list (need to create a new model for each experiment)\nmodels = [\"effnetb0\", \"effnetb2\"]\n\n# 3. Create dataloaders dictionary for various dataloaders\ntrain_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,\n                     \"data_20_percent\": train_dataloader_20_percent}\n</pre> # 1. Create epochs list num_epochs = [5, 10]  # 2. Create models list (need to create a new model for each experiment) models = [\"effnetb0\", \"effnetb2\"]  # 3. Create dataloaders dictionary for various dataloaders train_dataloaders = {\"data_10_percent\": train_dataloader_10_percent,                      \"data_20_percent\": train_dataloader_20_percent} <p>\u200b\u5217\u8868\u200b\u548c\u200b\u5b57\u5178\u200b\u5df2\u7ecf\u200b\u521b\u5efa\u200b\u597d\u200b\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7f16\u5199\u200b\u4ee3\u7801\u200b\u6765\u200b\u904d\u5386\u200b\u6bcf\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u9009\u9879\u200b\uff0c\u200b\u5e76\u200b\u5c1d\u8bd5\u200b\u6bcf\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u7ec4\u5408\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u4f1a\u200b\u5728\u200b\u6bcf\u6b21\u200b\u5b9e\u9a8c\u200b\u7ed3\u675f\u200b\u65f6\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u7a0d\u540e\u200b\u53ef\u4ee5\u200b\u52a0\u8f7d\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u5e76\u200b\u7528\u4e8e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6309\u7167\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u8fdb\u884c\u200b\uff1a</p> <ol> <li>\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff08\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u662f\u200b\u53ef\u200b\u590d\u73b0\u200b\u7684\u200b\uff0c\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b ~3 \u200b\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u79cd\u5b50\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u76f8\u540c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u5e76\u200b\u5e73\u5747\u200b\u7ed3\u679c\u200b\uff09\u3002</li> <li>\u200b\u8ddf\u8e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u7f16\u53f7\u200b\uff08\u200b\u8fd9\u200b\u4e3b\u8981\u200b\u662f\u200b\u4e3a\u4e86\u200b\u6253\u5370\u8f93\u51fa\u200b\u66f4\u200b\u7f8e\u89c2\u200b\uff09\u3002</li> <li>\u200b\u904d\u5386\u200b <code>train_dataloaders</code> \u200b\u5b57\u5178\u200b\u9879\u200b\uff0c\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b DataLoader \u200b\u8fdb\u884c\u200b\u5faa\u73af\u200b\u3002</li> <li>\u200b\u904d\u5386\u200b epoch \u200b\u6570\u91cf\u200b\u7684\u200b\u5217\u8868\u200b\u3002</li> <li>\u200b\u904d\u5386\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b\u7684\u200b\u5217\u8868\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u5f53\u524d\u200b\u8fd0\u884c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u521b\u5efa\u200b\u4fe1\u606f\u200b\u6253\u5370\u8f93\u51fa\u200b\uff08\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u77e5\u9053\u200b\u53d1\u751f\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff09\u3002</li> <li>\u200b\u68c0\u67e5\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u662f\u200b\u54ea\u200b\u4e00\u4e2a\u200b\uff0c\u200b\u5e76\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b EffNetB0 \u200b\u6216\u200b EffNetB2 \u200b\u5b9e\u4f8b\u200b\uff08\u200b\u6211\u4eec\u200b\u6bcf\u6b21\u200b\u5b9e\u9a8c\u200b\u90fd\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u90fd\u200b\u4ece\u200b\u76f8\u540c\u200b\u7684\u200b\u8d77\u70b9\u200b\u5f00\u59cb\u200b\uff09\u3002</li> <li>\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u65b0\u200b\u5b9e\u9a8c\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08<code>torch.nn.CrossEntropyLoss()</code>\uff09\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>torch.optim.Adam(params=model.parameters(), lr=0.001)</code>\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u4fee\u6539\u200b\u540e\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u9002\u5f53\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>writer</code> \u200b\u53c2\u6570\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>utils.py</code> \u200b\u4e2d\u200b\u7684\u200b <code>save_model()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u200b\u9002\u5f53\u200b\u7684\u200b\u6587\u4ef6\u540d\u200b\u4fdd\u5b58\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>%%time</code> \u200b\u9b54\u6cd5\u200b\u6765\u200b\u67e5\u770b\u200b\u6240\u6709\u200b\u5b9e\u9a8c\u200b\u5728\u200b\u4e00\u4e2a\u200b Jupyter/Google Colab \u200b\u5355\u5143\u683c\u200b\u4e2d\u200b\u603b\u5171\u200b\u9700\u8981\u200b\u591a\u957f\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u5f00\u59cb\u200b\u5427\u200b\uff01</p> In\u00a0[29]: Copied! <pre>%%time\nfrom going_modular.going_modular.utils import save_model\n\n# 1. Set the random seeds\nset_seeds(seed=42)\n\n# 2. Keep track of experiment numbers\nexperiment_number = 0\n\n# 3. Loop through each DataLoader\nfor dataloader_name, train_dataloader in train_dataloaders.items():\n\n    # 4. Loop through each number of epochs\n    for epochs in num_epochs: \n\n        # 5. Loop through each model name and create a new model based on the name\n        for model_name in models:\n\n            # 6. Create information print outs\n            experiment_number += 1\n            print(f\"[INFO] Experiment number: {experiment_number}\")\n            print(f\"[INFO] Model: {model_name}\")\n            print(f\"[INFO] DataLoader: {dataloader_name}\")\n            print(f\"[INFO] Number of epochs: {epochs}\")  \n\n            # 7. Select the model\n            if model_name == \"effnetb0\":\n                model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)\n            else:\n                model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)\n            \n            # 8. Create a new loss and optimizer for every model\n            loss_fn = nn.CrossEntropyLoss()\n            optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)\n\n            # 9. Train target model with target dataloaders and track experiments\n            train(model=model,\n                  train_dataloader=train_dataloader,\n                  test_dataloader=test_dataloader, \n                  optimizer=optimizer,\n                  loss_fn=loss_fn,\n                  epochs=epochs,\n                  device=device,\n                  writer=create_writer(experiment_name=dataloader_name,\n                                       model_name=model_name,\n                                       extra=f\"{epochs}_epochs\"))\n            \n            # 10. Save the model to file so we can get back the best model\n            save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"\n            save_model(model=model,\n                       target_dir=\"models\",\n                       model_name=save_filepath)\n            print(\"-\"*50 + \"\\n\")\n</pre> %%time from going_modular.going_modular.utils import save_model  # 1. Set the random seeds set_seeds(seed=42)  # 2. Keep track of experiment numbers experiment_number = 0  # 3. Loop through each DataLoader for dataloader_name, train_dataloader in train_dataloaders.items():      # 4. Loop through each number of epochs     for epochs in num_epochs:           # 5. Loop through each model name and create a new model based on the name         for model_name in models:              # 6. Create information print outs             experiment_number += 1             print(f\"[INFO] Experiment number: {experiment_number}\")             print(f\"[INFO] Model: {model_name}\")             print(f\"[INFO] DataLoader: {dataloader_name}\")             print(f\"[INFO] Number of epochs: {epochs}\")                # 7. Select the model             if model_name == \"effnetb0\":                 model = create_effnetb0() # creates a new model each time (important because we want each experiment to start from scratch)             else:                 model = create_effnetb2() # creates a new model each time (important because we want each experiment to start from scratch)                          # 8. Create a new loss and optimizer for every model             loss_fn = nn.CrossEntropyLoss()             optimizer = torch.optim.Adam(params=model.parameters(), lr=0.001)              # 9. Train target model with target dataloaders and track experiments             train(model=model,                   train_dataloader=train_dataloader,                   test_dataloader=test_dataloader,                    optimizer=optimizer,                   loss_fn=loss_fn,                   epochs=epochs,                   device=device,                   writer=create_writer(experiment_name=dataloader_name,                                        model_name=model_name,                                        extra=f\"{epochs}_epochs\"))                          # 10. Save the model to file so we can get back the best model             save_filepath = f\"07_{model_name}_{dataloader_name}_{epochs}_epochs.pth\"             save_model(model=model,                        target_dir=\"models\",                        model_name=save_filepath)             print(\"-\"*50 + \"\\n\") <pre>[INFO] Experiment number: 1\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 2\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 3\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0528 | train_acc: 0.4961 | test_loss: 0.9217 | test_acc: 0.4678\nEpoch: 2 | train_loss: 0.8747 | train_acc: 0.6992 | test_loss: 0.8138 | test_acc: 0.6203\nEpoch: 3 | train_loss: 0.8099 | train_acc: 0.6445 | test_loss: 0.7175 | test_acc: 0.8258\nEpoch: 4 | train_loss: 0.7097 | train_acc: 0.7578 | test_loss: 0.5897 | test_acc: 0.8864\nEpoch: 5 | train_loss: 0.5980 | train_acc: 0.9141 | test_loss: 0.5676 | test_acc: 0.8864\nEpoch: 6 | train_loss: 0.5611 | train_acc: 0.8984 | test_loss: 0.5949 | test_acc: 0.8864\nEpoch: 7 | train_loss: 0.5573 | train_acc: 0.7930 | test_loss: 0.5566 | test_acc: 0.8864\nEpoch: 8 | train_loss: 0.4702 | train_acc: 0.9492 | test_loss: 0.5176 | test_acc: 0.8759\nEpoch: 9 | train_loss: 0.5728 | train_acc: 0.7773 | test_loss: 0.5095 | test_acc: 0.8873\nEpoch: 10 | train_loss: 0.4794 | train_acc: 0.8242 | test_loss: 0.4640 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 4\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_10_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_10_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 1.0928 | train_acc: 0.3711 | test_loss: 0.9557 | test_acc: 0.6610\nEpoch: 2 | train_loss: 0.9247 | train_acc: 0.6445 | test_loss: 0.8711 | test_acc: 0.8144\nEpoch: 3 | train_loss: 0.8086 | train_acc: 0.7656 | test_loss: 0.7511 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.7191 | train_acc: 0.8867 | test_loss: 0.7150 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.6851 | train_acc: 0.7695 | test_loss: 0.7076 | test_acc: 0.8873\nEpoch: 6 | train_loss: 0.6111 | train_acc: 0.7812 | test_loss: 0.6325 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.6127 | train_acc: 0.8008 | test_loss: 0.6404 | test_acc: 0.8769\nEpoch: 8 | train_loss: 0.5202 | train_acc: 0.9336 | test_loss: 0.6200 | test_acc: 0.8977\nEpoch: 9 | train_loss: 0.5425 | train_acc: 0.8008 | test_loss: 0.6227 | test_acc: 0.8466\nEpoch: 10 | train_loss: 0.4908 | train_acc: 0.8125 | test_loss: 0.5870 | test_acc: 0.8873\n[INFO] Saving model to: models/07_effnetb2_data_10_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 5\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 6\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 5\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/5_epochs...\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_5_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 7\n[INFO] Model: effnetb0\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb0 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb0/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9577 | train_acc: 0.6167 | test_loss: 0.6545 | test_acc: 0.8655\nEpoch: 2 | train_loss: 0.6881 | train_acc: 0.8438 | test_loss: 0.5798 | test_acc: 0.9176\nEpoch: 3 | train_loss: 0.5798 | train_acc: 0.8604 | test_loss: 0.4575 | test_acc: 0.9176\nEpoch: 4 | train_loss: 0.4930 | train_acc: 0.8646 | test_loss: 0.4458 | test_acc: 0.9176\nEpoch: 5 | train_loss: 0.4886 | train_acc: 0.8500 | test_loss: 0.3909 | test_acc: 0.9176\nEpoch: 6 | train_loss: 0.3705 | train_acc: 0.8854 | test_loss: 0.3568 | test_acc: 0.9072\nEpoch: 7 | train_loss: 0.3551 | train_acc: 0.9250 | test_loss: 0.3187 | test_acc: 0.9072\nEpoch: 8 | train_loss: 0.3745 | train_acc: 0.8938 | test_loss: 0.3349 | test_acc: 0.8873\nEpoch: 9 | train_loss: 0.2972 | train_acc: 0.9396 | test_loss: 0.3092 | test_acc: 0.9280\nEpoch: 10 | train_loss: 0.3620 | train_acc: 0.8479 | test_loss: 0.2780 | test_acc: 0.9072\n[INFO] Saving model to: models/07_effnetb0_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\n[INFO] Experiment number: 8\n[INFO] Model: effnetb2\n[INFO] DataLoader: data_20_percent\n[INFO] Number of epochs: 10\n[INFO] Created new effnetb2 model.\n[INFO] Created SummaryWriter, saving to: runs/2022-06-23/data_20_percent/effnetb2/10_epochs...\n</pre> <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9830 | train_acc: 0.5521 | test_loss: 0.7767 | test_acc: 0.8153\nEpoch: 2 | train_loss: 0.7298 | train_acc: 0.7604 | test_loss: 0.6673 | test_acc: 0.8873\nEpoch: 3 | train_loss: 0.6022 | train_acc: 0.8458 | test_loss: 0.5622 | test_acc: 0.9280\nEpoch: 4 | train_loss: 0.5435 | train_acc: 0.8354 | test_loss: 0.5679 | test_acc: 0.9186\nEpoch: 5 | train_loss: 0.4404 | train_acc: 0.9042 | test_loss: 0.4462 | test_acc: 0.9489\nEpoch: 6 | train_loss: 0.3889 | train_acc: 0.9104 | test_loss: 0.4555 | test_acc: 0.8977\nEpoch: 7 | train_loss: 0.3483 | train_acc: 0.9271 | test_loss: 0.4227 | test_acc: 0.9384\nEpoch: 8 | train_loss: 0.3862 | train_acc: 0.8771 | test_loss: 0.4344 | test_acc: 0.9280\nEpoch: 9 | train_loss: 0.3308 | train_acc: 0.8979 | test_loss: 0.4242 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.3383 | train_acc: 0.8896 | test_loss: 0.3906 | test_acc: 0.9384\n[INFO] Saving model to: models/07_effnetb2_data_20_percent_10_epochs.pth\n--------------------------------------------------\n\nCPU times: user 29.5 s, sys: 1min 28s, total: 1min 58s\nWall time: 2min 33s\n</pre> In\u00a0[30]: Copied! <pre># Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance)\n# %load_ext tensorboard\n# %tensorboard --logdir runs\n</pre> # Viewing TensorBoard in Jupyter and Google Colab Notebooks (uncomment to view full TensorBoard instance) # %load_ext tensorboard # %tensorboard --logdir runs <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u5355\u5143\u683c\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e94\u8be5\u200b\u5f97\u5230\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6839\u636e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u548c\u200b\u786c\u4ef6\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6570\u5b57\u200b\u53ef\u80fd\u200b\u4e0e\u200b\u8fd9\u91cc\u200b\u5c55\u793a\u200b\u7684\u200b\u4e0d\u200b\u5b8c\u5168\u76f8\u540c\u200b\u3002\u200b\u8fd9\u662f\u200b\u6b63\u5e38\u200b\u7684\u200b\u3002\u200b\u8fd9\u662f\u200b\u7531\u4e8e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u56fa\u6709\u200b\u7684\u200b\u968f\u673a\u6027\u200b\u6240\u81f4\u200b\u3002\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u8d8b\u52bf\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6570\u5b57\u200b\u8d70\u5411\u200b\u4f55\u65b9\u200b\u3002\u200b\u5982\u679c\u200b\u5b83\u4eec\u200b\u504f\u79bb\u200b\u5f88\u5927\u200b\uff0c\u200b\u53ef\u80fd\u200b\u662f\u200b\u6709\u200b\u95ee\u9898\u200b\uff0c\u200b\u6700\u597d\u200b\u56de\u53bb\u200b\u68c0\u67e5\u200b\u4ee3\u7801\u200b\u3002\u200b\u4f46\u200b\u5982\u679c\u200b\u5b83\u4eec\u200b\u53ea\u662f\u200b\u5c0f\u5e45\u5ea6\u200b\u504f\u79bb\u200b\uff08\u200b\u6bd4\u5982\u200b\u5c0f\u6570\u70b9\u200b\u540e\u200b\u51e0\u4f4d\u200b\uff09\uff0c\u200b\u90a3\u200b\u662f\u200b\u53ef\u4ee5\u200b\u63a5\u53d7\u200b\u7684\u200b\u3002</p> <p>\u200b\u5728\u200bTensorBoard\u200b\u4e2d\u200b\u53ef\u89c6\u5316\u200b\u4e0d\u540c\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u503c\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u4e86\u200b10\u200b\u4e2a\u200b\u5468\u671f\u200b\u7684\u200bEffNetB0\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f7f\u7528\u200b\u4e86\u200b20%\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u8fbe\u5230\u200b\u4e86\u200b\u6700\u4f4e\u200b\u7684\u200b\u635f\u5931\u200b\u3002\u200b\u8fd9\u200b\u7b26\u5408\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u6574\u4f53\u200b\u8d8b\u52bf\u200b\uff1a\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u3001\u200b\u66f4\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u66f4\u957f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u901a\u5e38\u200b\u4f1a\u200b\u66f4\u597d\u200b\u3002</p> <p>\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4f60\u200b\u7684\u200bTensorBoard\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u4e0a\u200b\u4f20\u5230\u200btensorboard.dev\uff0c\u200b\u514d\u8d39\u200b\u516c\u5f00\u200b\u6258\u7ba1\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8fd0\u884c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u4ee3\u7801\u200b\uff1a</p> In\u00a0[31]: Copied! <pre># # Upload the results to TensorBoard.dev (uncomment to try it out)\n# !tensorboard dev upload --logdir runs \\\n#     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\\n#     --description \"Comparing results of different model size, training data amount and training time.\"\n</pre> # # Upload the results to TensorBoard.dev (uncomment to try it out) # !tensorboard dev upload --logdir runs \\ #     --name \"07. PyTorch Experiment Tracking: FoodVision Mini model results\" \\ #     --description \"Comparing results of different model size, training data amount and training time.\" <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u5355\u5143\u683c\u200b\u540e\u200b\uff0c\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u5c06\u200b\u5728\u200b\u4ee5\u4e0b\u200b\u7f51\u5740\u200b\u516c\u5f00\u200b\u53ef\u89c1\u200b\uff1ahttps://tensorboard.dev/experiment/VySxUYY7Rje0xREYvCvZXA/</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u60a8\u200b\u4e0a\u200b\u4f20\u5230\u200b tensorboard.dev \u200b\u7684\u200b\u4efb\u4f55\u200b\u5185\u5bb9\u200b\u90fd\u200b\u662f\u200b\u516c\u5f00\u200b\u7684\u200b\uff0c\u200b\u4efb\u4f55\u4eba\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u60a8\u200b\u4e0a\u4f20\u200b\u4e86\u200b\u5b9e\u9a8c\u200b\u6570\u636e\u200b\uff0c\u200b\u8bf7\u200b\u786e\u4fdd\u200b\u5b83\u4eec\u200b\u4e0d\u200b\u5305\u542b\u200b\u654f\u611f\u200b\u4fe1\u606f\u200b\u3002</p> In\u00a0[32]: Copied! <pre># Setup the best model filepath\nbest_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"\n\n# Instantiate a new instance of EffNetB2 (to load the saved state_dict() to)\nbest_model = create_effnetb2()\n\n# Load the saved best model state_dict()\nbest_model.load_state_dict(torch.load(best_model_path))\n</pre> # Setup the best model filepath best_model_path = \"models/07_effnetb2_data_20_percent_10_epochs.pth\"  # Instantiate a new instance of EffNetB2 (to load the saved state_dict() to) best_model = create_effnetb2()  # Load the saved best model state_dict() best_model.load_state_dict(torch.load(best_model_path)) <pre>[INFO] Created new effnetb2 model.\n</pre> Out[32]: <pre>&lt;All keys matched successfully&gt;</pre> <p>\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u5df2\u200b\u52a0\u8f7d\u200b\uff01</p> <p>\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u4e0d\u59a8\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u5b83\u200b\u7684\u200b\u6587\u4ef6\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5728\u200b\u540e\u7eed\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\uff08\u200b\u5c06\u200b\u5176\u200b\u6574\u5408\u200b\u5230\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u4e2d\u200b\uff09\u200b\u65f6\u200b\u662f\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u7684\u200b\u8003\u8651\u200b\u56e0\u7d20\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6a21\u578b\u200b\u8fc7\u5927\u200b\uff0c\u200b\u90e8\u7f72\u200b\u8d77\u6765\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f88\u200b\u56f0\u96be\u200b\u3002</p> In\u00a0[33]: Copied! <pre># Check the model file size\nfrom pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\neffnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024)\nprint(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\")\n</pre> # Check the model file size from pathlib import Path  # Get the model size in bytes then convert to megabytes effnetb2_model_size = Path(best_model_path).stat().st_size // (1024*1024) print(f\"EfficientNetB2 feature extractor model size: {effnetb2_model_size} MB\") <pre>EfficientNetB2 feature extractor model size: 29 MB\n</pre> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u76ee\u524d\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u4e3a\u200b29 MB\u3002\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u4ee5\u540e\u200b\u60f3\u8981\u200b\u90e8\u7f72\u200b\u5b83\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u8bb0\u4f4f\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\u5e76\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b6\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b<code>pred_and_plot_image()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ece\u200b<code>going_modular.going_modular.predictions.py</code>\u200b\u5bfc\u5165\u200b\u8be5\u200b\u51fd\u6570\u200b\u6765\u200b\u91cd\u7528\u200b\u5b83\u200b\uff08\u200b\u6211\u200b\u5c06\u200b<code>pred_and_plot_image()</code>\u200b\u51fd\u6570\u200b\u653e\u5728\u200b\u4e00\u4e2a\u200b\u811a\u672c\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91cd\u7528\u200b\u5b83\u200b\uff09\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u5bf9\u6a21\u578b\u200b\u4e4b\u524d\u200b\u672a\u89c1\u200b\u8fc7\u200b\u7684\u200b\u5404\u79cd\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4ece\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u56fe\u50cf\u6587\u4ef6\u200b\u8def\u5f84\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u7136\u540e\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u8fd9\u4e9b\u200b\u8def\u5f84\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5b50\u96c6\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6211\u4eec\u200b\u7684\u200b<code>pred_and_plot_image()</code>\u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[34]: Copied! <pre># Import function to make predictions on images and plot them \n# See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Get a random list of 3 images from 20% test set\nimport random\nnum_images_to_plot = 3\ntest_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset\ntest_image_path_sample = random.sample(population=test_image_path_list,\n                                       k=num_images_to_plot) # randomly select k number of images\n\n# Iterate through random test image paths, make predictions on them and plot them\nfor image_path in test_image_path_sample:\n    pred_and_plot_image(model=best_model,\n                        image_path=image_path,\n                        class_names=class_names,\n                        image_size=(224, 224))\n</pre> # Import function to make predictions on images and plot them  # See the function previously created in section: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set from going_modular.going_modular.predictions import pred_and_plot_image  # Get a random list of 3 images from 20% test set import random num_images_to_plot = 3 test_image_path_list = list(Path(data_20_percent_path / \"test\").glob(\"*/*.jpg\")) # get all test image paths from 20% dataset test_image_path_sample = random.sample(population=test_image_path_list,                                        k=num_images_to_plot) # randomly select k number of images  # Iterate through random test image paths, make predictions on them and plot them for image_path in test_image_path_sample:     pred_and_plot_image(model=best_model,                         image_path=image_path,                         class_names=class_names,                         image_size=(224, 224)) <p>\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u597d\u200b\uff0c\u200b\u5e76\u4e14\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u901a\u5e38\u200b\u6bd4\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u6784\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u66f4\u9ad8\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u8868\u660e\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5176\u200b\u505a\u51fa\u200b\u7684\u200b\u51b3\u7b56\u200b\u66f4\u52a0\u200b\u81ea\u4fe1\u200b\u3002</p> In\u00a0[35]: Copied! <pre># Download custom image\nimport requests\n\n# Setup custom image path\ncustom_image_path = Path(\"data/04-pizza-dad.jpeg\")\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=model,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> # Download custom image import requests  # Setup custom image path custom_image_path = Path(\"data/04-pizza-dad.jpeg\")  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=model,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u518d\u6b21\u200b\u53cc\u8d5e\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u4e86\u200b\u201c\u200b\u62ab\u8428\u200b\u201d\uff0c\u200b\u5e76\u4e14\u200b\u8fd9\u6b21\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff080.978\uff09\u200b\u6bd4\u200b\u6211\u4eec\u200b\u5728\u200b06. PyTorch\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b6.1\u200b\u8282\u4e2d\u200b\u8bad\u7ec3\u200b\u548c\u200b\u4f7f\u7528\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u6a21\u578b\u200b\u66f4\u9ad8\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u518d\u6b21\u200b\u8868\u660e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\uff08\u200b\u5728\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4e86\u200b10\u200b\u4e2a\u200b\u5468\u671f\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff09\u200b\u5df2\u7ecf\u200b\u5b66\u4e60\u200b\u4e86\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u66f4\u200b\u6709\u200b\u4fe1\u5fc3\u200b\u505a\u51fa\u200b\u9884\u6d4b\u200b\u62ab\u8428\u200b\u7684\u200b\u51b3\u5b9a\u200b\u3002</p> <p>\u200b\u6211\u200b\u60f3\u200b\u77e5\u9053\u200b\u8fd8\u6709\u200b\u4ec0\u4e48\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u6027\u80fd\u200b\uff1f</p> <p>\u200b\u6211\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u7559\u7ed9\u200b\u4f60\u200b\u4f5c\u4e3a\u200b\u63a2\u7d22\u200b\u7684\u200b\u6311\u6218\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#07-pytorch","title":"07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>torchvision</code> \u200b\u7684\u200b\u65b0\u200b \u200b\u591a\u200b\u6743\u91cd\u200b\u652f\u6301\u200b API\uff08\u200b\u9002\u7528\u200b\u4e8e\u200b <code>torchvision</code> v0.13+\uff09\u3002</p> <p>\u200b\u5728\u200b\u5236\u4f5c\u200b FoodVision Mini\uff08\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5206\u7c7b\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u7684\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6a21\u578b\u200b\uff09\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b Python \u200b\u5b57\u5178\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u4ec5\u4ec5\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u671f\u95f4\u200b\u6253\u5370\u200b\u7684\u200b\u6307\u6807\u200b\u6765\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u4e00\u6b21\u6027\u200b\u8fd0\u884c\u200b\u5341\u51e0\u4e2a\u200b\uff08\u200b\u6216\u200b\u66f4\u200b\u591a\u200b\uff09\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8be5\u200b\u600e\u4e48\u529e\u200b\uff1f</p> <p>\u200b\u5f53\u7136\u200b\u6709\u200b\u66f4\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b...</p> <p>\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u5bf9\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u81f3\u5173\u91cd\u8981\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u89c6\u4e3a\u200b\u4f60\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b\u3002</p> <p>\u200b\u6b22\u8fce\u200b\u6765\u5230\u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b 1\uff1aFoodVision Mini \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u56de\u7b54\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff1a\u200b\u5982\u4f55\u200b\u8ddf\u8e2a\u200b\u6211\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\uff1f</p>"},{"location":"07_pytorch_experiment_tracking/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\uff1f\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u662f\u200b\u975e\u5e38\u200b\u5b9e\u9a8c\u6027\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f60\u200b\u9700\u8981\u200b\u6234\u4e0a\u200b\u827a\u672f\u5bb6\u200b\u7684\u200b\u8d1d\u96f7\u5e3d\u200b/\u200b\u53a8\u5e08\u200b\u7684\u200b\u5e3d\u5b50\u200b\uff0c\u200b\u6765\u200b\u521b\u9020\u200b\u51fa\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u4f60\u200b\u8fd8\u200b\u9700\u8981\u200b\u7a7f\u200b\u4e0a\u200b\u79d1\u5b66\u5bb6\u200b\u7684\u200b\u767d\u5927\u8902\u200b\uff0c\u200b\u6765\u200b\u8ffd\u8e2a\u200b\u5404\u79cd\u200b\u6570\u636e\u200b\u7ec4\u5408\u200b\u3001\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u548c\u200b\u8bad\u7ec3\u5236\u5ea6\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u4f5c\u7528\u200b\u6240\u5728\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u8fdb\u884c\u200b\u5927\u91cf\u200b\u7684\u200b\u4e0d\u540c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u5f04\u6e05\u695a\u200b\u54ea\u4e9b\u200b\u65b9\u6cd5\u200b\u6709\u6548\u200b\uff0c\u200b\u54ea\u4e9b\u200b\u65e0\u6548\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/","title":"\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8ddf\u8e2a\u200b\u5b9e\u9a8c\u200b\uff1f\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u53ea\u200b\u8fd0\u884c\u200b\u4e86\u200b\u5c11\u6570\u51e0\u4e2a\u200b\u6a21\u578b\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u76ee\u524d\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff09\uff0c\u200b\u53ef\u80fd\u200b\u53ea\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b\u6253\u5370\u8f93\u51fa\u200b\u548c\u200b\u51e0\u4e2a\u200b\u5b57\u5178\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u5b83\u4eec\u200b\u7684\u200b\u7ed3\u679c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u4e86\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u968f\u7740\u200b\u4f60\u200b\u8fd0\u884c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u6570\u91cf\u200b\u5f00\u59cb\u200b\u589e\u52a0\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u7b80\u5355\u200b\u7684\u200b\u8ddf\u8e2a\u200b\u65b9\u5f0f\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u96be\u4ee5\u200b\u7ba1\u7406\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u9075\u5faa\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4ece\u4e1a\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\u2014\u2014\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u518d\u200b\u5b9e\u9a8c\u200b\uff01\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u9700\u8981\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u8fd9\u4e9b\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u5728\u200b\u6784\u5efa\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u6a21\u578b\u200b\u5e76\u200b\u8ddf\u8e2a\u200b\u5b83\u4eec\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e4b\u540e\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u5f00\u59cb\u200b\u6ce8\u610f\u200b\u5230\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4f1a\u200b\u591a\u4e48\u200b\u8fc5\u901f\u200b\u5730\u200b\u53d8\u5f97\u200b\u96be\u4ee5\u200b\u7ba1\u7406\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/","title":"\u8ddf\u8e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u4e0d\u540c\u200b\u65b9\u6cd5\u200b\u00b6","text":"<p>\u200b\u8ddf\u8e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u65b9\u6cd5\u200b\u548c\u200b\u5b9e\u9a8c\u200b\u672c\u8eab\u200b\u4e00\u6837\u200b\u591a\u200b\u3002</p> <p>\u200b\u4e0b\u8868\u200b\u6db5\u76d6\u200b\u4e86\u200b\u51e0\u79cd\u200b\u65b9\u6cd5\u200b\u3002</p> \u200b\u65b9\u6cd5\u200b \u200b\u8bbe\u7f6e\u200b \u200b\u4f18\u70b9\u200b \u200b\u7f3a\u70b9\u200b \u200b\u6210\u672c\u200b Python \u200b\u5b57\u5178\u200b\u3001CSV \u200b\u6587\u4ef6\u200b\u3001\u200b\u6253\u5370\u8f93\u51fa\u200b \u200b\u65e0\u200b \u200b\u6613\u4e8e\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u7eaf\u200b Python \u200b\u8fd0\u884c\u200b \u200b\u96be\u4ee5\u200b\u8ddf\u8e2a\u200b\u5927\u91cf\u200b\u5b9e\u9a8c\u200b \u200b\u514d\u8d39\u200b TensorBoard \u200b\u6700\u5c0f\u5316\u200b\uff0c\u200b\u5b89\u88c5\u200b <code>tensorboard</code> \u200b\u5185\u200b\u7f6e\u4e8e\u200b PyTorch \u200b\u7684\u200b\u6269\u5c55\u200b\uff0c\u200b\u5e7f\u6cdb\u200b\u8ba4\u53ef\u200b\u548c\u200b\u4f7f\u7528\u200b\uff0c\u200b\u6613\u4e8e\u200b\u6269\u5c55\u200b \u200b\u7528\u6237\u200b\u4f53\u9a8c\u200b\u4e0d\u5982\u200b\u5176\u4ed6\u200b\u9009\u9879\u200b \u200b\u514d\u8d39\u200b Weights &amp; Biases \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u6700\u5c0f\u5316\u200b\uff0c\u200b\u5b89\u88c5\u200b <code>wandb</code>\uff0c\u200b\u521b\u5efa\u200b\u8d26\u6237\u200b \u200b\u51fa\u8272\u200b\u7684\u200b\u7528\u6237\u200b\u4f53\u9a8c\u200b\uff0c\u200b\u516c\u5f00\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u8ddf\u8e2a\u200b\u4efb\u4f55\u200b\u5185\u5bb9\u200b \u200b\u9700\u8981\u200b PyTorch \u200b\u4e4b\u5916\u200b\u7684\u200b\u5916\u90e8\u200b\u8d44\u6e90\u200b \u200b\u4e2a\u4eba\u200b\u4f7f\u7528\u200b\u514d\u8d39\u200b MLFlow \u200b\u6700\u5c0f\u5316\u200b\uff0c\u200b\u5b89\u88c5\u200b <code>mlflow</code> \u200b\u5e76\u200b\u5f00\u59cb\u200b\u8ddf\u8e2a\u200b \u200b\u5b8c\u5168\u200b\u5f00\u6e90\u200b\u7684\u200b MLOps \u200b\u751f\u547d\u5468\u671f\u200b\u7ba1\u7406\u200b\uff0c\u200b\u8bb8\u591a\u200b\u96c6\u6210\u200b \u200b\u8bbe\u7f6e\u200b\u8fdc\u7a0b\u200b\u8ddf\u8e2a\u200b\u670d\u52a1\u5668\u200b\u6bd4\u200b\u5176\u4ed6\u200b\u670d\u52a1\u200b\u7a0d\u96be\u200b \u200b\u514d\u8d39\u200b <p>\u200b\u8ddf\u8e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u5404\u79cd\u200b\u5730\u65b9\u200b\u548c\u200b\u6280\u672f\u200b\u3002\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd8\u6709\u200b\u5176\u4ed6\u200b\u7c7b\u4f3c\u200b Weights &amp; Biases \u200b\u7684\u200b\u9009\u9879\u200b\u548c\u200b\u7c7b\u4f3c\u200b MLflow \u200b\u7684\u200b\u5f00\u6e90\u200b\u9009\u9879\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u7b80\u6d01\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6211\u200b\u6ca1\u6709\u200b\u5217\u51fa\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u641c\u7d22\u200b\u201c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u201d\u200b\u627e\u5230\u200b\u66f4\u200b\u591a\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/","title":"\u6211\u4eec\u200b\u5c06\u8981\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd0\u884c\u200b\u591a\u4e2a\u200b\u4e0d\u540c\u200b\u5c42\u6b21\u200b\u7684\u200b\u6570\u636e\u200b\u3001\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u548c\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u7684\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4ee5\u200b\u5c1d\u8bd5\u200b\u6539\u8fdb\u200b FoodVision Mini\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u5176\u200b\u4e0e\u200b PyTorch \u200b\u7684\u200b\u7d27\u5bc6\u200b\u96c6\u6210\u200b\u548c\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\uff0c\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u4f7f\u7528\u200b TensorBoard \u200b\u6765\u200b\u8ddf\u8e2a\u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u539f\u5219\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u6240\u6709\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u5de5\u5177\u200b\u4e2d\u200b\u7684\u200b\u539f\u5219\u200b\u76f8\u4f3c\u200b\u3002</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b \u200b\u6211\u4eec\u200b\u5728\u200b\u8fc7\u53bb\u200b\u51e0\u8282\u200b\u4e2d\u200b\u7f16\u5199\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u6709\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002 1. \u200b\u83b7\u53d6\u6570\u636e\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u83b7\u53d6\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4ee5\u200b\u5c1d\u8bd5\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002 2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b \u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5728\u200b\u7b2c\u200b05\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b <code>data_setup.py</code> \u200b\u811a\u672c\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002 3. \u200b\u83b7\u53d6\u200b\u5e76\u200b\u81ea\u5b9a\u4e49\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u5c31\u200b\u50cf\u200b\u4e0a\u200b\u4e00\u8282\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u4e0b\u8f7d\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u81ea\u5b9a\u4e49\u200b\u4e3a\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002 4. \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u8ddf\u8e2a\u200b\u7ed3\u679c\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u4f7f\u7528\u200b TensorBoard \u200b\u8bad\u7ec3\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u5355\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u3002 5. \u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u67e5\u770b\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b \u200b\u4e4b\u524d\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u53ef\u89c6\u5316\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u7684\u200b\u6837\u5b50\u200b\u3002 6. \u200b\u521b\u5efa\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u4ee5\u200b\u8ddf\u8e2a\u200b\u5b9e\u9a8c\u200b \u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6253\u7b97\u200b\u9075\u5faa\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u8df5\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff1a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff01\uff0c\u200b\u6211\u4eec\u200b\u6700\u597d\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u4fdd\u5b58\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u3002 7. \u200b\u8bbe\u7f6e\u200b\u4e00\u7cfb\u5217\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b \u200b\u4e0e\u5176\u200b\u4e00\u6b21\u200b\u8fd0\u884c\u200b\u4e00\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4e0d\u5982\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\u6765\u200b\u4e00\u6b21\u200b\u8fd0\u884c\u200b\u591a\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u3001\u200b\u4e0d\u540c\u200b\u6570\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\u548c\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u3002 8. \u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u67e5\u770b\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b \u200b\u5230\u200b\u8fd9\u200b\u4e00\u6b65\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e00\u6b21\u6027\u200b\u8fd0\u884c\u200b\u4e86\u200b\u516b\u4e2a\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u6709\u200b\u5f88\u591a\u200b\u9700\u8981\u200b\u8ddf\u8e2a\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u7684\u200b\u7ed3\u679c\u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u7684\u200b\u6837\u5b50\u200b\u3002 9. \u200b\u52a0\u8f7d\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5b83\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u76ee\u7684\u200b\u662f\u200b\u627e\u51fa\u200b\u54ea\u4e2a\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u52a0\u8f7d\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5b83\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4ee5\u200b \u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01\u3002"},{"location":"07_pytorch_experiment_tracking/","title":"\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u6750\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub \u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u6587\u6863\u200b\u548c\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#0","title":"0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u672c\u8282\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u5757\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8282\u7701\u200b\u7f16\u5199\u200b\u989d\u5916\u200b\u4ee3\u7801\u200b\u7684\u200b\u65f6\u95f4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5229\u7528\u200b\u5728\u200b05. PyTorch Going Modular\u200b\u90e8\u5206\u200b\u521b\u5efa\u200b\u7684\u200b\u4e00\u4e9b\u200bPython\u200b\u811a\u672c\u200b\uff08\u200b\u5982\u200b<code>data_setup.py</code>\u200b\u548c\u200b<code>engine.py</code>\uff09\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b<code>pytorch-deep-learning</code>\u200b\u4ed3\u5e93\u200b\u4e0b\u8f7d\u200b<code>going_modular</code>\u200b\u76ee\u5f55\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u4e0b\u8f7d\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u83b7\u53d6\u200b<code>torchinfo</code>\u200b\u5305\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u5b89\u88c5\u200b\uff09\u3002</p> <p><code>torchinfo</code>\u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u751f\u6210\u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u6458\u8981\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u8f83\u200b\u65b0\u200b\u7248\u672c\u200b\u7684\u200b<code>torchvision</code>\u200b\u5305\u200b\uff08\u200b\u622a\u81f3\u200b2022\u200b\u5e74\u200b6\u200b\u6708\u200b\u4e3a\u200bv0.13\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u786e\u4fdd\u200b\u5b89\u88c5\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/","title":"\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u7684\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u5728\u200b\u524d\u9762\u200b\u7684\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u591a\u6b21\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u4e0d\u5982\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u51fd\u6570\u200b\u5316\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>set_seeds()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\u6765\u200b\u201c\u200b\u8bbe\u7f6e\u200b\u79cd\u5b50\u200b\u201d\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u662f\u200b\u8ba1\u7b97\u673a\u200b\u751f\u6210\u200b\u968f\u673a\u6027\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u3002\u200b\u5728\u200b\u8fd0\u884c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u65f6\u200b\uff0c\u200b\u5e76\u200b\u4e0d\u200b\u603b\u662f\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u5b83\u4eec\u200b\u6709\u52a9\u4e8e\u200b\u786e\u4fdd\u200b\u53ef\u91cd\u590d\u6027\u200b\uff08\u200b\u6211\u200b\u4ee3\u7801\u751f\u6210\u200b\u7684\u200b\u6570\u5b57\u200b\u4e0e\u200b\u4f60\u200b\u4ee3\u7801\u751f\u6210\u200b\u7684\u200b\u6570\u5b57\u200b\u76f8\u4f3c\u200b\uff09\u3002\u200b\u5728\u200b\u6559\u80b2\u200b\u6216\u200b\u5b9e\u9a8c\u200b\u73af\u5883\u200b\u4e4b\u5916\u200b\uff0c\u200b\u901a\u5e38\u200b\u4e0d\u200b\u9700\u8981\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u4e00\u5982\u65e2\u5f80\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u8fd0\u884c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u5c1d\u8bd5\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u5728\u200b FoodVision Mini \u200b\u4e0a\u200b\u83b7\u5f97\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u5728\u200b\u4e0a\u200b\u4e00\u8282\u200b\uff0c06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4e86\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u548c\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u5728\u200b\u5206\u7c7b\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u65f6\u200b\u7684\u200b\u5f3a\u5927\u200b\u4e4b\u200b\u5904\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u8fd0\u884c\u200b\u4e00\u4e9b\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5c1d\u8bd5\u200b\u8fdb\u4e00\u6b65\u200b\u6539\u8fdb\u200b\u6211\u4eec\u200b\u7684\u200b\u7ed3\u679c\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4e0a\u200b\u4e00\u8282\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u4ee3\u7801\u200b\u6765\u200b\u4e0b\u8f7d\u200b <code>pizza_steak_sushi.zip</code>\uff08\u200b\u5982\u679c\u200b\u6570\u636e\u200b\u5c1a\u672a\u200b\u5b58\u5728\u200b\uff09\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u5b83\u200b\u5df2\u7ecf\u200b\u88ab\u200b\u51fd\u6570\u200b\u5316\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c06\u200b\u4f7f\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u5728\u200b\u4ee5\u540e\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#2","title":"2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b PyTorch \u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b 05. PyTorch Going Modular part 2 \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>create_dataloaders()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u548c\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u83b7\u53d6\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f6c\u6362\u200b\u6765\u200b\u6b63\u786e\u200b\u51c6\u5907\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\uff1a</p> <ol> <li>\u200b\u4f7f\u7528\u200b <code>torchvision.transforms</code> \u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torchvision.models.MODEL_NAME.MODEL_WEIGHTS.DEFAULT.transforms()</code> \u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002<ul> <li>\u200b\u5176\u4e2d\u200b <code>MODEL_NAME</code> \u200b\u662f\u200b\u7279\u5b9a\u200b\u7684\u200b <code>torchvision.models</code> \u200b\u67b6\u6784\u200b\uff0c<code>MODEL_WEIGHTS</code> \u200b\u662f\u200b\u7279\u5b9a\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\u96c6\u200b\uff0c<code>DEFAULT</code> \u200b\u8868\u793a\u200b\u201c\u200b\u6700\u4f73\u200b\u53ef\u7528\u200b\u6743\u91cd\u200b\u201d\u3002</li> </ul> </li> </ol> <p>\u200b\u6211\u4eec\u200b\u5728\u200b 06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b 2 \u200b\u8282\u200b \u200b\u4e2d\u200b\u770b\u5230\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u200b\u4e00\u4e2a\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b <code>torchvision.transforms</code> \u200b\u7ba1\u9053\u200b\u7684\u200b\u793a\u4f8b\u200b\uff08\u200b\u4ee5\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u7ba1\u9053\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u6700\u5927\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u6027\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u8f6c\u6362\u200b\u4e0e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u4e0d\u200b\u5339\u914d\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u6027\u80fd\u200b\u4e0b\u964d\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u7684\u200b\u4e3b\u8981\u200b\u624b\u52a8\u200b\u8f6c\u6362\u200b\u662f\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u90fd\u200b\u4ee5\u200b ImageNet \u200b\u683c\u5f0f\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\uff08\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b <code>torchvision.models</code> \u200b\u90fd\u200b\u662f\u200b\u5728\u200b ImageNet \u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8fd9\u6837\u200b\u505a\u200b\uff1a</p> <pre>normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n</pre>"},{"location":"07_pytorch_experiment_tracking/#21","title":"2.1 \u200b\u4f7f\u7528\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\u53d8\u6362\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u00b6","text":""},{"location":"07_pytorch_experiment_tracking/#22-dataloaders","title":"2.2 \u200b\u4f7f\u7528\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\u7684\u200b\u53d8\u6362\u200b\u521b\u5efa\u200b DataLoaders\u00b6","text":"<p>\u200b\u6570\u636e\u200b\u5df2\u200b\u53d8\u6362\u200b\u5e76\u200b\u521b\u5efa\u200b\u4e86\u200b DataLoaders\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u540c\u6837\u200b\u7684\u200b\u53d8\u6362\u200b\u6d41\u7a0b\u200b\u5728\u200b\u4f7f\u7528\u200b\u81ea\u52a8\u200b\u53d8\u6362\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u9996\u5148\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u7ec4\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\uff08\u200b\u4f8b\u5982\u200b <code>weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT</code>\uff09\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u5176\u200b\u8c03\u7528\u200b <code>transforms()</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#3","title":"3. \u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u51bb\u7ed3\u200b\u57fa\u7840\u200b\u5c42\u200b\u5e76\u200b\u66f4\u6539\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u8fd0\u884c\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u591a\u4e2a\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u6765\u200b\u770b\u770b\u200b\u5982\u4f55\u200b\u8fd0\u884c\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u5355\u4e2a\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200b\u5c31\u662f\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b <code>torchvision.models.efficientnet_b0()</code> \u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\uff0c\u200b\u5e76\u200b\u51c6\u5907\u200b\u597d\u200b\u5c06\u200b\u5176\u200b\u7528\u4e8e\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#4","title":"4. \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u8ddf\u8e2a\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u6a21\u578b\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u6765\u200b\u51c6\u5907\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.nn.CrossEntropyLoss()</code> \u200b\u4f5c\u4e3a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u4f7f\u7528\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b <code>0.001</code> \u200b\u7684\u200b <code>torch.optim.Adam()</code> \u200b\u4f5c\u4e3a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#train-summarywriter","title":"\u8c03\u6574\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u4ee5\u200b\u4f7f\u7528\u200b <code>SummaryWriter()</code> \u200b\u8ddf\u8e2a\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u5f00\u59cb\u200b\u9010\u6e10\u200b\u6574\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6dfb\u52a0\u200b\u6700\u540e\u200b\u4e00\u5757\u200b\u62fc\u56fe\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u591a\u4e2a\u200b Python \u200b\u5b57\u5178\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u6a21\u578b\u200b\u4e00\u4e2a\u200b\uff09\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u6211\u4eec\u200b\u7684\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u60f3\u8c61\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u8fd0\u884c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u4e0d\u6b62\u200b\u51e0\u4e2a\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u65b9\u6cd5\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u96be\u4ee5\u200b\u7ba1\u7406\u200b\u3002</p> <p>\u200b\u4e0d\u7528\u200b\u62c5\u5fc3\u200b\uff0c\u200b\u6709\u200b\u4e00\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u9009\u62e9\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b PyTorch \u200b\u7684\u200b <code>torch.utils.tensorboard.SummaryWriter()</code> \u200b\u7c7b\u200b\u5c06\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u5404\u4e2a\u200b\u90e8\u5206\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c<code>SummaryWriter()</code> \u200b\u7c7b\u4f1a\u200b\u5c06\u200b\u6709\u5173\u200b\u6a21\u578b\u200b\u7684\u200b\u5404\u79cd\u200b\u4fe1\u606f\u200b\u4fdd\u5b58\u200b\u5230\u200b\u7531\u200b <code>log_dir</code> \u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u7684\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u3002</p> <p><code>log_dir</code> \u200b\u7684\u200b\u9ed8\u8ba4\u200b\u4f4d\u7f6e\u200b\u662f\u200b <code>runs/CURRENT_DATETIME_HOSTNAME</code>\uff0c\u200b\u5176\u4e2d\u200b <code>HOSTNAME</code> \u200b\u662f\u200b\u4f60\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u540d\u79f0\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u81ea\u5b9a\u4e49\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u7684\u200b\u4f4d\u7f6e\u200b\uff08\u200b\u6587\u4ef6\u540d\u200b\u53ef\u4ee5\u200b\u968f\u5fc3\u6240\u6b32\u200b\u5730\u200b\u5b9a\u5236\u200b\uff09\u3002</p> <p><code>SummaryWriter()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u4ee5\u200b TensorBoard \u200b\u683c\u5f0f\u200b \u200b\u4fdd\u5b58\u200b\u3002</p> <p>TensorBoard \u200b\u662f\u200b TensorFlow \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5e93\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\uff0c\u200b\u662f\u200b\u53ef\u89c6\u5316\u200b\u6a21\u578b\u200b\u4e0d\u540c\u200b\u90e8\u5206\u200b\u7684\u200b\u4f18\u79c0\u200b\u5de5\u5177\u200b\u3002</p> <p>\u200b\u8981\u200b\u5f00\u59cb\u200b\u8ddf\u8e2a\u200b\u6211\u4eec\u200b\u7684\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u9ed8\u8ba4\u200b\u7684\u200b <code>SummaryWriter()</code> \u200b\u5b9e\u4f8b\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#5-tensorboard","title":"5. \u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u67e5\u770b\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u00b6","text":"<p><code>SummaryWriter()</code> \u200b\u7c7b\u200b\u9ed8\u8ba4\u200b\u5c06\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u4ee5\u200b TensorBoard \u200b\u683c\u5f0f\u200b\u5b58\u50a8\u200b\u5728\u200b\u540d\u4e3a\u200b <code>runs/</code> \u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>TensorBoard \u200b\u662f\u200b\u7531\u200b TensorFlow \u200b\u56e2\u961f\u200b\u521b\u5efa\u200b\u7684\u200b\u4e00\u4e2a\u200b\u53ef\u89c6\u5316\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u7528\u4e8e\u200b\u67e5\u770b\u200b\u548c\u200b\u68c0\u67e5\u200b\u6709\u5173\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4f60\u200b\u77e5\u9053\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4ec0\u4e48\u200b\u5417\u200b\uff1f</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u9075\u5faa\u200b\u6570\u636e\u200b\u53ef\u89c6\u5316\u200b\u5668\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u591a\u79cd\u200b\u65b9\u5f0f\u200b\u67e5\u770b\u200b TensorBoard\uff1a</p> \u200b\u4ee3\u7801\u200b\u73af\u5883\u200b \u200b\u5982\u4f55\u200b\u67e5\u770b\u200b TensorBoard \u200b\u8d44\u6e90\u200b VS Code\uff08\u200b\u7b14\u8bb0\u672c\u200b\u6216\u200b Python \u200b\u811a\u672c\u200b\uff09 \u200b\u6309\u200b <code>SHIFT + CMD + P</code> \u200b\u6253\u5f00\u200b\u547d\u4ee4\u200b\u9762\u677f\u200b\uff0c\u200b\u641c\u7d22\u200b\u547d\u4ee4\u200b \"Python: Launch TensorBoard\"\u3002 VS Code \u200b\u6307\u5357\u200b\uff1aTensorBoard \u200b\u548c\u200b PyTorch Jupyter \u200b\u548c\u200b Colab \u200b\u7b14\u8bb0\u672c\u200b \u200b\u786e\u4fdd\u200b TensorBoard \u200b\u5df2\u200b\u5b89\u88c5\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>%load_ext tensorboard</code> \u200b\u52a0\u8f7d\u200b\u5b83\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>%tensorboard --logdir DIR_WITH_LOGS</code> \u200b\u67e5\u770b\u200b\u7ed3\u679c\u200b\u3002 <code>torch.utils.tensorboard</code> \u200b\u548c\u200b TensorBoard \u200b\u5165\u95e8\u200b <p>\u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u4e0a\u200b\u4f20\u5230\u200b tensorboard.dev\uff0c\u200b\u4ee5\u4fbf\u200b\u4e0e\u200b\u4ed6\u4eba\u200b\u516c\u5f00\u200b\u5206\u4eab\u200b\u3002</p> <p>\u200b\u5728\u200b Google Colab \u200b\u6216\u200b Jupyter Notebook \u200b\u4e2d\u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u5c06\u200b\u542f\u52a8\u200b\u4e00\u4e2a\u200b\u4ea4\u4e92\u5f0f\u200b TensorBoard \u200b\u4f1a\u8bdd\u200b\uff0c\u200b\u4ee5\u200b\u67e5\u770b\u200b <code>runs/</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b TensorBoard \u200b\u6587\u4ef6\u200b\u3002</p> <pre>%load_ext tensorboard # \u200b\u52a0\u8f7d\u200b TensorBoard \u200b\u7684\u200b\u884c\u200b\u9b54\u6cd5\u200b\n%tensorboard --logdir runs # \u200b\u4f7f\u7528\u200b \"runs/\" \u200b\u76ee\u5f55\u200b\u8fd0\u884c\u200b TensorBoard \u200b\u4f1a\u8bdd\u200b\n</pre>"},{"location":"07_pytorch_experiment_tracking/#6-summarywriter","title":"6. \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u6765\u200b\u6784\u5efa\u200b <code>SummaryWriter()</code> \u200b\u5b9e\u4f8b\u200b\u00b6","text":"<p><code>SummaryWriter()</code> \u200b\u7c7b\u200b\u5c06\u200b\u5404\u79cd\u200b\u4fe1\u606f\u200b\u8bb0\u5f55\u200b\u5230\u200b\u7531\u200b <code>log_dir</code> \u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\uff0c\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u81ea\u5b9a\u4e49\u200b\u76ee\u5f55\u200b\uff1f</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u90fd\u200b\u6709\u200b\u81ea\u5df1\u200b\u7684\u200b\u65e5\u5fd7\u200b\u76ee\u5f55\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u8ddf\u8e2a\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> <ul> <li>\u200b\u5b9e\u9a8c\u200b\u65e5\u671f\u200b/\u200b\u65f6\u95f4\u200b\u6233\u200b - \u200b\u5b9e\u9a8c\u200b\u662f\u200b\u5728\u200b\u4f55\u65f6\u200b\u8fdb\u884c\u200b\u7684\u200b\uff1f</li> <li>\u200b\u5b9e\u9a8c\u200b\u540d\u79f0\u200b - \u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u60f3\u8981\u200b\u4e3a\u200b\u5b9e\u9a8c\u200b\u53d6\u200b\u4e00\u4e2a\u200b\u540d\u79f0\u200b\uff1f</li> <li>\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b - \u200b\u4f7f\u7528\u200b\u4e86\u200b\u54ea\u4e2a\u200b\u6a21\u578b\u200b\uff1f</li> <li>\u200b\u989d\u5916\u200b\u4fe1\u606f\u200b - \u200b\u662f\u5426\u200b\u9700\u8981\u200b\u8ddf\u8e2a\u200b\u5176\u4ed6\u200b\u4efb\u4f55\u200b\u5185\u5bb9\u200b\uff1f</li> </ul> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u8ddf\u8e2a\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5e76\u4e14\u200b\u53ef\u4ee5\u200b\u5c3d\u60c5\u200b\u53d1\u6325\u200b\u521b\u610f\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u4e9b\u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>create_writer()</code> \u200b\u7684\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u751f\u6210\u200b\u4e00\u4e2a\u200b <code>SummaryWriter()</code> \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u8bb0\u5f55\u200b\u5230\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b <code>log_dir</code>\u3002</p> <p>\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b <code>log_dir</code> \u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\uff1a</p> <p><code>runs/YYYY-MM-DD/experiment_name/model_name/extra</code></p> <p>\u200b\u5176\u4e2d\u200b <code>YYYY-MM-DD</code> \u200b\u662f\u200b\u5b9e\u9a8c\u200b\u8fd0\u884c\u200b\u7684\u200b\u65e5\u671f\u200b\uff08\u200b\u5982\u679c\u200b\u4f60\u200b\u613f\u610f\u200b\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u6dfb\u52a0\u200b\u65f6\u95f4\u200b\uff09\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#61-train-writer","title":"6.1 \u200b\u66f4\u65b0\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u4ee5\u200b\u5305\u542b\u200b <code>writer</code> \u200b\u53c2\u6570\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>create_writer()</code> \u200b\u51fd\u6570\u200b\u8868\u73b0\u200b\u975e\u5e38\u200b\u51fa\u8272\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u589e\u52a0\u200b\u4e00\u4e2a\u200b <code>writer</code> \u200b\u53c2\u6570\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6bcf\u6b21\u200b\u8c03\u7528\u200b <code>train()</code> \u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u90fd\u200b\u80fd\u200b\u4e3b\u52a8\u200b\u66f4\u65b0\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b <code>SummaryWriter()</code> \u200b\u5b9e\u4f8b\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u8fd0\u884c\u200b\u4e00\u7cfb\u5217\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u591a\u6b21\u200b\u8c03\u7528\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u8bad\u7ec3\u200b\u591a\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b <code>writer</code> \u200b\u4f1a\u200b\u5f88\u200b\u6709\u7528\u200b\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u4e00\u4e2a\u200b <code>writer</code> = \u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u4e00\u4e2a\u200b\u65e5\u5fd7\u200b\u76ee\u5f55\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8c03\u6574\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5411\u200b\u51fd\u6570\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b <code>writer</code> \u200b\u53c2\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u6dfb\u52a0\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\u6765\u200b\u68c0\u67e5\u200b\u662f\u5426\u200b\u6709\u200b <code>writer</code>\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u90a3\u91cc\u200b\u8bb0\u5f55\u200b\u6211\u4eec\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#7","title":"7. \u200b\u8bbe\u7f6e\u200b\u4e00\u7cfb\u5217\u200b\u5efa\u6a21\u200b\u5b9e\u9a8c\u200b\u00b6","text":"<p>\u200b\u662f\u200b\u65f6\u5019\u200b\u63d0\u5347\u200b\u4e00\u4e2a\u200b\u6863\u6b21\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e4b\u524d\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5404\u79cd\u200b\u5b9e\u9a8c\u200b\u5e76\u200b\u9010\u4e00\u200b\u68c0\u67e5\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u80fd\u200b\u540c\u65f6\u200b\u8fd0\u884c\u200b\u591a\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u7136\u540e\u200b\u4e00\u8d77\u200b\u68c0\u67e5\u200b\u7ed3\u679c\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u51c6\u5907\u200b\u597d\u4e86\u5417\u200b\uff1f</p> <p>\u200b\u6765\u200b\u5427\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#71","title":"7.1 \u200b\u4f60\u200b\u5e94\u8be5\u200b\u8fdb\u884c\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff1f\u00b6","text":"<p>\u200b\u8fd9\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u7684\u200b\u767e\u4e07\u7f8e\u5143\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u771f\u7684\u200b\u6ca1\u6709\u200b\u9650\u5236\u200b\u3002</p> <p>\u200b\u6b63\u662f\u200b\u8fd9\u79cd\u200b\u81ea\u7531\u200b\u4f7f\u5f97\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u65e2\u200b\u4ee4\u4eba\u5174\u594b\u200b\u53c8\u200b\u4ee4\u4eba\u200b\u6050\u60e7\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u4f60\u200b\u5fc5\u987b\u200b\u7a7f\u200b\u4e0a\u200b\u79d1\u5b66\u5bb6\u200b\u7684\u200b\u5916\u8863\u200b\uff0c\u200b\u5e76\u200b\u8bb0\u4f4f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u8df5\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff1a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff01</p> <p>\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u8d85\u200b\u53c2\u6570\u200b\u90fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u8d77\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u6539\u53d8\u200b epoch \u200b\u7684\u200b\u6570\u91cf\u200b\u3002</li> <li>\u200b\u6539\u53d8\u200b \u200b\u5c42\u6570\u200b/\u200b\u9690\u85cf\u200b\u5355\u5143\u200b \u200b\u7684\u200b\u6570\u91cf\u200b\u3002</li> <li>\u200b\u6539\u53d8\u200b \u200b\u6570\u636e\u200b \u200b\u7684\u200b\u6570\u91cf\u200b\u3002</li> <li>\u200b\u6539\u53d8\u200b \u200b\u5b66\u4e60\u200b\u7387\u200b\u3002</li> <li>\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u3002</li> <li>\u200b\u9009\u62e9\u200b\u4e0d\u540c\u200b\u7684\u200b \u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u3002</li> </ul> <p>\u200b\u901a\u8fc7\u200b\u5b9e\u8df5\u200b\u548c\u200b\u8fd0\u884c\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4f60\u200b\u5c06\u200b\u5f00\u59cb\u200b\u5efa\u7acb\u200b\u4e00\u79cd\u200b\u76f4\u89c9\u200b\uff0c\u200b\u4e86\u89e3\u200b\u4ec0\u4e48\u200b \u200b\u53ef\u80fd\u200b \u200b\u6709\u52a9\u4e8e\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u200b\u6545\u610f\u200b\u8bf4\u200b \u200b\u53ef\u80fd\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6ca1\u6709\u200b\u4fdd\u8bc1\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u9274\u4e8e\u200b The Bitter Lesson\uff08\u200b\u6211\u200b\u5df2\u7ecf\u200b\u63d0\u5230\u200b\u8fc7\u200b\u4e24\u6b21\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u662f\u200b\u4eba\u5de5\u667a\u80fd\u200b\u9886\u57df\u200b\u4e2d\u200b\u4e00\u7bc7\u200b\u91cd\u8981\u200b\u7684\u200b\u6587\u7ae0\u200b\uff09\uff0c\u200b\u901a\u5e38\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u8d8a\u5927\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u53c2\u6570\u200b\uff09\uff0c\u200b\u4f60\u200b\u62e5\u6709\u200b\u7684\u200b\u6570\u636e\u200b\u8d8a\u200b\u591a\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5b66\u4e60\u200b\u673a\u4f1a\u200b\uff09\uff0c\u200b\u6027\u80fd\u200b\u5c31\u200b\u8d8a\u200b\u597d\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u7b2c\u4e00\u6b21\u200b\u9762\u5bf9\u200b\u4e00\u4e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\u65f6\u200b\uff1a\u200b\u4ece\u5c0f\u200b\u89c4\u6a21\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5982\u679c\u200b\u67d0\u4e2a\u200b\u65b9\u6cd5\u200b\u6709\u6548\u200b\uff0c\u200b\u518d\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u3002</p> <p>\u200b\u4f60\u200b\u7684\u200b\u7b2c\u4e00\u6279\u200b\u5b9e\u9a8c\u200b\u5e94\u8be5\u200b\u53ea\u200b\u9700\u200b\u51e0\u79d2\u200b\u5230\u200b\u51e0\u5206\u949f\u200b\u5c31\u200b\u80fd\u200b\u8fd0\u884c\u200b\u5b8c\u6bd5\u200b\u3002</p> <p>\u200b\u4f60\u200b\u8d8a\u200b\u5feb\u200b\u80fd\u200b\u8fdb\u884c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5c31\u200b\u80fd\u200b\u8d8a\u200b\u5feb\u200b\u627e\u51fa\u200b\u4ec0\u4e48\u200b \u200b\u4e0d\u200b \u200b\u6709\u6548\u200b\uff0c\u200b\u8fdb\u800c\u200b\u8d8a\u200b\u5feb\u200b\u627e\u51fa\u200b\u4ec0\u4e48\u200b \u200b\u6709\u6548\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#72","title":"7.2 \u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u54ea\u4e9b\u200b\u5b9e\u9a8c\u200b\uff1f\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u6539\u8fdb\u200b\u9a71\u52a8\u200b FoodVision Mini \u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u540c\u65f6\u200b\u907f\u514d\u200b\u6a21\u578b\u200b\u53d8\u5f97\u200b\u8fc7\u5927\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u7406\u60f3\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8fbe\u5230\u200b\u9ad8\u200b\u51c6\u786e\u7387\u200b\uff0890%+\uff09\uff0c\u200b\u4f46\u200b\u8bad\u7ec3\u200b\u548c\u200b\u63a8\u7406\u200b\uff08\u200b\u9884\u6d4b\u200b\uff09\u200b\u65f6\u95f4\u200b\u4e0d\u4f1a\u200b\u592a\u200b\u957f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6709\u200b\u5f88\u591a\u200b\u9009\u62e9\u200b\uff0c\u200b\u4f46\u200b\u4e0d\u59a8\u200b\u4fdd\u6301\u200b\u7b80\u5355\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u4ee5\u4e0b\u200b\u7ec4\u5408\u200b\uff1a</p> <ol> <li>\u200b\u4e0d\u540c\u200b\u6570\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\uff0810% \u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b vs. 20%\uff09</li> <li>\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff08<code>torchvision.models.efficientnet_b0</code> vs. <code>torchvision.models.efficientnet_b2</code>\uff09</li> <li>\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff085 \u200b\u4e2a\u200b epoch vs. 10 \u200b\u4e2a\u200b epoch\uff09</li> </ol> <p>\u200b\u5177\u4f53\u200b\u5206\u89e3\u200b\u5982\u4e0b\u200b\uff1a</p> \u200b\u5b9e\u9a8c\u200b\u7f16\u53f7\u200b \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u6a21\u578b\u200b\uff08\u200b\u5728\u200b ImageNet \u200b\u4e0a\u9884\u200b\u8bad\u7ec3\u200b\uff09 \u200b\u8bad\u7ec3\u200b\u8f6e\u6570\u200b 1 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 10% EfficientNetB0 5 2 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 10% EfficientNetB2 5 3 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 10% EfficientNetB0 10 4 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 10% EfficientNetB2 10 5 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 20% EfficientNetB0 5 6 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 20% EfficientNetB2 5 7 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 20% EfficientNetB0 10 8 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 20% EfficientNetB2 10 <p>\u200b\u6ce8\u610f\u200b\u6211\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u9010\u6b65\u200b\u589e\u52a0\u200b\u5b9e\u9a8c\u200b\u89c4\u6a21\u200b\u7684\u200b\u3002</p> <p>\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u6211\u4eec\u200b\u90fd\u200b\u9010\u6e10\u200b\u589e\u52a0\u200b\u6570\u636e\u91cf\u200b\u3001\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u548c\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u957f\u200b\u3002</p> <p>\u200b\u5230\u200b\u6700\u540e\u200b\uff0c\u200b\u5b9e\u9a8c\u200b 8 \u200b\u5c06\u200b\u4f7f\u7528\u200b\u6bd4\u200b\u5b9e\u9a8c\u200b 1 \u200b\u591a\u4e00\u500d\u200b\u7684\u200b\u6570\u636e\u200b\u3001\u200b\u5927\u200b\u4e00\u500d\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u957f\u200b\u4e00\u500d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u200b\u60f3\u200b\u660e\u786e\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4f60\u200b\u771f\u6b63\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u6570\u91cf\u200b\u662f\u200b\u6ca1\u6709\u200b\u9650\u5236\u200b\u7684\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u8bbe\u8ba1\u200b\u7684\u200b\u53ea\u662f\u200b\u975e\u5e38\u200b\u5c0f\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u9009\u9879\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u6d4b\u8bd5\u200b\u6240\u6709\u200b\u4e1c\u897f\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6700\u597d\u200b\u5148\u200b\u5c1d\u8bd5\u200b\u4e00\u4e9b\u200b\uff0c\u200b\u7136\u540e\u200b\u8ddf\u8fdb\u200b\u90a3\u4e9b\u200b\u6548\u679c\u200b\u6700\u597d\u200b\u7684\u200b\u3002</p> <p>\u200b\u53e6\u5916\u200b\u63d0\u9192\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u662f\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u7684\u200b\u4e00\u4e2a\u200b\u5b50\u96c6\u200b\uff083 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b 101 \u200b\u4e2a\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u53ea\u200b\u4f7f\u7528\u200b\u4e86\u200b 10% \u200b\u548c\u200b 20% \u200b\u7684\u200b\u56fe\u7247\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b 100%\u3002\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u6210\u529f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u5728\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u5b9e\u9a8c\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u8fd9\u200b\u5c06\u200b\u9700\u8981\u200b\u66f4\u957f\u200b\u7684\u200b\u8ba1\u7b97\u200b\u65f6\u95f4\u200b\uff09\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>04_custom_data_creation.ipynb</code> \u200b\u7b14\u8bb0\u672c\u200b \u200b\u67e5\u770b\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u7684\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#73","title":"7.3 \u200b\u4e0b\u8f7d\u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u6211\u4eec\u200b\u5f00\u59cb\u8fd0\u884c\u200b\u4e00\u7cfb\u5217\u200b\u5b9e\u9a8c\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u786e\u4fdd\u200b\u6570\u636e\u200b\u96c6\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u5c31\u7eea\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e24\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff1a</p> <ol> <li>\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b Food101 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b 10% \u200b\u6570\u636e\u200b \u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\uff08\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5728\u200b\u4e0a\u9762\u200b\u521b\u5efa\u200b\u4e86\u200b\u8fd9\u4e2a\u200b\uff0c\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u5b8c\u6574\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u518d\u6b21\u200b\u521b\u5efa\u200b\uff09\u3002</li> <li>\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b Food101 \u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b 20% \u200b\u6570\u636e\u200b \u200b\u7684\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u3002</li> </ol> <p>\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u4e00\u81f4\u6027\u200b\uff0c\u200b\u6240\u6709\u200b\u5b9e\u9a8c\u200b\u5c06\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u6765\u81ea\u200b 10% \u200b\u6570\u636e\u200b\u5206\u5272\u200b\u7684\u200b\u90a3\u4e2a\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b <code>download_data()</code> \u200b\u51fd\u6570\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200b\u5404\u79cd\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6570\u636e\u200b\u96c6\u90fd\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub \u200b\u4e0a\u200b\u83b7\u53d6\u200b\uff1a</p> <ol> <li>\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 10% \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002</li> <li>\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b 20% \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002</li> </ol>"},{"location":"07_pytorch_experiment_tracking/#74-dataloader","title":"7.4 \u200b\u8f6c\u6362\u200b\u6570\u636e\u200b\u96c6\u200b\u5e76\u200b\u521b\u5efa\u200b DataLoader\u00b6","text":"<p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u7cfb\u5217\u200b\u8f6c\u6362\u200b\uff0c\u200b\u4ee5\u200b\u51c6\u5907\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u4e00\u81f4\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u624b\u52a8\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f6c\u6362\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4e0a\u9762\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff09\uff0c\u200b\u5e76\u200b\u5728\u200b\u6240\u6709\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u8f6c\u6362\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u8c03\u6574\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u7684\u200b\u5927\u5c0f\u200b\uff08\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b 224x224\uff0c\u200b\u4f46\u200b\u8fd9\u4e2a\u200b\u5c3a\u5bf8\u200b\u53ef\u4ee5\u200b\u66f4\u6539\u200b\uff09\u3002</li> <li>\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u503c\u200b\u5728\u200b 0 \u200b\u5230\u200b 1 \u200b\u4e4b\u95f4\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</li> <li>\u200b\u4ee5\u200b\u67d0\u79cd\u200b\u65b9\u5f0f\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u5206\u5e03\u200b\u4e0e\u200b ImageNet \u200b\u6570\u636e\u200b\u96c6\u200b\u4e00\u81f4\u200b\uff08\u200b\u6211\u4eec\u200b\u8fd9\u6837\u200b\u505a\u200b\u662f\u56e0\u4e3a\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u83b7\u53d6\u200b\u7684\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u5728\u200b ImageNet \u200b\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u9884\u200b\u8bad\u7ec3\u200b\uff09\u3002</li> </ol>"},{"location":"07_pytorch_experiment_tracking/#75","title":"7.5 \u200b\u521b\u5efa\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u662f\u200b\u65f6\u5019\u200b\u5f00\u59cb\u200b\u6784\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\uff1a</p> <ol> <li><code>torchvision.models.efficientnet_b0()</code> \u200b\u9884\u200b\u8bad\u7ec3\u200b\u4e3b\u5e72\u200b + \u200b\u81ea\u5b9a\u4e49\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\uff08\u200b\u7b80\u79f0\u200b EffNetB0\uff09\u3002</li> <li><code>torchvision.models.efficientnet_b2()</code> \u200b\u9884\u200b\u8bad\u7ec3\u200b\u4e3b\u5e72\u200b + \u200b\u81ea\u5b9a\u4e49\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\uff08\u200b\u7b80\u79f0\u200b EffNetB2\uff09\u3002</li> </ol> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u51bb\u7ed3\u200b\u57fa\u7840\u200b\u5c42\u200b\uff08\u200b\u7279\u5f81\u200b\u5c42\u200b\uff09\u200b\u5e76\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\uff08\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff09\uff0c\u200b\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b3.4\u200b\u8282\u4e2d\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u200b\u4e00\u7ae0\u200b\u4e2d\u200b\u770b\u5230\u200b\uff0cEffNetB0 \u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u7684\u200b <code>in_features</code> \u200b\u53c2\u6570\u200b\u662f\u200b <code>1280</code>\uff08\u200b\u4e3b\u5e72\u200b\u5c06\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5927\u5c0f\u200b\u4e3a\u200b <code>1280</code> \u200b\u7684\u200b\u7279\u5f81\u5411\u91cf\u200b\uff09\u3002</p> <p>\u200b\u7531\u4e8e\u200b EffNetB2 \u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u6570\u91cf\u200b\u7684\u200b\u5c42\u200b\u548c\u200b\u53c2\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u76f8\u5e94\u200b\u5730\u200b\u8c03\u6574\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6bcf\u5f53\u200b\u4f60\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u9996\u5148\u200b\u8981\u200b\u68c0\u67e5\u200b\u7684\u200b\u662f\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002\u200b\u8fd9\u6837\u200b\u4f60\u200b\u5c31\u200b\u77e5\u9053\u200b\u5982\u4f55\u200b\u51c6\u5907\u200b\u8f93\u5165\u200b\u6570\u636e\u200b/\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u6b63\u786e\u200b\u7684\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchinfo.summary()</code> \u200b\u5e76\u200b\u4f20\u5165\u200b <code>input_size=(32, 3, 224, 224)</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u627e\u5230\u200b EffNetB2 \u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\uff08<code>(32, 3, 224, 224)</code> \u200b\u76f8\u5f53\u4e8e\u200b <code>(batch_size, color_channels, height, width)</code>\uff0c\u200b\u5373\u200b\u6211\u4eec\u200b\u4f20\u5165\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u8bf4\u660e\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u7684\u200b\u6570\u636e\u200b\u4f1a\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bb8\u591a\u200b\u73b0\u4ee3\u200b\u6a21\u578b\u200b\u7531\u4e8e\u200b <code>torch.nn.AdaptiveAvgPool2d()</code> \u200b\u5c42\u200b\u7684\u200b\u5b58\u5728\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8be5\u5c42\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u81ea\u200b\u9002\u5e94\u200b\u5730\u200b\u8c03\u6574\u200b\u7ed9\u5b9a\u200b\u8f93\u5165\u200b\u7684\u200b <code>output_size</code>\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5411\u200b <code>torchinfo.summary()</code> \u200b\u6216\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u6a21\u578b\u200b\u4f20\u9012\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u627e\u5230\u200b EffNetB2 \u200b\u6700\u7ec8\u200b\u5c42\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>torchvision.models.efficientnet_b2(pretrained=True)</code> \u200b\u7684\u200b\u5b9e\u4f8b\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b <code>torchinfo.summary()</code> \u200b\u67e5\u770b\u200b\u5404\u79cd\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u68c0\u67e5\u200b EffNetB2 \u200b\u5206\u7c7b\u5668\u200b\u90e8\u5206\u200b\u7684\u200b <code>state_dict()</code> \u200b\u5e76\u6253\u5370\u200b\u6743\u91cd\u200b\u77e9\u9635\u200b\u7684\u200b\u957f\u5ea6\u200b\u6765\u200b\u6253\u5370\u200b <code>in_features</code> \u200b\u7684\u200b\u6570\u91cf\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u68c0\u67e5\u200b <code>effnetb2.classifier</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</li> </ul> </li> </ol>"},{"location":"07_pytorch_experiment_tracking/#76","title":"7.6 \u200b\u521b\u5efa\u200b\u5b9e\u9a8c\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u6570\u636e\u200b\u5e76\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u6a21\u578b\u200b\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u8bbe\u7f6e\u200b\u4e00\u4e9b\u200b\u5b9e\u9a8c\u200b\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u5217\u8868\u200b\u548c\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u5f00\u59cb\u200b\uff1a</p> <ol> <li>\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u6d4b\u8bd5\u200b\u7684\u200b epoch \u200b\u6570\u200b\u5217\u8868\u200b\uff08<code>[5, 10]</code>\uff09</li> <li>\u200b\u6211\u4eec\u200b\u60f3\u8981\u200b\u6d4b\u8bd5\u200b\u7684\u200b\u6a21\u578b\u200b\u5217\u8868\u200b\uff08<code>[\"effnetb0\", \"effnetb2\"]</code>\uff09</li> <li>\u200b\u4e0d\u540c\u200b\u8bad\u7ec3\u200b DataLoader \u200b\u7684\u200b\u5b57\u5178\u200b</li> </ol>"},{"location":"07_pytorch_experiment_tracking/#8-tensorboard","title":"8. \u200b\u5728\u200b TensorBoard \u200b\u4e2d\u200b\u67e5\u770b\u200b\u5b9e\u9a8c\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u54e6\u200b\uff0c\u200b\u54e6\u200b\uff01</p> <p>\u200b\u770b\u200b\u6211\u4eec\u200b\u8fdb\u5c55\u200b\u5f97\u200b\u591a\u200b\u5feb\u200b\uff01</p> <p>\u200b\u4e00\u6b21\u200b\u8bad\u7ec3\u200b\u516b\u4e2a\u200b\u6a21\u578b\u200b\uff1f</p> <p>\u200b\u8fd9\u624d\u200b\u7b26\u5408\u200b\u6211\u4eec\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff01</p> <p>\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u770b\u770b\u200b TensorBoard \u200b\u4e2d\u200b\u7684\u200b\u7ed3\u679c\u200b\u5462\u200b\uff1f</p>"},{"location":"07_pytorch_experiment_tracking/#9","title":"9. \u200b\u52a0\u8f7d\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5b83\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u67e5\u770b\u200b\u6211\u4eec\u200b\u516b\u4e2a\u200b\u5b9e\u9a8c\u200b\u7684\u200b TensorBoard \u200b\u65e5\u5fd7\u200b\uff0c\u200b\u4f3c\u4e4e\u200b\u7b2c\u516b\u4e2a\u200b\u5b9e\u9a8c\u200b\u53d6\u5f97\u200b\u4e86\u200b\u6700\u4f73\u200b\u6574\u4f53\u200b\u7ed3\u679c\u200b\uff08\u200b\u6700\u9ad8\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u7b2c\u4e8c\u200b\u4f4e\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u5b9e\u9a8c\u200b\u4f7f\u7528\u200b\u4e86\u200b\uff1a</p> <ul> <li>EffNetB2\uff08\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u662f\u200b EffNetB0 \u200b\u7684\u200b\u4e24\u500d\u200b\uff09</li> <li>20% \u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff08\u200b\u8bad\u7ec3\u200b\u6570\u636e\u91cf\u200b\u662f\u200b\u539f\u59cb\u6570\u636e\u200b\u7684\u200b\u4e24\u500d\u200b\uff09</li> <li>10 \u200b\u4e2a\u200b\u5468\u671f\u200b\uff08\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u662f\u200b\u539f\u59cb\u200b\u65f6\u95f4\u200b\u7684\u200b\u4e24\u500d\u200b\uff09</li> </ul> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u53d6\u5f97\u200b\u4e86\u200b\u6700\u4f73\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u8fd9\u4e9b\u200b\u7ed3\u679c\u200b\u5e76\u200b\u6ca1\u6709\u200b\u6bd4\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\u597d\u200b\u5f88\u591a\u200b\u3002</p> <p>\u200b\u5728\u200b\u76f8\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u5185\u200b\uff08\u200b\u5b9e\u9a8c\u200b\u7f16\u53f7\u200b 6\uff09\uff0c\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u53d6\u5f97\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u8868\u660e\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u53ef\u80fd\u200b\u6700\u5177\u200b\u5f71\u54cd\u529b\u200b\u7684\u200b\u90e8\u5206\u200b\u662f\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u548c\u200b\u6570\u636e\u91cf\u200b\u3002</p> <p>\u200b\u8fdb\u4e00\u6b65\u200b\u68c0\u67e5\u200b\u7ed3\u679c\u200b\uff0c\u200b\u4f3c\u4e4e\u200b\u901a\u5e38\u200b\u5177\u6709\u200b\u66f4\u200b\u591a\u200b\u53c2\u6570\u200b\uff08EffNetB2\uff09\u200b\u548c\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\uff0820% \u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff09\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\uff08\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u66f4\u200b\u4f4e\u200b\uff0c\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\u66f4\u9ad8\u200b\uff09\u3002</p> <p>\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u66f4\u200b\u591a\u200b\u5b9e\u9a8c\u200b\u6765\u200b\u8fdb\u4e00\u6b65\u200b\u9a8c\u8bc1\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4f46\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u7b2c\u516b\u4e2a\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u5bfc\u5165\u200b\u8868\u73b0\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u4fdd\u5b58\u200b\u8def\u5f84\u200b\u4e3a\u200b\uff1a<code>models/07_effnetb2_data_20_percent_10_epochs.pth</code>\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u8bfe\u7a0b\u200b GitHub \u200b\u4e0b\u8f7d\u200b\u6b64\u200b\u6a21\u578b\u200b\uff09\u200b\u5e76\u200b\u8fdb\u884c\u200b\u4e00\u4e9b\u200b\u5b9a\u6027\u200b\u8bc4\u4f30\u200b\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b \u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b <code>create_effnetb2()</code> \u200b\u51fd\u6570\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b EffNetB2 \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>torch.load()</code> \u200b\u52a0\u8f7d\u200b\u4fdd\u5b58\u200b\u7684\u200b <code>state_dict()</code> \u200b\u6765\u200b\u5bfc\u5165\u200b\u6700\u4f73\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/#91","title":"9.1 \u200b\u4f7f\u7528\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5f88\u9177\u200b\uff0c\u200b\u4f46\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u771f\u6b63\u200b\u9b45\u529b\u200b\u5728\u4e8e\u200b\u80fd\u591f\u200b\u5bf9\u200b\u60a8\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bfc\u5165\u200b\u53ef\u9760\u200b\u7684\u200b\u62ab\u8428\u200b\u7238\u7238\u200b\u56fe\u7247\u200b\uff08\u200b\u4e00\u5f20\u200b\u6211\u200b\u7238\u7238\u200b\u7ad9\u200b\u5728\u200b\u62ab\u8428\u200b\u524d\u200b\u7684\u200b\u7167\u7247\u200b\uff09\uff0c\u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u524d\u200b\u51e0\u8282\u200b\u4e2d\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\uff0c\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u5b83\u200b\u4e0a\u9762\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u3002</p>"},{"location":"07_pytorch_experiment_tracking/","title":"\u4e3b\u8981\u200b\u6536\u83b7\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5b8c\u6574\u200b\u5730\u200b\u56de\u987e\u200b\u4e86\u200b\u5728\u200b 01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200b PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u6570\u636e\u200b\uff0c\u200b\u6784\u5efa\u200b\u5e76\u200b\u9009\u62e9\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5404\u79cd\u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\u6765\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u672c\u6b21\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u4e00\u7cfb\u5217\u200b\u5b9e\u9a8c\u200b\u6765\u200b\u6539\u8fdb\u200b\u4e86\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u3002</p> <p></p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4e3a\u200b\u81ea\u5df1\u200b\u611f\u5230\u200b\u9a84\u50b2\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u9879\u200b\u4e0d\u5c0f\u200b\u7684\u200b\u6210\u5c31\u200b\uff01</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4ece\u200b\u672c\u6b21\u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b1\u200b\u4e2d\u200b\u5e26\u8d70\u200b\u7684\u200b\u4e3b\u8981\u200b\u60f3\u6cd5\u200b\u662f\u200b\uff1a</p> <ul> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u8df5\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff1a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u5b9e\u9a8c\u200b\uff01\uff08\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u505a\u200b\u4e86\u200b\u5f88\u591a\u200b\u8fd9\u6837\u200b\u7684\u200b\u5de5\u4f5c\u200b\uff09\u3002</li> <li>\u200b\u5728\u200b\u5f00\u59cb\u200b\u65f6\u200b\uff0c\u200b\u4fdd\u6301\u200b\u5b9e\u9a8c\u200b\u89c4\u6a21\u200b\u5c0f\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5feb\u901f\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u524d\u51e0\u6b21\u200b\u5b9e\u9a8c\u200b\u4e0d\u200b\u5e94\u8be5\u200b\u82b1\u8d39\u200b\u8d85\u8fc7\u200b\u51e0\u79d2\u200b\u5230\u200b\u51e0\u5206\u949f\u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u200b\u8fd0\u884c\u200b\u3002</li> <li>\u200b\u4f60\u200b\u505a\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u4f60\u200b\u5c31\u200b\u80fd\u200b\u8d8a\u200b\u5feb\u200b\u5730\u200b\u627e\u51fa\u200b\u4ec0\u4e48\u200b\u4e0d\u8d77\u4f5c\u7528\u200b\u3002</li> <li>\u200b\u5f53\u200b\u4f60\u200b\u627e\u5230\u200b\u6709\u6548\u200b\u7684\u200b\u65b9\u6848\u200b\u65f6\u200b\uff0c\u200b\u518d\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6027\u80fd\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b EffNetB2 \u200b\u4f5c\u4e3a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff0c\u200b\u4e5f\u8bb8\u200b\u4f60\u200b\u73b0\u5728\u200b\u60f3\u200b\u770b\u770b\u200b\u5f53\u200b\u4f60\u200b\u5c06\u200b\u5176\u200b\u6269\u5c55\u200b\u5230\u200b\u6574\u4e2a\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u65f6\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</li> <li>\u200b\u4ee5\u200b\u7f16\u7a0b\u200b\u65b9\u5f0f\u200b\u8ddf\u8e2a\u200b\u4f60\u200b\u7684\u200b\u5b9e\u9a8c\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u6b65\u9aa4\u200b\u6765\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u4f46\u200b\u4ece\u200b\u957f\u8fdc\u200b\u6765\u770b\u200b\u662f\u200b\u503c\u5f97\u200b\u7684\u200b\uff0c\u200b\u8fd9\u6837\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u627e\u51fa\u200b\u4ec0\u4e48\u200b\u6709\u6548\u200b\uff0c\u200b\u4ec0\u4e48\u200b\u65e0\u6548\u200b\u3002<ul> <li>\u200b\u6709\u200b\u5f88\u591a\u200b\u4e0d\u540c\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u5668\u200b\uff0c\u200b\u6240\u4ee5\u200b\u63a2\u7d22\u200b\u4e00\u4e9b\u200b\u5e76\u200b\u5c1d\u8bd5\u200b\u5b83\u4eec\u200b\u3002</li> </ul> </li> </ul>"},{"location":"07_pytorch_experiment_tracking/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u671f\u671b\u200b\u4f7f\u7528\u200b <code>torchvision</code> v0.13+\uff082022\u200b\u5e74\u200b7\u200b\u6708\u200b\u53d1\u5e03\u200b\uff09\uff0c\u200b\u4e4b\u524d\u200b\u7684\u200b\u7248\u672c\u200b\u53ef\u80fd\u200b\u4e5f\u200b\u80fd\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u4f46\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u96c6\u4e2d\u200b\u5728\u200b\u7ec3\u4e60\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u6216\u200b\u9075\u5faa\u200b\u6240\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u5e94\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u7b2c\u200b07\u200b\u8282\u200b\u7684\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b</li> <li>\u200b\u7b2c\u200b07\u200b\u8282\u200b\u7684\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\u4e4b\u524d\u200b\u67e5\u770b\u200b\u6b64\u200b\u5185\u5bb9\u200b\uff09<ul> <li>\u200b\u5728\u200bYouTube\u200b\u4e0a\u200b\u89c2\u770b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7684\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b\uff08\u200b\u5305\u62ec\u200b\u6240\u6709\u200b\u9519\u8bef\u200b\uff09</li> </ul> </li> </ul> <ol> <li>\u200b\u4ece\u200b<code>torchvision.models</code>\u200b\u4e2d\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u66f4\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u6dfb\u52a0\u200b\u5230\u200b\u5b9e\u9a8c\u200b\u5217\u8868\u200b\u4e2d\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cEffNetB3\u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\uff09\u3002<ul> <li>\u200b\u5b83\u200b\u7684\u200b\u8868\u73b0\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200b\u5982\u4f55\u200b\uff1f</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u5f15\u5165\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u5230\u200b\u5b9e\u9a8c\u200b\u5217\u8868\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u4f1a\u200b\u6709\u200b\u4ec0\u4e48\u200b\u53d8\u5316\u200b\u5417\u200b\uff1f<ul> <li>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6709\u200b\u4e00\u4e2a\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u8bad\u7ec3\u200bDataLoader\uff08\u200b\u4f8b\u5982\u200b<code>train_dataloader_20_percent_aug</code>\u200b\u548c\u200b<code>train_dataloader_20_percent_no_aug</code>\uff09\uff0c\u200b\u7136\u540e\u200b\u6bd4\u8f83\u200b\u4e24\u79cd\u200b\u76f8\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u8fd9\u4e9b\u200bDataLoader\u200b\u4e0a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\u3002</li> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4fee\u6539\u200b<code>create_dataloaders()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u80fd\u591f\u200b\u63a5\u53d7\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u7684\u200b\u53d8\u6362\u200b\uff08\u200b\u56e0\u4e3a\u200b\u4f60\u200b\u4e0d\u200b\u9700\u8981\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u8fdb\u884c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff09\u3002\u200b\u53c2\u89c1\u200b04. PyTorch\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u7b2c\u200b6\u200b\u8282\u4e2d\u200b\u4f7f\u7528\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u793a\u4f8b\u200b\uff0c\u200b\u6216\u200b\u4e0b\u9762\u200b\u7684\u200b\u811a\u672c\u200b\u793a\u4f8b\u200b\uff1a</li> </ul> </li> </ol> <pre># \u200b\u6ce8\u610f\u200b\uff1a\u200b\u8fd9\u79cd\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u53d8\u6362\u200b\u53ea\u5e94\u200b\u5728\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\ntrain_transform_data_aug = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.TrivialAugmentWide(),\n    transforms.ToTensor(),\n    normalize\n])\n\n# \u200b\u8f85\u52a9\u200b\u51fd\u6570\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5728\u200bDataLoader\u200b\u4e2d\u200b\u67e5\u770b\u200b\u56fe\u50cf\u200b\uff08\u200b\u9002\u7528\u200b\u4e8e\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u53d8\u6362\u200b\u6216\u200b\u4e0d\u200b\u9002\u7528\u200b\uff09\ndef view_dataloader_images(dataloader, n=10):\n    if n &gt; 10:\n        print(f\"n\u200b\u9ad8\u4e8e\u200b10\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u56fe\u50cf\u200b\u6df7\u4e71\u200b\uff0c\u200b\u964d\u4f4e\u200b\u5230\u200b10\u3002\")\n        n = 10\n    imgs, labels = next(iter(dataloader))\n    plt.figure(figsize=(16, 8))\n    for i in range(n):\n        # \u200b\u6700\u5c0f\u200b-\u200b\u6700\u5927\u200b\u7f29\u653e\u200b\u56fe\u50cf\u200b\u4ee5\u4fbf\u200b\u663e\u793a\u200b\n        targ_image = imgs[i]\n        sample_min, sample_max = targ_image.min(), targ_image.max()\n        sample_scaled = (targ_image - sample_min)/(sample_max - sample_min)\n\n        # \u200b\u7ed8\u5236\u200b\u56fe\u50cf\u200b\u5e76\u200b\u663e\u793a\u200b\u76f8\u5e94\u200b\u7684\u200b\u8f74\u200b\u4fe1\u606f\u200b\n        plt.subplot(1, 10, i+1)\n        plt.imshow(sample_scaled.permute(1, 2, 0)) # \u200b\u8c03\u6574\u200b\u5c3a\u5bf8\u200b\u4ee5\u200b\u7b26\u5408\u200bMatplotlib\u200b\u8981\u6c42\u200b\n        plt.title(class_names[labels[i]])\n        plt.axis(False)\n\n# \u200b\u9700\u8981\u200b\u66f4\u65b0\u200b`create_dataloaders()`\u200b\u4ee5\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u7684\u200b\u589e\u5f3a\u200b\nimport os\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\n\nNUM_WORKERS = os.cpu_count() # \u200b\u4f7f\u7528\u200b\u6700\u5927\u200bCPU\u200b\u6570\u91cf\u200b\u4ee5\u200b\u52a0\u5feb\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\n\n# \u200b\u6ce8\u610f\u200b\uff1a\u200b\u8fd9\u662f\u200bdata_setup.create_dataloaders\u200b\u7684\u200b\u66f4\u65b0\u200b\u7248\u672c\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u53d8\u6362\u200b\ndef create_dataloaders(\n    train_dir, \n    test_dir, \n    train_transform, # \u200b\u6dfb\u52a0\u200b\u8bad\u7ec3\u200b\u53d8\u6362\u200b\u53c2\u6570\u200b\uff08\u200b\u5e94\u7528\u200b\u4e8e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\uff09\n    test_transform,  # \u200b\u6dfb\u52a0\u200b\u6d4b\u8bd5\u200b\u53d8\u6362\u200b\u53c2\u6570\u200b\uff08\u200b\u5e94\u7528\u200b\u4e8e\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\uff09\n    batch_size=32, num_workers=NUM_WORKERS\n):\n    # \u200b\u4f7f\u7528\u200bImageFolder\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\n    train_data = datasets.ImageFolder(train_dir, transform=train_transform)\n    test_data = datasets.ImageFolder(test_dir, transform=test_transform)\n\n    # \u200b\u83b7\u53d6\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\n    class_names = train_data.classes\n\n    # \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\n    train_dataloader = DataLoader(\n        train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n    test_dataloader = DataLoader(\n        test_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=True,\n    )\n\n    return train_dataloader, test_dataloader, class_names\n</pre> <ol> <li>\u200b\u6269\u5c55\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5c06\u200bFoodVision Mini\u200b\u53d8\u6210\u200bFoodVision Big\uff0c\u200b\u4f7f\u7528\u200b<code>torchvision.models</code>\u200b\u4e2d\u200b\u7684\u200b\u6574\u4e2a\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u200b<ul> <li>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u4f60\u200b\u7684\u200b\u5404\u79cd\u200b\u5b9e\u9a8c\u200b\u4e2d\u200b\u9009\u62e9\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6216\u8005\u200b\u751a\u81f3\u200b\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u7bc7\u200b\u7b14\u8bb0\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u5728\u200b\u6240\u6709\u200bFood101\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b5\u200b\u4e2a\u200b\u5468\u671f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u679c\u200b\u5982\u4f55\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4f60\u200b\u5c1d\u8bd5\u200b\u4e86\u200b\u591a\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u6700\u597d\u200b\u8ddf\u8e2a\u200b\u6bcf\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4f60\u200b\u4ece\u200b<code>torchvision.models</code>\u200b\u52a0\u8f7d\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u521b\u5efa\u200bPyTorch DataLoaders\u200b\u4ee5\u4fbf\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u3002</li> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7531\u4e8e\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u6bd4\u200b\u6211\u4eec\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\u5c06\u200b\u9700\u8981\u200b\u66f4\u957f\u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u200b\u8bad\u7ec3\u200b\u3002</li> </ul> </li> </ol>"},{"location":"07_pytorch_experiment_tracking/","title":"\u8bfe\u5916\u200b\u62d3\u5c55\u200b\u00b6","text":"<ul> <li>\u200b\u9605\u8bfb\u200b Richard Sutton \u200b\u7684\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200b The Bitter Lesson\uff0c\u200b\u4e86\u89e3\u200b\u8bb8\u591a\u200b\u6700\u65b0\u200b\u7684\u200b AI \u200b\u8fdb\u5c55\u200b\u662f\u200b\u5982\u4f55\u200b\u6765\u81ea\u200b\u89c4\u6a21\u200b\u6269\u5927\u200b\uff08\u200b\u66f4\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\uff09\u200b\u548c\u200b\u66f4\u200b\u901a\u7528\u200b\uff08\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u7cbe\u5fc3\u8bbe\u8ba1\u200b\uff09\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u82b1\u200b 20 \u200b\u5206\u949f\u200b\u6d4f\u89c8\u200b PyTorch YouTube/\u200b\u4ee3\u7801\u200b\u6559\u7a0b\u200b \u200b\u4e2d\u200b\u7684\u200b TensorBoard \u200b\u6559\u7a0b\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b\u4ee3\u7801\u200b\u6709\u4f55\u200b\u4e0d\u540c\u200b\u3002</li> <li>\u200b\u4e5f\u8bb8\u200b\u4f60\u200b\u60f3\u200b\u7528\u200b DataFrame \u200b\u67e5\u770b\u200b\u548c\u200b\u91cd\u65b0\u6392\u5217\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b TensorBoard \u200b\u65e5\u5fd7\u200b\uff08\u200b\u8fd9\u6837\u200b\u4f60\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u6309\u200b\u6700\u4f4e\u200b\u635f\u5931\u200b\u6216\u200b\u6700\u9ad8\u200b\u51c6\u786e\u7387\u200b\u6392\u5e8f\u200b\u7ed3\u679c\u200b\uff09\uff0cTensorBoard \u200b\u6587\u6863\u200b\u4e2d\u6709\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u6307\u5357\u200b \u200b\u5728\u200b TensorBoard \u200b\u6587\u6863\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u4f60\u200b\u559c\u6b22\u200b\u4f7f\u7528\u200b VSCode \u200b\u8fdb\u884c\u200b\u811a\u672c\u200b\u6216\u200b\u7b14\u8bb0\u672c\u200b\u5f00\u53d1\u200b\uff08VSCode \u200b\u73b0\u5728\u200b\u53ef\u4ee5\u200b\u539f\u751f\u200b\u4f7f\u7528\u200b Jupyter Notebooks\uff09\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b PyTorch \u200b\u5728\u200b VSCode \u200b\u4e2d\u200b\u7684\u200b\u5f00\u53d1\u200b\u6307\u5357\u200b \u200b\u5728\u200b VSCode \u200b\u4e2d\u200b\u76f4\u63a5\u200b\u8bbe\u7f6e\u200b TensorBoard\u3002</li> <li>\u200b\u5982\u679c\u200b\u60f3\u200b\u8fdb\u4e00\u6b65\u200b\u8fdb\u884c\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\uff0c\u200b\u5e76\u200b\u4ece\u200b\u901f\u5ea6\u200b\u89d2\u5ea6\u200b\u67e5\u770b\u200b\u4f60\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\uff08\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u53ef\u4ee5\u200b\u6539\u8fdb\u200b\u7684\u200b\u74f6\u9888\u200b\u4ee5\u200b\u52a0\u5feb\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\uff1f\uff09\uff0c\u200b\u8bf7\u53c2\u9605\u200b PyTorch \u200b\u6587\u6863\u200b\u4e2d\u200b\u7684\u200b PyTorch profiler\u3002</li> <li>Made With ML \u200b\u662f\u200b\u7531\u200b Goku Mohandas \u200b\u63d0\u4f9b\u200b\u7684\u200b\u5173\u4e8e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u4f18\u79c0\u200b\u8d44\u6e90\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u7684\u200b \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u6307\u5357\u200b \u200b\u5305\u542b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b\u4f7f\u7528\u200b MLflow \u200b\u8ddf\u8e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7cbe\u5f69\u200b\u4ecb\u7ecd\u200b\u3002</li> </ul>"},{"location":"08_pytorch_paper_replicating/","title":"08. PyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12 or int(torch.__version__.split(\".\")[0]) == 2, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 2.1.0+cu118\ntorchvision version: 0.16.0+cu118\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\u5f00\u59cb\u200b\u5b89\u88c5\u200b\u5404\u79cd\u200b\u8f6f\u4ef6\u5305\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u5728\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u5355\u5143\u683c\u200b\u540e\u200b\u91cd\u542f\u200b\u8fd0\u884c\u200b\u65f6\u200b\u3002\u200b\u91cd\u542f\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u8fd0\u884c\u200b\u8be5\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u5e76\u200b\u9a8c\u8bc1\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u4e86\u200b\u6b63\u786e\u200b\u7248\u672c\u200b\u7684\u200b <code>torch</code> \u200b\u548c\u200b <code>torchvision</code>\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u8fdb\u884c\u200b\u5e38\u89c4\u200b\u5bfc\u5165\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u4ece\u200b GitHub \u200b\u83b7\u53d6\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u3002</p> <p><code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u5305\u542b\u200b\u6211\u4eec\u200b\u5728\u200b\u524d\u200b\u51e0\u8282\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u51e0\u4e2a\u200b\u51fd\u6570\u200b\uff1a</p> <ul> <li><code>set_seeds()</code> \u200b\u7528\u4e8e\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff08\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u90e8\u5206\u200b 0 \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff09\u3002</li> <li><code>download_data()</code> \u200b\u7528\u4e8e\u200b\u6839\u636e\u200b\u94fe\u63a5\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u6e90\u200b\uff08\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u90e8\u5206\u200b 1 \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff09\u3002</li> <li><code>plot_loss_curves()</code> \u200b\u7528\u4e8e\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\uff08\u200b\u5728\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u90e8\u5206\u200b 7.8 \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff09\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5c06\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u51fd\u6570\u200b\u5408\u5e76\u200b\u5230\u200b <code>going_modular/going_modular/utils.py</code> \u200b\u4e2d\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u4e3b\u610f\u200b\uff0c\u200b\u4e5f\u8bb8\u200b\u8fd9\u200b\u662f\u200b\u4f60\u200b\u60f3\u200b\u5c1d\u8bd5\u200b\u7684\u200b\u6269\u5c55\u200b\u3002</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <pre>[INFO] Couldn't find torchinfo... installing it.\n[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\nCloning into 'pytorch-deep-learning'...\nremote: Enumerating objects: 4033, done.\nremote: Counting objects: 100% (1224/1224), done.\nremote: Compressing objects: 100% (225/225), done.\nremote: Total 4033 (delta 1067), reused 1097 (delta 996), pack-reused 2809\nReceiving objects: 100% (4033/4033), 649.59 MiB | 34.16 MiB/s, done.\nResolving deltas: 100% (2358/2358), done.\nUpdating files: 100% (248/248), done.\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u4f7f\u7528\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u542f\u7528\u200b GPU\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u901a\u8fc7\u200b <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code> \u200b\u6765\u200b\u542f\u7528\u200b\u4e00\u4e2a\u200b GPU \u200b\u4e86\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[\u00a0]: <pre>'cuda'</pre> In\u00a0[\u00a0]: Copied! <pre># Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[\u00a0]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u6570\u636e\u200b\u5df2\u200b\u4e0b\u8f7d\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Setup directory paths to train and test images\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\n</pre> # Setup directory paths to train and test images train_dir = image_path / \"train\" test_dir = image_path / \"test\" In\u00a0[\u00a0]: Copied! <pre># Create image size (from Table 3 in the ViT paper)\nIMG_SIZE = 224\n\n# Create transform pipeline manually\nmanual_transforms = transforms.Compose([\n    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n    transforms.ToTensor(),\n])\nprint(f\"Manually created transforms: {manual_transforms}\")\n</pre> # Create image size (from Table 3 in the ViT paper) IMG_SIZE = 224  # Create transform pipeline manually manual_transforms = transforms.Compose([     transforms.Resize((IMG_SIZE, IMG_SIZE)),     transforms.ToTensor(), ]) print(f\"Manually created transforms: {manual_transforms}\") <pre>Manually created transforms: Compose(\n    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=None)\n    ToTensor()\n)\n</pre> In\u00a0[\u00a0]: Copied! <pre># Set the batch size\nBATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small\n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=manual_transforms, # use manually created transforms\n    batch_size=BATCH_SIZE\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Set the batch size BATCH_SIZE = 32 # this is lower than the ViT paper but it's because we're starting small  # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=manual_transforms, # use manually created transforms     batch_size=BATCH_SIZE )  train_dataloader, test_dataloader, class_names Out[\u00a0]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f18845ff0d0&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f17f3f5f520&gt;,\n ['pizza', 'steak', 'sushi'])</pre> In\u00a0[\u00a0]: Copied! <pre># Get a batch of images\nimage_batch, label_batch = next(iter(train_dataloader))\n\n# Get a single image from the batch\nimage, label = image_batch[0], label_batch[0]\n\n# View the batch shapes\nimage.shape, label\n</pre> # Get a batch of images image_batch, label_batch = next(iter(train_dataloader))  # Get a single image from the batch image, label = image_batch[0], label_batch[0]  # View the batch shapes image.shape, label Out[\u00a0]: <pre>(torch.Size([3, 224, 224]), tensor(2))</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>matplotlib</code> \u200b\u7ed8\u5236\u200b\u56fe\u50cf\u200b\u53ca\u5176\u200b\u6807\u7b7e\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Plot image with matplotlib\nplt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels]\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # Plot image with matplotlib plt.imshow(image.permute(1, 2, 0)) # rearrange image dimensions to suit matplotlib [color_channels, height, width] -&gt; [height, width, color_channels] plt.title(class_names[label]) plt.axis(False); <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u7247\u200b\u5bfc\u5165\u200b\u6ca1\u6709\u200b\u95ee\u9898\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u8fdb\u884c\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u5de5\u4f5c\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Create example values\nheight = 224 # H (\"The training resolution is 224.\")\nwidth = 224 # W\ncolor_channels = 3 # C\npatch_size = 16 # P\n\n# Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\nprint(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\")\n</pre> # Create example values height = 224 # H (\"The training resolution is 224.\") width = 224 # W color_channels = 3 # C patch_size = 16 # P  # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2) print(f\"Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}\") <pre>Number of patches (N) with image height (H=224), width (W=224) and patch size (P=16): 196\n</pre> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5f97\u5230\u200b\u4e86\u200b\u8865\u4e01\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u8f93\u51fa\u200b\u56fe\u50cf\u200b\u7684\u200b\u5927\u5c0f\u200b\u5462\u200b\uff1f</p> <p>\u200b\u66f4\u597d\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u590d\u5236\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\uff1a</p> <ul> <li>\u200b\u8f93\u5165\u200b\uff1a \u200b\u56fe\u50cf\u200b\u6700\u521d\u200b\u662f\u200b\u4e8c\u7ef4\u200b\u7684\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b ${H \\times W \\times C}$\u3002</li> <li>\u200b\u8f93\u51fa\u200b\uff1a \u200b\u56fe\u50cf\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7cfb\u5217\u200b\u5c55\u5e73\u200b\u7684\u200b\u4e8c\u7ef4\u200b\u8865\u4e01\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b ${N \\times\\left(P^{2} \\cdot C\\right)}$\u3002</li> </ul> In\u00a0[\u00a0]: Copied! <pre># Input shape (this is the size of a single image)\nembedding_layer_input_shape = (height, width, color_channels)\n\n# Output shape\nembedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)\n\nprint(f\"Input shape (single 2D image): {embedding_layer_input_shape}\")\nprint(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\")\n</pre> # Input shape (this is the size of a single image) embedding_layer_input_shape = (height, width, color_channels)  # Output shape embedding_layer_output_shape = (number_of_patches, patch_size**2 * color_channels)  print(f\"Input shape (single 2D image): {embedding_layer_input_shape}\") print(f\"Output shape (single 2D image flattened into patches): {embedding_layer_output_shape}\") <pre>Input shape (single 2D image): (224, 224, 3)\nOutput shape (single 2D image flattened into patches): (196, 768)\n</pre> <p>\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u5df2\u200b\u83b7\u53d6\u200b\uff01</p> In\u00a0[\u00a0]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5c06\u200b\u8fd9\u5f20\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u6210\u200b\u4e0e\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u56fe\u200b1\u200b\u6240\u793a\u200b\u7684\u200b\u5185\u8054\u200b\u56fe\u50cf\u200b\u5757\u200b\u3002</p> <p>\u200b\u4e0d\u5982\u200b\u6211\u4eec\u200b\u5148\u200b\u4ece\u200b\u53ef\u89c6\u5316\u200b\u6700\u200b\u4e0a\u9762\u200b\u4e00\u884c\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u50cf\u7d20\u200b\u5f00\u59cb\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7d22\u5f15\u200b\u4e0d\u540c\u200b\u7684\u200b\u56fe\u50cf\u200b\u7ef4\u5ea6\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels)\nimage_permuted = image.permute(1, 2, 0)\n\n# Index to plot the top row of patched pixels\npatch_size = 16\nplt.figure(figsize=(patch_size, patch_size))\nplt.imshow(image_permuted[:patch_size, :, :]);\n</pre> # Change image shape to be compatible with matplotlib (color_channels, height, width) -&gt; (height, width, color_channels) image_permuted = image.permute(1, 2, 0)  # Index to plot the top row of patched pixels patch_size = 16 plt.figure(figsize=(patch_size, patch_size)) plt.imshow(image_permuted[:patch_size, :, :]); <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u9876\u884c\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u5c0f\u5757\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fed\u4ee3\u200b\u9876\u884c\u200b\u4e2d\u5c06\u200b\u4f1a\u200b\u6709\u200b\u7684\u200b\u5c0f\u5757\u200b\u6570\u91cf\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size\nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\nprint(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=1,\n                        ncols=img_size // patch_size, # one column for each patch\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Iterate through number of patches in the top row\nfor i, patch in enumerate(range(0, img_size, patch_size)):\n    axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index\n    axs[i].set_xlabel(i+1) # set the label\n    axs[i].set_xticks([])\n    axs[i].set_yticks([])\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Create a series of subplots fig, axs = plt.subplots(nrows=1,                         ncols=img_size // patch_size, # one column for each patch                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Iterate through number of patches in the top row for i, patch in enumerate(range(0, img_size, patch_size)):     axs[i].imshow(image_permuted[:patch_size, patch:patch+patch_size, :]); # keep height index constant, alter the width index     axs[i].set_xlabel(i+1) # set the label     axs[i].set_xticks([])     axs[i].set_yticks([]) <pre>Number of patches per row: 14.0\nPatch size: 16 pixels x 16 pixels\n</pre> <p>\u200b\u8fd9\u4e9b\u200b\u8865\u4e01\u200b\u770b\u8d77\u6765\u200b\u5f88\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u5c06\u200b\u6574\u4e2a\u200b\u56fe\u50cf\u200b\u90fd\u200b\u8fd9\u6837\u200b\u505a\u200b\u5462\u200b\uff1f</p> <p>\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u904d\u5386\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u7684\u200b\u7d22\u5f15\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u8865\u4e01\u200b\u7ed8\u5236\u200b\u5728\u200b\u81ea\u5df1\u200b\u7684\u200b\u5b50\u200b\u56fe\u200b\u4e2d\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Setup hyperparameters and make sure img_size and patch_size are compatible\nimg_size = 224\npatch_size = 16\nnum_patches = img_size/patch_size\nassert img_size % patch_size == 0, \"Image size must be divisible by patch size\"\nprint(f\"Number of patches per row: {num_patches}\\\n        \\nNumber of patches per column: {num_patches}\\\n        \\nTotal patches: {num_patches*num_patches}\\\n        \\nPatch size: {patch_size} pixels x {patch_size} pixels\")\n\n# Create a series of subplots\nfig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float\n                        ncols=img_size // patch_size,\n                        figsize=(num_patches, num_patches),\n                        sharex=True,\n                        sharey=True)\n\n# Loop through height and width of image\nfor i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height\n    for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width\n\n        # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))\n        axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height\n                                        patch_width:patch_width+patch_size, # iterate through width\n                                        :]) # get all color channels\n\n        # Set up label information, remove the ticks for clarity and set labels to outside\n        axs[i, j].set_ylabel(i+1,\n                             rotation=\"horizontal\",\n                             horizontalalignment=\"right\",\n                             verticalalignment=\"center\")\n        axs[i, j].set_xlabel(j+1)\n        axs[i, j].set_xticks([])\n        axs[i, j].set_yticks([])\n        axs[i, j].label_outer()\n\n# Set a super title\nfig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16)\nplt.show()\n</pre> # Setup hyperparameters and make sure img_size and patch_size are compatible img_size = 224 patch_size = 16 num_patches = img_size/patch_size assert img_size % patch_size == 0, \"Image size must be divisible by patch size\" print(f\"Number of patches per row: {num_patches}\\         \\nNumber of patches per column: {num_patches}\\         \\nTotal patches: {num_patches*num_patches}\\         \\nPatch size: {patch_size} pixels x {patch_size} pixels\")  # Create a series of subplots fig, axs = plt.subplots(nrows=img_size // patch_size, # need int not float                         ncols=img_size // patch_size,                         figsize=(num_patches, num_patches),                         sharex=True,                         sharey=True)  # Loop through height and width of image for i, patch_height in enumerate(range(0, img_size, patch_size)): # iterate through height     for j, patch_width in enumerate(range(0, img_size, patch_size)): # iterate through width          # Plot the permuted image patch (image_permuted -&gt; (Height, Width, Color Channels))         axs[i, j].imshow(image_permuted[patch_height:patch_height+patch_size, # iterate through height                                         patch_width:patch_width+patch_size, # iterate through width                                         :]) # get all color channels          # Set up label information, remove the ticks for clarity and set labels to outside         axs[i, j].set_ylabel(i+1,                              rotation=\"horizontal\",                              horizontalalignment=\"right\",                              verticalalignment=\"center\")         axs[i, j].set_xlabel(j+1)         axs[i, j].set_xticks([])         axs[i, j].set_yticks([])         axs[i, j].label_outer()  # Set a super title fig.suptitle(f\"{class_names[label]} -&gt; Patchified\", fontsize=16) plt.show() <pre>Number of patches per row: 14.0        \nNumber of patches per column: 14.0        \nTotal patches: 196.0        \nPatch size: 16 pixels x 16 pixels\n</pre> <p>\u200b\u56fe\u50cf\u200b\u88ab\u200b\u5206\u5757\u200b\u4e86\u200b\uff01</p> <p>\u200b\u54c7\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u5f88\u9177\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u5206\u5757\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5d4c\u5165\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5e8f\u5217\u200b\u5462\u200b\uff1f</p> <p>\u200b\u63d0\u793a\u200b\uff1a\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5c42\u200b\u3002\u200b\u4f60\u200b\u80fd\u200b\u731c\u200b\u5230\u200b\u662f\u200b\u54ea\u4e9b\u200b\u5417\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre>from torch import nn\n\n# Set the patch size\npatch_size=16\n\n# Create the Conv2d layer with hyperparameters from the ViT paper\nconv2d = nn.Conv2d(in_channels=3, # number of color channels\n                   out_channels=768, # from Table 1: Hidden size D, this is the embedding size\n                   kernel_size=patch_size, # could also use (patch_size, patch_size)\n                   stride=patch_size,\n                   padding=0)\n</pre> from torch import nn  # Set the patch size patch_size=16  # Create the Conv2d layer with hyperparameters from the ViT paper conv2d = nn.Conv2d(in_channels=3, # number of color channels                    out_channels=768, # from Table 1: Hidden size D, this is the embedding size                    kernel_size=patch_size, # could also use (patch_size, patch_size)                    stride=patch_size,                    padding=0) <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5377\u79ef\u200b\u5c42\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5f53\u200b\u6211\u4eec\u200b\u5c06\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u901a\u8fc7\u200b\u5b83\u200b\u65f6\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\n</pre> # View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); In\u00a0[\u00a0]: Copied! <pre># Pass the image through the convolutional layer\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels)\nprint(image_out_of_conv.shape)\n</pre> # Pass the image through the convolutional layer image_out_of_conv = conv2d(image.unsqueeze(0)) # add a single batch dimension (height, width, color_channels) -&gt; (batch, height, width, color_channels) print(image_out_of_conv.shape) <pre>torch.Size([1, 768, 14, 14])\n</pre> <p>\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u901a\u8fc7\u200b\u5377\u79ef\u200b\u5c42\u200b\u5904\u7406\u200b\u540e\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u53d8\u6210\u200b\u4e00\u7cfb\u5217\u200b\u7684\u200b 768 \u200b\u4e2a\u200b\uff08\u200b\u8fd9\u662f\u200b\u5d4c\u5165\u200b\u5927\u5c0f\u200b\u6216\u200b $D$\uff09\u200b\u7279\u5f81\u200b/\u200b\u6fc0\u6d3b\u200b\u56fe\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5176\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u53ef\u4ee5\u200b\u8868\u793a\u200b\u4e3a\u200b\uff1a</p> <pre>torch.Size([1, 768, 14, 14]) -&gt; [\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b, \u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b, \u200b\u7279\u5f81\u200b\u56fe\u200b\u9ad8\u5ea6\u200b, \u200b\u7279\u5f81\u200b\u56fe\u200b\u5bbd\u5ea6\u200b]\n</pre> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u53ef\u89c6\u5316\u200b\u4e94\u4e2a\u200b\u968f\u673a\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Plot random 5 convolutional feature maps\nimport random\nrandom_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size\nprint(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")\n\n# Create plot\nfig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))\n\n# Plot random image feature maps\nfor i, idx in enumerate(random_indexes):\n    image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer\n    axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())\n    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);\n</pre> # Plot random 5 convolutional feature maps import random random_indexes = random.sample(range(0, 758), k=5) # pick 5 numbers between 0 and the embedding size print(f\"Showing random convolutional feature maps from indexes: {random_indexes}\")  # Create plot fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12, 12))  # Plot random image feature maps for i, idx in enumerate(random_indexes):     image_conv_feature_map = image_out_of_conv[:, idx, :, :] # index on the output tensor of the convolutional layer     axs[i].imshow(image_conv_feature_map.squeeze().detach().numpy())     axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]); <pre>Showing random convolutional feature maps from indexes: [571, 727, 734, 380, 90]\n</pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7279\u5f81\u200b\u56fe\u200b\u5728\u200b\u4e00\u5b9a\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u90fd\u200b\u53cd\u6620\u200b\u4e86\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\uff0c\u200b\u901a\u8fc7\u200b\u591a\u200b\u89c2\u5bdf\u200b\u51e0\u4e2a\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u770b\u5230\u200b\u4e0d\u540c\u200b\u7684\u200b\u4e3b\u8981\u200b\u8f6e\u5ed3\u200b\u548c\u200b\u4e00\u4e9b\u200b\u4e3b\u8981\u200b\u7279\u5f81\u200b\u3002</p> <p>\u200b\u9700\u8981\u200b\u6ce8\u610f\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u968f\u7740\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5b66\u4e60\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7279\u5f81\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\u3002</p> <p>\u200b\u6b63\u56e0\u5982\u6b64\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7279\u5f81\u200b\u56fe\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u6211\u4eec\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ee5\u200b\u6570\u503c\u200b\u5f62\u5f0f\u200b\u67e5\u770b\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Get a single feature map in tensor form\nsingle_feature_map = image_out_of_conv[:, 0, :, :]\nsingle_feature_map, single_feature_map.requires_grad\n</pre> # Get a single feature map in tensor form single_feature_map = image_out_of_conv[:, 0, :, :] single_feature_map, single_feature_map.requires_grad Out[\u00a0]: <pre>(tensor([[[ 0.4732,  0.3567,  0.3377,  0.3736,  0.3208,  0.3913,  0.3464,\n            0.3702,  0.2541,  0.3594,  0.1984,  0.3982,  0.3741,  0.1251],\n          [ 0.4178,  0.4771,  0.3374,  0.3353,  0.3159,  0.4008,  0.3448,\n            0.3345,  0.5850,  0.4115,  0.2969,  0.2751,  0.6150,  0.4188],\n          [ 0.3209,  0.3776,  0.4970,  0.4272,  0.3301,  0.4787,  0.2754,\n            0.3726,  0.3298,  0.4631,  0.3087,  0.4915,  0.4129,  0.4592],\n          [ 0.4540,  0.4930,  0.5570,  0.2660,  0.2150,  0.2044,  0.2766,\n            0.2076,  0.3278,  0.3727,  0.2637,  0.2493,  0.2782,  0.3664],\n          [ 0.4920,  0.5671,  0.3298,  0.2992,  0.1437,  0.1701,  0.1554,\n            0.1375,  0.1377,  0.3141,  0.2694,  0.2771,  0.2412,  0.3700],\n          [ 0.5783,  0.5790,  0.4229,  0.5032,  0.1216,  0.1000,  0.0356,\n            0.1258, -0.0023,  0.1640,  0.2809,  0.2418,  0.2606,  0.3787],\n          [ 0.5334,  0.5645,  0.4781,  0.3307,  0.2391,  0.0461,  0.0095,\n            0.0542,  0.1012,  0.1331,  0.2446,  0.2526,  0.3323,  0.4120],\n          [ 0.5724,  0.2840,  0.5188,  0.3934,  0.1328,  0.0776,  0.0235,\n            0.1366,  0.3149,  0.2200,  0.2793,  0.2351,  0.4722,  0.4785],\n          [ 0.4009,  0.4570,  0.4972,  0.5785,  0.2261,  0.1447, -0.0028,\n            0.2772,  0.2697,  0.4008,  0.3606,  0.3372,  0.4535,  0.4492],\n          [ 0.5678,  0.5870,  0.5824,  0.3438,  0.5113,  0.0757,  0.1772,\n            0.3677,  0.3572,  0.3742,  0.3820,  0.4868,  0.3781,  0.4694],\n          [ 0.5845,  0.5877,  0.5826,  0.3212,  0.5276,  0.4840,  0.4825,\n            0.5523,  0.5308,  0.5085,  0.5606,  0.5720,  0.4928,  0.5581],\n          [ 0.5853,  0.5849,  0.5793,  0.3410,  0.4428,  0.4044,  0.3275,\n            0.4958,  0.4366,  0.5750,  0.5494,  0.5868,  0.5557,  0.5069],\n          [ 0.5880,  0.5888,  0.5796,  0.3377,  0.2635,  0.2347,  0.3145,\n            0.3486,  0.5158,  0.5722,  0.5347,  0.5753,  0.5816,  0.4378],\n          [ 0.5692,  0.5843,  0.5721,  0.5081,  0.2694,  0.2032,  0.1589,\n            0.3464,  0.5349,  0.5768,  0.5739,  0.5764,  0.5394,  0.4482]]],\n        grad_fn=&lt;SliceBackward0&gt;),\n True)</pre> <p><code>single_feature_map</code> \u200b\u7684\u200b <code>grad_fn</code> \u200b\u8f93\u51fa\u200b\u4ee5\u53ca\u200b <code>requires_grad=True</code> \u200b\u5c5e\u6027\u200b\u610f\u5473\u7740\u200b PyTorch \u200b\u6b63\u5728\u200b\u8ddf\u8e2a\u200b\u6b64\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u68af\u5ea6\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u4e2d\u5c06\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Current tensor shape\nprint(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\")\n</pre> # Current tensor shape print(f\"Current tensor shape: {image_out_of_conv.shape} -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\") <pre>Current tensor shape: torch.Size([1, 768, 14, 14]) -&gt; [batch, embedding_dim, feature_map_height, feature_map_width]\n</pre> <p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5f97\u5230\u200b\u4e86\u200b 768 \u200b\u90e8\u5206\u200b\uff08$(P^{2} \\cdot C)$\uff09\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u8fd8\u200b\u9700\u8981\u200b\u8865\u4e01\u200b\u7684\u200b\u6570\u91cf\u200b\uff08$N$\uff09\u3002</p> <p>\u200b\u56de\u987e\u200b ViT \u200b\u8bba\u6587\u200b\u7684\u200b\u7b2c\u200b 3.1 \u200b\u8282\u200b\uff0c\u200b\u5b83\u200b\u63d0\u5230\u200b\uff08\u200b\u52a0\u7c97\u200b\u90e8\u5206\u200b\u4e3a\u200b\u6211\u200b\u6240\u52a0\u200b\uff09\uff1a</p> <p>\u200b\u4f5c\u4e3a\u200b\u4e00\u79cd\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\uff0c\u200b\u8865\u4e01\u200b\u53ef\u4ee5\u200b\u5177\u6709\u200b $1 \\times 1$ \u200b\u7684\u200b\u7a7a\u95f4\u200b\u5927\u5c0f\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u662f\u200b\u901a\u8fc7\u200b\u7b80\u5355\u200b\u5730\u200b\u5c55\u5e73\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u7a7a\u95f4\u200b\u7ef4\u5ea6\u200b\u5e76\u200b\u6295\u5f71\u200b\u5230\u200b Transformer \u200b\u7ef4\u5ea6\u200b\u6765\u200b\u83b7\u5f97\u200b\u7684\u200b\u3002</p> <p>\u200b\u5c55\u5e73\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u7a7a\u95f4\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5bf9\u200b\u5427\u200b\uff1f</p> <p>\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u6709\u200b\u4ec0\u4e48\u200b\u5c42\u200b\u53ef\u4ee5\u200b\u7528\u6765\u200b\u5c55\u5e73\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6bd4\u5982\u200b <code>torch.nn.Flatten()</code>\uff1f</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u4e0d\u60f3\u200b\u5c55\u5e73\u200b\u6574\u4e2a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u60f3\u200b\u5c55\u5e73\u200b\u201c\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u7a7a\u95f4\u200b\u7ef4\u5ea6\u200b\u201d\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u6307\u200b\u7684\u200b\u662f\u200b <code>image_out_of_conv</code> \u200b\u7684\u200b <code>feature_map_height</code> \u200b\u548c\u200b <code>feature_map_width</code> \u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>torch.nn.Flatten()</code> \u200b\u5c42\u200b\uff0c\u200b\u53ea\u200b\u5c55\u5e73\u200b\u8fd9\u4e9b\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>start_dim</code> \u200b\u548c\u200b <code>end_dim</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u5417\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre># Create flatten layer\nflatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)\n                     end_dim=3) # flatten feature_map_width (dimension 3)\n</pre> # Create flatten layer flatten = nn.Flatten(start_dim=2, # flatten feature_map_height (dimension 2)                      end_dim=3) # flatten feature_map_width (dimension 3) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u5b83\u4eec\u200b\u6574\u5408\u200b\u8d77\u6765\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u53d6\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5377\u79ef\u200b\u5c42\u200b\uff08<code>conv2d</code>\uff09\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e8c\u7ef4\u200b\u7279\u5f81\u200b\u56fe\u200b\uff08patch embeddings\uff09\u3002</li> <li>\u200b\u5c06\u200b\u4e8c\u7ef4\u200b\u7279\u5f81\u200b\u56fe\u200b\u5c55\u5e73\u200b\u6210\u200b\u4e00\u4e2a\u200b\u5e8f\u5217\u200b\u3002</li> </ol> In\u00a0[\u00a0]: Copied! <pre># 1. View single image\nplt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib\nplt.title(class_names[label])\nplt.axis(False);\nprint(f\"Original image shape: {image.shape}\")\n\n# 2. Turn image into feature maps\nimage_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors\nprint(f\"Image feature map shape: {image_out_of_conv.shape}\")\n\n# 3. Flatten the feature maps\nimage_out_of_conv_flattened = flatten(image_out_of_conv)\nprint(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\")\n</pre> # 1. View single image plt.imshow(image.permute(1, 2, 0)) # adjust for matplotlib plt.title(class_names[label]) plt.axis(False); print(f\"Original image shape: {image.shape}\")  # 2. Turn image into feature maps image_out_of_conv = conv2d(image.unsqueeze(0)) # add batch dimension to avoid shape errors print(f\"Image feature map shape: {image_out_of_conv.shape}\")  # 3. Flatten the feature maps image_out_of_conv_flattened = flatten(image_out_of_conv) print(f\"Flattened image feature map shape: {image_out_of_conv_flattened.shape}\") <pre>Original image shape: torch.Size([3, 224, 224])\nImage feature map shape: torch.Size([1, 768, 14, 14])\nFlattened image feature map shape: torch.Size([1, 768, 196])\n</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b <code>image_out_of_conv_flattened</code> \u200b\u5f62\u72b6\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\u6211\u4eec\u200b\u671f\u671b\u200b\u7684\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\uff1a</p> <ul> <li>\u200b\u671f\u671b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff08\u200b\u5c55\u5e73\u200b\u7684\u200b2D\u200b\u5757\u200b\uff09\uff1a (196, 768) -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> <li>\u200b\u5f53\u524d\u200b\u5f62\u72b6\u200b\uff1a (1, 768, 196)</li> </ul> <p>\u200b\u552f\u4e00\u200b\u7684\u200b\u533a\u522b\u200b\u662f\u200b\u5f53\u524d\u200b\u5f62\u72b6\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff0c\u200b\u5e76\u4e14\u200b\u7ef4\u5ea6\u200b\u987a\u5e8f\u200b\u4e0e\u200b\u671f\u671b\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5462\u200b\uff1f</p> <p>\u200b\u55ef\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u91cd\u65b0\u6392\u5217\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5bf9\u200b\u5427\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.Tensor.permute()</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b\u4f7f\u7528\u200b matplotlib \u200b\u7ed8\u5236\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u65f6\u200b\u91cd\u65b0\u6392\u5217\u200b\u7ef4\u5ea6\u200b\u4e00\u6837\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u770b\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Get flattened image patch embeddings in right shape\nimage_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\nprint(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\")\n</pre> # Get flattened image patch embeddings in right shape image_out_of_conv_flattened_reshaped = image_out_of_conv_flattened.permute(0, 2, 1) # [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] print(f\"Patch embedding sequence shape: {image_out_of_conv_flattened_reshaped.shape} -&gt; [batch_size, num_patches, embedding_size]\") <pre>Patch embedding sequence shape: torch.Size([1, 196, 768]) -&gt; [batch_size, num_patches, embedding_size]\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4f7f\u7528\u200b\u51e0\u4e2a\u200b PyTorch \u200b\u5c42\u200b\u6210\u529f\u200b\u5339\u914d\u200b\u4e86\u200b ViT \u200b\u67b6\u6784\u200b\u4e2d\u200b patch embedding \u200b\u5c42\u200b\u7684\u200b\u671f\u671b\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u53ef\u89c6\u5316\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u5c55\u5e73\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u5462\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre># Get a single flattened feature map\nsingle_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)\n\n# Plot the flattened feature map visually\nplt.figure(figsize=(22, 22))\nplt.imshow(single_flattened_feature_map.detach().numpy())\nplt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\")\nplt.axis(False);\n</pre> # Get a single flattened feature map single_flattened_feature_map = image_out_of_conv_flattened_reshaped[:, :, 0] # index: (batch_size, number_of_patches, embedding_dimension)  # Plot the flattened feature map visually plt.figure(figsize=(22, 22)) plt.imshow(single_flattened_feature_map.detach().numpy()) plt.title(f\"Flattened feature map shape: {single_flattened_feature_map.shape}\") plt.axis(False); <p>\u200b\u55ef\u200b\uff0c\u200b\u4ece\u200b\u89c6\u89c9\u200b\u4e0a\u200b\u770b\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5c55\u5e73\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u4f3c\u4e4e\u200b\u5e76\u200b\u4e0d\u8d77\u773c\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u662f\u200b\u6211\u4eec\u200b\u5173\u6ce8\u200b\u7684\u200b\u91cd\u70b9\u200b\u3002\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u540c\u65f6\u200b\u4e5f\u200b\u662f\u200bViT\u200b\u67b6\u6784\u200b\u5176\u4f59\u90e8\u5206\u200b\u7684\u200b\u8f93\u5165\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u539f\u59cb\u200b\u7684\u200bTransformer\u200b\u67b6\u6784\u200b \u200b\u662f\u200b\u4e3a\u200b\u5904\u7406\u200b\u6587\u672c\u200b\u800c\u200b\u8bbe\u8ba1\u200b\u7684\u200b\u3002\u200b\u800c\u200bVision Transformer\u200b\u67b6\u6784\u200b\uff08ViT\uff09\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u5229\u7528\u200b\u539f\u59cb\u200b\u7684\u200bTransformer\u200b\u6765\u200b\u5904\u7406\u200b\u56fe\u50cf\u200b\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u4e3a\u4ec0\u4e48\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u8f93\u5165\u200b\u9700\u8981\u200b\u4ee5\u200b\u8fd9\u79cd\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\u3002\u200b\u6211\u4eec\u200b\u5b9e\u9645\u4e0a\u200b\u662f\u200b\u5728\u200b\u5c06\u200b\u4e00\u4e2a\u200b2D\u200b\u56fe\u50cf\u200b\u683c\u5f0f\u5316\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u5448\u73b0\u200b\u4e3a\u200b1D\u200b\u7684\u200b\u6587\u672c\u200b\u5e8f\u5217\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u8fd9\u4e2a\u200b\u5c55\u5e73\u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\u5728\u200b\u5f20\u91cf\u200b\u5f62\u5f0f\u200b\u4e0b\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u5427\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre># See the flattened feature map as a tensor\nsingle_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape\n</pre> # See the flattened feature map as a tensor single_flattened_feature_map, single_flattened_feature_map.requires_grad, single_flattened_feature_map.shape Out[\u00a0]: <pre>(tensor([[ 0.4732,  0.3567,  0.3377,  0.3736,  0.3208,  0.3913,  0.3464,  0.3702,\n           0.2541,  0.3594,  0.1984,  0.3982,  0.3741,  0.1251,  0.4178,  0.4771,\n           0.3374,  0.3353,  0.3159,  0.4008,  0.3448,  0.3345,  0.5850,  0.4115,\n           0.2969,  0.2751,  0.6150,  0.4188,  0.3209,  0.3776,  0.4970,  0.4272,\n           0.3301,  0.4787,  0.2754,  0.3726,  0.3298,  0.4631,  0.3087,  0.4915,\n           0.4129,  0.4592,  0.4540,  0.4930,  0.5570,  0.2660,  0.2150,  0.2044,\n           0.2766,  0.2076,  0.3278,  0.3727,  0.2637,  0.2493,  0.2782,  0.3664,\n           0.4920,  0.5671,  0.3298,  0.2992,  0.1437,  0.1701,  0.1554,  0.1375,\n           0.1377,  0.3141,  0.2694,  0.2771,  0.2412,  0.3700,  0.5783,  0.5790,\n           0.4229,  0.5032,  0.1216,  0.1000,  0.0356,  0.1258, -0.0023,  0.1640,\n           0.2809,  0.2418,  0.2606,  0.3787,  0.5334,  0.5645,  0.4781,  0.3307,\n           0.2391,  0.0461,  0.0095,  0.0542,  0.1012,  0.1331,  0.2446,  0.2526,\n           0.3323,  0.4120,  0.5724,  0.2840,  0.5188,  0.3934,  0.1328,  0.0776,\n           0.0235,  0.1366,  0.3149,  0.2200,  0.2793,  0.2351,  0.4722,  0.4785,\n           0.4009,  0.4570,  0.4972,  0.5785,  0.2261,  0.1447, -0.0028,  0.2772,\n           0.2697,  0.4008,  0.3606,  0.3372,  0.4535,  0.4492,  0.5678,  0.5870,\n           0.5824,  0.3438,  0.5113,  0.0757,  0.1772,  0.3677,  0.3572,  0.3742,\n           0.3820,  0.4868,  0.3781,  0.4694,  0.5845,  0.5877,  0.5826,  0.3212,\n           0.5276,  0.4840,  0.4825,  0.5523,  0.5308,  0.5085,  0.5606,  0.5720,\n           0.4928,  0.5581,  0.5853,  0.5849,  0.5793,  0.3410,  0.4428,  0.4044,\n           0.3275,  0.4958,  0.4366,  0.5750,  0.5494,  0.5868,  0.5557,  0.5069,\n           0.5880,  0.5888,  0.5796,  0.3377,  0.2635,  0.2347,  0.3145,  0.3486,\n           0.5158,  0.5722,  0.5347,  0.5753,  0.5816,  0.4378,  0.5692,  0.5843,\n           0.5721,  0.5081,  0.2694,  0.2032,  0.1589,  0.3464,  0.5349,  0.5768,\n           0.5739,  0.5764,  0.5394,  0.4482]], grad_fn=&lt;SelectBackward0&gt;),\n True,\n torch.Size([1, 196]))</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u5355\u4e00\u200b\u7684\u200b2D\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u6210\u200b\u4e86\u200b\u4e00\u7ef4\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u5d4c\u5165\u200b\u5411\u91cf\u200b\uff08\u200b\u6216\u8005\u200b\u5728\u200bViT\u200b\u8bba\u6587\u200b\u56fe\u200b1\u200b\u4e2d\u200b\u79f0\u4e3a\u200b\u201c\u200b\u5c55\u5e73\u200b\u8865\u4e01\u200b\u7684\u200b\u7ebf\u6027\u200b\u6295\u5f71\u200b\u201d\uff09\u3002</p> In\u00a0[\u00a0]: Copied! <pre># 1. Create a class which subclasses nn.Module\nclass PatchEmbedding(nn.Module):\n    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n\n    Args:\n        in_channels (int): Number of color channels for the input images. Defaults to 3.\n        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n    \"\"\"\n    # 2. Initialize the class with appropriate variables\n    def __init__(self,\n                 in_channels:int=3,\n                 patch_size:int=16,\n                 embedding_dim:int=768):\n        super().__init__()\n\n        # 3. Create a layer to turn an image into patches\n        self.patcher = nn.Conv2d(in_channels=in_channels,\n                                 out_channels=embedding_dim,\n                                 kernel_size=patch_size,\n                                 stride=patch_size,\n                                 padding=0)\n\n        # 4. Create a layer to flatten the patch feature maps into a single dimension\n        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n                                  end_dim=3)\n\n    # 5. Define the forward method\n    def forward(self, x):\n        # Create assertion to check that inputs are the correct shape\n        image_resolution = x.shape[-1]\n        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n\n        # Perform the forward pass\n        x_patched = self.patcher(x)\n        x_flattened = self.flatten(x_patched)\n        # 6. Make sure the output shape has the right order\n        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C]\n</pre> # 1. Create a class which subclasses nn.Module class PatchEmbedding(nn.Module):     \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.      Args:         in_channels (int): Number of color channels for the input images. Defaults to 3.         patch_size (int): Size of patches to convert input image into. Defaults to 16.         embedding_dim (int): Size of embedding to turn image into. Defaults to 768.     \"\"\"     # 2. Initialize the class with appropriate variables     def __init__(self,                  in_channels:int=3,                  patch_size:int=16,                  embedding_dim:int=768):         super().__init__()          # 3. Create a layer to turn an image into patches         self.patcher = nn.Conv2d(in_channels=in_channels,                                  out_channels=embedding_dim,                                  kernel_size=patch_size,                                  stride=patch_size,                                  padding=0)          # 4. Create a layer to flatten the patch feature maps into a single dimension         self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector                                   end_dim=3)      # 5. Define the forward method     def forward(self, x):         # Create assertion to check that inputs are the correct shape         image_resolution = x.shape[-1]         assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"          # Perform the forward pass         x_patched = self.patcher(x)         x_flattened = self.flatten(x_patched)         # 6. Make sure the output shape has the right order         return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2\u2022C, N] -&gt; [batch_size, N, P^2\u2022C] <p><code>PatchEmbedding</code> \u200b\u5c42\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>set_seeds()\n\n# Create an instance of patch embedding layer\npatchify = PatchEmbedding(in_channels=3,\n                          patch_size=16,\n                          embedding_dim=768)\n\n# Pass a single image through\nprint(f\"Input image shape: {image.unsqueeze(0).shape}\")\npatch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error\nprint(f\"Output patch embedding shape: {patch_embedded_image.shape}\")\n</pre> set_seeds()  # Create an instance of patch embedding layer patchify = PatchEmbedding(in_channels=3,                           patch_size=16,                           embedding_dim=768)  # Pass a single image through print(f\"Input image shape: {image.unsqueeze(0).shape}\") patch_embedded_image = patchify(image.unsqueeze(0)) # add an extra batch dimension on the 0th index, otherwise will error print(f\"Output patch embedding shape: {patch_embedded_image.shape}\") <pre>Input image shape: torch.Size([1, 3, 224, 224])\nOutput patch embedding shape: torch.Size([1, 196, 768])\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u8f93\u51fa\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0e\u200b\u6211\u4eec\u200b\u671f\u671b\u200b\u7684\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u7406\u60f3\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u76f8\u5339\u914d\u200b\uff1a</p> <ul> <li>\u200b\u8f93\u5165\u200b\uff1a \u200b\u56fe\u50cf\u200b\u6700\u521d\u200b\u662f\u200b\u4e8c\u7ef4\u200b\u7684\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b ${H \\times W \\times C}$\u3002</li> <li>\u200b\u8f93\u51fa\u200b\uff1a \u200b\u56fe\u50cf\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7ef4\u200b\u7684\u200b\u6241\u5e73\u5316\u200b\u4e8c\u7ef4\u200b\u8865\u4e01\u200b\u5e8f\u5217\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b ${N \\times \\left(P^{2} \\cdot C\\right)}$\u3002</li> </ul> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li>$(H, W)$ \u200b\u662f\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\u3002</li> <li>$C$ \u200b\u662f\u200b\u901a\u9053\u200b\u6570\u200b\u3002</li> <li>$(P, P)$ \u200b\u662f\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u8865\u4e01\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\uff08\u200b\u8865\u4e01\u200b\u5927\u5c0f\u200b\uff09\u3002</li> <li>$N = H W / P^{2}$ \u200b\u662f\u200b\u751f\u6210\u200b\u7684\u200b\u8865\u4e01\u200b\u6570\u91cf\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u4f5c\u4e3a\u200bTransformer\u200b\u7684\u200b\u6709\u6548\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\u3002</li> </ul> <p>\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5df2\u7ecf\u200b\u590d\u5236\u200b\u4e86\u200b\u516c\u5f0f\u200b1\u200b\u4e2d\u200b\u7684\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\uff0c\u200b\u4f46\u200b\u6ca1\u6709\u200b\u590d\u5236\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b/\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u5904\u7406\u200b\u8fd9\u4e9b\u200b\u3002</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>PatchEmbedding</code> \u200b\u7c7b\u200b\uff08\u200b\u53f3\u4fa7\u200b\uff09\u200b\u590d\u5236\u200b\u4e86\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u56fe\u200b1\u200b\u548c\u200b\u516c\u5f0f\u200b1\u200b\u7684\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\uff08\u200b\u5de6\u4fa7\u200b\uff09\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u7c7b\u522b\u200b\u5d4c\u5165\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5c1a\u672a\u200b\u521b\u5efa\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u603b\u7ed3\u200b\u4e00\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200b <code>PatchEmbedding</code> \u200b\u5c42\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Create random input sizes\nrandom_input_image = (1, 3, 224, 224)\nrandom_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size\n\n# # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output)\n# summary(PatchEmbedding(),\n#         input_size=random_input_image, # try swapping this for \"random_input_image_error\"\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # Create random input sizes random_input_image = (1, 3, 224, 224) random_input_image_error = (1, 3, 250, 250) # will error because image size is incompatible with patch_size  # # Get a summary of the input and outputs of PatchEmbedding (uncomment for full output) # summary(PatchEmbedding(), #         input_size=random_input_image, # try swapping this for \"random_input_image_error\" #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) In\u00a0[\u00a0]: Copied! <pre># View the patch embedding and patch embedding shape\nprint(patch_embedded_image)\nprint(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # View the patch embedding and patch embedding shape print(patch_embedded_image) print(f\"Patch embedding shape: {patch_embedded_image.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],\n         [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],\n         [-0.7589,  0.2633, -0.1695,  ...,  0.5897, -0.3980,  0.0761],\n         ...,\n         [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],\n         [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],\n         [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],\n       grad_fn=&lt;PermuteBackward0&gt;)\nPatch embedding shape: torch.Size([1, 196, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>\u200b\u8981\u200b\u5c06\u200b\"\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u5d4c\u5165\u200b\u6dfb\u52a0\u200b\u5230\u200b\u5d4c\u5165\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e8f\u5217\u200b\u7684\u200b\u524d\u9762\u200b\"\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>embedding_dimension</code>\uff08$D$\uff09\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u5d4c\u5165\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5176\u200b\u6dfb\u52a0\u200b\u5230\u200b<code>number_of_patches</code>\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u7528\u4f2a\u200b\u4ee3\u7801\u200b\u8868\u793a\u200b\uff1a</p> <pre>patch_embedding = [image_patch_1, image_patch_2, image_patch_3...]\nclass_token = learnable_embedding\npatch_embedding_with_class_token = torch.cat((class_token, patch_embedding), dim=1)\n</pre> <p>\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u7684\u200b\u62fc\u63a5\u200b\uff08<code>torch.cat()</code>\uff09\u200b\u53d1\u751f\u200b\u5728\u200b<code>dim=1</code>\uff08\u200b\u5373\u200b<code>number_of_patches</code>\u200b\u7ef4\u5ea6\u200b\uff09\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u83b7\u53d6\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u548c\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5f62\u72b6\u200b\uff0c\u200b\u7136\u540e\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u4e3a\u200b<code>[batch_size, 1, embedding_dimension]</code>\u200b\u7684\u200b<code>torch.ones()</code>\u200b\u5f20\u91cf\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u5c06\u200b\u8be5\u200b\u5f20\u91cf\u200b\u4f20\u9012\u200b\u7ed9\u200b<code>nn.Parameter()</code>\u200b\u5e76\u200b\u8bbe\u7f6e\u200b<code>requires_grad=True</code>\uff0c\u200b\u4f7f\u200b\u5176\u200b\u6210\u4e3a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u53c2\u6570\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Get the batch size and embedding dimension\nbatch_size = patch_embedded_image.shape[0]\nembedding_dimension = patch_embedded_image.shape[-1]\n\n# Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D)\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]\n                           requires_grad=True) # make sure the embedding is learnable\n\n# Show the first 10 examples of the class_token\nprint(class_token[:, :, :10])\n\n# Print the class_token shape\nprint(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\")\n</pre> # Get the batch size and embedding dimension batch_size = patch_embedded_image.shape[0] embedding_dimension = patch_embedded_image.shape[-1]  # Create the class token embedding as a learnable parameter that shares the same size as the embedding dimension (D) class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), # [batch_size, number_of_tokens, embedding_dimension]                            requires_grad=True) # make sure the embedding is learnable  # Show the first 10 examples of the class_token print(class_token[:, :, :10])  # Print the class_token shape print(f\"Class token shape: {class_token.shape} -&gt; [batch_size, number_of_tokens, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nClass token shape: torch.Size([1, 1, 768]) -&gt; [batch_size, number_of_tokens, embedding_dimension]\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u91cc\u200b\u6211\u4eec\u200b\u4ec5\u200b\u51fa\u4e8e\u200b\u6f14\u793a\u200b\u76ee\u7684\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torch.ones()</code> \u200b\u521b\u5efa\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u3002\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4f7f\u7528\u200b <code>torch.randn()</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\uff08\u200b\u56e0\u4e3a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u6838\u5fc3\u200b\u5728\u4e8e\u200b\u5229\u7528\u200b\u53d7\u63a7\u200b\u968f\u673a\u6027\u200b\u7684\u200b\u529b\u91cf\u200b\uff0c\u200b\u901a\u5e38\u200b\u4ece\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u6539\u8fdb\u200b\u5b83\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b <code>class_token</code> \u200b\u7684\u200b <code>number_of_tokens</code> \u200b\u7ef4\u5ea6\u200b\u4e3a\u200b <code>1</code>\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u53ea\u60f3\u200b\u5728\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u5e8f\u5217\u200b\u7684\u200b\u5f00\u5934\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u503c\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u5c06\u200b\u5176\u200b\u6dfb\u52a0\u200b\u5230\u200b\u56fe\u50cf\u200b\u8865\u4e01\u200b\u5e8f\u5217\u200b <code>patch_embedded_image</code> \u200b\u7684\u200b\u5f00\u5934\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.cat()</code> \u200b\u5e76\u200b\u8bbe\u7f6e\u200b <code>dim=1</code>\uff08\u200b\u8fd9\u6837\u200b <code>class_token</code> \u200b\u7684\u200b <code>number_of_tokens</code> \u200b\u7ef4\u5ea6\u200b\u4f1a\u200b\u6dfb\u52a0\u200b\u5230\u200b <code>patch_embedded_image</code> \u200b\u7684\u200b <code>number_of_patches</code> \u200b\u7ef4\u5ea6\u200b\u4e4b\u524d\u200b\uff09\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Add the class token embedding to the front of the patch embedding\npatch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),\n                                                      dim=1) # concat on first dimension\n\n# Print the sequence of patch embeddings with the prepended class token embedding\nprint(patch_embedded_image_with_class_embedding)\nprint(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the class token embedding to the front of the patch embedding patch_embedded_image_with_class_embedding = torch.cat((class_token, patch_embedded_image),                                                       dim=1) # concat on first dimension  # Print the sequence of patch embeddings with the prepended class token embedding print(patch_embedded_image_with_class_embedding) print(f\"Sequence of patch embeddings with class token prepended shape: {patch_embedded_image_with_class_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n         [-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],\n         [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],\n         ...,\n         [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],\n         [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],\n         [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],\n       grad_fn=&lt;CatBackward0&gt;)\nSequence of patch embeddings with class token prepended shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u5df2\u200b\u6dfb\u52a0\u200b\u5230\u200b\u5e8f\u5217\u200b\u524d\u7aef\u200b\uff01</p> <p></p> <p>\u200b\u56de\u987e\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff1a\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b <code>PatchEmbedding()</code> \u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u751f\u6210\u200b\u4e00\u7cfb\u5217\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\uff1b\u200b\u63a5\u7740\u200b\uff0c\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u7c7b\u200b\u6807\u8bb0\u200b\uff0c\u200b\u5176\u200b\u7ef4\u5ea6\u200b\u4e0e\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\u4e00\u81f4\u200b\uff1b\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u6dfb\u52a0\u200b\u5230\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u5e8f\u5217\u200b\u7684\u200b\u524d\u7aef\u200b\u3002\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f7f\u7528\u200b <code>torch.ones()</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u4e3b\u8981\u200b\u662f\u200b\u4e3a\u4e86\u200b\u6f14\u793a\u200b\u76ee\u7684\u200b\uff0c\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u7528\u200b <code>torch.randn()</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># View the sequence of patch embeddings with the prepended class embedding\npatch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape\n</pre> # View the sequence of patch embeddings with the prepended class embedding patch_embedded_image_with_class_embedding, patch_embedded_image_with_class_embedding.shape Out[\u00a0]: <pre>(tensor([[[ 1.0000,  1.0000,  1.0000,  ...,  1.0000,  1.0000,  1.0000],\n          [-0.9145,  0.2454, -0.2292,  ...,  0.6768, -0.4515,  0.3496],\n          [-0.7427,  0.1955, -0.3570,  ...,  0.5823, -0.3458,  0.3261],\n          ...,\n          [-1.0072,  0.2795, -0.2804,  ...,  0.7624, -0.4584,  0.3581],\n          [-0.9839,  0.1652, -0.1576,  ...,  0.7489, -0.5478,  0.3486],\n          [-0.9260,  0.1383, -0.1157,  ...,  0.5847, -0.4717,  0.3112]]],\n        grad_fn=&lt;CatBackward0&gt;),\n torch.Size([1, 197, 768]))</pre> <p>\u200b\u516c\u5f0f\u200b1\u200b\u6307\u51fa\u200b\uff0c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff08$\\mathbf{E}_{\\text {pos }}$\uff09\u200b\u7684\u200b\u5f62\u72b6\u200b\u5e94\u4e3a\u200b $(N + 1) \\times D$\uff1a</p> <p>$$\\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D}$$</p> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li>$N=H W / P^{2}$ \u200b\u662f\u200b\u751f\u6210\u200b\u7684\u200b\u8865\u4e01\u200b\u6570\u91cf\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u4f5c\u4e3a\u200bTransformer\u200b\u7684\u200b\u6709\u6548\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\uff08\u200b\u8865\u4e01\u200b\u6570\u91cf\u200b\uff09\u3002</li> <li>$D$ \u200b\u662f\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u8868\u200b1\u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u4e0d\u540c\u200b\u7684\u200b $D$ \u200b\u503c\u200b\uff08\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff09\u3002</li> </ul> <p>\u200b\u5e78\u8fd0\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u503c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>torch.ones()</code> \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b1D\u200b\u5d4c\u5165\u200b $\\mathbf{E}_{\\text {pos }}$\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Calculate N (number of patches)\nnumber_of_patches = int((height * width) / patch_size**2)\n\n# Get embedding dimension\nembedding_dimension = patch_embedded_image_with_class_embedding.shape[2]\n\n# Create the learnable 1D position embedding\nposition_embedding = nn.Parameter(torch.ones(1,\n                                             number_of_patches+1,\n                                             embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding\nprint(position_embedding[:, :10, :10])\nprint(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Calculate N (number of patches) number_of_patches = int((height * width) / patch_size**2)  # Get embedding dimension embedding_dimension = patch_embedded_image_with_class_embedding.shape[2]  # Create the learnable 1D position embedding position_embedding = nn.Parameter(torch.ones(1,                                              number_of_patches+1,                                              embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # Show the first 10 sequences and 10 position embedding values and check the shape of the position embedding print(position_embedding[:, :10, :10]) print(f\"Position embeddding shape: {position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n         [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]], grad_fn=&lt;SliceBackward0&gt;)\nPosition embeddding shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ec5\u200b\u51fa\u4e8e\u200b\u6f14\u793a\u200b\u76ee\u7684\u200b\uff0c\u200b\u521b\u5efa\u200b\u4e86\u200b <code>torch.ones()</code> \u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4f7f\u7528\u200b <code>torch.randn()</code> \u200b\u521b\u5efa\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff08\u200b\u4ece\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b\u5e76\u200b\u901a\u8fc7\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\u8fdb\u884c\u200b\u6539\u8fdb\u200b\uff09\u3002</p> <p>\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u6dfb\u52a0\u200b\u5230\u200b\u9884\u5148\u200b\u9644\u52a0\u200b\u4e86\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u7684\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Add the position embedding to the patch and class token embedding\npatch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding\nprint(patch_and_position_embedding)\nprint(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\")\n</pre> # Add the position embedding to the patch and class token embedding patch_and_position_embedding = patch_embedded_image_with_class_embedding + position_embedding print(patch_and_position_embedding) print(f\"Patch embeddings, class token prepended and positional embeddings added shape: {patch_and_position_embedding.shape} -&gt; [batch_size, number_of_patches, embedding_dimension]\") <pre>tensor([[[ 2.0000,  2.0000,  2.0000,  ...,  2.0000,  2.0000,  2.0000],\n         [ 0.0855,  1.2454,  0.7708,  ...,  1.6768,  0.5485,  1.3496],\n         [ 0.2573,  1.1955,  0.6430,  ...,  1.5823,  0.6542,  1.3261],\n         ...,\n         [-0.0072,  1.2795,  0.7196,  ...,  1.7624,  0.5416,  1.3581],\n         [ 0.0161,  1.1652,  0.8424,  ...,  1.7489,  0.4522,  1.3486],\n         [ 0.0740,  1.1383,  0.8843,  ...,  1.5847,  0.5283,  1.3112]]],\n       grad_fn=&lt;AddBackward0&gt;)\nPatch embeddings, class token prepended and positional embeddings added shape: torch.Size([1, 197, 768]) -&gt; [batch_size, number_of_patches, embedding_dimension]\n</pre> <p>\u200b\u6ce8\u610f\u200b\u5230\u200b\u5d4c\u5165\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u7684\u200b\u503c\u200b\u90fd\u200b\u589e\u52a0\u200b\u4e86\u200b1\uff08\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u662f\u200b\u7528\u200b<code>torch.ones()</code>\u200b\u521b\u5efa\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u613f\u610f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5206\u522b\u200b\u653e\u5165\u200b\u5b83\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u5c42\u200b\u4e2d\u200b\u3002\u200b\u4f46\u200b\u7a0d\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b8\u200b\u8282\u4e2d\u200b\u770b\u5230\u200b\uff0c\u200b\u5b83\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u88ab\u200b\u6574\u5408\u200b\u5230\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b<code>forward()</code>\u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u7684\u200b\u3002</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u7528\u4e8e\u200b\u5c06\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u6dfb\u52a0\u200b\u5230\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u548c\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u5e8f\u5217\u200b\u4e2d\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002\u200b\u6ce8\u610f\u200b\uff1a <code>torch.ones()</code>\u200b\u4ec5\u200b\u7528\u4e8e\u200b\u521b\u5efa\u200b\u5d4c\u5165\u200b\u4ee5\u200b\u8fdb\u884c\u200b\u8bf4\u660e\u200b\uff0c\u200b\u5b9e\u9645\u4e0a\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4f7f\u7528\u200b<code>torch.randn()</code>\u200b\u6765\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u968f\u673a\u6570\u200b\u5f00\u59cb\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>set_seeds()\n\n# 1. Set patch size\npatch_size = 16\n\n# 2. Print shape of original image tensor and get the image dimensions\nprint(f\"Image tensor shape: {image.shape}\")\nheight, width = image.shape[1], image.shape[2]\n\n# 3. Get image tensor and add batch dimension\nx = image.unsqueeze(0)\nprint(f\"Input image with batch dimension shape: {x.shape}\")\n\n# 4. Create patch embedding layer\npatch_embedding_layer = PatchEmbedding(in_channels=3,\n                                       patch_size=patch_size,\n                                       embedding_dim=768)\n\n# 5. Pass image through patch embedding layer\npatch_embedding = patch_embedding_layer(x)\nprint(f\"Patching embedding shape: {patch_embedding.shape}\")\n\n# 6. Create class token embedding\nbatch_size = patch_embedding.shape[0]\nembedding_dimension = patch_embedding.shape[-1]\nclass_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),\n                           requires_grad=True) # make sure it's learnable\nprint(f\"Class token embedding shape: {class_token.shape}\")\n\n# 7. Prepend class token embedding to patch embedding\npatch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\nprint(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n\n# 8. Create position embedding\nnumber_of_patches = int((height * width) / patch_size**2)\nposition_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n                                  requires_grad=True) # make sure it's learnable\n\n# 9. Add position embedding to patch embedding with class token\npatch_and_position_embedding = patch_embedding_class_token + position_embedding\nprint(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n</pre> set_seeds()  # 1. Set patch size patch_size = 16  # 2. Print shape of original image tensor and get the image dimensions print(f\"Image tensor shape: {image.shape}\") height, width = image.shape[1], image.shape[2]  # 3. Get image tensor and add batch dimension x = image.unsqueeze(0) print(f\"Input image with batch dimension shape: {x.shape}\")  # 4. Create patch embedding layer patch_embedding_layer = PatchEmbedding(in_channels=3,                                        patch_size=patch_size,                                        embedding_dim=768)  # 5. Pass image through patch embedding layer patch_embedding = patch_embedding_layer(x) print(f\"Patching embedding shape: {patch_embedding.shape}\")  # 6. Create class token embedding batch_size = patch_embedding.shape[0] embedding_dimension = patch_embedding.shape[-1] class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension),                            requires_grad=True) # make sure it's learnable print(f\"Class token embedding shape: {class_token.shape}\")  # 7. Prepend class token embedding to patch embedding patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1) print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")  # 8. Create position embedding number_of_patches = int((height * width) / patch_size**2) position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),                                   requires_grad=True) # make sure it's learnable  # 9. Add position embedding to patch embedding with class token patch_and_position_embedding = patch_embedding_class_token + position_embedding print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\") <pre>Image tensor shape: torch.Size([3, 224, 224])\nInput image with batch dimension shape: torch.Size([1, 3, 224, 224])\nPatching embedding shape: torch.Size([1, 196, 768])\nClass token embedding shape: torch.Size([1, 1, 768])\nPatch embedding with class token shape: torch.Size([1, 197, 768])\nPatch and position embedding shape: torch.Size([1, 197, 768])\n</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u4ece\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u5230\u200b\u5206\u5757\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff0c\u200b\u53ea\u200b\u9700\u200b\u4e00\u884c\u200b\u4ee3\u7801\u200b\u3002</p> <p></p> <p>\u200b\u5c06\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u65b9\u7a0b\u200b1\u200b\u6620\u5c04\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200bPyTorch\u200b\u4ee3\u7801\u200b\u3002\u200b\u8fd9\u662f\u200b\u590d\u73b0\u200b\u8bba\u6587\u200b\u7684\u200b\u7cbe\u9ad3\u200b\uff0c\u200b\u5c06\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u53ef\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u65b9\u6cd5\u200b\u6765\u200b\u7f16\u7801\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u4f20\u9012\u200b\u7ed9\u200bViT\u200b\u8bba\u6587\u200b\u56fe\u200b1\u200b\u4e2d\u200b\u7684\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u3002</p> <p>\u200b\u52a8\u753b\u200b\u5c55\u793a\u200b\u6574\u4e2a\u200bViT\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff1a\u200b\u4ece\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\u5230\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u518d\u200b\u5230\u200bMLP\u200b\u5934\u90e8\u200b\u3002</p> <p>\u200b\u4ece\u200b\u4ee3\u7801\u200b\u7684\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0c\u200b\u521b\u5efa\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\u53ef\u80fd\u200b\u662f\u200b\u590d\u73b0\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u6700\u5927\u200b\u7684\u200b\u90e8\u5206\u200b\u3002</p> <p>ViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u90e8\u5206\u200b\uff0c\u200b\u5982\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u548c\u200b\u5f52\u4e00\u5316\u200b\u5c42\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u73b0\u6709\u200b\u7684\u200bPyTorch\u200b\u5c42\u6765\u200b\u521b\u5efa\u200b\u3002</p> <p>\u200b\u7ee7\u7eed\u524d\u8fdb\u200b\uff01</p> In\u00a0[\u00a0]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MultiheadSelfAttentionBlock(nn.Module):\n    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n    \"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multi-Head Attention (MSA) layer\n        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n                                                    num_heads=num_heads,\n                                                    dropout=attn_dropout,\n                                                    batch_first=True) # does our batch dimension come first?\n\n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n                                             key=x, # key embeddings\n                                             value=x, # value embeddings\n                                             need_weights=False) # do we need the weights or just the layer outputs?\n        return attn_output\n</pre> # 1. Create a class that inherits from nn.Module class MultiheadSelfAttentionBlock(nn.Module):     \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).     \"\"\"     # 2. Initialize the class with hyperparameters from Table 1     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks         super().__init__()          # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)          # 4. Create the Multi-Head Attention (MSA) layer         self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,                                                     num_heads=num_heads,                                                     dropout=attn_dropout,                                                     batch_first=True) # does our batch dimension come first?      # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         attn_output, _ = self.multihead_attn(query=x, # query embeddings                                              key=x, # key embeddings                                              value=x, # value embeddings                                              need_weights=False) # do we need the weights or just the layer outputs?         return attn_output <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0e\u200b\u56fe\u200b1\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b <code>MultiheadSelfAttentionBlock</code> \u200b\u4e0d\u200b\u5305\u542b\u200b\u8df3\u8dc3\u200b\u6216\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08\u200b\u516c\u5f0f\u200b2\u200b\u4e2d\u200b\u7684\u200b\u201c$+\\mathbf{z}_{\\ell-1}$\u201d\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b7.1\u200b\u8282\u200b\u521b\u5efa\u200b\u5b8c\u6574\u200b\u7684\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u65f6\u200b\u6dfb\u52a0\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>MSABlock\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>MultiheadSelfAttentionBlock</code> \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b4.8\u200b\u8282\u200b\u521b\u5efa\u200b\u7684\u200b <code>patch_and_position_embedding</code> \u200b\u53d8\u91cf\u200b\u4f20\u9012\u6570\u636e\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Create an instance of MSABlock\nmultihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1\n                                                             num_heads=12) # from Table 1\n\n# Pass patch and position image embedding through MSABlock\npatched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding)\nprint(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\")\nprint(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\")\n</pre> # Create an instance of MSABlock multihead_self_attention_block = MultiheadSelfAttentionBlock(embedding_dim=768, # from Table 1                                                              num_heads=12) # from Table 1  # Pass patch and position image embedding through MSABlock patched_image_through_msa_block = multihead_self_attention_block(patch_and_position_embedding) print(f\"Input shape of MSA block: {patch_and_position_embedding.shape}\") print(f\"Output shape MSA block: {patched_image_through_msa_block.shape}\") <pre>Input shape of MSA block: torch.Size([1, 197, 768])\nOutput shape MSA block: torch.Size([1, 197, 768])\n</pre> <p>\u200b\u6ce8\u610f\u200b\u5230\u200b\u5f53\u200b\u6570\u636e\u200b\u901a\u8fc7\u200bMSA\u200b\u5757\u200b\u65f6\u200b\uff0c\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u5f62\u72b6\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u200b\u610f\u5473\u7740\u200b\u6570\u636e\u200b\u5728\u200b\u901a\u8fc7\u200b\u65f6\u200b\u6ca1\u6709\u200b\u53d8\u5316\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u6253\u5370\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f20\u91cf\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u662f\u200b\u5982\u4f55\u200b\u53d8\u5316\u200b\u7684\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u8fd9\u79cd\u200b\u53d8\u5316\u200b\u4f1a\u200b\u8de8\u8d8a\u200b <code>1 * 197 * 768</code> \u200b\u4e2a\u503c\u200b\uff0c\u200b\u53ef\u80fd\u200b\u96be\u4ee5\u200b\u76f4\u89c2\u200b\u5c55\u793a\u200b\uff09\u3002</p> <p></p> <p>***\u200b\u5de6\u56fe\u200b\uff1a** \u200b\u56fe\u200b1\u200b\u4e2d\u200b\u7684\u200bVision Transformer\u200b\u67b6\u6784\u200b\uff0c\u200b\u9ad8\u4eae\u200b\u663e\u793a\u200b\u4e86\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u548c\u200bLayerNorm\u200b\u5c42\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u6784\u6210\u200b\u4e86\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u4e2d\u200b\u7684\u200b\u516c\u5f0f\u200b2\u3002\u200b\u53f3\u56fe\u200b\uff1a \u200b\u4f7f\u7528\u200bPyTorch\u200b\u5c42\u200b\u590d\u5236\u200b\u516c\u5f0f\u200b2\uff08\u200b\u4e0d\u200b\u5305\u62ec\u200b\u6700\u540e\u200b\u7684\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\uff09\u3002*</p> <p>\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u6b63\u5f0f\u200b\u590d\u5236\u200b\u4e86\u200b\u516c\u5f0f\u200b2\uff08\u200b\u9664\u4e86\u200b\u6700\u540e\u200b\u7684\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b7.1\u200b\u8282\u4e2d\u200b\u8ba8\u8bba\u200b\uff09\uff01</p> <p>\u200b\u7ee7\u7eed\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\uff01</p> In\u00a0[\u00a0]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass MLPBlock(nn.Module):\n    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n        super().__init__()\n\n        # 3. Create the Norm layer (LN)\n        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n\n        # 4. Create the Multilayer perceptron (MLP) layer(s)\n        self.mlp = nn.Sequential(\n            nn.Linear(in_features=embedding_dim,\n                      out_features=mlp_size),\n            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n            nn.Dropout(p=dropout),\n            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n                      out_features=embedding_dim), # take back to embedding_dim\n            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n        )\n\n    # 5. Create a forward() method to pass the data throguh the layers\n    def forward(self, x):\n        x = self.layer_norm(x)\n        x = self.mlp(x)\n        return x\n</pre> # 1. Create a class that inherits from nn.Module class MLPBlock(nn.Module):     \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  dropout:float=0.1): # Dropout from Table 3 for ViT-Base         super().__init__()          # 3. Create the Norm layer (LN)         self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)          # 4. Create the Multilayer perceptron (MLP) layer(s)         self.mlp = nn.Sequential(             nn.Linear(in_features=embedding_dim,                       out_features=mlp_size),             nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"             nn.Dropout(p=dropout),             nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above                       out_features=embedding_dim), # take back to embedding_dim             nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"         )      # 5. Create a forward() method to pass the data throguh the layers     def forward(self, x):         x = self.layer_norm(x)         x = self.mlp(x)         return x <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0e\u200b\u56fe\u200b1\u200b\u4e0d\u540c\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b <code>MLPBlock()</code> \u200b\u4e0d\u200b\u5305\u542b\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\u6216\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08\u200b\u65b9\u7a0b\u200b3\u200b\u4e2d\u200b\u7684\u200b\"$+\\mathbf{z}_{\\ell}^{\\prime}$\"\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7a0d\u540e\u200b\u521b\u5efa\u200b\u6574\u4e2a\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u65f6\u200b\u52a0\u5165\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>MLPBlock \u200b\u7c7b\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>MLPBlock</code> \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b5.3\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>patched_image_through_msa_block</code> \u200b\u53d8\u91cf\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u5b83\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Create an instance of MLPBlock\nmlp_block = MLPBlock(embedding_dim=768, # from Table 1\n                     mlp_size=3072, # from Table 1\n                     dropout=0.1) # from Table 3\n\n# Pass output of MSABlock through MLPBlock\npatched_image_through_mlp_block = mlp_block(patched_image_through_msa_block)\nprint(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\")\nprint(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\")\n</pre> # Create an instance of MLPBlock mlp_block = MLPBlock(embedding_dim=768, # from Table 1                      mlp_size=3072, # from Table 1                      dropout=0.1) # from Table 3  # Pass output of MSABlock through MLPBlock patched_image_through_mlp_block = mlp_block(patched_image_through_msa_block) print(f\"Input shape of MLP block: {patched_image_through_msa_block.shape}\") print(f\"Output shape MLP block: {patched_image_through_mlp_block.shape}\") <pre>Input shape of MLP block: torch.Size([1, 197, 768])\nOutput shape MLP block: torch.Size([1, 197, 768])\n</pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u5f53\u200b\u6570\u636e\u200b\u8fdb\u5165\u200b\u548c\u200b\u79bb\u5f00\u200bMLP\u200b\u5757\u200b\u65f6\u200b\uff0c\u200b\u5176\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u518d\u6b21\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5f53\u200b\u6570\u636e\u200b\u901a\u8fc7\u200bMLP\u200b\u5757\u200b\u5185\u90e8\u200b\u7684\u200b<code>nn.Linear()</code>\u200b\u5c42\u65f6\u200b\uff0c\u200b\u5f62\u72b6\u200b\u786e\u5b9e\u200b\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff08\u200b\u4ece\u8868\u200b1\u200b\u4e2d\u200b\u7684\u200bMLP\u200b\u5927\u5c0f\u200b\u6269\u5c55\u200b\uff0c\u200b\u7136\u540e\u200b\u538b\u7f29\u200b\u56de\u8868\u200b1\u200b\u4e2d\u200b\u7684\u200b\u9690\u85cf\u200b\u5927\u5c0f\u200b$D$\uff09\u3002</p> <p></p> <p>\u200b\u5de6\u200b\uff1a\u200b\u56fe\u200b1\u200b\u4e2d\u200b\u7684\u200bVision Transformer\u200b\u67b6\u6784\u200b\uff0c\u200b\u5176\u4e2d\u200bMLP\u200b\u548c\u200bNorm\u200b\u5c42\u9ad8\u200b\u4eae\u200b\u663e\u793a\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u6784\u6210\u200b\u4e86\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u4e2d\u200b\u7684\u200b\u65b9\u7a0b\u200b3\u3002\u200b\u53f3\u200b\uff1a\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5c42\u200b\u590d\u5236\u200b\u65b9\u7a0b\u200b3\uff08\u200b\u4e0d\u200b\u5305\u62ec\u200b\u672b\u7aef\u200b\u7684\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\uff09\u3002</p> <p>\u200b\u5475\u5475\u200b\uff01</p> <p>\u200b\u65b9\u7a0b\u200b3\u200b\u5df2\u200b\u590d\u5236\u200b\uff08\u200b\u9664\u4e86\u200b\u672b\u7aef\u200b\u7684\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b7.1\u200b\u8282\u200b\u8ba8\u8bba\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u65b9\u7a0b\u200b2\u200b\u548c\u200b3\u200b\u8f6c\u6362\u200b\u4e3a\u200bPyTorch\u200b\u4ee3\u7801\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u7ed3\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u521b\u5efa\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># 1. Create a class that inherits from nn.Module\nclass TransformerEncoderBlock(nn.Module):\n    \"\"\"Creates a Transformer Encoder block.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                 attn_dropout:float=0): # Amount of dropout for attention layers\n        super().__init__()\n\n        # 3. Create MSA block (equation 2)\n        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n                                                     num_heads=num_heads,\n                                                     attn_dropout=attn_dropout)\n\n        # 4. Create MLP block (equation 3)\n        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n                                   mlp_size=mlp_size,\n                                   dropout=mlp_dropout)\n\n    # 5. Create a forward() method\n    def forward(self, x):\n\n        # 6. Create residual connection for MSA block (add the input to the output)\n        x =  self.msa_block(x) + x\n\n        # 7. Create residual connection for MLP block (add the input to the output)\n        x = self.mlp_block(x) + x\n\n        return x\n</pre> # 1. Create a class that inherits from nn.Module class TransformerEncoderBlock(nn.Module):     \"\"\"Creates a Transformer Encoder block.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                  attn_dropout:float=0): # Amount of dropout for attention layers         super().__init__()          # 3. Create MSA block (equation 2)         self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,                                                      num_heads=num_heads,                                                      attn_dropout=attn_dropout)          # 4. Create MLP block (equation 3)         self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,                                    mlp_size=mlp_size,                                    dropout=mlp_dropout)      # 5. Create a forward() method     def forward(self, x):          # 6. Create residual connection for MSA block (add the input to the output)         x =  self.msa_block(x) + x          # 7. Create residual connection for MLP block (add the input to the output)         x = self.mlp_block(x) + x          return x <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>Transformer Encoder \u200b\u6a21\u5757\u200b\u521b\u5efa\u200b\u6210\u529f\u200b\uff01</p> <p></p> <p>***\u200b\u5de6\u4fa7\u200b\uff1a** \u200b\u6765\u81ea\u200b ViT \u200b\u8bba\u6587\u200b\u7684\u200b\u56fe\u200b 1\uff0c\u200b\u5176\u4e2d\u200b ViT \u200b\u67b6\u6784\u200b\u7684\u200b Transformer Encoder \u200b\u90e8\u5206\u200b\u88ab\u200b\u9ad8\u4eae\u200b\u663e\u793a\u200b\u3002 \u200b\u53f3\u4fa7\u200b\uff1a Transformer Encoder \u200b\u5bf9\u5e94\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u516c\u5f0f\u200b 2 \u200b\u548c\u200b\u516c\u5f0f\u200b 3\uff0cTransformer Encoder \u200b\u7531\u200b\u4ea4\u66ff\u200b\u7684\u200b\u516c\u5f0f\u200b 2\uff08\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff09\u200b\u548c\u200b\u516c\u5f0f\u200b 3\uff08\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff09\u200b\u7ec4\u6210\u200b\u3002*</p> <p>\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u50cf\u200b\u642d\u79ef\u6728\u200b\u4e00\u6837\u200b\uff0c\u200b\u4e00\u6b65\u200b\u4e00\u6b65\u200b\u5730\u200b\u5c06\u200b\u6574\u4e2a\u200b\u67b6\u6784\u200b\u62fc\u51d1\u200b\u8d77\u6765\u200b\u7684\u200b\uff0c\u200b\u4e00\u6b21\u200b\u7f16\u7801\u200b\u4e00\u4e2a\u200b\u201c\u200b\u7816\u5757\u200b\u201d\uff08\u200b\u6216\u200b\u516c\u5f0f\u200b\uff09\u3002</p> <p></p> <p>\u200b\u5c06\u200b ViT \u200b\u7684\u200b Transformer Encoder \u200b\u6620\u5c04\u200b\u5230\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u6ce8\u610f\u200b\u5230\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u8868\u200b 1 \u200b\u6709\u200b\u4e00\u4e2a\u200b Layers \u200b\u5217\u200b\u3002\u200b\u8fd9\u6307\u200b\u7684\u200b\u662f\u200b\u7279\u5b9a\u200b ViT \u200b\u67b6\u6784\u200b\u4e2d\u200b Transformer Encoder \u200b\u5757\u200b\u7684\u200b\u6570\u91cf\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u4e8e\u200b ViT-Base\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5806\u53e0\u200b 12 \u200b\u4e2a\u200b\u8fd9\u6837\u200b\u7684\u200b Transformer Encoder \u200b\u5757\u200b\u6765\u200b\u6784\u6210\u200b\u6211\u4eec\u200b\u67b6\u6784\u200b\u7684\u200b\u4e3b\u5e72\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b 8 \u200b\u8282\u4e2d\u200b\u8be6\u7ec6\u200b\u8ba8\u8bba\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>torchinfo.summary()</code> \u200b\u6765\u200b\u67e5\u770b\u200b\u5c06\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(1, 197, 768) -&gt; (batch_size, num_patches, embedding_dimension)</code> \u200b\u7684\u200b\u8f93\u5165\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6211\u4eec\u200b\u7684\u200b Transformer Encoder \u200b\u5757\u200b\u7684\u200b\u6982\u8981\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Create an instance of TransformerEncoderBlock\ntransformer_encoder_block = TransformerEncoderBlock()\n\n# # Print an input and output summary of our Transformer Encoder (uncomment for full output)\n# summary(model=transformer_encoder_block,\n#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # Create an instance of TransformerEncoderBlock transformer_encoder_block = TransformerEncoderBlock()  # # Print an input and output summary of our Transformer Encoder (uncomment for full output) # summary(model=transformer_encoder_block, #         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension) #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>\u200b\u54c7\u200b\uff01\u200b\u770b\u770b\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\uff01</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u5728\u200b\u901a\u8fc7\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u5757\u200b\u4e2d\u200b\u7684\u200bMSA\u200b\u5757\u200b\u548c\u200bMLP\u200b\u5757\u200b\u7684\u200b\u6240\u6709\u200b\u4e0d\u540c\u200b\u5c42\u65f6\u200b\uff0c\u200b\u5f62\u72b6\u200b\u53d1\u751f\u200b\u4e86\u200b\u53d8\u5316\u200b\uff0c\u200b\u6700\u540e\u200b\u5728\u200b\u672b\u7aef\u200b\u6062\u590d\u200b\u5230\u200b\u539f\u6765\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5c3d\u7ba1\u200b\u8f93\u5165\u200b\u5230\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u5757\u200b\u7684\u200b\u5f62\u72b6\u200b\u5728\u200b\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u5904\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u200b\u610f\u5473\u7740\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u6ca1\u6709\u200b\u88ab\u200b\u5904\u7406\u200b\u3002Transformer\u200b\u7f16\u7801\u5668\u200b\u5757\u200b\uff08\u200b\u4ee5\u53ca\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\uff09\u200b\u7684\u200b\u6574\u4e2a\u200b\u76ee\u6807\u200b\u662f\u200b\u901a\u8fc7\u200b\u4e2d\u95f4\u200b\u7684\u200b\u5404\u79cd\u200b\u5c42\u6765\u200b\u5b66\u4e60\u200b\u8f93\u5165\u200b\u7684\u200b\u6df1\u5c42\u200b\u8868\u793a\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Create the same as above with torch.nn.TransformerEncoderLayer()\ntorch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base\n                                                             nhead=12, # Heads from Table 1 for ViT-Base\n                                                             dim_feedforward=3072, # MLP size from Table 1 for ViT-Base\n                                                             dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n                                                             activation=\"gelu\", # GELU non-linear activation\n                                                             batch_first=True, # Do our batches come first?\n                                                             norm_first=True) # Normalize first or after MSA/MLP layers?\n\ntorch_transformer_encoder_layer\n</pre> # Create the same as above with torch.nn.TransformerEncoderLayer() torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=768, # Hidden size D from Table 1 for ViT-Base                                                              nhead=12, # Heads from Table 1 for ViT-Base                                                              dim_feedforward=3072, # MLP size from Table 1 for ViT-Base                                                              dropout=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base                                                              activation=\"gelu\", # GELU non-linear activation                                                              batch_first=True, # Do our batches come first?                                                              norm_first=True) # Normalize first or after MSA/MLP layers?  torch_transformer_encoder_layer Out[\u00a0]: <pre>TransformerEncoderLayer(\n  (self_attn): MultiheadAttention(\n    (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n  )\n  (linear1): Linear(in_features=768, out_features=3072, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n  (linear2): Linear(in_features=3072, out_features=768, bias=True)\n  (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  (dropout1): Dropout(p=0.1, inplace=False)\n  (dropout2): Dropout(p=0.1, inplace=False)\n)</pre> <p>\u200b\u4e3a\u4e86\u200b\u8fdb\u4e00\u6b65\u200b\u68c0\u67e5\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>torchinfo.summary()</code> \u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u6458\u8981\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output)\n# summary(model=torch_transformer_encoder_layer,\n#         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> # # Get the output of PyTorch's version of the Transformer Encoder (uncomment for full output) # summary(model=torch_transformer_encoder_layer, #         input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension) #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>\u200b\u7531\u4e8e\u200b <code>torch.nn.TransformerEncoderLayer()</code> \u200b\u6784\u5efa\u200b\u5c42\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8f93\u51fa\u200b\u7684\u200b\u603b\u7ed3\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7565\u6709\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5b83\u200b\u4f7f\u7528\u200b\u7684\u200b\u5c42\u200b\u3001\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u4ee5\u53ca\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u60f3\u200b\uff1a\u201c\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7528\u200b PyTorch \u200b\u5c42\u200b\u8fd9\u4e48\u200b\u5feb\u5730\u200b\u521b\u5efa\u200b Transformer Encoder\uff0c\u200b\u4e3a\u4ec0\u4e48\u200b\u8fd8\u8981\u200b\u8d39\u5fc3\u200b\u91cd\u73b0\u200b\u65b9\u7a0b\u200b 2 \u200b\u548c\u200b 3 \u200b\u5462\u200b\uff1f\u201d</p> <p>\u200b\u7b54\u6848\u200b\u662f\u200b\uff1a\u200b\u7ec3\u4e60\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4ece\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\u4e2d\u200b\u590d\u5236\u200b\u4e86\u200b\u4e00\u7cfb\u5217\u200b\u65b9\u7a0b\u200b\u548c\u200b\u5c42\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u9700\u8981\u200b\u6539\u53d8\u200b\u5c42\u200b\u5e76\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u4e1c\u897f\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u505a\u5230\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4f7f\u7528\u200b PyTorch \u200b\u9884\u200b\u6784\u5efa\u200b\u5c42\u200b\u7684\u200b\u4f18\u70b9\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li>\u200b\u66f4\u200b\u4e0d\u200b\u5bb9\u6613\u200b\u51fa\u9519\u200b - \u200b\u901a\u5e38\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u88ab\u200b\u7eb3\u5165\u200b PyTorch \u200b\u6807\u51c6\u200b\u5e93\u200b\uff0c\u200b\u5b83\u200b\u5df2\u7ecf\u200b\u8fc7\u200b\u6d4b\u8bd5\u200b\u5e76\u200b\u88ab\u200b\u8bc1\u660e\u200b\u662f\u200b\u6709\u6548\u200b\u7684\u200b\u3002</li> <li>\u200b\u6f5c\u5728\u200b\u7684\u200b\u66f4\u597d\u200b\u6027\u80fd\u200b - \u200b\u622a\u81f3\u200b 2022 \u200b\u5e74\u200b 7 \u200b\u6708\u200b\u548c\u200b PyTorch 1.12\uff0cPyTorch \u200b\u5b9e\u73b0\u200b\u7684\u200b <code>torch.nn.TransformerEncoderLayer()</code> \u200b\u5728\u200b\u8bb8\u591a\u200b\u5e38\u89c1\u200b\u5de5\u4f5c\u200b\u8d1f\u8f7d\u200b\u4e0a\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b \u200b\u8d85\u8fc7\u200b 2 \u200b\u500d\u200b\u7684\u200b\u52a0\u901f\u200b\u3002</li> </ul> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u7531\u4e8e\u200b ViT \u200b\u67b6\u6784\u200b\u5728\u200b\u5b8c\u6574\u200b\u7684\u200b\u67b6\u6784\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u4e86\u200b\u591a\u4e2a\u200b Transformer \u200b\u5c42\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\uff08\u200b\u8868\u200b 1 \u200b\u663e\u793a\u200b\u4e86\u200b ViT-Base \u200b\u60c5\u51b5\u200b\u4e0b\u200b\u6709\u200b 12 \u200b\u5c42\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.nn.TransformerEncoder(encoder_layer, num_layers)</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>encoder_layer</code> - \u200b\u4f7f\u7528\u200b <code>torch.nn.TransformerEncoderLayer()</code> \u200b\u521b\u5efa\u200b\u7684\u200b\u76ee\u6807\u200b Transformer Encoder \u200b\u5c42\u200b\u3002</li> <li><code>num_layers</code> - \u200b\u8981\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\u7684\u200b Transformer Encoder \u200b\u5c42\u6570\u200b\u3002</li> </ul> In\u00a0[\u00a0]: Copied! <pre># 1. Create a ViT class that inherits from nn.Module\nclass ViT(nn.Module):\n    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n    def __init__(self,\n                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n                 in_channels:int=3, # Number of channels in input image\n                 patch_size:int=16, # Patch size\n                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n                 attn_dropout:float=0, # Dropout for attention projection\n                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n                 num_classes:int=1000): # Default for ImageNet but can customize this\n        super().__init__() # don't forget the super().__init__()!\n\n        # 3. Make the image size is divisble by the patch size\n        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n\n        # 4. Calculate number of patches (height * width/patch^2)\n        self.num_patches = (img_size * img_size) // patch_size**2\n\n        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n                                            requires_grad=True)\n\n        # 6. Create learnable position embedding\n        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n                                               requires_grad=True)\n\n        # 7. Create embedding dropout value\n        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n\n        # 8. Create patch embedding layer\n        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n                                              patch_size=patch_size,\n                                              embedding_dim=embedding_dim)\n\n        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n        # Note: The \"*\" means \"all\"\n        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n                                                                            num_heads=num_heads,\n                                                                            mlp_size=mlp_size,\n                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n\n        # 10. Create classifier head\n        self.classifier = nn.Sequential(\n            nn.LayerNorm(normalized_shape=embedding_dim),\n            nn.Linear(in_features=embedding_dim,\n                      out_features=num_classes)\n        )\n\n    # 11. Create a forward() method\n    def forward(self, x):\n\n        # 12. Get batch size\n        batch_size = x.shape[0]\n\n        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n\n        # 14. Create patch embedding (equation 1)\n        x = self.patch_embedding(x)\n\n        # 15. Concat class embedding and patch embedding (equation 1)\n        x = torch.cat((class_token, x), dim=1)\n\n        # 16. Add position embedding to patch embedding (equation 1)\n        x = self.position_embedding + x\n\n        # 17. Run embedding dropout (Appendix B.1)\n        x = self.embedding_dropout(x)\n\n        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)\n        x = self.transformer_encoder(x)\n\n        # 19. Put 0 index logit through classifier (equation 4)\n        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n\n        return x\n</pre> # 1. Create a ViT class that inherits from nn.Module class ViT(nn.Module):     \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"     # 2. Initialize the class with hyperparameters from Table 1 and Table 3     def __init__(self,                  img_size:int=224, # Training resolution from Table 3 in ViT paper                  in_channels:int=3, # Number of channels in input image                  patch_size:int=16, # Patch size                  num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base                  embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base                  mlp_size:int=3072, # MLP size from Table 1 for ViT-Base                  num_heads:int=12, # Heads from Table 1 for ViT-Base                  attn_dropout:float=0, # Dropout for attention projection                  mlp_dropout:float=0.1, # Dropout for dense/MLP layers                  embedding_dropout:float=0.1, # Dropout for patch and position embeddings                  num_classes:int=1000): # Default for ImageNet but can customize this         super().__init__() # don't forget the super().__init__()!          # 3. Make the image size is divisble by the patch size         assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"          # 4. Calculate number of patches (height * width/patch^2)         self.num_patches = (img_size * img_size) // patch_size**2          # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)         self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),                                             requires_grad=True)          # 6. Create learnable position embedding         self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),                                                requires_grad=True)          # 7. Create embedding dropout value         self.embedding_dropout = nn.Dropout(p=embedding_dropout)          # 8. Create patch embedding layer         self.patch_embedding = PatchEmbedding(in_channels=in_channels,                                               patch_size=patch_size,                                               embedding_dim=embedding_dim)          # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())         # Note: The \"*\" means \"all\"         self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,                                                                             num_heads=num_heads,                                                                             mlp_size=mlp_size,                                                                             mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])          # 10. Create classifier head         self.classifier = nn.Sequential(             nn.LayerNorm(normalized_shape=embedding_dim),             nn.Linear(in_features=embedding_dim,                       out_features=num_classes)         )      # 11. Create a forward() method     def forward(self, x):          # 12. Get batch size         batch_size = x.shape[0]          # 13. Create class token embedding and expand it to match the batch size (equation 1)         class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)          # 14. Create patch embedding (equation 1)         x = self.patch_embedding(x)          # 15. Concat class embedding and patch embedding (equation 1)         x = torch.cat((class_token, x), dim=1)          # 16. Add position embedding to patch embedding (equation 1)         x = self.position_embedding + x          # 17. Run embedding dropout (Appendix B.1)         x = self.embedding_dropout(x)          # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 &amp; 3)         x = self.transformer_encoder(x)          # 19. Put 0 index logit through classifier (equation 4)         x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index          return x <ol> <li>\ud83d\udd7a\ud83d\udc83\ud83e\udd73 \u200b\u54c7\u547c\u200b\uff01\u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u6784\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u89c6\u89c9\u200b\u53d8\u6362\u5668\u200b\uff01</li> </ol> <p>\u200b\u771f\u662f\u200b\u8d39\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u529f\u592b\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u4e00\u6b65\u6b65\u200b\u5730\u200b\u521b\u5efa\u200b\u4e86\u200b\u5c42\u200b\u548c\u200b\u5757\u200b\uff0c\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5168\u90e8\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u6784\u5efa\u200b\u4e86\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200bViT\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5feb\u901f\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6f14\u793a\u200b\uff0c\u200b\u5c55\u793a\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u5728\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u4e0a\u200b\u7684\u200b\u6269\u5c55\u200b\u60c5\u51b5\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Example of creating the class embedding and expanding over a batch dimension\nbatch_size = 32\nclass_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token\nclass_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"\n\n# Print out the change in shapes\nprint(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\")\nprint(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\")\n</pre> # Example of creating the class embedding and expanding over a batch dimension batch_size = 32 class_token_embedding_single = nn.Parameter(data=torch.randn(1, 1, 768)) # create a single learnable class token class_token_embedding_expanded = class_token_embedding_single.expand(batch_size, -1, -1) # expand the single learnable class token across the batch dimension, \"-1\" means to \"infer the dimension\"  # Print out the change in shapes print(f\"Shape of class token embedding single: {class_token_embedding_single.shape}\") print(f\"Shape of class token embedding expanded: {class_token_embedding_expanded.shape}\") <pre>Shape of class token embedding single: torch.Size([1, 1, 768])\nShape of class token embedding expanded: torch.Size([32, 1, 768])\n</pre> <p>\u200b\u6ce8\u610f\u200b\u7b2c\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u662f\u200b\u5982\u4f55\u200b\u6269\u5c55\u200b\u5230\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u5176\u4ed6\u200b\u7ef4\u5ea6\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff08\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u662f\u200b\u901a\u8fc7\u200b <code>.expand(batch_size, -1, -1)</code> \u200b\u4e2d\u200b\u7684\u200b \"<code>-1</code>\" \u200b\u7ef4\u5ea6\u200b\u63a8\u65ad\u200b\u51fa\u6765\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b <code>ViT()</code> \u200b\u7c7b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e0e\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u76f8\u540c\u200b\u7684\u200b\u968f\u673a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>ViT</code> \u200b\u7684\u200b\u4e00\u4e2a\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>set_seeds()\n\n# Create a random tensor with same shape as a single image\nrandom_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n\n# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\nvit = ViT(num_classes=len(class_names))\n\n# Pass the random image tensor to our ViT instance\nvit(random_image_tensor)\n</pre> set_seeds()  # Create a random tensor with same shape as a single image random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)  # Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi) vit = ViT(num_classes=len(class_names))  # Pass the random image tensor to our ViT instance vit(random_image_tensor) Out[\u00a0]: <pre>tensor([[-0.2377,  0.7360,  1.2137]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u968f\u673a\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u5df2\u7ecf\u200b\u987a\u5229\u200b\u901a\u8fc7\u200b\u4e86\u200bViT\u200b\u67b6\u6784\u200b\uff0c\u200b\u5e76\u200b\u8f93\u51fa\u200b\u4e86\u200b\u4e09\u4e2a\u200b\u5bf9\u6570\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u4e00\u4e2a\u200b\uff09\u3002</p> <p>\u200b\u800c\u4e14\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b<code>ViT</code>\u200b\u7c7b\u6709\u200b\u8bb8\u591a\u200b\u53ef\u200b\u5b9a\u5236\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u613f\u610f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b<code>img_size</code>\u3001<code>patch_size</code>\u200b\u6216\u200b<code>num_classes</code>\u3002</p> In\u00a0[\u00a0]: Copied! <pre>from torchinfo import summary\n\n# # Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n# summary(model=vit,\n#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> from torchinfo import summary  # # Print a summary of our custom ViT model using torchinfo (uncomment for actual output) # summary(model=vit, #         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width) #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p>\u200b\u73b0\u5728\u200b\u8fd9\u4e9b\u200b\u770b\u8d77\u6765\u200b\u771f\u662f\u200b\u4e0d\u9519\u200b\u7684\u200b\u5c42\u200b\uff01</p> <p>\u200b\u4e5f\u200b\u6765\u200b\u770b\u770b\u200b\u603b\u200b\u7684\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff0c85,800,963\uff0c\u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u8fd9\u4e2a\u200b\u6570\u5b57\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200bPyTorch\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bViT-Base\u200b\u6a21\u578b\u200b\uff08patch\u200b\u5927\u5c0f\u200b\u4e3a\u200b16\uff09\u200b\u5728\u200b<code>torch.vision.models.vit_b_16()</code>\u200b\u4e2d\u200b\u7684\u200b86,567,656\u200b\u4e2a\u200b\u603b\u200b\u53c2\u6570\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u8fd9\u4e9b\u200b\u53c2\u6570\u200b\u662f\u200b\u9488\u5bf9\u200bImageNet\u200b\u4e2d\u200b\u7684\u200b1000\u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09\u3002</p> <p>\u200b\u7ec3\u4e60\u200b\uff1a \u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b<code>ViT()</code>\u200b\u6a21\u578b\u200b\u7684\u200b<code>num_classes</code>\u200b\u53c2\u6570\u200b\u6539\u4e3a\u200b1000\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b<code>torchinfo.summary()</code>\u200b\u521b\u5efa\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u603b\u7ed3\u200b\uff0c\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u7684\u200b\u4ee3\u7801\u200b\u548c\u200b<code>torchvision.models.vit_b_16()</code>\u200b\u4e4b\u95f4\u200b\u7684\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u662f\u5426\u200b\u4e00\u81f4\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k\n                             betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)\n                             weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k\n\n# Setup the loss function for multi-class classification\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set the seeds\nset_seeds()\n\n# Train the model and save the training results to a dictionary\nresults = engine.train(model=vit,\n                       train_dataloader=train_dataloader,\n                       test_dataloader=test_dataloader,\n                       optimizer=optimizer,\n                       loss_fn=loss_fn,\n                       epochs=10,\n                       device=device)\n</pre> from going_modular.going_modular import engine  # Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper optimizer = torch.optim.Adam(params=vit.parameters(),                              lr=3e-3, # Base LR from Table 3 for ViT-* ImageNet-1k                              betas=(0.9, 0.999), # default values but also mentioned in ViT paper section 4.1 (Training &amp; Fine-tuning)                              weight_decay=0.3) # from the ViT paper section 4.1 (Training &amp; Fine-tuning) and Table 3 for ViT-* ImageNet-1k  # Setup the loss function for multi-class classification loss_fn = torch.nn.CrossEntropyLoss()  # Set the seeds set_seeds()  # Train the model and save the training results to a dictionary results = engine.train(model=vit,                        train_dataloader=train_dataloader,                        test_dataloader=test_dataloader,                        optimizer=optimizer,                        loss_fn=loss_fn,                        epochs=10,                        device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 4.8759 | train_acc: 0.2891 | test_loss: 1.0465 | test_acc: 0.5417\nEpoch: 2 | train_loss: 1.5900 | train_acc: 0.2617 | test_loss: 1.5876 | test_acc: 0.1979\nEpoch: 3 | train_loss: 1.4644 | train_acc: 0.2617 | test_loss: 1.2738 | test_acc: 0.1979\nEpoch: 4 | train_loss: 1.3159 | train_acc: 0.2773 | test_loss: 1.7498 | test_acc: 0.1979\nEpoch: 5 | train_loss: 1.3114 | train_acc: 0.3008 | test_loss: 1.7444 | test_acc: 0.2604\nEpoch: 6 | train_loss: 1.2445 | train_acc: 0.3008 | test_loss: 1.9704 | test_acc: 0.1979\nEpoch: 7 | train_loss: 1.2050 | train_acc: 0.3984 | test_loss: 3.5480 | test_acc: 0.1979\nEpoch: 8 | train_loss: 1.4368 | train_acc: 0.4258 | test_loss: 1.8324 | test_acc: 0.2604\nEpoch: 9 | train_loss: 1.5757 | train_acc: 0.2344 | test_loss: 1.2848 | test_acc: 0.5417\nEpoch: 10 | train_loss: 1.4658 | train_acc: 0.4023 | test_loss: 1.2389 | test_acc: 0.2604\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u7ec8\u4e8e\u200b\u6d3b\u200b\u8d77\u6765\u200b\u4e86\u200b\uff01</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b\u4f3c\u4e4e\u200b\u4e0d\u592a\u200b\u7406\u60f3\u200b\u3002</p> <p>\u200b\u6216\u8bb8\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u9057\u6f0f\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u4e1c\u897f\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Plot our ViT model's loss curves\nplot_loss_curves(results)\n</pre> from helper_functions import plot_loss_curves  # Plot our ViT model's loss curves plot_loss_curves(results) <p>\u200b\u55ef\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u6ce2\u52a8\u200b\u5f88\u5927\u200b\u3002</p> <p>\u200b\u81f3\u5c11\u200b\u635f\u5931\u200b\u770b\u8d77\u6765\u200b\u662f\u200b\u5728\u200b\u671d\u7740\u200b\u6b63\u786e\u200b\u7684\u200b\u65b9\u5411\u200b\u53d1\u5c55\u200b\uff0c\u200b\u4f46\u200b\u51c6\u786e\u7387\u200b\u66f2\u7ebf\u200b\u5e76\u200b\u6ca1\u6709\u200b\u663e\u793a\u200b\u51fa\u592a\u591a\u200b\u5e0c\u671b\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u7ed3\u679c\u200b\u5f88\u200b\u53ef\u80fd\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u4e0e\u200bViT\u200b\u8bba\u6587\u200b\u5728\u200b\u6570\u636e\u200b\u8d44\u6e90\u200b\u548c\u200b\u8bad\u7ec3\u200b\u673a\u5236\u200b\u4e0a\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e25\u91cd\u200b\u6b20\u200b\u62df\u5408\u200b\uff08\u200b\u672a\u80fd\u200b\u8fbe\u5230\u200b\u6211\u4eec\u200b\u671f\u671b\u200b\u7684\u200b\u7ed3\u679c\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u8003\u8651\u200b\u5f15\u5165\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre># The following requires torch v0.12+ and torchvision v0.13+\nimport torch\nimport torchvision\nprint(torch.__version__)\nprint(torchvision.__version__)\n</pre> # The following requires torch v0.12+ and torchvision v0.13+ import torch import torchvision print(torch.__version__) print(torchvision.__version__) <pre>1.12.0+cu102\n0.13.0+cu102\n</pre> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[\u00a0]: <pre>'cuda'</pre> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT-Base\uff08\u200b\u8865\u4e01\u200b\u5927\u5c0f\u200b\u4e3a\u200b16\uff09\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u51c6\u5907\u200b\u4e3a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u7528\u4f8b\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u4ece\u200b <code>torchvision.models.ViT_B_16_Weights.DEFAULT</code> \u200b\u83b7\u53d6\u200b\u5728\u200b ImageNet-1k \u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT-Base \u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\uff08<code>DEFAULT</code> \u200b\u8868\u793a\u200b\u6700\u4f73\u200b\u53ef\u7528\u200b\u6743\u91cd\u200b\uff09\u3002</li> <li>\u200b\u901a\u8fc7\u200b <code>torchvision.models.vit_b_16</code> \u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b ViT \u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u4f20\u9012\u200b\u6b65\u9aa4\u200b1\u200b\u4e2d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5c06\u200b\u6b65\u9aa4\u200b2\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u57fa\u7840\u200b ViT \u200b\u6a21\u578b\u200b\u7684\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u7684\u200b <code>requires_grad</code> \u200b\u5c5e\u6027\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>False</code>\uff0c\u200b\u51bb\u7ed3\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u3002</li> <li>\u200b\u66f4\u65b0\u200b\u6b65\u9aa4\u200b2\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\uff0c\u200b\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u5c06\u200b <code>out_features</code> \u200b\u7684\u200b\u6570\u91cf\u200b\u66f4\u200b\u6539\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u200b\uff08\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\uff09\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u5728\u200b 06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u7684\u200b 3.2 \u200b\u8282\u200b\uff1a\u200b\u8bbe\u7f6e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b \u200b\u548c\u200b 3.4 \u200b\u8282\u200b\uff1a\u200b\u51bb\u7ed3\u200b\u57fa\u7840\u200b\u6a21\u578b\u200b\u5e76\u200b\u66f4\u6539\u200b\u8f93\u51fa\u200b\u5c42\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b\u9700\u6c42\u200b \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u6b65\u9aa4\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># 1. Get pretrained weights for ViT-Base\npretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available\n\n# 2. Setup a ViT model instance with pretrained weights\npretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)\n\n# 3. Freeze the base parameters\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n\n# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device)\n# pretrained_vit # uncomment for model output\n</pre> # 1. Get pretrained weights for ViT-Base pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires torchvision &gt;= 0.13, \"DEFAULT\" means best available  # 2. Setup a ViT model instance with pretrained weights pretrained_vit = torchvision.models.vit_b_16(weights=pretrained_vit_weights).to(device)  # 3. Freeze the base parameters for parameter in pretrained_vit.parameters():     parameter.requires_grad = False  # 4. Change the classifier head (set the seeds to ensure same initialization with linear head) set_seeds() pretrained_vit.heads = nn.Linear(in_features=768, out_features=len(class_names)).to(device) # pretrained_vit # uncomment for model output <p>\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u6253\u5370\u200b <code>torchinfo.summary()</code> \u200b\u6765\u200b\u67e5\u770b\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># # Print a summary using torchinfo (uncomment for actual output)\n# summary(model=pretrained_vit,\n#         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n#         # col_names=[\"input_size\"], # uncomment for smaller output\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"]\n# )\n</pre> # # Print a summary using torchinfo (uncomment for actual output) # summary(model=pretrained_vit, #         input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width) #         # col_names=[\"input_size\"], # uncomment for smaller output #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"] # ) <p></p> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff0c\u200b\u53ea\u6709\u200b\u8f93\u51fa\u200b\u5c42\u200b\u662f\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u800c\u200b\u5176\u4f59\u200b\u6240\u6709\u200b\u5c42\u200b\u90fd\u200b\u662f\u200b\u4e0d\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff08\u200b\u51bb\u7ed3\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u5e76\u4e14\u200b\uff0c\u200b\u603b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b85,800,963\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u4f46\u200b<code>pretrained_vit</code>\u200b\u7684\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u8fdc\u8fdc\u200b\u4f4e\u4e8e\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b<code>vit</code>\uff0c\u200b\u4ec5\u200b\u6709\u200b2,307\u200b\u4e2a\u200b\uff0c\u200b\u800c\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b<code>vit</code>\u200b\u6709\u200b85,800,963\u200b\u4e2a\u200b\uff08\u200b\u5728\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b<code>vit</code>\u200b\u4e2d\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\u90fd\u200b\u662f\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u8bad\u7ec3\u200b\u5f97\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u6211\u4eec\u200b\u751a\u81f3\u200b\u53ef\u80fd\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u66f4\u5927\u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8f83\u5c11\u200b\u7684\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u4f1a\u200b\u5360\u7528\u200b\u5185\u5b58\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>from helper_functions import download_data\n\n# Download pizza, steak, sushi images from GitHub\nimage_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",\n                           destination=\"pizza_steak_sushi\")\nimage_path\n</pre> from helper_functions import download_data  # Download pizza, steak, sushi images from GitHub image_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\",                            destination=\"pizza_steak_sushi\") image_path <pre>[INFO] data/pizza_steak_sushi directory exists, skipping download.\n</pre> Out[\u00a0]: <pre>PosixPath('data/pizza_steak_sushi')</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\u7684\u200b\u8def\u5f84\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Setup train and test directory paths\ntrain_dir = image_path / \"train\"\ntest_dir = image_path / \"test\"\ntrain_dir, test_dir\n</pre> # Setup train and test directory paths train_dir = image_path / \"train\" test_dir = image_path / \"test\" train_dir, test_dir Out[\u00a0]: <pre>(PosixPath('data/pizza_steak_sushi/train'),\n PosixPath('data/pizza_steak_sushi/test'))</pre> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5f20\u91cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b DataLoader\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u6765\u81ea\u200b <code>torchvision.models</code> \u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u56e0\u6b64\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b\u5176\u200b <code>transforms()</code> \u200b\u65b9\u6cd5\u200b\u6765\u200b\u83b7\u53d6\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u6253\u7b97\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u901a\u5e38\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\u786e\u4fdd\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u4ee5\u200b\u4e0e\u200b\u539f\u59cb\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u76f8\u540c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b/\u200b\u683c\u5f0f\u5316\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b 06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b 2.2 \u200b\u8282\u200b \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u8fd9\u79cd\u200b\u201c\u200b\u81ea\u52a8\u200b\u201d\u200b\u521b\u5efa\u200b\u8f6c\u6362\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Get automatic transforms from pretrained ViT weights\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)\n</pre> # Get automatic transforms from pretrained ViT weights pretrained_vit_transforms = pretrained_vit_weights.transforms() print(pretrained_vit_transforms) <pre>ImageClassification(\n    crop_size=[224]\n    resize_size=[256]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u53d8\u6362\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b05. PyTorch Going Modular \u200b\u7b2c\u200b2\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>data_setup.create_dataloaders()</code>\u200b\u65b9\u6cd5\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200bDataLoaders\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u7279\u5f81\u63d0\u53d6\u200b\u6a21\u578b\u200b\uff08\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b\u8f83\u200b\u5c11\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u589e\u52a0\u200b\u5230\u200b\u4e00\u4e2a\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u503c\u200b\uff08\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b1024\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6a21\u4eff\u200b\u5728\u200bBetter plain ViT baselines for ImageNet-1k\u200b\u4e00\u6587\u200b\u4e2d\u200b\u53d1\u73b0\u200b\u7684\u200b\u6539\u8fdb\u200b\uff0c\u200b\u8be5\u6587\u200b\u6539\u8fdb\u200b\u4e86\u200b\u539f\u59cb\u200bViT\u200b\u8bba\u6587\u200b\u5e76\u200b\u5efa\u8bae\u200b\u989d\u5916\u200b\u9605\u8bfb\u200b\uff09\u3002\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u603b\u5171\u200b\u53ea\u6709\u200b\u7ea6\u200b200\u200b\u4e2a\u200b\u8bad\u7ec3\u6837\u672c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u575a\u6301\u200b\u4f7f\u7528\u200b32\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Setup dataloaders\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)\n</pre> # Setup dataloaders train_dataloader_pretrained, test_dataloader_pretrained, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                      test_dir=test_dir,                                                                                                      transform=pretrained_vit_transforms,                                                                                                      batch_size=32) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)  In\u00a0[\u00a0]: Copied! <pre>from going_modular.going_modular import engine\n\n# Create optimizer and loss function\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train the classifier head of the pretrained ViT feature extractor model\nset_seeds()\npretrained_vit_results = engine.train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)\n</pre> from going_modular.going_modular import engine  # Create optimizer and loss function optimizer = torch.optim.Adam(params=pretrained_vit.parameters(),                              lr=1e-3) loss_fn = torch.nn.CrossEntropyLoss()  # Train the classifier head of the pretrained ViT feature extractor model set_seeds() pretrained_vit_results = engine.train(model=pretrained_vit,                                       train_dataloader=train_dataloader_pretrained,                                       test_dataloader=test_dataloader_pretrained,                                       optimizer=optimizer,                                       loss_fn=loss_fn,                                       epochs=10,                                       device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7665 | train_acc: 0.7227 | test_loss: 0.5432 | test_acc: 0.8665\nEpoch: 2 | train_loss: 0.3428 | train_acc: 0.9453 | test_loss: 0.3263 | test_acc: 0.8977\nEpoch: 3 | train_loss: 0.2064 | train_acc: 0.9531 | test_loss: 0.2707 | test_acc: 0.9081\nEpoch: 4 | train_loss: 0.1556 | train_acc: 0.9570 | test_loss: 0.2422 | test_acc: 0.9081\nEpoch: 5 | train_loss: 0.1246 | train_acc: 0.9727 | test_loss: 0.2279 | test_acc: 0.8977\nEpoch: 6 | train_loss: 0.1216 | train_acc: 0.9766 | test_loss: 0.2129 | test_acc: 0.9280\nEpoch: 7 | train_loss: 0.0938 | train_acc: 0.9766 | test_loss: 0.2352 | test_acc: 0.8883\nEpoch: 8 | train_loss: 0.0797 | train_acc: 0.9844 | test_loss: 0.2281 | test_acc: 0.8778\nEpoch: 9 | train_loss: 0.1098 | train_acc: 0.9883 | test_loss: 0.2074 | test_acc: 0.9384\nEpoch: 10 | train_loss: 0.0650 | train_acc: 0.9883 | test_loss: 0.1804 | test_acc: 0.9176\n</pre> <p>\u200b\u6211\u200b\u7684\u200b\u5929\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u8868\u73b0\u200b\u8fdc\u8d85\u200b\u6211\u4eec\u200b\u4ece\u96f6\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b ViT \u200b\u6a21\u578b\u200b\uff08\u200b\u5728\u200b\u76f8\u540c\u200b\u65f6\u95f4\u200b\u5185\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u76f4\u89c2\u200b\u5730\u200b\u770b\u770b\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre># Plot the loss curves\nfrom helper_functions import plot_loss_curves\n\nplot_loss_curves(pretrained_vit_results)\n</pre> # Plot the loss curves from helper_functions import plot_loss_curves  plot_loss_curves(pretrained_vit_results) <p>\u200b\u54c7\u585e\u200b\uff01</p> <p>\u200b\u8fd9\u4e9b\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u770b\u8d77\u6765\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\u6559\u79d1\u4e66\u200b\u4e0a\u200b\u7684\u200b\u7406\u60f3\u200b\u66f2\u7ebf\u200b\uff08\u200b\u771f\u7684\u200b\u5f88\u68d2\u200b\uff09\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b8\u200b\u8282\u6765\u200b\u4e86\u89e3\u200b\u7406\u60f3\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u5e94\u8be5\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u5a01\u529b\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u53d6\u5f97\u200b\u4e86\u200b\u51fa\u8272\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200b\u5b9e\u73b0\u200b\u662f\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff08\u200b\u6027\u80fd\u200b\u8f83\u5dee\u200b\uff09\uff0c\u200b\u800c\u200b\u8fd9\u4e2a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u80cc\u540e\u200b\u6709\u7740\u200b\u6765\u81ea\u200bImageNet\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\u3002</p> <p>\u200b\u4f60\u200b\u89c9\u5f97\u200b\u5462\u200b\uff1f</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u8bad\u7ec3\u200b\u8fd9\u4e2a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u6027\u80fd\u200b\u4f1a\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u5417\u200b\uff1f</p> In\u00a0[\u00a0]: Copied! <pre># Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=pretrained_vit,\n                 target_dir=\"models\",\n                 model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\")\n</pre> # Save the model from going_modular.going_modular import utils  utils.save_model(model=pretrained_vit,                  target_dir=\"models\",                  model_name=\"08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\") <pre>[INFO] Saving model to: models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\n</pre> <p>\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u5728\u200b\u8003\u8651\u200b\u90e8\u7f72\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e86\u89e3\u200b\u5b83\u200b\u7684\u200b\u5927\u5c0f\u200b\uff08\u200b\u4ee5\u5146\u200b\u5b57\u8282\u200b\u6216\u200bMB\u200b\u4e3a\u200b\u5355\u4f4d\u200b\uff09\u200b\u4f1a\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200bFood Vision Mini\u200b\u5e94\u7528\u200b\u7a0b\u5e8f\u8fd0\u884c\u200b\u5f97\u200b\u5feb\u200b\uff0c\u200b\u901a\u5e38\u200b\u4e00\u4e2a\u200b\u6027\u80fd\u200b\u826f\u597d\u200b\u4f46\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u4f1a\u200b\u6bd4\u200b\u4e00\u4e2a\u200b\u6027\u80fd\u200b\u6781\u4f73\u200b\u4f46\u200b\u8f83\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u66f4\u4f18\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bPython\u200b\u7684\u200b<code>pathlib.Path().stat()</code>\u200b\u65b9\u6cd5\u200b\u7684\u200b<code>st_size</code>\u200b\u5c5e\u6027\u200b\uff0c\u200b\u540c\u65f6\u200b\u4f20\u5165\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u540d\u200b\uff0c\u200b\u6765\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\uff08\u200b\u4ee5\u200b\u5b57\u8282\u200b\u4e3a\u200b\u5355\u4f4d\u200b\uff09\u3002</p> <p>\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5b57\u8282\u200b\u5927\u5c0f\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5146\u200b\u5b57\u8282\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)\nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_vit_model_size = Path(\"models/08_pretrained_vit_feature_extractor_pizza_steak_sushi.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <pre>Pretrained ViT feature extractor model size: 327 MB\n</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7528\u4e8e\u200bFood Vision Mini\u200b\u7684\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u7ea6\u200b\u4e3a\u200b327 MB\u3002</p> <p>\u200b\u8fd9\u200b\u4e0e\u200b07. PyTorch\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u90e8\u5206\u200b9\u200b\u4e2d\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200b\u5982\u4f55\u200b\uff1f</p> \u200b\u6a21\u578b\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b (MB) \u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b \u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u5ea6\u200b EffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b^ 29 ~0.3906 ~0.9384 ViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b 327 ~0.1084 ~0.9384 <p>\u200b\u6ce8\u610f\u200b\uff1a ^ \u200b\u53c2\u8003\u200b\u4e2d\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u662f\u200b\u4f7f\u7528\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\uff08\u200b\u56fe\u7247\u200b\u6570\u91cf\u200b\u662f\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u4e24\u500d\u200b\uff09\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u800c\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u5219\u200b\u662f\u200b\u4f7f\u7528\u200b10%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u3002\u200b\u4e00\u4e2a\u200b\u7ec3\u4e60\u200b\u662f\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u6570\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\uff0820%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\uff09\u200b\u8bad\u7ec3\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u770b\u200b\u7ed3\u679c\u200b\u80fd\u200b\u63d0\u9ad8\u200b\u591a\u5c11\u200b\u3002</p> <p>EffNetB2\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\u7ea6\u200b\u4e3a\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b1/11\uff0c\u200b\u4f46\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u548c\u200b\u51c6\u786e\u5ea6\u200b\u7ed3\u679c\u200b\u76f8\u4f3c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5f53\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\uff0820%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\uff09\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0cViT\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8fdb\u4e00\u6b65\u63d0\u9ad8\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5728\u200b\u90e8\u7f72\u200b\u65b9\u9762\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6bd4\u8f83\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8003\u8651\u200b\u7684\u200b\u662f\u200b\uff0cViT\u200b\u6a21\u578b\u200b\u989d\u5916\u200b\u63d0\u9ad8\u200b\u7684\u200b\u51c6\u786e\u5ea6\u200b\u662f\u5426\u200b\u503c\u5f97\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u589e\u52a0\u200b\u7ea6\u200b11\u200b\u500d\u200b\uff1f</p> <p>\u200b\u4e5f\u8bb8\u200b\u8fd9\u6837\u200b\u4e00\u4e2a\u200b\u5927\u578b\u200b\u6a21\u578b\u200b\u52a0\u8f7d\u200b/\u200b\u8fd0\u884c\u200b\u65f6\u95f4\u200b\u4f1a\u200b\u66f4\u957f\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4e0d\u4f1a\u200b\u50cf\u200b\u6027\u80fd\u200b\u76f8\u4f3c\u200b\u4f46\u200b\u5c3a\u5bf8\u200b\u5927\u5e45\u200b\u51cf\u5c0f\u200b\u7684\u200bEffNetB2\u200b\u90a3\u6837\u200b\u63d0\u4f9b\u200b\u826f\u597d\u200b\u7684\u200b\u4f53\u9a8c\u200b\u3002</p> In\u00a0[\u00a0]: Copied! <pre>import requests\n\n# Import function to make predictions on images and plot them\nfrom going_modular.going_modular.predictions import pred_and_plot_image\n\n# Setup custom image path\ncustom_image_path = image_path / \"04-pizza-dad.jpeg\"\n\n# Download the image if it doesn't already exist\nif not custom_image_path.is_file():\n    with open(custom_image_path, \"wb\") as f:\n        # When downloading from GitHub, need to use the \"raw\" file link\n        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n        print(f\"Downloading {custom_image_path}...\")\n        f.write(request.content)\nelse:\n    print(f\"{custom_image_path} already exists, skipping download.\")\n\n# Predict on custom image\npred_and_plot_image(model=pretrained_vit,\n                    image_path=custom_image_path,\n                    class_names=class_names)\n</pre> import requests  # Import function to make predictions on images and plot them from going_modular.going_modular.predictions import pred_and_plot_image  # Setup custom image path custom_image_path = image_path / \"04-pizza-dad.jpeg\"  # Download the image if it doesn't already exist if not custom_image_path.is_file():     with open(custom_image_path, \"wb\") as f:         # When downloading from GitHub, need to use the \"raw\" file link         request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")         print(f\"Downloading {custom_image_path}...\")         f.write(request.content) else:     print(f\"{custom_image_path} already exists, skipping download.\")  # Predict on custom image pred_and_plot_image(model=pretrained_vit,                     image_path=custom_image_path,                     class_names=class_names) <pre>data/pizza_steak_sushi/04-pizza-dad.jpeg already exists, skipping download.\n</pre> <p>\u200b\u53cc\u8d5e\u200b\uff01</p> <p>\u200b\u606d\u559c\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4ece\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u4e00\u8def\u200b\u8d70\u5230\u200b\u4e86\u200b\u5728\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u53ef\u7528\u200b\u7684\u200b\u6a21\u578b\u200b\u4ee3\u7801\u200b\uff01</p>"},{"location":"08_pytorch_paper_replicating/#08-pytorch","title":"08. PyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u00b6","text":"<p>\u200b\u6b22\u8fce\u200b\u6765\u5230\u200b\u7b2c\u4e8c\u9636\u6bb5\u200b\u9879\u76ee\u200b\uff1aPyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\uff01</p> <p>\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u590d\u73b0\u200b\u4e00\u7bc7\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b PyTorch \u200b\u4ece\u5934\u5f00\u59cb\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u89c6\u89c9\u200b\u53d8\u6362\u5668\u200b\uff08Vision Transformer\uff0cViT\uff09\u3002</p> <p>\u200b\u968f\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u770b\u5230\u200b\u8fd9\u79cd\u200b\u5148\u8fdb\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u67b6\u6784\u200b ViT \u200b\u5728\u200b FoodVision Mini \u200b\u95ee\u9898\u200b\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b\u3002</p> <p>\u200b\u5728\u200b\u7b2c\u4e8c\u9636\u6bb5\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u91cd\u5efa\u200b\u89c6\u89c9\u200b\u53d8\u6362\u5668\u200b\uff08ViT\uff09\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u67b6\u6784\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5e94\u7528\u200b\u4e8e\u200b FoodVision Mini \u200b\u95ee\u9898\u200b\uff0c\u200b\u4ee5\u200b\u5bf9\u200b\u4e0d\u540c\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\uff1f\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u5feb\u901f\u200b\u53d1\u5c55\u200b\u5df2\u200b\u4e0d\u662f\u200b\u79d8\u5bc6\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u8fdb\u6b65\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u90fd\u200b\u4f1a\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u4e2d\u200b\u53d1\u8868\u200b\u3002</p> <p>\u200b\u800c\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u5229\u7528\u200b\u4ee3\u7801\u200b\u590d\u73b0\u200b\u8fd9\u4e9b\u200b\u8fdb\u6b65\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4f60\u200b\u80fd\u591f\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6280\u672f\u200b\u5e94\u7528\u200b\u4e8e\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5047\u8bbe\u200b\u6709\u200b\u4e00\u79cd\u200b\u65b0\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u53d1\u5e03\u200b\uff0c\u200b\u5b83\u200b\u5728\u200b\u5404\u79cd\u200b\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b\u4e2d\u200b\u8868\u73b0\u200b\u4f18\u4e8e\u200b\u4ee5\u5f80\u200b\u4efb\u4f55\u200b\u67b6\u6784\u200b\uff0c\u200b\u5c06\u200b\u8fd9\u79cd\u200b\u67b6\u6784\u200b\u5e94\u7528\u200b\u5230\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u95ee\u9898\u200b\u4e0a\u200b\u5c82\u200b\u4e0d\u662f\u200b\u5f88\u68d2\u200b\uff1f</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u6d89\u53ca\u200b\u5c06\u200b\u5305\u542b\u200b\u56fe\u50cf\u200b/\u200b\u56fe\u8868\u200b\u3001\u200b\u6570\u5b66\u200b\u548c\u200b\u6587\u672c\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bba\u6587\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u53ef\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u662f\u200b\u53ef\u7528\u200b\u7684\u200bPyTorch\u200b\u4ee3\u7801\u200b\u3002\u200b\u56fe\u8868\u200b\u3001\u200b\u6570\u5b66\u200b\u65b9\u7a0b\u200b\u548c\u200b\u6587\u672c\u200b\u6765\u81ea\u200bViT\u200b\u8bba\u6587\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\uff1f\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u662f\u200b\u4e00\u7bc7\u200b\u79d1\u5b66\u8bba\u6587\u200b\uff0c\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u5728\u200b\u7279\u5b9a\u200b\u9886\u57df\u200b\u7684\u200b\u7814\u7a76\u6210\u679c\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u5185\u5bb9\u200b\u53ef\u80fd\u200b\u56e0\u200b\u8bba\u6587\u200b\u800c\u5f02\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u901a\u5e38\u200b\u9075\u5faa\u200b\u4ee5\u4e0b\u200b\u7ed3\u6784\u200b\uff1a</p> \u200b\u90e8\u5206\u200b \u200b\u5185\u5bb9\u200b \u200b\u6458\u8981\u200b \u200b\u8bba\u6587\u200b\u4e3b\u8981\u200b\u53d1\u73b0\u200b/\u200b\u8d21\u732e\u200b\u7684\u200b\u6982\u8ff0\u200b/\u200b\u603b\u7ed3\u200b\u3002 \u200b\u5f15\u8a00\u200b \u200b\u8bba\u6587\u200b\u7684\u200b\u4e3b\u8981\u200b\u95ee\u9898\u200b\u53ca\u200b\u4ee5\u5f80\u200b\u7528\u4e8e\u200b\u5c1d\u8bd5\u200b\u89e3\u51b3\u200b\u8be5\u200b\u95ee\u9898\u200b\u7684\u200b\u65b9\u6cd5\u200b\u7ec6\u8282\u200b\u3002 \u200b\u65b9\u6cd5\u200b \u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u662f\u200b\u5982\u4f55\u200b\u8fdb\u884c\u200b\u7814\u7a76\u200b\u7684\u200b\uff1f\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f7f\u7528\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u6a21\u578b\u200b\u3001\u200b\u6570\u636e\u6e90\u200b\u3001\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\uff1f \u200b\u7ed3\u679c\u200b \u200b\u8bba\u6587\u200b\u7684\u200b\u6210\u679c\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\u200b\u5982\u679c\u200b\u4f7f\u7528\u200b\u4e86\u200b\u65b0\u200b\u7c7b\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u6216\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5176\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u4ee5\u5f80\u200b\u7814\u7a76\u200b\u76f8\u6bd4\u200b\u5982\u4f55\u200b\uff1f\uff08\u200b\u8fd9\u91cc\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\uff09 \u200b\u7ed3\u8bba\u200b \u200b\u6240\u200b\u63d0\u51fa\u200b\u65b9\u6cd5\u200b\u7684\u200b\u5c40\u9650\u6027\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\u200b\u7814\u7a76\u200b\u793e\u533a\u200b\u7684\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f \u200b\u53c2\u8003\u6587\u732e\u200b \u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u53c2\u8003\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u8d44\u6e90\u200b/\u200b\u5176\u4ed6\u200b\u8bba\u6587\u200b\u6765\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u7814\u7a76\u6210\u679c\u200b\uff1f \u200b\u9644\u5f55\u200b \u200b\u662f\u5426\u200b\u6709\u200b\u672a\u200b\u5305\u542b\u200b\u5728\u200b\u4e0a\u8ff0\u200b\u4efb\u4f55\u200b\u90e8\u5206\u200b\u4e2d\u200b\u7684\u200b\u989d\u5916\u200b\u8d44\u6e90\u200b/\u200b\u53d1\u73b0\u200b\uff1f"},{"location":"08_pytorch_paper_replicating/","title":"\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u590d\u73b0\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\uff1f\u00b6","text":"<p>\u200b\u4e00\u7bc7\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u901a\u5e38\u200b\u662f\u200b\u4e16\u754c\u200b\u4e0a\u200b\u6700\u200b\u4f18\u79c0\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u56e2\u961f\u200b\u6570\u6708\u200b\u5de5\u4f5c\u200b\u548c\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u6d53\u7f29\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u88ab\u200b\u7cbe\u7b80\u200b\u6210\u200b\u51e0\u9875\u200b\u6587\u5b57\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u8fd9\u4e9b\u200b\u5b9e\u9a8c\u200b\u5728\u200b\u4f60\u200b\u6240\u200b\u7814\u7a76\u200b\u7684\u200b\u95ee\u9898\u200b\u76f8\u5173\u200b\u9886\u57df\u200b\u53d6\u5f97\u200b\u4e86\u200b\u66f4\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u53bb\u200b\u68c0\u9a8c\u200b\u4e00\u4e0b\u200b\u5b83\u4eec\u200b\u662f\u200b\u5f88\u200b\u6709\u200b\u610f\u4e49\u200b\u7684\u200b\u3002</p> <p>\u200b\u6b64\u5916\u200b\uff0c\u200b\u590d\u73b0\u200b\u4ed6\u4eba\u200b\u7684\u200b\u5de5\u4f5c\u200b\u662f\u200b\u953b\u70bc\u200b\u4f60\u200b\u6280\u80fd\u200b\u7684\u200b\u7edd\u4f73\u200b\u65b9\u5f0f\u200b\u3002</p> <p></p> <p>\u200b\u4e54\u6cbb\u200b\u00b7\u200b\u970d\u8328\u200b\u662f\u200bcomma.ai\u200b\u7684\u200b\u521b\u59cb\u4eba\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u5bb6\u200b\u81ea\u52a8\u200b\u9a7e\u9a76\u200b\u6c7d\u8f66\u200b\u516c\u53f8\u200b\uff0c\u200b\u4ed6\u200b\u5728\u200bTwitch\u200b\u4e0a\u200b\u76f4\u64ad\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7f16\u7a0b\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u89c6\u9891\u200b\u5b8c\u6574\u200b\u5730\u200b\u53d1\u5e03\u200b\u5728\u200bYouTube\u200b\u4e0a\u200b\u3002\u200b\u6211\u200b\u4ece\u200b\u4ed6\u200b\u7684\u200b\u4e00\u6b21\u200b\u76f4\u64ad\u200b\u4e2d\u200b\u6458\u5f55\u200b\u4e86\u200b\u8fd9\u53e5\u200b\u8bdd\u200b\u3002\"\u066d\"\u200b\u8868\u793a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\u901a\u5e38\u200b\u6d89\u53ca\u200b\u989d\u5916\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u5982\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u548c\u200b\u4f7f\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u200b\u4f9b\u200b\u4ed6\u4eba\u200b\u4f7f\u7528\u200b\uff08\u200b\u90e8\u7f72\u200b\uff09\u3002</p> <p>\u200b\u5f53\u200b\u4f60\u200b\u521a\u200b\u5f00\u59cb\u200b\u5c1d\u8bd5\u200b\u590d\u73b0\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u65f6\u200b\uff0c\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u611f\u5230\u200b\u4e0d\u77e5\u6240\u63aa\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u6b63\u5e38\u200b\u7684\u200b\u3002</p> <p>\u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u82b1\u8d39\u200b\u6570\u5468\u200b\u3001\u200b\u6570\u6708\u200b\u751a\u81f3\u200b\u6570\u5e74\u200b\u65f6\u95f4\u200b\u6765\u200b\u521b\u9020\u200b\u8fd9\u4e9b\u200b\u6210\u679c\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5982\u679c\u200b\u4f60\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u65f6\u95f4\u200b\u6765\u200b\u9605\u8bfb\u200b\u751a\u81f3\u200b\u91cd\u73b0\u200b\u8fd9\u4e9b\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u8fd9\u662f\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u7684\u200b\u3002</p> <p>\u200b\u590d\u73b0\u200b\u7814\u7a76\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5982\u6b64\u200b\u8270\u5de8\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4ee5\u81f3\u4e8e\u200b\u8bde\u751f\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u5353\u8d8a\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5e93\u200b\u548c\u200b\u5de5\u5177\u200b\uff0c\u200b\u5982\u200bHuggingFace\u3001PyTorch Image Models\uff08<code>timm</code>\u200b\u5e93\u200b\uff09\u200b\u548c\u200bfast.ai\uff0c\u200b\u5b83\u4eec\u200b\u81f4\u529b\u4e8e\u200b\u4f7f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u66f4\u52a0\u200b\u6613\u4e8e\u200b\u8bbf\u95ee\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/","title":"\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b\uff1f\u00b6","text":"<p>\u200b\u5f53\u200b\u4f60\u200b\u5f00\u59cb\u200b\u63a5\u89e6\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u65f6\u200b\uff0c\u200b\u9996\u5148\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u7684\u200b\u662f\u200b\uff1a\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u7814\u7a76\u200b\u975e\u5e38\u200b\u591a\u200b\u3002</p> <p>\u200b\u6240\u4ee5\u200b\u8981\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8bd5\u56fe\u200b\u8ddf\u4e0a\u200b\u8fd9\u200b\u4e00\u5207\u200b\u5c31\u200b\u50cf\u662f\u200b\u8bd5\u56fe\u200b\u8dd1\u200b\u8d62\u200b\u4ed3\u9f20\u200b\u8f6e\u200b\u4e00\u6837\u200b\u3002</p> <p>\u200b\u8ddf\u968f\u200b\u4f60\u200b\u7684\u200b\u5174\u8da3\u200b\uff0c\u200b\u6311\u9009\u200b\u4e00\u4e9b\u200b\u5bf9\u200b\u4f60\u200b\u6765\u8bf4\u200b\u7a81\u51fa\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u8bf4\u6765\u200b\uff0c\u200b\u6709\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\u53ef\u4ee5\u200b\u627e\u5230\u200b\u5e76\u200b\u9605\u8bfb\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\uff08\u200b\u53ca\u5176\u200b\u4ee3\u7801\u200b\uff09\uff1a</p> \u200b\u8d44\u6e90\u200b \u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f arXiv \u200b\u53d1\u97f3\u200b\u4e3a\u200b \"archive\"\uff0carXiv \u200b\u662f\u200b\u4e00\u4e2a\u200b\u514d\u8d39\u200b\u4e14\u200b\u5f00\u653e\u200b\u7684\u200b\u8d44\u6e90\u200b\uff0c\u200b\u7528\u4e8e\u200b\u9605\u8bfb\u200b\u4ece\u200b\u7269\u7406\u5b66\u200b\u5230\u200b\u8ba1\u7b97\u673a\u79d1\u5b66\u200b\uff08\u200b\u5305\u62ec\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff09\u200b\u7684\u200b\u5404\u79cd\u200b\u6280\u672f\u200b\u6587\u7ae0\u200b\u3002 AK Twitter AK Twitter \u200b\u8d26\u53f7\u200b\u53d1\u5e03\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u4eae\u70b9\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u6bcf\u5929\u200b\u90fd\u200b\u6709\u200b\u5b9e\u65f6\u200b\u6f14\u793a\u200b\u3002\u200b\u6211\u200b\u4e0d\u200b\u7406\u89e3\u200b 9/10 \u200b\u7684\u200b\u5e16\u5b50\u200b\uff0c\u200b\u4f46\u200b\u6211\u200b\u53d1\u73b0\u200b\u5076\u5c14\u200b\u63a2\u7d22\u200b\u4e00\u4e0b\u200b\u5f88\u200b\u6709\u8da3\u200b\u3002 Papers with Code \u200b\u4e00\u4e2a\u200b\u7cbe\u9009\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bba\u6587\u200b\u96c6\u5408\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bb8\u591a\u200b\u8bba\u6587\u200b\u9644\u6709\u200b\u4ee3\u7801\u200b\u8d44\u6e90\u200b\u3002\u200b\u8fd8\u200b\u5305\u62ec\u200b\u4e00\u7cfb\u5217\u200b\u5e38\u89c1\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u96c6\u200b\u3001\u200b\u57fa\u51c6\u200b\u548c\u200b\u5f53\u524d\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u6a21\u578b\u200b\u3002 lucidrains \u200b\u7684\u200b <code>vit-pytorch</code> GitHub \u200b\u4ed3\u5e93\u200b \u200b\u4e0e\u5176\u8bf4\u662f\u200b\u4e00\u4e2a\u200b\u5bfb\u627e\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u5730\u65b9\u200b\uff0c\u200b\u4e0d\u5982\u8bf4\u662f\u200b\u4e00\u4e2a\u200b\u5927\u89c4\u6a21\u200b\u3001\u200b\u7279\u5b9a\u200b\u7126\u70b9\u200b\u4e0b\u7528\u200b\u4ee3\u7801\u200b\u590d\u73b0\u200b\u8bba\u6587\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002<code>vit-pytorch</code> \u200b\u4ed3\u5e93\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6536\u96c6\u200b\u4e86\u200b\u6765\u81ea\u200b\u5404\u79cd\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b Vision Transformer \u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff0c\u200b\u7528\u200b PyTorch \u200b\u4ee3\u7801\u200b\u590d\u73b0\u200b\u7684\u200b\u96c6\u5408\u200b\uff08\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u8bb8\u591a\u200b\u7075\u611f\u200b\u6765\u81ea\u200b\u4e8e\u200b\u6b64\u200b\u4ed3\u5e93\u200b\uff09\u3002 <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e2a\u200b\u5217\u8868\u200b\u8fdc\u975e\u200b\u8be6\u5c3d\u200b\u3002\u200b\u6211\u200b\u53ea\u200b\u5217\u51fa\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u5730\u65b9\u200b\uff0c\u200b\u662f\u200b\u6211\u200b\u4e2a\u4eba\u200b\u6700\u200b\u5e38\u200b\u4f7f\u7528\u200b\u7684\u200b\u3002\u200b\u6240\u4ee5\u200b\u8981\u200b\u6ce8\u610f\u200b\u504f\u89c1\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u200b\u53d1\u73b0\u200b\u5373\u4f7f\u200b\u662f\u200b\u8fd9\u4e2a\u200b\u7b80\u77ed\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u4e5f\u200b\u5e38\u5e38\u200b\u80fd\u200b\u6ee1\u8db3\u200b\u6211\u200b\u5bf9\u200b\u9886\u57df\u200b\u5185\u200b\u52a8\u6001\u200b\u7684\u200b\u4e86\u89e3\u200b\u9700\u6c42\u200b\u3002\u200b\u518d\u200b\u591a\u4e00\u4e9b\u200b\uff0c\u200b\u6211\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u75af\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/","title":"\u6211\u4eec\u200b\u5c06\u8981\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u4e0e\u5176\u200b\u8ba8\u8bba\u200b\u5982\u4f55\u200b\u590d\u5236\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u5982\u200b\u4eb2\u81ea\u52a8\u624b\u200b\uff0c\u200b\u771f\u6b63\u200b\u5730\u200b\u590d\u5236\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\u3002</p> <p>\u200b\u590d\u5236\u200b\u6240\u6709\u200b\u8bba\u6587\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u4f1a\u200b\u6709\u6240\u4e0d\u540c\u200b\uff0c\u200b\u4f46\u200b\u901a\u8fc7\u200b\u4f53\u9a8c\u200b\u4e00\u6b21\u200b\u590d\u5236\u200b\u7684\u200b\u8fc7\u7a0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u83b7\u5f97\u200b\u7ee7\u7eed\u200b\u590d\u5236\u200b\u7684\u200b\u52a8\u529b\u200b\u3002</p> <p>\u200b\u66f4\u200b\u5177\u4f53\u5730\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u590d\u5236\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200bAn Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\uff08ViT\u200b\u8bba\u6587\u200b\uff09\uff0c\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5b9e\u73b0\u200b\u3002</p> <p>Transformer\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\u6700\u521d\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200bAttention is all you need\u200b\u4e2d\u200b\u88ab\u200b\u4ecb\u7ecd\u200b\u3002</p> <p>\u200b\u800c\u200b\u6700\u521d\u200b\u7684\u200bTransformer\u200b\u67b6\u6784\u200b\u88ab\u200b\u8bbe\u8ba1\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u4e00\u7ef4\u200b\uff081D\uff09\u200b\u7684\u200b\u6587\u672c\u200b\u5e8f\u5217\u200b\u3002</p> <p>Transformer\u200b\u67b6\u6784\u200b\u901a\u5e38\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u4efb\u4f55\u200b\u4f7f\u7528\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b)\u200b\u4f5c\u4e3a\u200b\u5176\u200b\u4e3b\u8981\u200b\u5b66\u4e60\u200b\u5c42\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08CNN\uff09\u200b\u4f7f\u7528\u200b\u5377\u79ef\u200b\u4f5c\u4e3a\u200b\u5176\u200b\u4e3b\u8981\u200b\u5b66\u4e60\u200b\u5c42\u200b\u3002</p> <p>\u200b\u6b63\u5982\u200b\u540d\u79f0\u200b\u6240\u793a\u200b\uff0cVision Transformer\uff08ViT\uff09\u200b\u67b6\u6784\u200b\u65e8\u5728\u200b\u5c06\u200b\u539f\u59cb\u200bTransformer\u200b\u67b6\u6784\u200b\u9002\u5e94\u200b\u4e8e\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\uff08\u200b\u5206\u7c7b\u200b\u662f\u200b\u7b2c\u4e00\u4e2a\u200b\uff0c\u200b\u6b64\u540e\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u95ee\u9898\u200b\u4e5f\u200b\u968f\u4e4b\u800c\u6765\u200b\uff09\u3002</p> <p>\u200b\u6700\u521d\u200b\u7684\u200bVision Transformer\u200b\u5728\u200b\u8fc7\u53bb\u200b\u51e0\u5e74\u200b\u4e2d\u200b\u7ecf\u5386\u200b\u4e86\u200b\u51e0\u6b21\u200b\u8fed\u4ee3\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u590d\u5236\u200b\u6700\u521d\u200b\u7684\u200b\u7248\u672c\u200b\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u201c\u200b\u539f\u59cb\u200bVision Transformer\u201d\u3002\u200b\u56e0\u4e3a\u200b\u5982\u679c\u200b\u4f60\u200b\u80fd\u200b\u91cd\u73b0\u200b\u539f\u59cb\u200b\u7248\u672c\u200b\uff0c\u200b\u4f60\u200b\u5c31\u200b\u80fd\u200b\u9002\u5e94\u200b\u5176\u4ed6\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u6309\u7167\u200b\u539f\u59cb\u200bViT\u200b\u8bba\u6587\u200b\u6784\u5efa\u200bViT\u200b\u67b6\u6784\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u5e94\u7528\u200b\u4e8e\u200bFoodVision Mini\u3002</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b \u200b\u6211\u4eec\u200b\u5728\u200b\u8fc7\u53bb\u200b\u7684\u200b\u51e0\u4e2a\u200b\u90e8\u5206\u200b\u4e2d\u200b\u7f16\u5199\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u6709\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u5b83\u200b\u5e76\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002 1. \u200b\u83b7\u53d6\u6570\u636e\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u83b7\u53d6\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5e76\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200bVision Transformer\u200b\u6765\u200b\u5c1d\u8bd5\u200b\u6539\u8fdb\u200bFoodVision Mini\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002 2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b \u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5728\u200b\u7b2c\u200b05\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u7f16\u5199\u200b\u7684\u200b<code>data_setup.py</code>\u200b\u811a\u672c\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3002 3. \u200b\u590d\u5236\u200bViT\u200b\u8bba\u6587\u200b\uff1a\u200b\u6982\u8ff0\u200b \u200b\u590d\u5236\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u76f8\u5f53\u200b\u5927\u200b\u7684\u200b\u6311\u6218\u200b\uff0c\u200b\u6240\u4ee5\u200b\u5728\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200bViT\u200b\u8bba\u6587\u200b\u5206\u89e3\u6210\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9010\u200b\u90e8\u5206\u200b\u590d\u5236\u200b\u8bba\u6587\u200b\u3002 4. \u200b\u516c\u5f0f\u200b1\uff1aPatch Embedding ViT\u200b\u67b6\u6784\u200b\u7531\u200b\u56db\u4e2a\u200b\u4e3b\u8981\u200b\u516c\u5f0f\u200b\u7ec4\u6210\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200b\u662f\u200bpatch\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u3002\u200b\u6216\u8005\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7cfb\u5217\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200bpatch\u3002 5. \u200b\u516c\u5f0f\u200b2\uff1a\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09 \u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b/\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09\u200b\u673a\u5236\u200b\u662f\u200b\u6bcf\u4e2a\u200bTransformer\u200b\u67b6\u6784\u200b\u7684\u200b\u6838\u5fc3\u200b\uff0c\u200b\u5305\u62ec\u200bViT\u200b\u67b6\u6784\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bPyTorch\u200b\u7684\u200b\u5185\u7f6e\u200b\u5c42\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200bMSA\u200b\u5757\u200b\u3002 6. \u200b\u516c\u5f0f\u200b3\uff1a\u200b\u591a\u5c42\u200b\u611f\u77e5\u5668\u200b\uff08MLP\uff09 ViT\u200b\u67b6\u6784\u200b\u4f7f\u7528\u200b\u591a\u5c42\u200b\u611f\u77e5\u5668\u200b\u4f5c\u4e3a\u200b\u5176\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u53ca\u5176\u200b\u8f93\u51fa\u200b\u5c42\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4e3a\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200bMLP\u3002 7. \u200b\u521b\u5efa\u200bTransformer\u200b\u7f16\u7801\u5668\u200b Transformer\u200b\u7f16\u7801\u5668\u200b\u901a\u5e38\u200b\u7531\u200bMSA\uff08\u200b\u516c\u5f0f\u200b2\uff09\u200b\u548c\u200bMLP\uff08\u200b\u516c\u5f0f\u200b3\uff09\u200b\u4ea4\u66ff\u200b\u5c42\u200b\u7ec4\u6210\u200b\uff0c\u200b\u901a\u8fc7\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u8fde\u63a5\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5c06\u200b\u7b2c\u200b5\u200b\u8282\u200b\u548c\u200b\u7b2c\u200b6\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u5c42\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u200b\u8d77\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u3002 8. \u200b\u5c06\u200b\u6240\u6709\u200b\u90e8\u5206\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\u521b\u5efa\u200bViT \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u521b\u5efa\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u6240\u6709\u200b\u62fc\u56fe\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u5168\u90e8\u200b\u7ec4\u5408\u200b\u5230\u200b\u4e00\u4e2a\u200b\u7c7b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8c03\u7528\u200b\u5b83\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002 9. \u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b \u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200bViT\u200b\u5b9e\u73b0\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6240\u6709\u200b\u5176\u4ed6\u200b\u6a21\u578b\u200b\u3002\u200b\u5e76\u4e14\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5728\u200b<code>engine.py</code>\u200b\u4e2d\u200b\u7684\u200b<code>train()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u7528\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u3002 10. \u200b\u4f7f\u7528\u200b<code>torchvision.models</code>\u200b\u4e2d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200bViT \u200b\u8bad\u7ec3\u200b\u50cf\u200bViT\u200b\u8fd9\u6837\u200b\u7684\u200b\u5927\u578b\u200b\u6a21\u578b\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u5927\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u53ea\u200b\u5904\u7406\u200b\u5c11\u91cf\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u5229\u7528\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u529b\u91cf\u200b\u6765\u200b\u63d0\u9ad8\u200b\u6211\u4eec\u200b\u7684\u200b\u6027\u80fd\u200b\u3002 11. \u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u795e\u5947\u200b\u4e4b\u200b\u5904\u200b\u5728\u4e8e\u200b\u770b\u5230\u200b\u5b83\u200b\u5728\u200b\u60a8\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91c7\u7528\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u81ed\u540d\u662d\u8457\u200b\u7684\u200bpizza-dad\u200b\u56fe\u50cf\u200b\uff08\u200b\u4e00\u5f20\u200b\u6211\u200b\u7238\u7238\u200b\u5403\u200b\u62ab\u8428\u200b\u7684\u200b\u7167\u7247\u200b\uff09\u200b\u4e0a\u200b\u6d4b\u8bd5\u200bFoodVision Mini\u3002 <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u590d\u5236\u200bViT\u200b\u8bba\u6587\u200b\uff0c\u200b\u4f46\u200b\u8981\u200b\u907f\u514d\u200b\u8fc7\u4e8e\u200b\u6c89\u8ff7\u4e8e\u200b\u67d0\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u66f4\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u5f80\u5f80\u200b\u4f1a\u200b\u5f88\u5feb\u200b\u51fa\u73b0\u200b\uff0c\u200b\u6240\u4ee5\u200b\u6280\u80fd\u200b\u5e94\u8be5\u200b\u662f\u200b\u4fdd\u6301\u200b\u597d\u5947\u5fc3\u200b\uff0c\u200b\u540c\u65f6\u200b\u5efa\u7acb\u200b\u5c06\u200b\u6570\u5b66\u200b\u548c\u200b\u9875\u9762\u200b\u4e0a\u200b\u7684\u200b\u6587\u5b57\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u5de5\u4f5c\u200b\u4ee3\u7801\u200b\u7684\u200b\u57fa\u672c\u6280\u80fd\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/","title":"\u672f\u8bed\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u5c06\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u76f8\u5f53\u200b\u591a\u200b\u7684\u200b\u7f29\u5199\u200b\u8bcd\u200b\u3002</p> <p>\u200b\u9274\u4e8e\u200b\u6b64\u200b\uff0c\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e9b\u200b\u5b9a\u4e49\u200b\uff1a</p> <ul> <li>ViT - \u200b\u4ee3\u8868\u200b Vision Transformer\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e3b\u8981\u200b\u5173\u6ce8\u200b\u590d\u5236\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\uff09\u3002</li> <li>ViT \u200b\u8bba\u6587\u200b - \u200b\u6307\u200b\u4ecb\u7ecd\u200b ViT \u200b\u67b6\u6784\u200b\u7684\u200b\u539f\u59cb\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u7b80\u79f0\u200b\uff0c\u200b\u5373\u200b An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\u3002\u200b\u6bcf\u5f53\u200b\u63d0\u5230\u200b ViT \u200b\u8bba\u6587\u200b \u200b\u65f6\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u786e\u4fe1\u200b\u5b83\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u8fd9\u7bc7\u200b\u8bba\u6587\u200b\u3002</li> </ul>"},{"location":"08_pytorch_paper_replicating/","title":"\u5982\u4f55\u200b\u83b7\u53d6\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub Discussions \u200b\u9875\u9762\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200b PyTorch \u200b\u6587\u6863\u200b\u548c\u200b PyTorch \u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6240\u6709\u200b PyTorch \u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#0","title":"0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u00b6","text":"<p>\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u786e\u4fdd\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u4e86\u200b\u672c\u8282\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u5757\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bfc\u5165\u200b\u5728\u200b 05. PyTorch \u200b\u6a21\u5757\u5316\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b Python \u200b\u811a\u672c\u200b\uff08\u200b\u5982\u200b <code>data_setup.py</code> \u200b\u548c\u200b <code>engine.py</code>\uff09\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>pytorch-deep-learning</code> \u200b\u4ed3\u5e93\u200b\u4e0b\u8f7d\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u4e0b\u8f7d\u200b\u7684\u8bdd\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u83b7\u53d6\u200b <code>torchinfo</code> \u200b\u5305\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u5b89\u88c5\u200b\uff09\u3002</p> <p><code>torchinfo</code> \u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u76f4\u89c2\u200b\u5730\u200b\u5c55\u793a\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u7531\u4e8e\u200b\u7a0d\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torchvision</code> v0.13 \u200b\u7248\u672c\u200b\uff08\u200b\u81ea\u200b 2022 \u200b\u5e74\u200b 7 \u200b\u6708\u200b\u8d77\u200b\u53ef\u7528\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u786e\u4fdd\u200b\u5df2\u200b\u5b89\u88c5\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u4f7f\u7528\u200b FoodVision Mini\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u90e8\u5206\u200b 1\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>helper_functions.py</code> \u200b\u4e2d\u200b\u7684\u200b <code>download_data()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b <code>source</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>pizza_steak_sushi.zip</code> \u200b\u6570\u636e\u200b\u7684\u200b\u539f\u59cb\u200b GitHub \u200b\u94fe\u63a5\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b <code>destination</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>pizza_steak_sushi</code>\u3002</p>"},{"location":"08_pytorch_paper_replicating/#2","title":"2. \u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>data_setup.py</code> \u200b\u4e2d\u200b\u7684\u200b <code>create_dataloaders()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u8f6c\u6362\u200b\u6765\u200b\u51c6\u5907\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u8fd9\u662f\u200b\u7b2c\u4e00\u4e2a\u200b\u53c2\u8003\u200b ViT \u200b\u8bba\u6587\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p> <p>\u200b\u5728\u200b\u8868\u200b 3 \u200b\u4e2d\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u5206\u8fa8\u7387\u200b\u88ab\u200b\u63d0\u53ca\u200b\u4e3a\u200b 224\uff08\u200b\u9ad8\u5ea6\u200b=224\uff0c\u200b\u5bbd\u5ea6\u200b=224\uff09\u3002</p> <p></p> <p>\u200b\u4f60\u200b\u901a\u5e38\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8868\u683c\u200b\u4e2d\u200b\u627e\u5230\u200b\u5404\u79cd\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecd\u200b\u5728\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u5173\u6ce8\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u548c\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u7b49\u200b\u4e8b\u9879\u200b\u3002\u200b\u6765\u6e90\u200b\uff1aViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u8868\u200b 3\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u8f6c\u6362\u200b\u9002\u5f53\u200b\u5730\u200b\u8c03\u6574\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u4e00\u200b\u5f00\u59cb\u200b\u4e0d\u200b\u8fdb\u884c\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u4f1a\u200b\u50cf\u200b\u5728\u200b 06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u7b2c\u200b 2.1 \u200b\u8282\u200b \u200b\u4e2d\u200b\u90a3\u6837\u200b\u63d0\u4f9b\u200b <code>normalize</code> \u200b\u8f6c\u6362\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#21","title":"2.1 \u200b\u51c6\u5907\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u00b6","text":""},{"location":"08_pytorch_paper_replicating/#22-dataloader","title":"2.2 \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u00b6","text":"<p>\u200b\u53d8\u6362\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b <code>DataLoader</code>\u3002</p> <p>ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u63d0\u5230\u200b\u4f7f\u7528\u200b 4096 \u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u4f7f\u7528\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff0832\uff09\u200b\u7684\u200b 128 \u200b\u500d\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u575a\u6301\u200b\u4f7f\u7528\u200b 32 \u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u4e00\u4e9b\u200b\u786c\u4ef6\u200b\uff08\u200b\u5305\u62ec\u200b Google Colab \u200b\u7684\u200b\u514d\u8d39\u200b\u5c42\u7ea7\u200b\uff09\u200b\u53ef\u80fd\u200b\u65e0\u6cd5\u200b\u5904\u7406\u200b 4096 \u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b 4096 \u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u610f\u5473\u7740\u200b\u4e00\u6b21\u200b\u9700\u8981\u200b\u5c06\u200b 4096 \u200b\u5f20\u200b\u56fe\u50cf\u200b\u653e\u5165\u200b GPU \u200b\u5185\u5b58\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u5f53\u200b\u4f60\u200b\u62e5\u6709\u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u7684\u200b\u786c\u4ef6\u200b\u65f6\u200b\uff0c\u200b\u6bd4\u5982\u200b\u8c37\u6b4c\u200b\u7684\u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u7ecf\u5e38\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u8fd9\u662f\u200b\u53ef\u884c\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5f53\u200b\u4f60\u200b\u5728\u200b\u4e00\u53f0\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u65f6\u200b\uff08\u200b\u6bd4\u5982\u200b\u4f7f\u7528\u200b Google Colab\uff09\uff0c\u200b\u786e\u4fdd\u200b\u5728\u200b\u5c0f\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u4e0b\u5148\u200b\u8ba9\u200b\u4e8b\u60c5\u200b\u8fd0\u884c\u200b\u8d77\u6765\u200b\u662f\u200b\u4e2a\u200b\u597d\u200b\u4e3b\u610f\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u9879\u76ee\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6269\u5c55\u200b\u53ef\u80fd\u200b\u662f\u200b\u5c1d\u8bd5\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u503c\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u5728\u200b <code>create_dataloaders()</code> \u200b\u51fd\u6570\u200b\u4e2d\u200b\u4f7f\u7528\u200b <code>pin_memory=True</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u52a0\u901f\u200b\u8ba1\u7b97\u200b\u3002<code>pin_memory=True</code> \u200b\u901a\u8fc7\u200b\u201c\u200b\u56fa\u5b9a\u200b\u201d\u200b\u4e4b\u524d\u200b\u89c1\u200b\u8fc7\u200b\u7684\u200b\u793a\u4f8b\u200b\uff0c\u200b\u907f\u514d\u200b\u4e86\u200b CPU \u200b\u548c\u200b GPU \u200b\u5185\u5b58\u200b\u4e4b\u95f4\u200b\u4e0d\u5fc5\u8981\u200b\u7684\u200b\u5185\u5b58\u200b\u590d\u5236\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u8fd9\u79cd\u200b\u597d\u5904\u200b\u53ef\u80fd\u200b\u5728\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u5927\u5c0f\u200b\u4e0b\u200b\u624d\u80fd\u200b\u770b\u5230\u200b\uff08\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6570\u636e\u200b\u96c6\u200b\u76f8\u5f53\u200b\u5c0f\u200b\uff09\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8bbe\u7f6e\u200b <code>pin_memory=True</code> \u200b\u5e76\u200b\u4e0d\u200b\u603b\u662f\u200b\u80fd\u200b\u63d0\u9ad8\u200b\u6027\u80fd\u200b\uff08\u200b\u8fd9\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u90a3\u4e9b\u200b\u6709\u65f6\u200b\u6709\u6548\u200b\u6709\u65f6\u200b\u65e0\u6548\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e4b\u4e00\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u6700\u597d\u200b\u662f\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u518d\u200b\u5b9e\u9a8c\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b PyTorch \u200b\u7684\u200b <code>torch.utils.data.DataLoader</code> \u200b\u6587\u6863\u200b \u200b\u6216\u200b Horace He \u200b\u7684\u200b Making Deep Learning Go Brrrr from First Principles\u3002</p>"},{"location":"08_pytorch_paper_replicating/#23","title":"2.3 \u200b\u53ef\u89c6\u5316\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u52a0\u8f7d\u200b\u4e86\u200b\u6570\u636e\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u518d\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>ViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u6b65\u9aa4\u200b\u662f\u200b\u5c06\u200b\u56fe\u50cf\u200b\u51c6\u5907\u200b\u6210\u200b\u591a\u4e2a\u200b\u5c0f\u200b\u5757\u200b\uff08patches\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b4\u200b\u8282\u4e2d\u200b\u8be6\u7ec6\u200b\u89e3\u91ca\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4ec0\u4e48\u200b\uff0c\u200b\u4f46\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5148\u200b\u6765\u200b\u67e5\u770b\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u53ca\u5176\u200b\u6807\u7b7e\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u6279\u200b\u6570\u636e\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u548c\u200b\u6807\u7b7e\u200b\uff0c\u200b\u5e76\u200b\u68c0\u67e5\u200b\u5b83\u4eec\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#3-vit","title":"3. \u200b\u590d\u73b0\u200b ViT \u200b\u8bba\u6587\u200b\uff1a\u200b\u6982\u8ff0\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u66f4\u200b\u591a\u200b\u4ee3\u7801\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u8ba8\u8bba\u4e00\u4e0b\u200b\u6211\u4eec\u200b\u8981\u200b\u505a\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\u2014\u2014FoodVision Mini\uff0c\u200b\u590d\u73b0\u200b ViT \u200b\u8bba\u6587\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8f93\u5165\u200b\u662f\u200b\uff1a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u800c\u200b\u6211\u4eec\u200b\u7406\u60f3\u200b\u7684\u200b\u6a21\u578b\u200b\u8f93\u51fa\u200b\u662f\u200b\uff1a\u200b\u9884\u6d4b\u200b\u7684\u200b\u6807\u7b7e\u200b\uff0c\u200b\u5373\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b\u524d\u200b\u51e0\u8282\u200b\u4e2d\u6240\u200b\u505a\u200b\u7684\u200b\u6ca1\u6709\u200b\u4ec0\u4e48\u200b\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\u662f\u200b\uff1a\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u4ece\u200b\u8f93\u5165\u200b\u5230\u200b\u671f\u671b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff1f</p>"},{"location":"08_pytorch_paper_replicating/#31","title":"3.1 \u200b\u8f93\u5165\u200b\u4e0e\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5c42\u200b\u4e0e\u200b\u5757\u200b\u00b6","text":"<p>ViT \u200b\u662f\u200b\u4e00\u79cd\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\u3002</p> <p>\u200b\u4efb\u4f55\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u67b6\u6784\u200b\u901a\u5e38\u200b\u90fd\u200b\u7531\u200b \u200b\u5c42\u200b \u200b\u7ec4\u6210\u200b\u3002</p> <p>\u200b\u4e00\u7ec4\u200b\u5c42\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u4e00\u4e2a\u200b \u200b\u5757\u200b\u3002</p> <p>\u200b\u5c06\u200b\u8bb8\u591a\u5757\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\u5c31\u200b\u6784\u6210\u200b\u4e86\u200b\u6574\u4e2a\u200b\u67b6\u6784\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b \u200b\u5c42\u200b \u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\uff08\u200b\u6bd4\u5982\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\uff09\uff0c\u200b\u5bf9\u200b\u5176\u200b\u6267\u884c\u200b\u67d0\u79cd\u200b\u64cd\u4f5c\u200b\uff08\u200b\u4f8b\u5982\u200b\u5c42\u4e2d\u200b\u7684\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u6240\u200b\u5b9a\u4e49\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u8fd4\u56de\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b \u200b\u5355\u5c42\u200b \u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\u5e76\u200b\u4ea7\u751f\u200b\u8f93\u51fa\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4e00\u7ec4\u200b\u5c42\u200b\u6216\u200b\u4e00\u4e2a\u200b \u200b\u5757\u200b \u200b\u540c\u6837\u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\u5e76\u200b\u4ea7\u751f\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5177\u4f53\u5316\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1a</p> <ul> <li>\u200b\u5c42\u200b - \u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\uff0c\u200b\u5bf9\u200b\u5176\u200b\u6267\u884c\u200b\u67d0\u79cd\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u8f93\u51fa\u200b\u3002</li> <li>\u200b\u5757\u200b - \u200b\u4e00\u7ec4\u200b\u5c42\u200b\uff0c\u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\uff0c\u200b\u5bf9\u200b\u5176\u200b\u6267\u884c\u200b\u4e00\u7cfb\u5217\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u8f93\u51fa\u200b\u3002</li> <li>\u200b\u67b6\u6784\u200b\uff08\u200b\u6216\u200b\u6a21\u578b\u200b\uff09 - \u200b\u4e00\u7ec4\u200b\u5757\u200b\uff0c\u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\uff0c\u200b\u5bf9\u200b\u5176\u200b\u6267\u884c\u200b\u4e00\u7cfb\u5217\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u8fd4\u56de\u200b\u8f93\u51fa\u200b\u3002</li> </ul> <p>\u200b\u8fd9\u79cd\u200b\u7406\u5ff5\u200b\u6b63\u662f\u200b\u6211\u4eec\u200b\u5c06\u200b\u7528\u6765\u200b\u590d\u73b0\u200b ViT \u200b\u8bba\u6587\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9010\u5c42\u200b\u3001\u200b\u9010\u5757\u200b\u3001\u200b\u9010\u200b\u51fd\u6570\u200b\u5730\u200b\u6784\u5efa\u200b\uff0c\u200b\u50cf\u200b\u62fc\u4e50\u9ad8\u200b\u4e00\u6837\u200b\u5c06\u200b\u62fc\u56fe\u200b\u7684\u200b\u788e\u7247\u200b\u7ec4\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u6211\u4eec\u200b\u671f\u671b\u200b\u7684\u200b\u6574\u4f53\u200b\u67b6\u6784\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd9\u6837\u200b\u505a\u200b\u7684\u200b\u539f\u56e0\u200b\u662f\u200b\uff0c\u200b\u9605\u8bfb\u200b\u6574\u7bc7\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4ee4\u4eba\u200b\u671b\u800c\u751f\u754f\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u66f4\u597d\u200b\u5730\u200b\u7406\u89e3\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u5355\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f00\u59cb\u200b\uff0c\u200b\u9010\u6b65\u200b\u4e0a\u5347\u200b\u5230\u200b\u6574\u4e2a\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u73b0\u4ee3\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u67b6\u6784\u200b\u901a\u5e38\u200b\u662f\u200b\u4e00\u7ec4\u200b\u5c42\u200b\u548c\u200b\u5757\u200b\u3002\u200b\u5c42\u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\uff08\u200b\u6570\u636e\u200b\u4ee5\u200b\u6570\u503c\u200b\u5f62\u5f0f\u200b\u8868\u793a\u200b\uff09\uff0c\u200b\u901a\u8fc7\u200b\u67d0\u79cd\u200b\u64cd\u4f5c\u200b\uff08\u200b\u4f8b\u5982\u200b\u4e0a\u200b\u56fe\u200b\u6240\u793a\u200b\u7684\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u516c\u5f0f\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u64cd\u4f5c\u200b\u53ef\u4ee5\u200b\u662f\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u5f62\u5f0f\u200b\uff09\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\uff0c\u200b\u7136\u540e\u200b\u8f93\u51fa\u200b\u3002\u200b\u5757\u200b\u901a\u5e38\u200b\u662f\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\u7684\u200b\u5c42\u200b\uff0c\u200b\u5bf9\u200b\u5355\u5c42\u200b\u8fdb\u884c\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4f46\u200b\u6267\u884c\u200b\u591a\u6b21\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#32-vit","title":"3.2 \u200b\u6df1\u5165\u200b\u89e3\u6790\u200b\uff1aViT\u200b\u7684\u200b\u6784\u6210\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\u00b6","text":"<p>\u200b\u5173\u4e8e\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u7ec6\u8282\u200b\u5728\u200b\u8bba\u6587\u200b\u4e2d\u200b\u968f\u5904\u53ef\u89c1\u200b\u3002</p> <p>\u200b\u627e\u5230\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u7ec6\u8282\u200b\u5c31\u200b\u50cf\u662f\u200b\u4e00\u573a\u200b\u5927\u578b\u200b\u7684\u200b\u5bfb\u5b9d\u200b\u6e38\u620f\u200b\uff01</p> <p>\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u4e00\u7bc7\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u901a\u5e38\u200b\u662f\u200b\u6570\u6708\u200b\u5de5\u4f5c\u200b\u7684\u200b\u6d53\u7f29\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u7ec3\u4e60\u200b\u624d\u80fd\u200b\u590d\u5236\u200b\u5176\u4e2d\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u8fd9\u662f\u200b\u53ef\u4ee5\u200b\u7406\u89e3\u200b\u7684\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e3b\u8981\u200b\u53c2\u8003\u200b\u4ee5\u4e0b\u200b\u4e09\u4e2a\u200b\u8d44\u6e90\u200b\u6765\u200b\u4e86\u89e3\u200b\u67b6\u6784\u8bbe\u8ba1\u200b\uff1a</p> <ol> <li>\u200b\u56fe\u200b1 - \u200b\u8fd9\u200b\u4ece\u200b\u56fe\u5f62\u200b\u89d2\u5ea6\u200b\u6982\u8ff0\u200b\u4e86\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u4ec5\u51ed\u200b\u8fd9\u5f20\u200b\u56fe\u6765\u200b\u91cd\u5efa\u200b\u67b6\u6784\u200b\u3002</li> <li>\u200b\u7b2c\u200b3.1\u200b\u8282\u4e2d\u200b\u7684\u200b\u56db\u4e2a\u200b\u65b9\u7a0b\u200b - \u200b\u8fd9\u4e9b\u200b\u65b9\u7a0b\u200b\u4e3a\u200b\u56fe\u200b1\u200b\u4e2d\u200b\u7684\u200b\u5f69\u8272\u200b\u5757\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u5b66\u200b\u57fa\u7840\u200b\u3002</li> <li>\u200b\u8868\u200b1 - \u200b\u8be5\u8868\u200b\u5c55\u793a\u200b\u4e86\u200b\u4e0d\u540c\u200bViT\u200b\u6a21\u578b\u200b\u53d8\u4f53\u200b\u7684\u200b\u5404\u79cd\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\uff08\u200b\u5982\u200b\u5c42\u6570\u200b\u548c\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u200b\uff09\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b\u6700\u5c0f\u200b\u7248\u672c\u200b\uff0c\u200b\u5373\u200bViT-Base\u3002</li> </ol>"},{"location":"08_pytorch_paper_replicating/#321-1","title":"3.2.1 \u200b\u63a2\u7d22\u200b\u56fe\u200b1\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200bViT\u200b\u8bba\u6587\u200b\u7684\u200b\u56fe\u200b1\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e3b\u8981\u200b\u5173\u6ce8\u200b\u7684\u200b\u662f\u200b\uff1a</p> <ol> <li>\u200b\u5c42\u200b - \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\uff0c\u200b\u5bf9\u200b\u8f93\u5165\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\u6216\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ea7\u751f\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u3002</li> <li>\u200b\u5757\u200b - \u200b\u7531\u200b\u591a\u4e2a\u200b\u5c42\u200b\u7ec4\u6210\u200b\u7684\u200b\u96c6\u5408\u200b\uff0c\u200b\u540c\u6837\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u8f93\u5165\u200b\u5e76\u200b\u4ea7\u751f\u200b\u4e00\u4e2a\u200b\u8f93\u51fa\u200b\u3002</li> </ol> <p></p> <p>\u200b\u56fe\u200b1\u200b\u6765\u81ea\u200bViT\u200b\u8bba\u6587\u200b\uff0c\u200b\u5c55\u793a\u200b\u4e86\u200b\u6784\u6210\u200b\u67b6\u6784\u200b\u7684\u200b\u4e0d\u540c\u200b\u8f93\u5165\u200b\u3001\u200b\u8f93\u51fa\u200b\u3001\u200b\u5c42\u200b\u548c\u200b\u5757\u200b\u3002\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u4f7f\u7528\u200bPyTorch\u200b\u4ee3\u7801\u200b\u590d\u5236\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u3002</p> <p>ViT\u200b\u67b6\u6784\u200b\u5305\u542b\u200b\u51e0\u4e2a\u200b\u9636\u6bb5\u200b\uff1a</p> <ul> <li>\u200b\u5206\u5757\u200b + \u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff08\u200b\u8f93\u5165\u200b\uff09 - \u200b\u5c06\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e8f\u5217\u200b\uff0c\u200b\u5e76\u200b\u6dfb\u52a0\u200b\u4f4d\u7f6e\u200b\u7f16\u53f7\u200b\u4ee5\u200b\u6307\u5b9a\u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b\u987a\u5e8f\u200b\u3002</li> <li>\u200b\u6241\u5e73\u5316\u200b\u5206\u5757\u200b\u7684\u200b\u7ebf\u6027\u200b\u6295\u5f71\u200b\uff08\u200b\u5d4c\u5165\u200b\u5206\u5757\u200b\uff09 - \u200b\u56fe\u50cf\u200b\u5757\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5d4c\u5165\u200b\uff0c\u200b\u4f7f\u7528\u200b\u5d4c\u5165\u200b\u800c\u200b\u4e0d\u662f\u200b\u56fe\u50cf\u200b\u503c\u200b\u7684\u200b\u597d\u5904\u200b\u662f\u200b\u5d4c\u5165\u200b\u662f\u200b\u4e00\u79cd\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u8868\u793a\u200b\uff08\u200b\u901a\u5e38\u200b\u4ee5\u200b\u5411\u91cf\u200b\u5f62\u5f0f\u200b\uff09\uff0c\u200b\u53ef\u4ee5\u200b\u968f\u7740\u200b\u8bad\u7ec3\u200b\u800c\u200b\u6539\u8fdb\u200b\u3002</li> <li>\u200b\u5f52\u4e00\u5316\u200b - \u200b\u8fd9\u200b\u662f\u200b\u6307\u200b\u201c\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\u201d\u200b\u6216\u200b\u201cLayerNorm\u201d\uff0c\u200b\u662f\u200b\u4e00\u79cd\u200b\u7528\u4e8e\u200b\u6b63\u5219\u200b\u5316\u200b\uff08\u200b\u51cf\u5c11\u200b\u8fc7\u200b\u62df\u5408\u200b\uff09\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6280\u672f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bPyTorch\u200b\u5c42\u200b <code>torch.nn.LayerNorm()</code> \u200b\u4f7f\u7528\u200b\u3002</li> <li>\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b - \u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff0c\u200b\u7b80\u79f0\u200b\u201cMSA\u201d\u3002\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bPyTorch\u200b\u5c42\u200b <code>torch.nn.MultiheadAttention()</code> \u200b\u521b\u5efa\u200bMSA\u200b\u5c42\u200b\u3002</li> <li>MLP\uff08\u200b\u6216\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff09 - MLP\u200b\u901a\u5e38\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u4efb\u4f55\u200b\u524d\u9988\u200b\u5c42\u200b\u7684\u200b\u96c6\u5408\u200b\uff08\u200b\u6216\u200b\u5728\u200bPyTorch\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5177\u6709\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u7684\u200b\u5c42\u200b\u7684\u200b\u96c6\u5408\u200b\uff09\u3002\u200b\u5728\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\uff0c\u200b\u4f5c\u8005\u200b\u5c06\u200bMLP\u200b\u79f0\u4e3a\u200b\u201cMLP\u200b\u5757\u200b\u201d\uff0c\u200b\u5b83\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b <code>torch.nn.Linear()</code> \u200b\u5c42\u200b\uff0c\u200b\u4e2d\u95f4\u200b\u6709\u200b\u4e00\u4e2a\u200b <code>torch.nn.GELU()</code> \u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\uff09\uff0c\u200b\u6bcf\u4e2a\u200b\u5c42\u200b\u540e\u9762\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b <code>torch.nn.Dropout()</code> \u200b\u5c42\u200b\uff08\u200b\u9644\u5f55\u200bB.1\uff09\u3002</li> <li>Transformer\u200b\u7f16\u7801\u5668\u200b - Transformer\u200b\u7f16\u7801\u5668\u200b\u662f\u200b\u4e0a\u8ff0\u200b\u5c42\u200b\u7684\u200b\u96c6\u5408\u200b\u3002Transformer\u200b\u7f16\u7801\u5668\u200b\u5185\u90e8\u200b\u6709\u200b\u4e24\u4e2a\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\uff08\u201c+\u201d\u200b\u7b26\u53f7\u200b\uff09\uff0c\u200b\u610f\u5473\u7740\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u4e0d\u4ec5\u200b\u76f4\u63a5\u200b\u4f20\u9012\u200b\u5230\u200b\u540e\u7eed\u200b\u5c42\u200b\uff0c\u200b\u8fd8\u200b\u4f20\u9012\u200b\u5230\u200b\u7d27\u90bb\u200b\u7684\u200b\u5c42\u200b\u3002\u200b\u6574\u4e2a\u200bViT\u200b\u67b6\u6784\u200b\u7531\u200b\u591a\u4e2a\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\u7684\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u7ec4\u6210\u200b\u3002</li> <li>MLP\u200b\u5934\u200b - \u200b\u8fd9\u662f\u200b\u67b6\u6784\u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u8f93\u5165\u200b\u7684\u200b\u5b66\u4e60\u200b\u7279\u5f81\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u7c7b\u522b\u200b\u8f93\u51fa\u200b\u3002\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u8fdb\u884c\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u79f0\u5176\u4e3a\u200b\u201c\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u201d\u3002MLP\u200b\u5934\u200b\u7684\u200b\u7ed3\u6784\u200b\u7c7b\u4f3c\u200b\u4e8e\u200bMLP\u200b\u5757\u200b\u3002</li> </ul> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\uff0cViT\u200b\u67b6\u6784\u200b\u7684\u200b\u8bb8\u591a\u200b\u90e8\u5206\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u7528\u200b\u73b0\u6709\u200b\u7684\u200bPyTorch\u200b\u5c42\u200b\u521b\u5efa\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200bPyTorch\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u76ee\u7684\u200b\u4e4b\u4e00\u200b\u5c31\u662f\u200b\u4e3a\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u548c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4ece\u4e1a\u8005\u200b\u521b\u5efa\u200b\u53ef\u200b\u91cd\u7528\u200b\u7684\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\u3002</p> <p>\u200b\u95ee\u9898\u200b\uff1a \u200b\u4e3a\u4ec0\u4e48\u200b\u4e0d\u200b\u4ece\u5934\u5f00\u59cb\u200b\u7f16\u5199\u200b\u6240\u6709\u200b\u4ee3\u7801\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u5f53\u7136\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7528\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200bPyTorch\u200b\u5c42\u200b\u91cd\u73b0\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u6570\u5b66\u200b\u65b9\u7a0b\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8fd9\u200b\u80af\u5b9a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6709\u200b\u6559\u80b2\u200b\u610f\u4e49\u200b\u7684\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u4f7f\u7528\u200b\u73b0\u6709\u200b\u7684\u200bPyTorch\u200b\u5c42\u200b\u901a\u5e38\u200b\u66f4\u200b\u53d7\u6b22\u8fce\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u73b0\u6709\u200b\u7684\u200b\u5c42\u200b\u901a\u5e38\u200b\u7ecf\u8fc7\u200b\u5e7f\u6cdb\u200b\u6d4b\u8bd5\u200b\u548c\u200b\u6027\u80fd\u200b\u68c0\u67e5\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u5b83\u4eec\u200b\u6b63\u786e\u200b\u4e14\u200b\u5feb\u901f\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u7f16\u5199\u200bPyTorch\u200b\u4ee3\u7801\u200b\u6765\u200b\u521b\u5efa\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u3002\u200b\u5173\u4e8e\u200b\u6bcf\u4e2a\u200b\u5c42\u200b\u7684\u200b\u529f\u80fd\u200b\u80cc\u666f\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u9605\u8bfb\u200b\u5b8c\u6574\u200b\u7684\u200bViT\u200b\u8bba\u6587\u200b\u6216\u200b\u9605\u8bfb\u200b\u6bcf\u4e2a\u200b\u5c42\u200b\u7684\u200b\u94fe\u63a5\u200b\u8d44\u6e90\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u56fe\u200b1\u200b\u6539\u7f16\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200bFoodVision Mini\u200b\u95ee\u9898\u200b\uff0c\u200b\u5373\u5c06\u200b\u98df\u7269\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u3002</p> <p></p> <p>\u200b\u56fe\u200b1\u200b\u6765\u81ea\u200bViT\u200b\u8bba\u6587\u200b\u6539\u7f16\u200b\u4e3a\u200bFoodVision Mini\u3002\u200b\u98df\u7269\u200b\u56fe\u50cf\u200b\u8fdb\u5165\u200b\uff08\u200b\u62ab\u8428\u200b\uff09\uff0c\u200b\u56fe\u50cf\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5206\u5757\u200b\uff0c\u200b\u7136\u540e\u200b\u6295\u5f71\u200b\u5230\u200b\u5d4c\u5165\u200b\u4e2d\u200b\u3002\u200b\u5d4c\u5165\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u5404\u4e2a\u200b\u5c42\u200b\u548c\u200b\u5757\u200b\uff0c\u200b\u5e76\u200b\uff08\u200b\u5e0c\u671b\u200b\uff09\u200b\u8fd4\u56de\u200b\u7c7b\u522b\u200b\u201c\u200b\u62ab\u8428\u200b\u201d\u3002</p>"},{"location":"08_pytorch_paper_replicating/#322","title":"3.2.2 \u200b\u63a2\u7d22\u200b\u56db\u4e2a\u200b\u65b9\u7a0b\u200b\u00b6","text":"<p>\u200b\u63a5\u4e0b\u6765\u200b\u6211\u4eec\u200b\u8981\u200b\u91cd\u70b9\u200b\u7814\u7a76\u200b\u7684\u200b\u662f\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u56db\u4e2a\u200b\u65b9\u7a0b\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4f4d\u4e8e\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u3002</p> <p></p> <p>\u200b\u8fd9\u200b\u56db\u4e2a\u200b\u65b9\u7a0b\u200b\u4ee3\u8868\u200b\u4e86\u200bViT\u200b\u67b6\u6784\u200b\u4e2d\u200b\u56db\u4e2a\u200b\u4e3b\u8981\u200b\u90e8\u5206\u200b\u7684\u200b\u6570\u5b66\u539f\u7406\u200b\u3002</p> <p>\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u63cf\u8ff0\u200b\u4e86\u200b\u6bcf\u4e2a\u200b\u65b9\u7a0b\u200b\uff08\u200b\u4e3a\u4e86\u200b\u7b80\u6d01\u200b\uff0c\u200b\u7701\u7565\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u6587\u672c\u200b\uff0c\u200b\u52a0\u7c97\u200b\u7684\u200b\u6587\u672c\u200b\u662f\u200b\u6211\u200b\u6dfb\u52a0\u200b\u7684\u200b\uff09\uff1a</p> \u200b\u65b9\u7a0b\u200b\u7f16\u53f7\u200b ViT\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u7684\u200b\u63cf\u8ff0\u200b 1 ...Transformer\u200b\u5728\u200b\u6574\u4e2a\u200b\u5c42\u4e2d\u200b\u4f7f\u7528\u200b\u6052\u5b9a\u200b\u7684\u200b\u6f5c\u5728\u200b\u5411\u91cf\u200b\u5927\u5c0f\u200b$D$\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u5c06\u200b\u56fe\u50cf\u200b\u5757\u200b\u5c55\u5e73\u200b\u5e76\u200b\u901a\u8fc7\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7ebf\u6027\u200b\u6295\u5f71\u200b\u6620\u5c04\u200b\u5230\u200b$D$\u200b\u7ef4\u200b\uff08\u200b\u65b9\u7a0b\u200b1\uff09\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u6295\u5f71\u200b\u7684\u200b\u8f93\u51fa\u200b\u79f0\u4e3a\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b...\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u88ab\u200b\u6dfb\u52a0\u200b\u5230\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u4e2d\u4ee5\u200b\u4fdd\u7559\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u3002\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6807\u51c6\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b1D\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b... 2 Transformer\u200b\u7f16\u7801\u5668\u200b\uff08Vaswani\u200b\u7b49\u200b\u4eba\u200b\uff0c2017\uff09\u200b\u7531\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff0c\u200b\u89c1\u200b\u9644\u5f55\u200bA\uff09\u200b\u548c\u200bMLP\u200b\u5757\u200b\u4ea4\u66ff\u200b\u5c42\u200b\u7ec4\u6210\u200b\uff08\u200b\u65b9\u7a0b\u200b2, 3\uff09\u3002\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u5757\u200b\u4e4b\u524d\u200b\u5e94\u7528\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LN\uff09\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u5757\u200b\u4e4b\u540e\u200b\u5e94\u7528\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08Wang\u200b\u7b49\u200b\u4eba\u200b\uff0c2019\uff1bBaevski &amp; Auli\uff0c2019\uff09\u3002 3 \u200b\u4e0e\u200b\u65b9\u7a0b\u200b2\u200b\u76f8\u540c\u200b\u3002 4 \u200b\u7c7b\u4f3c\u200b\u4e8e\u200bBERT\u200b\u7684\u200b[class]\u200b\u4ee4\u724c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u5d4c\u5165\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e8f\u5217\u200b\u524d\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u5d4c\u5165\u200b $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$\uff0c\u200b\u5176\u200b\u5728\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u8f93\u51fa\u200b\u5904\u200b\u7684\u200b\u72b6\u6001\u200b $\\left(\\mathbf{z}_{L}^{0}\\right)$ \u200b\u4f5c\u4e3a\u200b\u56fe\u50cf\u200b\u8868\u793a\u200b $\\mathbf{y}$\uff08\u200b\u65b9\u7a0b\u200b4\uff09... <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u63cf\u8ff0\u200b\u6620\u5c04\u200b\u5230\u200b\u56fe\u200b1\u200b\u4e2d\u200b\u7684\u200bViT\u200b\u67b6\u6784\u200b\u3002</p> <p></p> <p>\u200b\u5c06\u200bViT\u200b\u8bba\u6587\u200b\u7684\u200b\u56fe\u200b1\u200b\u4e0e\u200b\u7b2c\u200b3.1\u200b\u8282\u4e2d\u200b\u63cf\u8ff0\u200b\u6bcf\u4e2a\u200b\u5c42\u200b/\u200b\u5757\u200b\u7684\u200b\u6570\u5b66\u539f\u7406\u200b\u7684\u200b\u56db\u4e2a\u200b\u65b9\u7a0b\u200b\u8fdb\u884c\u200b\u8fde\u63a5\u200b\u3002</p> <p>\u200b\u4e0a\u56fe\u200b\u4e2d\u200b\u6709\u200b\u8bb8\u591a\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4f46\u200b\u8ddf\u968f\u200b\u5f69\u8272\u200b\u7ebf\u6761\u200b\u548c\u200b\u7bad\u5934\u200b\u53ef\u4ee5\u200b\u63ed\u793a\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u4e3b\u8981\u200b\u6982\u5ff5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fdb\u4e00\u6b65\u200b\u5206\u89e3\u200b\u6bcf\u4e2a\u200b\u65b9\u7a0b\u200b\u5982\u4f55\u200b\uff08\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u4f7f\u7528\u200b\u4ee3\u7801\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u8fd9\u4e9b\u200b\u65b9\u7a0b\u200b\uff09\uff1f</p> <p>\u200b\u5728\u200b\u6240\u6709\u200b\u65b9\u7a0b\u200b\uff08\u200b\u9664\u4e86\u200b\u65b9\u7a0b\u200b4\uff09\u200b\u4e2d\u200b\uff0c\u201c$\\mathbf{z}$\u201d\u200b\u662f\u200b\u7279\u5b9a\u200b\u5c42\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\uff1a</p> <ol> <li>$\\mathbf{z}_{0}$ \u200b\u662f\u200b \"z\u200b\u96f6\u200b\"\uff08\u200b\u8fd9\u662f\u200b\u521d\u59cb\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\uff09\u3002</li> <li>$\\mathbf{z}_{\\ell}^{\\prime}$ \u200b\u662f\u200b \"z\u200b\u7279\u5b9a\u200b\u5c42\u200b prime\"\uff08\u200b\u6216\u200bz\u200b\u7684\u200b\u4e2d\u95f4\u200b\u503c\u200b\uff09\u3002</li> <li>$\\mathbf{z}_{\\ell}$ \u200b\u662f\u200b \"z\u200b\u7279\u5b9a\u200b\u5c42\u200b\"\u3002</li> </ol> <p>\u200b\u800c\u200b $\\mathbf{y}$ \u200b\u662f\u200b\u67b6\u6784\u200b\u7684\u200b\u6574\u4f53\u200b\u8f93\u51fa\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#323-1","title":"3.2.3 \u200b\u516c\u5f0f\u200b1\u200b\u6982\u8ff0\u200b\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$</p> <p>\u200b\u8be5\u200b\u516c\u5f0f\u200b\u5904\u7406\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u7684\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u3001\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff08$\\mathbf{E}$ \u200b\u8868\u793a\u200b\u5d4c\u5165\u200b\u77e9\u9635\u200b\uff09\u3002</p> <p>\u200b\u5728\u200b\u5411\u91cf\u200b\u5f62\u5f0f\u200b\u4e2d\u200b\uff0c\u200b\u5d4c\u5165\u200b\u53ef\u80fd\u200b\u770b\u8d77\u6765\u200b\u50cf\u200b\u8fd9\u6837\u200b\uff1a</p> <pre>x_input = [class_token, image_patch_1, image_patch_2, image_patch_3...] + [class_token_position, image_patch_1_position, image_patch_2_position, image_patch_3_position...]\n</pre> <p>\u200b\u5176\u4e2d\u200b\u5411\u91cf\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u90fd\u200b\u662f\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\uff08\u200b\u5b83\u4eec\u200b\u7684\u200b <code>requires_grad=True</code>\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#324-2","title":"3.2.4 \u200b\u516c\u5f0f\u200b2\u200b\u6982\u8ff0\u200b\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>\u200b\u8fd9\u200b\u8868\u793a\u200b\u5bf9\u4e8e\u200b\u4ece\u200b\u7b2c\u200b1\u200b\u5c42\u5230\u200b\u7b2c\u200b$L$\u200b\u5c42\u200b\uff08\u200b\u603b\u200b\u5c42\u6570\u200b\uff09\u200b\u7684\u200b\u6bcf\u200b\u4e00\u5c42\u200b\uff0c\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\uff08MSA\uff09\u200b\u5305\u88f9\u200b\u7740\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\u5c42\u200b\uff08LN\uff09\u3002</p> <p>\u200b\u672b\u5c3e\u200b\u7684\u200b\u52a0\u6cd5\u200b\u76f8\u5f53\u4e8e\u200b\u5c06\u200b\u8f93\u5165\u200b\u4e0e\u200b\u8f93\u51fa\u200b\u76f8\u52a0\u200b\uff0c\u200b\u5f62\u6210\u200b\u4e00\u4e2a\u200b\u8df3\u8dc3\u200b/\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u79f0\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u4e3a\u200b\u201cMSA\u200b\u5757\u200b\u201d\u3002</p> <p>\u200b\u7528\u4f2a\u200b\u4ee3\u7801\u200b\u8868\u793a\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u770b\u8d77\u6765\u200b\u50cf\u200b\uff1a</p> <pre>x_output_MSA_block = MSA_layer(LN_layer(x_input)) + x_input\n</pre> <p>\u200b\u6ce8\u610f\u200b\u672b\u5c3e\u200b\u7684\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\uff08\u200b\u5c06\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u4e0e\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u76f8\u52a0\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#325-3","title":"3.2.5 \u200b\u516c\u5f0f\u200b3\u200b\u6982\u8ff0\u200b\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\\\ \\end{aligned} $$</p> <p>\u200b\u8fd9\u200b\u8868\u793a\u200b\u5bf9\u4e8e\u200b\u4ece\u200b\u7b2c\u200b1\u200b\u5c42\u5230\u200b\u7b2c\u200b$L$\u200b\u5c42\u200b\uff08\u200b\u603b\u200b\u5c42\u6570\u200b\uff09\uff0c\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff08MLP\uff09\u200b\u5c42\u200b\u5305\u88f9\u200b\u7740\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LN\uff09\u200b\u5c42\u200b\u3002</p> <p>\u200b\u672b\u5c3e\u200b\u7684\u200b\u52a0\u6cd5\u200b\u8868\u793a\u200b\u5b58\u5728\u200b\u4e00\u4e2a\u200b\u8df3\u8dc3\u200b/\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u79f0\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u4e3a\u200b\u201cMLP\u200b\u5757\u200b\u201d\u3002</p> <p>\u200b\u5728\u200b\u4f2a\u200b\u4ee3\u7801\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u770b\u8d77\u6765\u200b\u50cf\u200b\uff1a</p> <pre>x_output_MLP_block = MLP_layer(LN_layer(x_output_MSA_block)) + x_output_MSA_block\n</pre> <p>\u200b\u6ce8\u610f\u200b\u672b\u5c3e\u200b\u7684\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\uff08\u200b\u5c06\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u4e0e\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u76f8\u52a0\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#326-4","title":"3.2.6 \u200b\u516c\u5f0f\u200b4\u200b\u6982\u8ff0\u200b\u00b6","text":"<p>$$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$</p> <p>\u200b\u8fd9\u200b\u8868\u793a\u200b\u5bf9\u4e8e\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b $L$\uff0c\u200b\u8f93\u51fa\u200b $y$ \u200b\u662f\u200b $z$ \u200b\u7684\u200b\u7b2c\u200b0\u200b\u4e2a\u200b\u7d22\u5f15\u200b\u6807\u8bb0\u200b\u7ecf\u8fc7\u200b LayerNorm \u200b\u5c42\u200b\uff08LN\uff09\u200b\u5904\u7406\u200b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c<code>x_output_MLP_block</code> \u200b\u7684\u200b\u7b2c\u200b0\u200b\u4e2a\u200b\u7d22\u5f15\u200b\uff1a</p> <pre>y = Linear_layer(LN_layer(x_output_MLP_block[0]))\n</pre> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u4e0a\u8ff0\u200b\u5185\u5bb9\u200b\u6709\u200b\u4e00\u4e9b\u200b\u7b80\u5316\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u7f16\u5199\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u7684\u200b PyTorch \u200b\u4ee3\u7801\u200b\u65f6\u200b\u5904\u7406\u200b\u8fd9\u4e9b\u200b\u7ec6\u8282\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0a\u8ff0\u200b\u90e8\u5206\u200b\u6db5\u76d6\u200b\u4e86\u200b\u5927\u91cf\u200b\u4fe1\u606f\u200b\u3002\u200b\u4f46\u200b\u8bf7\u200b\u8bb0\u4f4f\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u200b\u4e0d\u200b\u7406\u89e3\u200b\u7684\u200b\u5730\u65b9\u200b\uff0c\u200b\u4f60\u200b\u603b\u662f\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u7814\u7a76\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u63d0\u95ee\u200b\u201c\u200b\u4ec0\u4e48\u200b\u662f\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff1f\u201d\u200b\u6765\u200b\u6df1\u5165\u200b\u4e86\u89e3\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#327-1","title":"3.2.7 \u200b\u63a2\u7d22\u200b\u8868\u200b1\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5173\u6ce8\u200b\u7684\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u90e8\u5206\u200b\uff08\u200b\u76ee\u524d\u200b\uff09\u200b\u662f\u200b\u8868\u200b1\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u5c42\u6570\u200b \u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b $D$ MLP\u200b\u5927\u5c0f\u200b \u200b\u5934\u6570\u200b \u200b\u53c2\u6570\u200b\u6570\u91cf\u200b ViT-Base 12 768 3072 12 $86M$ ViT-Large 24 1024 4096 16 $307M$ ViT-Huge 32 1280 5120 16 $632M$ \u200b\u8868\u200b1\uff1aVision Transformer\u200b\u6a21\u578b\u200b\u53d8\u4f53\u200b\u7684\u200b\u8be6\u7ec6\u200b\u53c2\u6570\u200b\u3002\u200b\u6765\u6e90\u200b\uff1aViT\u200b\u8bba\u6587\u200b\u3002 <p>\u200b\u6b64\u8868\u200b\u5c55\u793a\u200b\u4e86\u200b\u6bcf\u79cd\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u5404\u79cd\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u4ece\u200bViT-Base\u200b\u5230\u200bViT-Huge\uff0c\u200b\u6570\u503c\u200b\u9010\u6e10\u200b\u589e\u52a0\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u590d\u73b0\u200bViT-Base\uff08\u200b\u4ece\u5c0f\u200b\u89c4\u6a21\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5fc5\u8981\u200b\u65f6\u200b\u518d\u200b\u6269\u5c55\u200b\uff09\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5c06\u200b\u7f16\u5199\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u6269\u5c55\u200b\u5230\u200b\u66f4\u200b\u5927\u200b\u53d8\u4f53\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u5206\u89e3\u200b\u8fd9\u4e9b\u200b\u8d85\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li>\u200b\u5c42\u6570\u200b - \u200b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u5757\u200b\uff1f\uff08\u200b\u6bcf\u4e2a\u200b\u5757\u200b\u5305\u542b\u200b\u4e00\u4e2a\u200bMSA\u200b\u5757\u200b\u548c\u200b\u4e00\u4e2a\u200bMLP\u200b\u5757\u200b\uff09</li> <li>\u200b\u9690\u85cf\u200b\u5c42\u200b\u5927\u5c0f\u200b $D$ - \u200b\u8fd9\u662f\u200b\u6574\u4e2a\u200b\u67b6\u6784\u200b\u4e2d\u200b\u7684\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u6211\u4eec\u200b\u56fe\u50cf\u200b\u5728\u200b\u88ab\u200b\u5206\u5757\u200b\u548c\u200b\u5d4c\u5165\u200b\u65f6\u200b\u7684\u200b\u5411\u91cf\u200b\u5927\u5c0f\u200b\u3002\u200b\u901a\u5e38\u200b\uff0c\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\u8d8a\u5927\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u6355\u83b7\u200b\u7684\u200b\u4fe1\u606f\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u7ed3\u679c\u200b\u8d8a\u200b\u597d\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u66f4\u5927\u200b\u7684\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\u4f1a\u200b\u589e\u52a0\u200b\u8ba1\u7b97\u6210\u672c\u200b\u3002</li> <li>MLP\u200b\u5927\u5c0f\u200b - MLP\u200b\u5c42\u4e2d\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u91cf\u200b\u662f\u200b\u591a\u5c11\u200b\uff1f</li> <li>\u200b\u5934\u6570\u200b - \u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c42\u4e2d\u200b\u6709\u200b\u591a\u5c11\u200b\u4e2a\u5934\u200b\uff1f</li> <li>\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b - \u200b\u6a21\u578b\u200b\u7684\u200b\u603b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u662f\u200b\u591a\u5c11\u200b\uff1f\u200b\u901a\u5e38\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u53c2\u6570\u200b\u4f1a\u200b\u5e26\u6765\u200b\u66f4\u597d\u200b\u7684\u200b\u6027\u80fd\u200b\uff0c\u200b\u4f46\u4f1a\u200b\u589e\u52a0\u200b\u8ba1\u7b97\u6210\u672c\u200b\u3002\u200b\u4f60\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\uff0c\u200b\u5373\u4f7f\u200b\u662f\u200bViT-Base\uff0c\u200b\u5176\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u4e5f\u200b\u8fdc\u8d85\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u4f7f\u7528\u200b\u7684\u200b\u4efb\u4f55\u200b\u6a21\u578b\u200b\u3002</li> </ul> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u4f5c\u4e3a\u200b\u6211\u4eec\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#33","title":"3.3 \u200b\u6211\u200b\u590d\u73b0\u200b\u8bba\u6587\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u00b6","text":"<p>\u200b\u5f53\u200b\u6211\u200b\u5f00\u59cb\u200b\u7740\u624b\u200b\u590d\u73b0\u200b\u4e00\u7bc7\u200b\u8bba\u6587\u200b\u65f6\u200b\uff0c\u200b\u6211\u4f1a\u200b\u6309\u7167\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u8fdb\u884c\u200b\uff1a</p> <ol> <li>\u200b\u4ece\u5934\u5230\u5c3e\u200b\u9605\u8bfb\u200b\u6574\u7bc7\u200b\u8bba\u6587\u200b\u4e00\u6b21\u200b\uff08\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u4e3b\u8981\u200b\u6982\u5ff5\u200b\uff09\u3002</li> <li>\u200b\u56de\u987e\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u662f\u200b\u5982\u4f55\u200b\u76f8\u4e92\u200b\u5173\u8054\u200b\u7684\u200b\uff0c\u200b\u5e76\u200b\u5f00\u59cb\u200b\u601d\u8003\u200b\u5982\u4f55\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4ee3\u7801\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u4e0a\u9762\u200b\u90a3\u6837\u200b\uff09\u3002</li> <li>\u200b\u91cd\u590d\u200b\u6b65\u9aa4\u200b2\uff0c\u200b\u76f4\u5230\u200b\u6211\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u76f8\u5f53\u200b\u597d\u200b\u7684\u200b\u5927\u7eb2\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b mathpix.com\uff08\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u65b9\u4fbf\u200b\u7684\u200b\u5de5\u5177\u200b\uff09\u200b\u5c06\u200b\u8bba\u6587\u200b\u7684\u200b\u4efb\u4f55\u200b\u90e8\u5206\u200b\u8f6c\u6362\u200b\u4e3a\u200bmarkdown/LaTeX\uff0c\u200b\u4ee5\u4fbf\u200b\u653e\u5165\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u590d\u73b0\u200b\u5c3d\u53ef\u80fd\u200b\u7b80\u5355\u200b\u7684\u200b\u6a21\u578b\u200b\u7248\u672c\u200b\u3002</li> <li>\u200b\u5982\u679c\u200b\u9047\u5230\u56f0\u96be\u200b\uff0c\u200b\u67e5\u627e\u200b\u5176\u4ed6\u200b\u793a\u4f8b\u200b\u3002</li> </ol> <p></p> <p>\u200b\u4f7f\u7528\u200b mathpix.com \u200b\u5c06\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u56db\u4e2a\u200b\u65b9\u7a0b\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u53ef\u200b\u7f16\u8f91\u200b\u7684\u200b LaTeX/markdown\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u4e0a\u8ff0\u200b\u7684\u200b\u524d\u200b\u51e0\u4e2a\u200b\u6b65\u9aa4\u200b\uff08\u200b\u5982\u679c\u200b\u4f60\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u9605\u8bfb\u200b\u5b8c\u6574\u7bc7\u200b\u8bba\u6587\u200b\uff0c\u200b\u6211\u200b\u9f13\u52b1\u200b\u4f60\u200b\u53bb\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\uff09\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u63a5\u4e0b\u6765\u200b\u5c06\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b\u6b65\u9aa4\u200b5\uff1a\u200b\u590d\u73b0\u200b\u5c3d\u53ef\u80fd\u200b\u7b80\u5355\u200b\u7684\u200b\u6a21\u578b\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u4e3a\u4ec0\u4e48\u200b\u4ece\u200b ViT-Base \u200b\u5f00\u59cb\u200b\u7684\u200b\u539f\u56e0\u200b\u3002</p> <p>\u200b\u590d\u73b0\u200b\u5c3d\u53ef\u80fd\u200b\u5c0f\u200b\u7684\u200b\u67b6\u6784\u200b\u7248\u672c\u200b\uff0c\u200b\u8ba9\u200b\u5b83\u200b\u8fd0\u884c\u200b\u8d77\u6765\u200b\uff0c\u200b\u7136\u540e\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u613f\u610f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u518d\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4ee5\u524d\u200b\u4ece\u672a\u200b\u9605\u8bfb\u200b\u8fc7\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\uff0c\u200b\u4e0a\u8ff0\u200b\u8bb8\u591a\u200b\u6b65\u9aa4\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u8ba9\u200b\u4eba\u200b\u611f\u5230\u200b\u754f\u60e7\u200b\u3002\u200b\u4f46\u522b\u200b\u62c5\u5fc3\u200b\uff0c\u200b\u50cf\u200b\u4efb\u4f55\u200b\u4e8b\u60c5\u200b\u4e00\u6837\u200b\uff0c\u200b\u4f60\u200b\u9605\u8bfb\u200b\u548c\u200b\u590d\u73b0\u200b\u8bba\u6587\u200b\u7684\u200b\u6280\u80fd\u200b\u4f1a\u200b\u968f\u7740\u200b\u7ec3\u4e60\u200b\u800c\u200b\u63d0\u9ad8\u200b\u3002\u200b\u522b\u5fd8\u4e86\u200b\uff0c\u200b\u4e00\u7bc7\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u901a\u5e38\u200b\u662f\u200b\u8bb8\u591a\u200b\u4eba\u200b\u51e0\u4e2a\u200b\u6708\u200b\u5de5\u4f5c\u200b\u7684\u200b\u6210\u679c\u200b\u538b\u7f29\u200b\u5728\u200b\u51e0\u9875\u200b\u7eb8\u4e0a\u200b\u3002\u200b\u6240\u4ee5\u200b\u5c1d\u8bd5\u200b\u81ea\u5df1\u200b\u590d\u73b0\u200b\u5b83\u200b\u7edd\u975e\u6613\u4e8b\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#4-1","title":"4. \u200b\u65b9\u7a0b\u200b1\uff1a\u200b\u5c06\u200b\u6570\u636e\u200b\u5206\u5272\u200b\u6210\u5757\u200b\u5e76\u200b\u521b\u5efa\u200b\u7c7b\u522b\u200b\u3001\u200b\u4f4d\u7f6e\u200b\u548c\u200b\u5757\u200b\u5d4c\u5165\u200b\u00b6","text":"<p>\u200b\u6211\u200b\u8bb0\u5f97\u200b\u6211\u200b\u7684\u200b\u4e00\u4f4d\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u5e08\u200b\u670b\u53cb\u200b\u66fe\u7ecf\u200b\u8bf4\u200b\u8fc7\u200b\uff1a\u201c\u200b\u4e00\u5207\u200b\u90fd\u200b\u4e0e\u200b\u5d4c\u5165\u200b\u6709\u5173\u200b\u3002\u201d</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u80fd\u200b\u4ee5\u200b\u4e00\u79cd\u200b\u597d\u200b\u7684\u200b\u3001\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff08\u200b\u5982\u200b\u5d4c\u5165\u200b\u662f\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u8868\u793a\u200b\uff09\u200b\u6765\u200b\u8868\u793a\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u90a3\u4e48\u200b\u4e00\u4e2a\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u4e0a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u3002</p> <p>\u200b\u8bdd\u867d\u5982\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u4e3a\u200bViT\u200b\u67b6\u6784\u200b\u521b\u5efa\u200b\u7c7b\u522b\u200b\u3001\u200b\u4f4d\u7f6e\u200b\u548c\u200b\u5757\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u5757\u200b\u5d4c\u5165\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u6210\u200b\u4e00\u7cfb\u5217\u200b\u7684\u200b\u5757\u200b\uff0c\u200b\u7136\u540e\u200b\u5d4c\u5165\u200b\u8fd9\u4e9b\u200b\u5757\u200b\u3002</p> <p>\u200b\u56de\u60f3\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u5d4c\u5165\u200b\u662f\u200b\u67d0\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u8868\u793a\u200b\uff0c\u200b\u901a\u5e38\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5411\u91cf\u200b\u3002</p> <p>\u201c\u200b\u53ef\u200b\u5b66\u4e60\u200b\u201d\u200b\u8fd9\u4e2a\u200b\u8bcd\u200b\u5f88\u200b\u91cd\u8981\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u7684\u200b\u6570\u503c\u200b\u8868\u793a\u200b\uff08\u200b\u6a21\u578b\u200b\u6240\u200b\u770b\u5230\u200b\u7684\u200b\uff09\u200b\u53ef\u4ee5\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u7684\u200b\u63a8\u79fb\u200b\u800c\u200b\u6539\u8fdb\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200bViT\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u7684\u200b\u5f15\u8a00\u200b\u6bb5\u843d\u200b\u5f00\u59cb\u200b\uff08\u200b\u52a0\u7c97\u200b\u90e8\u5206\u200b\u4e3a\u200b\u6211\u200b\u6240\u200b\u5f3a\u8c03\u200b\uff09\uff1a</p> <p>\u200b\u6807\u51c6\u200b\u7684\u200bTransformer\u200b\u63a5\u6536\u200b\u4e00\u4e2a\u200b1D\u200b\u5e8f\u5217\u200b\u7684\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\u3002\u200b\u4e3a\u4e86\u200b\u5904\u7406\u200b2D\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u56fe\u50cf\u200b$\\mathbf{x} \\in \\mathbb{R}^{H \\times W \\times C}$\u200b\u91cd\u5851\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u6241\u5e73\u5316\u200b\u7684\u200b2D\u200b\u5757\u200b\u5e8f\u5217\u200b$\\mathbf{x}_{p} \\in \\mathbb{R}^{N \\times\\left(P^{2} \\cdot C\\right)}$\uff0c\u200b\u5176\u4e2d\u200b$(H, W)$\u200b\u662f\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\uff0c$C$\u200b\u662f\u200b\u901a\u9053\u200b\u6570\u200b\uff0c$(P, P)$\u200b\u662f\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\uff0c$N=H W / P^{2}$\u200b\u662f\u200b\u751f\u6210\u200b\u7684\u200b\u5757\u200b\u6570\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u4f5c\u4e3a\u200bTransformer\u200b\u7684\u200b\u6709\u6548\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\u3002Transformer\u200b\u5728\u200b\u5176\u200b\u6240\u6709\u200b\u5c42\u4e2d\u200b\u4f7f\u7528\u200b\u6052\u5b9a\u200b\u7684\u200b\u6f5c\u5728\u200b\u5411\u91cf\u200b\u5927\u5c0f\u200b$D$\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u6241\u5e73\u5316\u200b\u8fd9\u4e9b\u200b\u5757\u200b\u5e76\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7ebf\u6027\u200b\u6295\u5f71\u200b\u6620\u5c04\u200b\u5230\u200b$D$\u200b\u7ef4\u200b\uff08\u200b\u65b9\u7a0b\u200b1\uff09\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u6295\u5f71\u200b\u7684\u200b\u8f93\u51fa\u200b\u79f0\u4e3a\u200b\u5757\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u8003\u8651\u200b\u5230\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bb0\u4f4f\u200bViT\u200b\u8bba\u6587\u200b\u8868\u200b3\u200b\u4e2d\u200b\u7684\u200b\u4e00\u884c\u200b\uff1a</p> <p>\u200b\u8bad\u7ec3\u200b\u5206\u8fa8\u7387\u200b\u4e3a\u200b224\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5206\u89e3\u200b\u4e0a\u9762\u200b\u7684\u200b\u6587\u672c\u200b\u3002</p> <ul> <li>$D$ \u200b\u662f\u200b\u5757\u200b\u5d4c\u5165\u200b\u7684\u200b\u5927\u5c0f\u200b\uff0c\u200b\u4e0d\u540c\u200b\u5927\u5c0f\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u5bf9\u5e94\u200b\u7684\u200b$D$\u200b\u503c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8868\u200b1\u200b\u4e2d\u200b\u627e\u5230\u200b\u3002</li> <li>\u200b\u56fe\u50cf\u200b\u6700\u521d\u200b\u662f\u200b2D\u200b\u7684\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b${H \\times W \\times C}$\u3002<ul> <li>$(H, W)$ \u200b\u662f\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\uff08\u200b\u9ad8\u5ea6\u200b\uff0c\u200b\u5bbd\u5ea6\u200b\uff09\u3002</li> <li>$C$ \u200b\u662f\u200b\u901a\u9053\u200b\u6570\u200b\u3002</li> </ul> </li> <li>\u200b\u56fe\u50cf\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u7cfb\u5217\u200b\u6241\u5e73\u5316\u200b\u7684\u200b2D\u200b\u5757\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b${N \\times\\left(P^{2} \\cdot C\\right)}$\u3002<ul> <li>$(P, P)$ \u200b\u662f\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b\u5206\u8fa8\u7387\u200b\uff08\u200b\u5757\u200b\u5927\u5c0f\u200b\uff09\u3002</li> <li>$N=H W / P^{2}$ \u200b\u662f\u200b\u751f\u6210\u200b\u7684\u200b\u5757\u200b\u6570\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u4f5c\u4e3a\u200bTransformer\u200b\u7684\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u957f\u5ea6\u200b\u3002</li> </ul> </li> </ul> <p></p> <p>\u200b\u5c06\u200b\u56fe\u200b1\u200b\u4e2d\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u5757\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u90e8\u5206\u200b\u6620\u5c04\u200b\u5230\u200b\u65b9\u7a0b\u200b1\u3002\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u7684\u200b\u5f15\u8a00\u200b\u6bb5\u843d\u200b\u63cf\u8ff0\u200b\u4e86\u200b\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u4e0d\u540c\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#41","title":"4.1 \u200b\u624b\u52a8\u200b\u8ba1\u7b97\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u624b\u52a8\u200b\u8ba1\u7b97\u200b\u8fd9\u4e9b\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u503c\u200b\u5f00\u59cb\u200b\u5427\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u53d8\u91cf\u200b\u6765\u200b\u6a21\u62df\u200b\u4e0a\u8ff0\u200b\u5404\u4e2a\u200b\u672f\u8bed\u200b\uff08\u200b\u5982\u200b $H$\u3001$W$ \u200b\u7b49\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b 16 \u200b\u7684\u200b\u8865\u4e01\u200b\u5927\u5c0f\u200b\uff08$P$\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u662f\u200b ViT-Base \u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u7248\u672c\u200b\u6240\u200b\u4f7f\u7528\u200b\u7684\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u8868\u200b 5 \u200b\u7684\u200b\u201cViT-B/16\u201d\u200b\u5217\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#42","title":"4.2 \u200b\u5c06\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u5757\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u4e86\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u7406\u60f3\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7740\u624b\u200b\u5b9e\u73b0\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u505a\u6cd5\u200b\u662f\u200b\u5c06\u200b\u6574\u4f53\u200b\u67b6\u6784\u200b\u5206\u89e3\u200b\u4e3a\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\uff0c\u200b\u91cd\u70b9\u200b\u5173\u6ce8\u200b\u5404\u4e2a\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u8bb2\u200b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u518d\u200b\u53ef\u89c6\u5316\u200b\uff01\u200b\u770b\u770b\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u5757\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u5f00\u59cb\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#43-torchnnconv2d","title":"4.3 \u200b\u4f7f\u7528\u200b <code>torch.nn.Conv2d()</code> \u200b\u521b\u5efa\u200b\u56fe\u50cf\u200b\u5757\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u56fe\u50cf\u200b\u5728\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u56fe\u50cf\u200b\u5757\u200b\u65f6\u200b\u7684\u200b\u6837\u5b50\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6765\u200b\u590d\u73b0\u200b\u8fd9\u4e9b\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u53ef\u89c6\u5316\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e86\u200b\u4ee3\u7801\u200b\u6765\u200b\u904d\u5386\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u7684\u200b\u4e0d\u540c\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u7ed8\u5236\u200b\u5355\u4e2a\u200b\u56fe\u50cf\u200b\u5757\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u64cd\u4f5c\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u5728\u200b 03. PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7ae0\u8282\u200b 7.1\uff1a\u200b\u9010\u6b65\u200b\u89e3\u6790\u200b <code>nn.Conv2d()</code> \u200b\u4e2d\u200b\u770b\u5230\u200b\u7684\u200b\u5377\u79ef\u200b\u64cd\u4f5c\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\u3002</p> <p>\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0cViT \u200b\u8bba\u6587\u200b\u7684\u200b\u4f5c\u8005\u200b\u5728\u200b\u7b2c\u200b 3.1 \u200b\u8282\u4e2d\u200b\u63d0\u5230\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08CNN\uff09\u200b\u6765\u200b\u5b9e\u73b0\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\uff1a</p> <p>\u200b\u6df7\u5408\u200b\u67b6\u6784\u200b\u3002 \u200b\u4f5c\u4e3a\u200b\u539f\u59cb\u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b\u66ff\u4ee3\u200b\u65b9\u6848\u200b\uff0c\u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u53ef\u4ee5\u200b\u7531\u200b CNN \u200b\u7684\u200b\u7279\u5f81\u200b\u56fe\u200b\uff08LeCun \u200b\u7b49\u200b\u4eba\u200b\uff0c1989\uff09\u200b\u5f62\u6210\u200b\u3002\u200b\u5728\u200b\u8fd9\u79cd\u200b\u6df7\u5408\u200b\u6a21\u578b\u200b\u4e2d\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u6295\u5f71\u200b $\\mathbf{E}$\uff08\u200b\u516c\u5f0f\u200b 1\uff09\u200b\u5e94\u7528\u200b\u4e8e\u200b\u4ece\u200b CNN \u200b\u7279\u5f81\u200b\u56fe\u200b \u200b\u4e2d\u200b\u63d0\u53d6\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u3002\u200b\u4f5c\u4e3a\u200b\u4e00\u79cd\u200b\u7279\u6b8a\u200b\u60c5\u51b5\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5757\u200b\u53ef\u4ee5\u200b\u5177\u6709\u200b\u7a7a\u95f4\u200b\u5927\u5c0f\u200b $1 \\times 1$\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b \u200b\u8f93\u5165\u200b\u5e8f\u5217\u200b\u662f\u200b\u901a\u8fc7\u200b\u7b80\u5355\u200b\u5730\u200b\u5c55\u5e73\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u7a7a\u95f4\u200b\u7ef4\u5ea6\u200b\u5e76\u200b\u6295\u5f71\u200b\u5230\u200b Transformer \u200b\u7ef4\u5ea6\u200b \u200b\u83b7\u5f97\u200b\u7684\u200b\u3002\u200b\u5982\u4e0a\u6240\u8ff0\u200b\uff0c\u200b\u6dfb\u52a0\u200b\u5206\u7c7b\u200b\u8f93\u5165\u200b\u5d4c\u5165\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u4ed6\u4eec\u200b\u63d0\u5230\u200b\u7684\u200b\u201c\u200b\u7279\u5f81\u200b\u56fe\u200b\u201d\u200b\u662f\u200b\u7531\u200b\u5377\u79ef\u200b\u5c42\u200b\u5728\u200b\u7ed9\u5b9a\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u751f\u6210\u200b\u7684\u200b\u6743\u91cd\u200b/\u200b\u6fc0\u6d3b\u200b\u3002</p> <p></p> <p>\u200b\u901a\u8fc7\u200b\u5c06\u200b <code>torch.nn.Conv2d()</code> \u200b\u5c42\u200b\u7684\u200b <code>kernel_size</code> \u200b\u548c\u200b <code>stride</code> \u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b <code>patch_size</code>\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u6709\u6548\u200b\u5730\u200b\u83b7\u5f97\u200b\u4e00\u4e2a\u200b\u5c06\u200b\u56fe\u50cf\u200b\u5206\u5272\u200b\u6210\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e76\u200b\u521b\u5efa\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u5d4c\u5165\u200b\uff08\u200b\u5728\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u79f0\u4e3a\u200b\u201c\u200b\u7ebf\u6027\u200b\u6295\u5f71\u200b\u201d\uff09\u200b\u7684\u200b\u5c42\u200b\u3002</p> <p>\u200b\u8fd8\u200b\u8bb0\u5f97\u200b\u6211\u4eec\u200b\u7406\u60f3\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u5417\u200b\uff1f</p> <ul> <li>\u200b\u8f93\u5165\u200b\uff1a \u200b\u56fe\u50cf\u200b\u5f00\u59cb\u200b\u65f6\u200b\u662f\u200b 2D \u200b\u7684\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b ${H \\times W \\times C}$\u3002</li> <li>\u200b\u8f93\u51fa\u200b\uff1a \u200b\u56fe\u50cf\u200b\u88ab\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5c55\u5e73\u200b\u7684\u200b 2D \u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b 1D \u200b\u5e8f\u5217\u200b\uff0c\u200b\u5927\u5c0f\u200b\u4e3a\u200b ${N \\times\\left(P^{2} \\cdot C\\right)}$\u3002</li> </ul> <p>\u200b\u6216\u8005\u200b\u5bf9\u4e8e\u200b\u5927\u5c0f\u200b\u4e3a\u200b 224 \u200b\u548c\u200b\u56fe\u50cf\u200b\u5757\u200b\u5927\u5c0f\u200b\u4e3a\u200b 16 \u200b\u7684\u200b\u56fe\u50cf\u200b\uff1a</p> <ul> <li>\u200b\u8f93\u5165\u200b\uff082D \u200b\u56fe\u50cf\u200b\uff09\uff1a (224, 224, 3) -&gt; (\u200b\u9ad8\u5ea6\u200b, \u200b\u5bbd\u5ea6\u200b, \u200b\u989c\u8272\u200b\u901a\u9053\u200b)</li> <li>\u200b\u8f93\u51fa\u200b\uff08\u200b\u5c55\u5e73\u200b\u7684\u200b 2D \u200b\u56fe\u50cf\u200b\u5757\u200b\uff09\uff1a (196, 768) -&gt; (\u200b\u56fe\u50cf\u200b\u5757\u200b\u6570\u91cf\u200b, \u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b)</li> </ul> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u65b9\u6cd5\u200b\u6765\u200b\u590d\u73b0\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\uff1a</p> <ul> <li><code>torch.nn.Conv2d()</code> \u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b CNN \u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u3002</li> <li><code>torch.nn.Flatten()</code> \u200b\u5c55\u5e73\u200b\u7279\u5f81\u200b\u56fe\u200b\u7684\u200b\u7a7a\u95f4\u200b\u7ef4\u5ea6\u200b\u3002</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b <code>torch.nn.Conv2d()</code> \u200b\u5c42\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b <code>kernel_size</code> \u200b\u548c\u200b <code>stride</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>patch_size</code> \u200b\u6765\u200b\u590d\u73b0\u200b\u56fe\u50cf\u200b\u5757\u200b\u7684\u200b\u521b\u5efa\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u6bcf\u4e2a\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u5927\u5c0f\u200b\u4e3a\u200b <code>(patch_size x patch_size)</code>\uff0c\u200b\u6216\u8005\u200b\u5982\u679c\u200b <code>patch_size=16</code>\uff0c\u200b\u5219\u200b\u4e3a\u200b <code>(16 x 16)</code>\uff08\u200b\u76f8\u5f53\u4e8e\u200b\u4e00\u4e2a\u200b\u5b8c\u6574\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\uff09\u3002</p> <p>\u200b\u5377\u79ef\u200b\u6838\u200b\u7684\u200b\u6bcf\u200b\u4e00\u6b65\u200b\u6216\u200b <code>stride</code> \u200b\u5c06\u200b\u662f\u200b <code>patch_size</code> \u200b\u50cf\u7d20\u200b\u957f\u200b\uff0c\u200b\u5373\u200b <code>16</code> \u200b\u50cf\u7d20\u200b\u957f\u200b\uff08\u200b\u76f8\u5f53\u4e8e\u200b\u79fb\u52a8\u200b\u5230\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\u56fe\u50cf\u200b\u5757\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b <code>in_channels=3</code> \u200b\u8868\u793a\u200b\u56fe\u50cf\u200b\u7684\u200b\u989c\u8272\u200b\u901a\u9053\u200b\u6570\u91cf\u200b\uff0c\u200b\u5e76\u200b\u8bbe\u7f6e\u200b <code>out_channels=768</code>\uff0c\u200b\u8fd9\u200b\u4e0e\u200b ViT-Base \u200b\u7684\u200b\u8868\u200b 1 \u200b\u4e2d\u200b\u7684\u200b $D$ \u200b\u503c\u200b\u76f8\u540c\u200b\uff08\u200b\u8fd9\u662f\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u56fe\u50cf\u200b\u5c06\u200b\u88ab\u200b\u5d4c\u5165\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5927\u5c0f\u200b\u4e3a\u200b 768 \u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b\u5411\u91cf\u200b\u4e2d\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#44-torchnnflatten-patch-embedding","title":"4.4 \u200b\u4f7f\u7528\u200b <code>torch.nn.Flatten()</code> \u200b\u5c55\u5e73\u200b patch embedding\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b patch embedding\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u4ecd\u7136\u200b\u662f\u200b\u4e8c\u7ef4\u200b\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u5982\u4f55\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u200b ViT \u200b\u6a21\u578b\u200b\u4e2d\u200b patch embedding \u200b\u5c42\u200b\u7684\u200b\u671f\u671b\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u5462\u200b\uff1f</p> <ul> <li>\u200b\u671f\u671b\u200b\u8f93\u51fa\u200b\uff08\u200b\u5c55\u5e73\u200b\u7684\u200b\u4e8c\u7ef4\u200b patch \u200b\u7684\u200b\u4e00\u7ef4\u200b\u5e8f\u5217\u200b\uff09\uff1a (196, 768) -&gt; \uff08patch \u200b\u6570\u91cf\u200b\uff0c\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff09 -&gt; ${N \\times\\left(P^{2} \\cdot C\\right)}$</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u200b\u5f53\u524d\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#45-vitpatchpytorch","title":"4.5 \u200b\u5c06\u200bViT\u200b\u7684\u200bpatch\u200b\u5d4c\u5165\u200b\u5c42\u200b\u8f6c\u5316\u200b\u4e3a\u200bPyTorch\u200b\u6a21\u5757\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u5c06\u200b\u6211\u4eec\u200b\u4e3a\u200b\u521b\u5efa\u200bpatch\u200b\u5d4c\u5165\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u6240\u6709\u200b\u5de5\u4f5c\u200b\u6574\u5408\u200b\u5230\u200b\u4e00\u4e2a\u200bPyTorch\u200b\u5c42\u4e2d\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5b50\u200b\u7c7b\u5316\u200b<code>nn.Module</code>\u200b\u5e76\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5c0f\u578b\u200b\u7684\u200bPyTorch\u201c\u200b\u6a21\u578b\u200b\u201d\u200b\u6765\u200b\u5b8c\u6210\u200b\u4e0a\u8ff0\u200b\u6240\u6709\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>PatchEmbedding</code>\u200b\u7684\u200b\u7c7b\u200b\uff0c\u200b\u8be5\u7c7b\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>nn.Module</code>\uff08\u200b\u4ee5\u4fbf\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u4f5c\u4e3a\u200bPyTorch\u200b\u5c42\u200b\u4f7f\u7528\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u53c2\u6570\u200b<code>in_channels=3</code>\u3001<code>patch_size=16</code>\uff08\u200b\u9002\u7528\u200b\u4e8e\u200bViT-Base\uff09\u200b\u548c\u200b<code>embedding_dim=768</code>\uff08\u200b\u8fd9\u200b\u662f\u200b\u8868\u200b1\u200b\u4e2d\u200bViT-Base\u200b\u7684\u200b$D$\uff09\u200b\u521d\u59cb\u5316\u200b\u8be5\u7c7b\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5c42\u200b\uff0c\u200b\u4f7f\u7528\u200b<code>nn.Conv2d()</code>\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200bpatches\uff08\u200b\u5c31\u200b\u50cf\u200b\u5728\u200b4.3\u200b\u4e2d\u200b\u4e00\u6837\u200b\uff09\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5c42\u200b\uff0c\u200b\u5c06\u200bpatch\u200b\u7279\u5f81\u200b\u56fe\u200b\u5c55\u5e73\u200b\u4e3a\u200b\u4e00\u7ef4\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u5728\u200b4.4\u200b\u4e2d\u200b\u4e00\u6837\u200b\uff09\u3002</li> <li>\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b<code>forward()</code>\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u63a5\u6536\u200b\u8f93\u5165\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u901a\u8fc7\u200b\u7b2c\u200b3\u200b\u548c\u200b\u7b2c\u200b4\u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5c42\u200b\u3002</li> <li>\u200b\u786e\u4fdd\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7b26\u5408\u200bViT\u200b\u67b6\u6784\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\uff08${N \\times\\left(P^{2} \\cdot C\\right)}$\uff09\u3002</li> </ol> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u5427\u200b\uff01</p>"},{"location":"08_pytorch_paper_replicating/#46","title":"4.6 \u200b\u521b\u5efa\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u4e86\u200b\u56fe\u50cf\u200b\u5757\u200b\u5d4c\u5165\u200b\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u7740\u624b\u200b\u5904\u7406\u200b\u7c7b\u522b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e5f\u200b\u5c31\u662f\u200b\u65b9\u7a0b\u200b1\u200b\u4e2d\u200b\u7684\u200b $\\mathbf{x}_\\text {class }$\u3002</p> <p></p> <p>\u200b\u5de6\u200b\uff1aViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u56fe\u200b1\uff0c\u200b\u5176\u4e2d\u200b\u7a81\u51fa\u200b\u663e\u793a\u200b\u4e86\u200b\u6211\u4eec\u200b\u5373\u5c06\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u7684\u200b\u201c\u200b\u5206\u7c7b\u200b\u6807\u8bb0\u200b\u201d\u200b\u6216\u200b<code>[class]</code>\u200b\u5d4c\u5165\u200b\u6807\u8bb0\u200b\u3002\u200b\u53f3\u200b\uff1aViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u4e0e\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7c7b\u522b\u200b\u5d4c\u5165\u200b\u6807\u8bb0\u200b\u76f8\u5173\u200b\u7684\u200b\u65b9\u7a0b\u200b1\u200b\u548c\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u3002</p> <p>\u200b\u9605\u8bfb\u200bViT\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u7684\u200b\u7b2c\u4e8c\u6bb5\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u4ee5\u4e0b\u200b\u63cf\u8ff0\u200b\uff1a</p> <p>\u200b\u7c7b\u4f3c\u200b\u4e8e\u200bBERT\u200b\u7684\u200b<code>[class]</code>\u200b\u6807\u8bb0\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u5d4c\u5165\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e8f\u5217\u200b\u524d\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u5d4c\u5165\u200b $\\left(\\mathbf{z}_{0}^{0}=\\mathbf{x}_{\\text {class }}\\right)$\uff0c\u200b\u5176\u200b\u5728\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u8f93\u51fa\u200b\u5904\u200b\u7684\u200b\u72b6\u6001\u200b $\\left(\\mathbf{z}_{L}^{0}\\right)$ \u200b\u4f5c\u4e3a\u200b\u56fe\u50cf\u200b\u8868\u793a\u200b $\\mathbf{y}$\uff08\u200b\u65b9\u7a0b\u200b4\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a BERT\uff08\u200b\u6765\u81ea\u200bTransformer\u200b\u7684\u200b\u53cc\u5411\u200b\u7f16\u7801\u5668\u200b\u8868\u793a\u200b\uff09\u200b\u662f\u200b\u6700\u65e9\u200b\u4f7f\u7528\u200bTransformer\u200b\u67b6\u6784\u200b\u5728\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff08NLP\uff09\u200b\u4efb\u52a1\u200b\u4e2d\u200b\u53d6\u5f97\u200b\u5353\u8d8a\u200b\u6210\u679c\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u4e4b\u4e00\u200b\uff0c\u200b\u4e5f\u200b\u662f\u200b\u5e8f\u5217\u200b\u5f00\u59cb\u200b\u5904\u200b\u4f7f\u7528\u200b<code>[class]</code>\u200b\u6807\u8bb0\u200b\u8fd9\u4e00\u200b\u60f3\u6cd5\u200b\u7684\u200b\u8d77\u6e90\u200b\uff0c\u200b\u5176\u4e2d\u200b\u201cclass\u201d\u200b\u662f\u200b\u5bf9\u200b\u5e8f\u5217\u200b\u6240\u5c5e\u200b\u7684\u200b\u201c\u200b\u5206\u7c7b\u200b\u201d\u200b\u7c7b\u522b\u200b\u7684\u200b\u63cf\u8ff0\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u201c\u200b\u5728\u200b\u5d4c\u5165\u200b\u7684\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e8f\u5217\u200b\u524d\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u5d4c\u5165\u200b\u201d\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u67e5\u770b\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b4.5\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u5d4c\u5165\u200b\u56fe\u50cf\u200b\u5757\u200b\u5e8f\u5217\u200b\u5f20\u91cf\u200b\u53ca\u5176\u200b\u5f62\u72b6\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#47","title":"4.7 \u200b\u521b\u5efa\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u7c7b\u522b\u200b\u4ee4\u724c\u200b\u5d4c\u5165\u200b\u548c\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e5f\u200b\u5c31\u662f\u200b\u516c\u5f0f\u200b1\u200b\u4e2d\u200b\u7684\u200b $\\mathbf{E}_{\\text {pos }}$\uff0c\u200b\u5176\u4e2d\u200b $E$ \u200b\u4ee3\u8868\u200b\u201c\u200b\u5d4c\u5165\u200b\u201d\u3002</p> <p></p> <p>\u200b\u5de6\u56fe\u200b\uff1a\u200b\u6765\u81ea\u200bViT\u200b\u8bba\u6587\u200b\u7684\u200b\u56fe\u200b1\uff0c\u200b\u6211\u4eec\u200b\u5c06\u8981\u200b\u91cd\u73b0\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u90e8\u5206\u200b\u88ab\u200b\u9ad8\u4eae\u200b\u663e\u793a\u200b\u3002\u200b\u53f3\u56fe\u200b\uff1a\u200b\u516c\u5f0f\u200b1\u200b\u548c\u200bViT\u200b\u8bba\u6587\u200b\u7684\u200b3.1\u200b\u8282\u200b\uff0c\u200b\u4e0e\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u76f8\u5173\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u9605\u8bfb\u200bViT\u200b\u8bba\u6587\u200b\u7684\u200b3.1\u200b\u8282\u6765\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff08\u200b\u52a0\u7c97\u200b\u90e8\u5206\u200b\u4e3a\u200b\u6211\u200b\u6240\u52a0\u200b\uff09\uff1a</p> <p>\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u88ab\u200b\u6dfb\u52a0\u200b\u5230\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\u4e2d\u4ee5\u200b\u4fdd\u7559\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u3002\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6807\u51c6\u200b\u7684\u200b\u53ef\u200b\u5b66\u4e60\u200b1D\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u6ca1\u6709\u200b\u89c2\u5bdf\u200b\u5230\u200b\u4f7f\u7528\u200b\u66f4\u200b\u9ad8\u7ea7\u200b\u7684\u200b2D\u200b\u611f\u77e5\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u4f1a\u200b\u5e26\u6765\u200b\u663e\u8457\u200b\u7684\u200b\u6027\u80fd\u200b\u63d0\u5347\u200b\uff08\u200b\u9644\u5f55\u200bD.4\uff09\u3002\u200b\u751f\u6210\u200b\u7684\u200b\u5d4c\u5165\u200b\u5411\u91cf\u200b\u5e8f\u5217\u200b\u4f5c\u4e3a\u200b\u7f16\u7801\u5668\u200b\u7684\u200b\u8f93\u5165\u200b\u3002</p> <p>\u200b\u4f5c\u8005\u200b\u6240\u8bf4\u200b\u7684\u200b\u201c\u200b\u4fdd\u7559\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u201d\u200b\u610f\u5473\u7740\u200b\u4ed6\u4eec\u200b\u5e0c\u671b\u200b\u67b6\u6784\u200b\u77e5\u9053\u200b\u5206\u5757\u200b\u7684\u200b\u201c\u200b\u987a\u5e8f\u200b\u201d\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5206\u5757\u200b\u4e8c\u200b\u7d27\u968f\u200b\u5206\u5757\u200b\u4e00\u200b\uff0c\u200b\u5206\u5757\u200b\u4e09\u200b\u7d27\u968f\u200b\u5206\u5757\u200b\u4e8c\u200b\uff0c\u200b\u4ee5\u6b64\u7c7b\u63a8\u200b\u3002</p> <p>\u200b\u8fd9\u79cd\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u5728\u200b\u8003\u8651\u200b\u56fe\u50cf\u200b\u5185\u5bb9\u200b\u65f6\u200b\u53ef\u80fd\u200b\u5f88\u200b\u91cd\u8981\u200b\uff08\u200b\u6ca1\u6709\u200b\u4f4d\u7f6e\u200b\u4fe1\u606f\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u6241\u5e73\u5316\u200b\u7684\u200b\u5e8f\u5217\u200b\u53ef\u80fd\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u6ca1\u6709\u200b\u987a\u5e8f\u200b\uff0c\u200b\u56e0\u6b64\u200b\u6ca1\u6709\u200b\u5206\u5757\u200b\u4e0e\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u5206\u5757\u200b\u76f8\u5173\u200b\uff09\u3002</p> <p>\u200b\u8981\u200b\u5f00\u59cb\u200b\u521b\u5efa\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u67e5\u770b\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b\u5d4c\u5165\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#48","title":"4.8 \u200b\u7efc\u5408\u200b\u5e94\u7528\u200b\uff1a\u200b\u4ece\u200b\u56fe\u50cf\u200b\u5230\u200b\u5d4c\u5165\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8d70\u200b\u4e86\u200b\u5f88\u957f\u200b\u7684\u200b\u8def\u200b\uff0c\u200b\u5c06\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5d4c\u5165\u200b\uff0c\u200b\u5e76\u200b\u590d\u73b0\u200b\u4e86\u200bViT\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u4e2d\u200b\u7684\u200b\u516c\u5f0f\u200b1\uff1a</p> <p>$$ \\begin{aligned} \\mathbf{z}_{0} &amp;=\\left[\\mathbf{x}_{\\text {class }} ; \\mathbf{x}_{p}^{1} \\mathbf{E} ; \\mathbf{x}_{p}^{2} \\mathbf{E} ; \\cdots ; \\mathbf{x}_{p}^{N} \\mathbf{E}\\right]+\\mathbf{E}_{\\text {pos }}, &amp; &amp; \\mathbf{E} \\in \\mathbb{R}^{\\left(P^{2} \\cdot C\\right) \\times D}, \\mathbf{E}_{\\text {pos }} \\in \\mathbb{R}^{(N+1) \\times D} \\end{aligned} $$</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5728\u200b\u4e00\u4e2a\u200b\u4ee3\u7801\u200b\u5355\u5143\u200b\u4e2d\u5c06\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u6574\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u4ece\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\uff08$\\mathbf{x}$\uff09\u200b\u5230\u200b\u8f93\u51fa\u200b\u5d4c\u5165\u200b\uff08$\\mathbf{z}_0$\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5b9e\u73b0\u200b\uff1a</p> <ol> <li>\u200b\u8bbe\u7f6e\u200b\u8865\u4e01\u200b\u5927\u5c0f\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>16</code>\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u200b\u5728\u200b\u8bba\u6587\u200b\u548c\u200bViT-Base\u200b\u6a21\u578b\u200b\u4e2d\u200b\u5e7f\u6cdb\u200b\u4f7f\u7528\u200b\uff09\u3002</li> <li>\u200b\u83b7\u53d6\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u6253\u5370\u200b\u5176\u200b\u5f62\u72b6\u200b\u5e76\u200b\u5b58\u50a8\u200b\u5176\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u6dfb\u52a0\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b <code>PatchEmbedding</code> \u200b\u5c42\u200b\u517c\u5bb9\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>PatchEmbedding</code> \u200b\u5c42\u200b\uff08\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b4.5\u200b\u8282\u4e2d\u200b\u5236\u4f5c\u200b\u7684\u200b\u90a3\u4e2a\u200b\uff09\uff0c\u200b\u8bbe\u7f6e\u200b <code>patch_size=16</code> \u200b\u548c\u200b <code>embedding_dim=768</code>\uff08\u200b\u6765\u81ea\u200bViT-Base\u200b\u7684\u200b\u8868\u200b1\uff09\u3002</li> <li>\u200b\u5c06\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u901a\u8fc7\u200b\u7b2c\u200b4\u200b\u6b65\u4e2d\u200b\u7684\u200b <code>PatchEmbedding</code> \u200b\u5c42\u200b\uff0c\u200b\u521b\u5efa\u200b\u4e00\u7cfb\u5217\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\uff0c\u200b\u5982\u200b\u7b2c\u200b4.6\u200b\u8282\u200b\u6240\u793a\u200b\u3002</li> <li>\u200b\u5c06\u7c7b\u200b\u6807\u8bb0\u200b\u5d4c\u5165\u200b\u6dfb\u52a0\u200b\u5230\u200b\u7b2c\u200b5\u200b\u6b65\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u4e4b\u524d\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\uff0c\u200b\u5982\u200b\u7b2c\u200b4.7\u200b\u8282\u200b\u6240\u793a\u200b\u3002</li> <li>\u200b\u5c06\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u6dfb\u52a0\u200b\u5230\u200b\u7b2c\u200b7\u200b\u6b65\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u7c7b\u200b\u6807\u8bb0\u200b\u548c\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u4e2d\u200b\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u786e\u4fdd\u200b\u4f7f\u7528\u200b <code>set_seeds()</code> \u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u6253\u5370\u200b\u4e0d\u540c\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#5-2msa","title":"5. \u200b\u65b9\u7a0b\u200b 2\uff1a\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u5206\u5757\u200b\u5e76\u200b\u5d4c\u5165\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u63a2\u8ba8\u200b ViT \u200b\u67b6\u6784\u200b\u7684\u200b\u4e0b\u200b\u4e00\u4e2a\u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u90e8\u5206\u200b\u5206\u89e3\u200b\u4e3a\u200b\u4e24\u4e2a\u200b\u90e8\u5206\u200b\uff08\u200b\u4ece\u200b\u7b80\u5355\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5fc5\u8981\u200b\u65f6\u200b\u589e\u52a0\u200b\u590d\u6742\u5ea6\u200b\uff09\u3002</p> <p>\u200b\u7b2c\u4e00\u200b\u90e8\u5206\u200b\u662f\u200b\u65b9\u7a0b\u200b 2\uff0c\u200b\u7b2c\u4e8c\u200b\u90e8\u5206\u200b\u662f\u200b\u65b9\u7a0b\u200b 3\u3002</p> <p>\u200b\u56de\u987e\u200b\u65b9\u7a0b\u200b 2 \u200b\u7684\u200b\u5185\u5bb9\u200b\uff1a</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell}^{\\prime} &amp;=\\operatorname{MSA}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell-1}\\right)\\right)+\\mathbf{z}_{\\ell-1}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>\u200b\u8fd9\u200b\u8868\u793a\u200b\u4e00\u4e2a\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09\u200b\u5c42\u200b\u88ab\u200b\u5305\u88f9\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LN\uff09\u200b\u5c42\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u5e26\u6709\u200b\u4e00\u4e2a\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u88ab\u200b\u52a0\u5230\u200b\u5c42\u200b\u7684\u200b\u8f93\u51fa\u200b\u4e0a\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u65b9\u7a0b\u200b 2 \u200b\u79f0\u4e3a\u200b\u201cMSA \u200b\u5757\u200b\u201d\u3002</p> <p>***\u200b\u5de6\u56fe\u200b\uff1a** \u200b\u6765\u81ea\u200b ViT \u200b\u8bba\u6587\u200b\u7684\u200b\u56fe\u200b 1\uff0c\u200b\u5176\u4e2d\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u5757\u200b\u5185\u200b\u7684\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u548c\u200b\u5f52\u4e00\u5316\u200b\u5c42\u200b\u4ee5\u53ca\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08+\uff09\u200b\u88ab\u200b\u9ad8\u4eae\u200b\u663e\u793a\u200b\u3002 \u200b\u53f3\u56fe\u200b\uff1a \u200b\u5c06\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09\u200b\u5c42\u200b\u3001\u200b\u5f52\u4e00\u5316\u200b\u5c42\u200b\u548c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u6620\u5c04\u200b\u5230\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u65b9\u7a0b\u200b 2 \u200b\u7684\u200b\u76f8\u5e94\u200b\u90e8\u5206\u200b\u3002*</p> <p>\u200b\u8bb8\u591a\u200b\u5728\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u4e2d\u200b\u53d1\u73b0\u200b\u7684\u200b\u5c42\u200b\u5df2\u7ecf\u200b\u5728\u200b\u73b0\u4ee3\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u4e2d\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u4f8b\u5982\u200b PyTorch\u3002</p> <p>\u200b\u5c3d\u7ba1\u5982\u6b64\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u7528\u200b PyTorch \u200b\u4ee3\u7801\u200b\u590d\u5236\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u548c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\uff1a</p> <ul> <li>\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09 - <code>torch.nn.MultiheadAttention()</code>\u3002</li> <li>\u200b\u5f52\u4e00\u5316\u200b\uff08LN \u200b\u6216\u200b LayerNorm\uff09 - <code>torch.nn.LayerNorm()</code>\u3002</li> <li>\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b - \u200b\u5c06\u200b\u8f93\u5165\u200b\u52a0\u200b\u5230\u200b\u8f93\u51fa\u200b\u4e0a\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b 7.1 \u200b\u8282\u200b\u521b\u5efa\u200b\u5b8c\u6574\u200b\u7684\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u5757\u200b\u65f6\u200b\u770b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u3002</li> </ul>"},{"location":"08_pytorch_paper_replicating/#51-layernorm","title":"5.1 LayerNorm\uff08\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff09\u200b\u5c42\u200b\u00b6","text":"<p>\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08<code>torch.nn.LayerNorm()</code> \u200b\u6216\u200b Norm \u200b\u6216\u200b LayerNorm \u200b\u6216\u200b LN\uff09\u200b\u5bf9\u200b\u8f93\u5165\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\u8fdb\u884c\u200b\u5f52\u4e00\u5316\u200b\u5904\u7406\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b PyTorch \u200b\u6587\u6863\u200b\u4e2d\u200b\u627e\u5230\u200b <code>torch.nn.LayerNorm()</code> \u200b\u7684\u200b\u6b63\u5f0f\u200b\u5b9a\u4e49\u200b\u3002</p> <p>PyTorch \u200b\u7684\u200b <code>torch.nn.LayerNorm()</code> \u200b\u7684\u200b\u4e3b\u8981\u53c2\u6570\u200b\u662f\u200b <code>normalized_shape</code>\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5f52\u4e00\u5316\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u5927\u5c0f\u200b\uff08\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u4e8e\u200b ViT-Base \u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u662f\u200b $D$ \u200b\u6216\u200b <code>768</code>\uff09\u3002</p> <p>\u200b\u5b83\u200b\u6709\u200b\u4ec0\u4e48\u200b\u4f5c\u7528\u200b\uff1f</p> <p>\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\u6709\u52a9\u4e8e\u200b\u6539\u5584\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u548c\u200b\u6a21\u578b\u200b\u7684\u200b\u6cdb\u5316\u200b\u80fd\u529b\u200b\uff08\u200b\u9002\u5e94\u200b\u672a\u89c1\u200b\u6570\u636e\u200b\u7684\u200b\u80fd\u529b\u200b\uff09\u3002</p> <p>\u200b\u6211\u200b\u559c\u6b22\u200b\u5c06\u200b\u4efb\u4f55\u200b\u7c7b\u578b\u200b\u7684\u200b\u5f52\u4e00\u5316\u200b\u89c6\u4e3a\u200b\u201c\u200b\u5c06\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u76f8\u4f3c\u200b\u7684\u200b\u683c\u5f0f\u200b\u201d\u200b\u6216\u200b\u201c\u200b\u5c06\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u76f8\u4f3c\u200b\u7684\u200b\u5206\u5e03\u200b\u201d\u3002</p> <p>\u200b\u60f3\u8c61\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u8bd5\u56fe\u200b\u8d70\u4e0a\u200b\uff08\u200b\u6216\u200b\u8d70\u200b\u4e0b\u200b\uff09\u200b\u4e00\u7ec4\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u957f\u5ea6\u200b\u5404\u4e0d\u76f8\u540c\u200b\u7684\u200b\u697c\u68af\u200b\u3002</p> <p>\u200b\u6bcf\u200b\u4e00\u6b65\u200b\u90fd\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u8c03\u6574\u200b\uff0c\u200b\u5bf9\u200b\u5427\u200b\uff1f</p> <p>\u200b\u800c\u4e14\u200b\u4f60\u200b\u5728\u200b\u6bcf\u200b\u4e00\u6b65\u200b\u5b66\u5230\u200b\u7684\u200b\u77e5\u8bc6\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u6709\u52a9\u4e8e\u200b\u4e0b\u200b\u4e00\u6b65\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u90fd\u200b\u4e0d\u540c\u200b\uff0c\u200b\u8fd9\u200b\u589e\u52a0\u200b\u4e86\u200b\u4f60\u200b\u7a7f\u8d8a\u200b\u697c\u68af\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u5f52\u4e00\u5316\u200b\uff08\u200b\u5305\u62ec\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff09\u200b\u76f8\u5f53\u4e8e\u200b\u8ba9\u200b\u6240\u6709\u200b\u697c\u68af\u200b\u7684\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u957f\u5ea6\u200b\u90fd\u200b\u76f8\u540c\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u91cc\u200b\u7684\u200b\u697c\u68af\u200b\u662f\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u4f60\u200b\u8d70\u4e0a\u200b\uff08\u200b\u6216\u200b\u8d70\u200b\u4e0b\u200b\uff09\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u957f\u5ea6\u200b\u76f8\u4f3c\u200b\u7684\u200b\u697c\u68af\u200b\u6bd4\u200b\u90a3\u4e9b\u200b\u9ad8\u5ea6\u200b\u548c\u200b\u5bbd\u5ea6\u200b\u4e0d\u200b\u7b49\u200b\u7684\u200b\u697c\u68af\u200b\u5bb9\u6613\u200b\u5f97\u200b\u591a\u200b\u4e00\u6837\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5728\u200b\u4f18\u5316\u200b\u5177\u6709\u200b\u76f8\u4f3c\u200b\u5206\u5e03\u200b\uff08\u200b\u76f8\u4f3c\u200b\u7684\u200b\u5747\u503c\u200b\u548c\u200b\u6807\u51c6\u5dee\u200b\uff09\u200b\u7684\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u65f6\u200b\uff0c\u200b\u6bd4\u200b\u90a3\u4e9b\u200b\u5206\u5e03\u200b\u5404\u5f02\u200b\u7684\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u66f4\u200b\u5bb9\u6613\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#52-msa","title":"5.2 \u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09\u200b\u5c42\u200b\u00b6","text":"<p>\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u548c\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff08\u200b\u591a\u6b21\u200b\u5e94\u7528\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff09\u200b\u7684\u200b\u5f3a\u5927\u200b\u80fd\u529b\u200b\u5728\u200b\u300a\u200b\u6ce8\u610f\u529b\u200b\u5c31\u662f\u200b\u4f60\u200b\u6240\u200b\u9700\u8981\u200b\u7684\u200b\u4e00\u5207\u200b\u300b\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u539f\u59cb\u200bTransformer\u200b\u67b6\u6784\u200b\u4e2d\u200b\u5f97\u5230\u200b\u4e86\u200b\u4f53\u73b0\u200b\u3002</p> <p>\u200b\u6700\u521d\u200b\u8bbe\u8ba1\u200b\u7528\u4e8e\u200b\u6587\u672c\u200b\u8f93\u5165\u200b\uff0c\u200b\u539f\u59cb\u200b\u7684\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u63a5\u53d7\u200b\u4e00\u7cfb\u5217\u200b\u5355\u8bcd\u200b\uff0c\u200b\u7136\u540e\u200b\u8ba1\u7b97\u200b\u54ea\u4e2a\u200b\u5355\u8bcd\u200b\u5e94\u8be5\u200b\u5bf9\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5355\u8bcd\u200b\u7ed9\u4e88\u200b\u66f4\u200b\u591a\u200b\u201c\u200b\u5173\u6ce8\u200b\u201d\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5728\u200b\u53e5\u5b50\u200b\u201cthe dog jumped over the fence\u201d\u200b\u4e2d\u200b\uff0c\u200b\u53ef\u80fd\u200b\u5355\u8bcd\u200b\u201cdog\u201d\u200b\u4e0e\u200b\u201cjumped\u201d\u200b\u548c\u200b\u201cfence\u201d\u200b\u6709\u200b\u5f88\u200b\u5f3a\u200b\u7684\u200b\u5173\u8054\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u867d\u7136\u200b\u7b80\u5316\u200b\u4e86\u200b\uff0c\u200b\u4f46\u200b\u524d\u63d0\u200b\u4ecd\u7136\u200b\u9002\u7528\u200b\u4e8e\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u662f\u200b\u4e00\u7cfb\u5217\u200b\u56fe\u50cf\u200b\u5757\u200b\u800c\u200b\u4e0d\u662f\u200b\u5355\u8bcd\u200b\uff0c\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u548c\u200b\u968f\u540e\u200b\u7684\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u56fe\u50cf\u200b\u7684\u200b\u54ea\u4e2a\u200b\u5757\u200b\u4e0e\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5757\u200b\u6700\u200b\u76f8\u5173\u200b\uff0c\u200b\u6700\u7ec8\u200b\u5f62\u6210\u200b\u56fe\u50cf\u200b\u7684\u200b\u5b66\u4e60\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8fd9\u200b\u4e00\u5c42\u200b\u5728\u200b\u7ed9\u5b9a\u200b\u6570\u636e\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u81ea\u884c\u200b\u5b8c\u6210\u200b\u8fd9\u4e00\u200b\u8fc7\u7a0b\u200b\uff08\u200b\u6211\u4eec\u200b\u4e0d\u200b\u544a\u8bc9\u200b\u5b83\u200b\u5b66\u4e60\u200b\u4ec0\u4e48\u200b\u6a21\u5f0f\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u901a\u8fc7\u200bMSA\u200b\u5f62\u6210\u200b\u7684\u200b\u5c42\u200b\u5b66\u4e60\u200b\u8868\u793a\u200b\u662f\u200b\u597d\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u6a21\u578b\u200b\u7684\u200b\u6027\u80fd\u200b\u4e2d\u200b\u770b\u5230\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u7f51\u4e0a\u200b\u6709\u200b\u8bb8\u591a\u200b\u8d44\u6e90\u200b\u53ef\u4ee5\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200bTransformer\u200b\u67b6\u6784\u200b\u548c\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f8b\u5982\u200bJay Alammar\u200b\u7cbe\u5f69\u200b\u7684\u200b\u300aIllustrated Transformer\u300b\u200b\u6587\u7ae0\u200b\u548c\u200b\u300aIllustrated Attention\u300b\u200b\u6587\u7ae0\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u66f4\u200b\u591a\u200b\u5730\u200b\u5173\u6ce8\u200b\u7f16\u5199\u200b\u73b0\u6709\u200b\u7684\u200bPyTorch MSA\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u9644\u5f55\u200bA\u200b\u4e2d\u200b\u627e\u5230\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200bMSA\u200b\u5b9e\u73b0\u200b\u7684\u200b\u6b63\u5f0f\u200b\u5b9a\u4e49\u200b\uff1a</p> <p>***\u200b\u5de6\u200b\uff1a** \u200b\u6765\u81ea\u200bViT\u200b\u8bba\u6587\u200b\u56fe\u200b1\u200b\u7684\u200bVision Transformer\u200b\u67b6\u6784\u200b\u6982\u89c8\u200b\u3002 \u200b\u53f3\u200b\uff1a \u200b\u7a81\u51fa\u200b\u663e\u793a\u200b\u4e86\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u516c\u5f0f\u200b2\u3001\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u548c\u200b\u9644\u5f55\u200bA\u200b\u7684\u200b\u5b9a\u4e49\u200b\uff0c\u200b\u4ee5\u200b\u53cd\u6620\u200b\u5b83\u4eec\u200b\u5728\u200b\u56fe\u200b1\u200b\u4e2d\u200b\u7684\u200b\u76f8\u5e94\u200b\u90e8\u5206\u200b\u3002*</p> <p>\u200b\u4e0a\u200b\u56fe\u200b\u7a81\u51fa\u200b\u663e\u793a\u200b\u4e86\u200b\u8f93\u5165\u200b\u5230\u200bMSA\u200b\u5c42\u200b\u7684\u200b\u4e09\u91cd\u200b\u5d4c\u5165\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u67e5\u8be2\u200b\u3001\u200b\u952e\u200b\u3001\u200b\u503c\u200b\u8f93\u5165\u200b\uff0c\u200b\u7b80\u79f0\u200bqkv\uff0c\u200b\u8fd9\u200b\u662f\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200b\u57fa\u7840\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u4e09\u91cd\u200b\u5d4c\u5165\u200b\u8f93\u5165\u200b\u5c06\u200b\u662f\u200bNorm\u200b\u5c42\u200b\u8f93\u51fa\u200b\u7684\u200b\u4e09\u4e2a\u200b\u7248\u672c\u200b\uff0c\u200b\u5206\u522b\u200b\u7528\u4e8e\u200b\u67e5\u8be2\u200b\u3001\u200b\u952e\u548c\u503c\u200b\u3002</p> <p>\u200b\u6216\u8005\u8bf4\u200b\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b4.8\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\u56fe\u50cf\u200b\u5757\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u7684\u200b\u4e09\u4e2a\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u4f7f\u7528\u200b<code>torch.nn.MultiheadAttention()</code>\u200b\u5b9e\u73b0\u200bMSA\u200b\u5c42\u200b\uff0c\u200b\u53c2\u6570\u200b\u5982\u4e0b\u200b\uff1a</p> <ul> <li><code>embed_dim</code> - \u200b\u6765\u81ea\u200b\u8868\u200b1\u200b\u7684\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff08\u200b\u9690\u85cf\u200b\u5927\u5c0f\u200b$D$\uff09\u3002</li> <li><code>num_heads</code> - \u200b\u4f7f\u7528\u200b\u591a\u5c11\u200b\u4e2a\u200b\u6ce8\u610f\u529b\u200b\u5934\u200b\uff08\u200b\u8fd9\u200b\u5c31\u662f\u200b\u201c\u200b\u591a\u5934\u200b\u201d\u200b\u8fd9\u4e00\u200b\u672f\u8bed\u200b\u7684\u200b\u6765\u6e90\u200b\uff09\uff0c\u200b\u8fd9\u4e2a\u200b\u503c\u200b\u4e5f\u200b\u5728\u200b\u8868\u200b1\u200b\u4e2d\u200b\uff08\u200b\u5934\u6570\u200b\uff09\u3002</li> <li><code>dropout</code> - \u200b\u662f\u5426\u200b\u5bf9\u200b\u6ce8\u610f\u529b\u200b\u5c42\u200b\u5e94\u7528\u200bdropout\uff08\u200b\u6839\u636e\u200b\u9644\u5f55\u200bB.1\uff0cqkv\u200b\u6295\u5f71\u200b\u540e\u200b\u4e0d\u200b\u4f7f\u7528\u200bdropout\uff09\u3002</li> <li><code>batch_first</code> - \u200b\u6211\u4eec\u200b\u7684\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u662f\u5426\u200b\u5728\u200b\u9996\u4f4d\u200b\uff1f\uff08\u200b\u662f\u200b\u7684\u200b\uff09</li> </ul>"},{"location":"08_pytorch_paper_replicating/#53-pytorch2","title":"5.3 \u200b\u4f7f\u7528\u200bPyTorch\u200b\u5c42\u200b\u91cd\u73b0\u200b\u516c\u5f0f\u200b2\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u516c\u5f0f\u200b2\u200b\u4e2d\u200b\u8ba8\u8bba\u200b\u7684\u200b\u5173\u4e8e\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LayerNorm\uff0cLN\uff09\u200b\u548c\u200b\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff08Multi-Head Attention\uff0cMSA\uff09\u200b\u5c42\u200b\u7684\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u4ed8\u8bf8\u5b9e\u8df5\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>MultiheadSelfAttentionBlock</code>\u200b\u7684\u200b\u7c7b\u200b\uff0c\u200b\u8be5\u7c7b\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>torch.nn.Module</code>\u3002</li> <li>\u200b\u4f7f\u7528\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u8868\u200b1\u200b\u7684\u200bViT-Base\u200b\u6a21\u578b\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u521d\u59cb\u5316\u200b\u8be5\u7c7b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b<code>torch.nn.LayerNorm()</code>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LN\uff09\u200b\u5c42\u200b\uff0c\u200b\u5176\u200b<code>normalized_shape</code>\u200b\u53c2\u6570\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff08\u200b\u8868\u200b1\u200b\u4e2d\u200b\u7684\u200b$D$\uff09\u200b\u76f8\u540c\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u9002\u5f53\u200b\u7684\u200b<code>embed_dim</code>\u3001<code>num_heads</code>\u3001<code>dropout</code>\u200b\u548c\u200b<code>batch_first</code>\u200b\u53c2\u6570\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u591a\u5934\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff09\u200b\u5c42\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u7c7b\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b<code>forward()</code>\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5c06\u200b\u8f93\u5165\u200b\u901a\u8fc7\u200bLN\u200b\u5c42\u200b\u548c\u200bMSA\u200b\u5c42\u200b\u3002</li> </ol>"},{"location":"08_pytorch_paper_replicating/#6-3mlp","title":"6. \u200b\u516c\u5f0f\u200b 3\uff1a\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff08MLP\uff09\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u8fdb\u5c55\u200b\u987a\u5229\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u524d\u8fdb\u200b\uff0c\u200b\u91cd\u73b0\u200b\u516c\u5f0f\u200b 3\uff1a</p> <p>$$ \\begin{aligned} \\mathbf{z}_{\\ell} &amp;=\\operatorname{MLP}\\left(\\operatorname{LN}\\left(\\mathbf{z}_{\\ell}^{\\prime}\\right)\\right)+\\mathbf{z}_{\\ell}^{\\prime}, &amp; &amp; \\ell=1 \\ldots L \\end{aligned} $$</p> <p>\u200b\u8fd9\u91cc\u200b MLP \u200b\u4ee3\u8868\u200b\u201c\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\u201d\uff0cLN \u200b\u4ee3\u8868\u200b\u201c\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\u201d\uff08\u200b\u5982\u4e0a\u6240\u8ff0\u200b\uff09\u3002</p> <p>\u200b\u672b\u5c3e\u200b\u7684\u200b\u52a0\u6cd5\u200b\u662f\u200b\u8df3\u8dc3\u200b/\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u516c\u5f0f\u200b 3 \u200b\u79f0\u4e3a\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u7684\u200b\u201cMLP \u200b\u5757\u200b\u201d\uff08\u200b\u6ce8\u610f\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u7ee7\u7eed\u200b\u5c06\u200b\u67b6\u6784\u200b\u5206\u89e3\u200b\u4e3a\u200b\u66f4\u200b\u5c0f\u200b\u7684\u200b\u90e8\u5206\u200b\uff09\u3002</p> <p>***\u200b\u5de6\u200b\uff1a** ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u56fe\u200b 1\uff0c\u200b\u7a81\u51fa\u200b\u663e\u793a\u200b\u4e86\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u5757\u200b\u4e2d\u200b\u7684\u200b MLP \u200b\u548c\u200b\u5f52\u4e00\u5316\u200b\u5c42\u200b\u4ee5\u53ca\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08+\uff09\u3002 \u200b\u53f3\u200b\uff1a \u200b\u5c06\u200b\u591a\u5c42\u200b\u611f\u77e5\u673a\u200b\uff08MLP\uff09\u200b\u5c42\u200b\u3001\u200b\u5f52\u4e00\u5316\u200b\u5c42\u200b\uff08LN\uff09\u200b\u548c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u6620\u5c04\u200b\u5230\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u516c\u5f0f\u200b 3 \u200b\u7684\u200b\u76f8\u5e94\u200b\u90e8\u5206\u200b\u3002*</p>"},{"location":"08_pytorch_paper_replicating/#61-mlp","title":"6.1 MLP\u200b\u5c42\u200b\u00b6","text":"<p>MLP\uff08\u200b\u591a\u5c42\u200b\u611f\u77e5\u5668\u200b\uff09\u200b\u8fd9\u4e2a\u200b\u672f\u8bed\u200b\u975e\u5e38\u200b\u5bbd\u6cdb\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u51e0\u4e4e\u200b\u53ef\u4ee5\u200b\u6307\u4ee3\u200b\u4efb\u4f55\u200b\u5f62\u5f0f\u200b\u7684\u200b\u591a\u5c42\u200b\u7ec4\u5408\u200b\uff08\u200b\u56e0\u6b64\u200b\u5f97\u540d\u200b\u201c\u200b\u591a\u5c42\u200b\u611f\u77e5\u5668\u200b\u201d\uff09\u3002</p> <p>\u200b\u4f46\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u7ed3\u6784\u200b\u9075\u5faa\u200b\u4ee5\u4e0b\u200b\u6a21\u5f0f\u200b\uff1a</p> <p><code>\u200b\u7ebf\u6027\u200b\u5c42\u200b -&gt; \u200b\u975e\u7ebf\u6027\u200b\u5c42\u200b -&gt; \u200b\u7ebf\u6027\u200b\u5c42\u200b -&gt; \u200b\u975e\u7ebf\u6027\u200b\u5c42\u200b</code></p> <p>\u200b\u5728\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\uff0cMLP\u200b\u7ed3\u6784\u200b\u5728\u200b3.1\u200b\u8282\u4e2d\u200b\u5b9a\u4e49\u200b\uff1a</p> <p>MLP\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b\u5c42\u200b\uff0c\u200b\u5e76\u200b\u5177\u6709\u200bGELU\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u201c\u200b\u4e24\u4e2a\u200b\u5c42\u200b\u201d\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u7ebf\u6027\u200b\u5c42\u200b\uff08\u200b\u5728\u200bPyTorch\u200b\u4e2d\u4e3a\u200b<code>torch.nn.Linear()</code>\uff09\uff0c\u200b\u800c\u200b\u201cGELU\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u201d\u200b\u662f\u200bGELU\uff08\u200b\u9ad8\u65af\u200b\u8bef\u5dee\u200b\u7ebf\u6027\u200b\u5355\u5143\u200b\uff09\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\uff08\u200b\u5728\u200bPyTorch\u200b\u4e2d\u4e3a\u200b<code>torch.nn.GELU()</code>\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u7ebf\u6027\u200b\u5c42\u200b\uff08<code>torch.nn.Linear()</code>\uff09\u200b\u6709\u65f6\u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u201d\u200b\u6216\u200b\u201c\u200b\u524d\u9988\u200b\u5c42\u200b\u201d\u3002\u200b\u67d0\u4e9b\u200b\u8bba\u6587\u200b\u751a\u81f3\u200b\u4f7f\u7528\u200b\u8fd9\u200b\u4e09\u79cd\u200b\u672f\u8bed\u200b\u6765\u200b\u63cf\u8ff0\u200b\u540c\u4e00\u200b\u4e8b\u7269\u200b\uff08\u200b\u5982\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u6240\u793a\u200b\uff09\u3002</p> <p>\u200b\u5173\u4e8e\u200bMLP\u200b\u5757\u200b\u7684\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7ec6\u8282\u200b\u76f4\u5230\u200b\u9644\u5f55\u200bB.1\uff08\u200b\u8bad\u7ec3\u200b\uff09\u200b\u4e2d\u624d\u200b\u51fa\u73b0\u200b\uff1a</p> <p>\u200b\u8868\u200b3\u200b\u603b\u7ed3\u200b\u4e86\u200b\u6211\u4eec\u200b\u4e0d\u540c\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\u3002...\u200b\u5f53\u200b\u4f7f\u7528\u200bDropout\u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u88ab\u200b\u5e94\u7528\u200b\u4e8e\u200b\u6bcf\u4e2a\u200b\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\u4e4b\u540e\u200b\uff0c\u200b\u9664\u4e86\u200bqkv\u200b\u6295\u5f71\u200b\u5c42\u200b\u548c\u200b\u76f4\u63a5\u200b\u5728\u200b\u6dfb\u52a0\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5230\u200b\u8865\u4e01\u200b\u5d4c\u5165\u200b\u4e4b\u540e\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200bMLP\u200b\u5757\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u7ebf\u6027\u200b\u5c42\u200b\uff08\u200b\u6216\u5168\u200b\u8fde\u63a5\u200b\u5c42\u200b\uff09\u200b\u90fd\u200b\u6709\u200b\u4e00\u4e2a\u200bDropout\u200b\u5c42\u200b\uff08\u200b\u5728\u200bPyTorch\u200b\u4e2d\u4e3a\u200b<code>torch.nn.Dropout()</code>\uff09\u3002</p> <p>\u200b\u5176\u503c\u200b\u53ef\u4ee5\u200b\u5728\u200bViT\u200b\u8bba\u6587\u200b\u7684\u200b\u8868\u200b3\u200b\u4e2d\u200b\u627e\u5230\u200b\uff08\u200b\u5bf9\u4e8e\u200bViT-Base\uff0c<code>dropout=0.1</code>\uff09\u3002</p> <p>\u200b\u4e86\u89e3\u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200bMLP\u200b\u5757\u200b\u7ed3\u6784\u200b\u5c06\u200b\u662f\u200b\uff1a</p> <p><code>\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b -&gt; \u200b\u7ebf\u6027\u200b\u5c42\u200b -&gt; \u200b\u975e\u7ebf\u6027\u200b\u5c42\u200b -&gt; Dropout -&gt; \u200b\u7ebf\u6027\u200b\u5c42\u200b -&gt; Dropout</code></p> <p>\u200b\u7ebf\u6027\u200b\u5c42\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u503c\u200b\u53ef\u200b\u4ece\u8868\u200b1\u200b\u4e2d\u200b\u83b7\u53d6\u200b\uff08MLP\u200b\u5927\u5c0f\u200b\u662f\u200b\u7ebf\u6027\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u6570\u200b\uff0c\u200b\u9690\u85cf\u200b\u5927\u5c0f\u200b$D$\u200b\u662f\u200bMLP\u200b\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u5927\u5c0f\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#62-pytorch3","title":"6.2 \u200b\u4f7f\u7528\u200bPyTorch\u200b\u5c42\u200b\u590d\u5236\u200b\u516c\u5f0f\u200b3\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u516c\u5f0f\u200b3\u200b\u4e2d\u200b\u8ba8\u8bba\u200b\u7684\u200b\u5173\u4e8e\u200bLayerNorm\uff08LN\uff09\u200b\u548c\u200bMLP\uff08MSA\uff09\u200b\u5c42\u200b\u7684\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u4ed8\u8bf8\u5b9e\u8df5\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>MLPBlock</code>\u200b\u7684\u200b\u7c7b\u200b\uff0c\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>torch.nn.Module</code>\u3002</li> <li>\u200b\u4f7f\u7528\u200bViT-Base\u200b\u6a21\u578b\u200b\u7684\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u8868\u200b1\u200b\u548c\u8868\u200b3\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u521d\u59cb\u5316\u200b\u8be5\u7c7b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b<code>torch.nn.LayerNorm()</code>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LN\uff09\u200b\u5c42\u200b\uff0c\u200b\u5176\u200b<code>normalized_shape</code>\u200b\u53c2\u6570\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b\u5d4c\u5165\u200b\u7ef4\u5ea6\u200b\uff08\u200b\u8868\u200b1\u200b\u4e2d\u200b\u7684\u200b$D$\uff09\u200b\u76f8\u540c\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b<code>torch.nn.Linear()</code>\u3001<code>torch.nn.Dropout()</code>\u200b\u548c\u200b<code>torch.nn.GELU()</code>\u200b\u521b\u5efa\u200b\u4e00\u7cfb\u5217\u200bMLP\u200b\u5c42\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u8868\u200b1\u200b\u548c\u8868\u200b3\u200b\u4e2d\u200b\u7684\u200b\u9002\u5f53\u200b\u8d85\u200b\u53c2\u6570\u503c\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u7c7b\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b<code>forward()</code>\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u901a\u8fc7\u200bLN\u200b\u5c42\u200b\u548c\u200bMLP\u200b\u5c42\u200b\u4f20\u9012\u200b\u8f93\u5165\u200b\u3002</li> </ol>"},{"location":"08_pytorch_paper_replicating/#7-transformer","title":"7. \u200b\u521b\u5efa\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b<code>MultiheadSelfAttentionBlock</code>\uff08\u200b\u65b9\u7a0b\u200b2\uff09\u200b\u548c\u200b<code>MLPBlock</code>\uff08\u200b\u65b9\u7a0b\u200b3\uff09\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u521b\u5efa\u200bViT\u200b\u67b6\u6784\u200b\u4e2d\u200b\u7684\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\"\u200b\u7f16\u7801\u5668\u200b\"\u200b\u6216\u200b\"\u200b\u81ea\u52a8\u200b\u7f16\u7801\u5668\u200b\"\u200b\u901a\u5e38\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u4e00\u7cfb\u5217\u200b\u5c42\u200b\uff0c\u200b\u7528\u4e8e\u200b\"\u200b\u7f16\u7801\u200b\"\u200b\u8f93\u5165\u200b\uff08\u200b\u5c06\u200b\u5176\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u67d0\u79cd\u200b\u5f62\u5f0f\u200b\u7684\u200b\u6570\u503c\u200b\u8868\u793a\u200b\uff09\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0cTransformer\u200b\u7f16\u7801\u5668\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e00\u7cfb\u5217\u200b\u4ea4\u66ff\u200b\u7684\u200bMSA\u200b\u5757\u200b\u548c\u200bMLP\u200b\u5757\u200b\uff0c\u200b\u6839\u636e\u200bViT\u200b\u8bba\u6587\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\u6240\u8ff0\u200b\uff0c\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u5206\u5757\u200b\u56fe\u50cf\u200b\u5d4c\u5165\u200b\u7f16\u7801\u200b\u4e3a\u200b\u5b66\u4e60\u200b\u5230\u200b\u7684\u200b\u8868\u793a\u200b\uff1a</p> <p>Transformer\u200b\u7f16\u7801\u5668\u200b\uff08Vaswani\u200b\u7b49\u200b\u4eba\u200b\uff0c2017\uff09\u200b\u7531\u200b\u4ea4\u66ff\u200b\u7684\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08MSA\uff0c\u200b\u89c1\u200b\u9644\u5f55\u200bA\uff09\u200b\u548c\u200bMLP\u200b\u5757\u200b\uff08\u200b\u65b9\u7a0b\u200b2\u30013\uff09\u200b\u7ec4\u6210\u200b\u3002\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u5757\u200b\u4e4b\u524d\u200b\u5e94\u7528\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08LN\uff09\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u5757\u200b\u4e4b\u540e\u200b\u5e94\u7528\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08Wang\u200b\u7b49\u200b\u4eba\u200b\uff0c2019\uff1bBaevski &amp; Auli\uff0c2019\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u521b\u5efa\u200b\u4e86\u200bMSA\u200b\u548c\u200bMLP\u200b\u5757\u200b\uff0c\u200b\u4f46\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u8df3\u8dc3\u200b\u8fde\u63a5\u200b\uff09\uff0c\u200b\u9996\u6b21\u200b\u5728\u200b\u8bba\u6587\u200b\u300a\u200b\u7528\u4e8e\u200b\u56fe\u50cf\u8bc6\u522b\u200b\u7684\u200b\u6df1\u5ea6\u200b\u6b8b\u5dee\u200b\u5b66\u4e60\u200b\u300b\u200b\u4e2d\u200b\u63d0\u51fa\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5c06\u200b\u5176\u200b\u8f93\u5165\u200b\u4e0e\u5176\u200b\u540e\u7eed\u200b\u8f93\u51fa\u200b\u76f8\u52a0\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</p> <p>\u200b\u540e\u7eed\u200b\u8f93\u51fa\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u6216\u200b\u591a\u4e2a\u200b\u5c42\u200b\u4e4b\u540e\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u5728\u200bViT\u200b\u67b6\u6784\u200b\u4e2d\u200b\uff0c\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u610f\u5473\u7740\u200bMSA\u200b\u5757\u200b\u7684\u200b\u8f93\u5165\u200b\u5728\u200b\u4f20\u9012\u200b\u5230\u200bMLP\u200b\u5757\u200b\u4e4b\u524d\u200b\u88ab\u52a0\u200b\u56de\u5230\u200bMSA\u200b\u5757\u200b\u7684\u200b\u8f93\u51fa\u200b\u3002</p> <p>MLP\u200b\u5757\u200b\u5728\u200b\u4f20\u9012\u200b\u5230\u200b\u4e0b\u200b\u4e00\u4e2a\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u5757\u200b\u4e4b\u524d\u200b\u4e5f\u200b\u4f1a\u200b\u53d1\u751f\u200b\u540c\u6837\u200b\u7684\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u7528\u4f2a\u200b\u4ee3\u7801\u200b\u8868\u793a\u200b\uff1a</p> <p><code>x_input -&gt; MSA_block -&gt; [MSA_block_output + x_input] -&gt; MLP_block -&gt; [MLP_block_output + MSA_block_output + x_input] -&gt; ...</code></p> <p>\u200b\u8fd9\u6837\u200b\u505a\u200b\u6709\u200b\u4ec0\u4e48\u200b\u4f5c\u7528\u200b\uff1f</p> <p>\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u7684\u200b\u4e3b\u8981\u200b\u601d\u60f3\u200b\u4e4b\u4e00\u200b\u662f\u200b\u9632\u6b62\u200b\u6743\u91cd\u200b\u503c\u200b\u548c\u200b\u68af\u5ea6\u200b\u66f4\u65b0\u200b\u53d8\u5f97\u200b\u8fc7\u200b\u5c0f\u200b\uff0c\u200b\u4ece\u800c\u200b\u5141\u8bb8\u200b\u66f4\u6df1\u200b\u7684\u200b\u7f51\u7edc\u200b\uff0c\u200b\u8fdb\u800c\u200b\u5141\u8bb8\u200b\u5b66\u4e60\u200b\u66f4\u6df1\u200b\u7684\u200b\u8868\u793a\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6807\u5fd7\u6027\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u67b6\u6784\u200b\"ResNet\"\u200b\u4e4b\u6240\u4ee5\u200b\u5982\u6b64\u200b\u547d\u540d\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200b\u5f15\u5165\u200b\u4e86\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b<code>torchvision.models</code>\u200b\u4e2d\u200b\u627e\u5230\u200b\u8bb8\u591a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bResNet\u200b\u67b6\u6784\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#71-transformer","title":"7.1 \u200b\u901a\u8fc7\u200b\u7ec4\u5408\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u7684\u200b\u5c42\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u00b6","text":"<p>\u200b\u8bf4\u591f\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\uff0c\u200b\u901a\u8fc7\u200b\u7ec4\u5408\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b\u5c42\u200b\u6765\u200b\u4f7f\u7528\u200bPyTorch\u200b\u5236\u4f5c\u200b\u4e00\u4e2a\u200bViT Transformer\u200b\u7f16\u7801\u5668\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>TransformerEncoderBlock</code>\u200b\u7684\u200b\u7c7b\u200b\uff0c\u200b\u7ee7\u627f\u200b\u81ea\u200b<code>torch.nn.Module</code>\u3002</li> <li>\u200b\u4f7f\u7528\u200bViT-Base\u200b\u6a21\u578b\u200b\u7684\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u8868\u200b1\u200b\u548c\u8868\u200b3\u200b\u7684\u200b hyperparameters \u200b\u521d\u59cb\u5316\u200b\u8be5\u7c7b\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7b2c\u200b5.2\u200b\u8282\u4e2d\u200b\u7684\u200b<code>MultiheadSelfAttentionBlock</code>\uff0c\u200b\u4ee5\u200b\u9002\u5f53\u200b\u7684\u200b\u53c2\u6570\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u516c\u5f0f\u200b2\u200b\u7684\u200bMSA\u200b\u5757\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7b2c\u200b6.2\u200b\u8282\u4e2d\u200b\u7684\u200b<code>MLPBlock</code>\uff0c\u200b\u4ee5\u200b\u9002\u5f53\u200b\u7684\u200b\u53c2\u6570\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u516c\u5f0f\u200b3\u200b\u7684\u200bMLP\u200b\u5757\u200b\u3002</li> <li>\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b<code>TransformerEncoderBlock</code>\u200b\u7c7b\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b<code>forward()</code>\u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u4e3a\u200bMSA\u200b\u5757\u200b\uff08\u200b\u7528\u4e8e\u200b\u516c\u5f0f\u200b2\uff09\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002</li> <li>\u200b\u4e3a\u200bMLP\u200b\u5757\u200b\uff08\u200b\u7528\u4e8e\u200b\u516c\u5f0f\u200b3\uff09\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6b8b\u5dee\u200b\u8fde\u63a5\u200b\u3002</li> </ol>"},{"location":"08_pytorch_paper_replicating/#72-pytorchtransformertransformer","title":"7.2 \u200b\u4f7f\u7528\u200bPyTorch\u200b\u7684\u200bTransformer\u200b\u5c42\u200b\u521b\u5efa\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u00b6","text":"<p>\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u81ea\u5df1\u200b\u6784\u5efa\u200b\u4e86\u200bTransformer\u200b\u7f16\u7801\u5668\u200b\u5c42\u200b\u7684\u200b\u5404\u4e2a\u200b\u7ec4\u4ef6\u200b\u53ca\u5176\u200b\u672c\u8eab\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200bTransformer\u200b\u7684\u200b\u6d41\u884c\u200b\u548c\u200b\u6709\u6548\u6027\u200b\uff0cPyTorch\u200b\u73b0\u5728\u200b\u5185\u7f6e\u200b\u4e86\u200bTransformer\u200b\u5c42\u200b\u4f5c\u4e3a\u200b<code>torch.nn</code>\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>torch.nn.TransformerEncoderLayer()</code>\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u521b\u5efa\u200b\u7684\u200b<code>TransformerEncoderBlock</code>\uff0c\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u4e0e\u200b\u4e0a\u8ff0\u200b\u76f8\u540c\u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#8-vit","title":"8. \u200b\u7efc\u5408\u200b\u6240\u6709\u200b\u90e8\u5206\u200b\u521b\u5efa\u200b ViT\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8d70\u200b\u4e86\u200b\u5f88\u957f\u200b\u7684\u200b\u8def\u200b\uff01</p> <p>\u200b\u4f46\u200b\u73b0\u5728\u200b\u662f\u200b\u8981\u200b\u505a\u200b\u6fc0\u52a8\u4eba\u5fc3\u200b\u7684\u200b\u4e8b\u60c5\u200b\u7684\u200b\u65f6\u5019\u200b\u4e86\u200b\uff0c\u200b\u628a\u200b\u6240\u6709\u200b\u7684\u200b\u62fc\u56fe\u200b\u788e\u7247\u200b\u62fc\u5728\u4e00\u8d77\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ed3\u5408\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u5757\u200b\u6765\u200b\u590d\u5236\u200b\u5b8c\u6574\u200b\u7684\u200b ViT \u200b\u67b6\u6784\u200b\u3002</p> <p>\u200b\u4ece\u200b\u5206\u5757\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5230\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u518d\u200b\u5230\u200b MLP \u200b\u5934\u90e8\u200b\u3002</p> <p>\u200b\u4f46\u662f\u200b\u7b49\u7b49\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u6ca1\u6709\u200b\u521b\u5efa\u200b\u65b9\u7a0b\u200b 4...</p> <p>$$ \\begin{aligned} \\mathbf{y} &amp;=\\operatorname{LN}\\left(\\mathbf{z}_{L}^{0}\\right) &amp; &amp; \\end{aligned} $$</p> <p>\u200b\u522b\u200b\u62c5\u5fc3\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u628a\u200b\u65b9\u7a0b\u200b 4 \u200b\u653e\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u6574\u4f53\u200b ViT \u200b\u67b6\u6784\u200b\u7c7b\u4e2d\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b <code>torch.nn.LayerNorm()</code> \u200b\u5c42\u200b\u548c\u200b\u4e00\u4e2a\u200b <code>torch.nn.Linear()</code> \u200b\u5c42\u200b\uff0c\u200b\u5c06\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u903b\u8f91\u200b\u8f93\u51fa\u200b\u7684\u200b\u7b2c\u200b 0 \u200b\u4e2a\u200b\u7d22\u5f15\u200b ($\\mathbf{z}_{L}^{0}$) \u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6211\u4eec\u200b\u62e5\u6709\u200b\u7684\u200b\u76ee\u6807\u200b\u7c7b\u522b\u200b\u6570\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u521b\u5efa\u200b\u5b8c\u6574\u200b\u7684\u200b\u67b6\u6784\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u9700\u8981\u200b\u5c06\u200b\u591a\u4e2a\u200b <code>TransformerEncoderBlock</code> \u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u5b83\u4eec\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>torch.nn.Sequential()</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\uff08\u200b\u8fd9\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u7cfb\u5217\u200b\u7684\u200b <code>TransformerEncoderBlock</code>\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5173\u6ce8\u200b\u8868\u200b 1 \u200b\u4e2d\u200b\u7684\u200b ViT-Base \u200b\u8d85\u200b\u53c2\u6570\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u4ee3\u7801\u200b\u5e94\u8be5\u200b\u9002\u7528\u200b\u4e8e\u200b\u5176\u4ed6\u200b ViT \u200b\u53d8\u4f53\u200b\u3002</p> <p>\u200b\u521b\u5efa\u200b ViT \u200b\u5c06\u200b\u662f\u200b\u6211\u4eec\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u5927\u200b\u7684\u200b\u4ee3\u7801\u200b\u5757\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u80fd\u200b\u505a\u5230\u200b\uff01</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u5b9e\u73b0\u200b\u6d3b\u200b\u8d77\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>ViT</code> \u200b\u7684\u200b\u7c7b\u200b\uff0c\u200b\u7ee7\u627f\u200b\u81ea\u200b <code>torch.nn.Module</code>\u3002</li> <li>\u200b\u4f7f\u7528\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u8868\u200b 1 \u200b\u548c\u8868\u200b 3 \u200b\u7684\u200b\u8d85\u200b\u53c2\u6570\u200b\u521d\u59cb\u5316\u200b\u7c7b\u200b\uff0c\u200b\u7528\u4e8e\u200b ViT-Base \u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u786e\u4fdd\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u80fd\u200b\u88ab\u200b\u5206\u5757\u200b\u5927\u5c0f\u200b\u6574\u9664\u200b\uff08\u200b\u56fe\u50cf\u200b\u5e94\u200b\u88ab\u200b\u5206\u5272\u200b\u6210\u200b\u5747\u5300\u200b\u7684\u200b\u5206\u5757\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u516c\u5f0f\u200b $N=H W / P^{2}$ \u200b\u8ba1\u7b97\u200b\u5206\u5757\u200b\u6570\u91cf\u200b\uff0c\u200b\u5176\u4e2d\u200b $H$ \u200b\u662f\u200b\u56fe\u50cf\u200b\u9ad8\u5ea6\u200b\uff0c$W$ \u200b\u662f\u200b\u56fe\u50cf\u200b\u5bbd\u5ea6\u200b\uff0c$P$ \u200b\u662f\u200b\u5206\u5757\u200b\u5927\u5c0f\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u7c7b\u522b\u200b\u5d4c\u5165\u200b\u4ee4\u724c\u200b\uff08\u200b\u65b9\u7a0b\u200b 1\uff09\uff0c\u200b\u5982\u4e0a\u200b\u6587\u7b2c\u200b 4.6 \u200b\u8282\u200b\u6240\u8ff0\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5b66\u4e60\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u5411\u91cf\u200b\uff08\u200b\u65b9\u7a0b\u200b 1\uff09\uff0c\u200b\u5982\u4e0a\u200b\u6587\u7b2c\u200b 4.7 \u200b\u8282\u200b\u6240\u8ff0\u200b\u3002</li> <li>\u200b\u8bbe\u7f6e\u200b\u5d4c\u5165\u200b\u4e22\u5f03\u200b\u5c42\u200b\uff0c\u200b\u5982\u200b ViT \u200b\u8bba\u6587\u200b\u9644\u5f55\u200b B.1 \u200b\u6240\u8ff0\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u7b2c\u200b 4.5 \u200b\u8282\u4e2d\u200b\u7684\u200b <code>PatchEmbedding</code> \u200b\u7c7b\u200b\u521b\u5efa\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\u5c42\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5c06\u200b\u7b2c\u200b 7.1 \u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>TransformerEncoderBlock</code> \u200b\u5217\u8868\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>torch.nn.Sequential()</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u7cfb\u5217\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u5757\u200b\uff08\u200b\u65b9\u7a0b\u200b 2 \u200b\u548c\u200b 3\uff09\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u4f20\u9012\u200b\u4e00\u4e2a\u200b <code>torch.nn.LayerNorm()</code>\uff08LN\uff09\u200b\u5c42\u200b\u548c\u200b\u4e00\u4e2a\u200b <code>torch.nn.Linear(out_features=num_classes)</code> \u200b\u5c42\u200b\uff08\u200b\u5176\u4e2d\u200b <code>num_classes</code> \u200b\u662f\u200b\u76ee\u6807\u200b\u7c7b\u522b\u200b\u6570\u200b\uff09\u200b\u5230\u200b <code>torch.nn.Sequential()</code> \u200b\u6765\u200b\u521b\u5efa\u200b MLP \u200b\u5934\u90e8\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\u6216\u200b\u65b9\u7a0b\u200b 4\uff09\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u63a5\u53d7\u200b\u8f93\u5165\u200b\u7684\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u83b7\u53d6\u200b\u8f93\u5165\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\uff08\u200b\u5f62\u72b6\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u7b2c\u200b 8 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5c42\u200b\u521b\u5efa\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\uff08\u200b\u65b9\u7a0b\u200b 1\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u7b2c\u200b 5 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5c42\u200b\u521b\u5efa\u200b\u7c7b\u522b\u200b\u4ee4\u724c\u200b\u5d4c\u5165\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torch.Tensor.expand()</code> \u200b\u5728\u200b\u7b2c\u200b 11 \u200b\u6b65\u200b\u627e\u5230\u200b\u7684\u200b\u6279\u6b21\u200b\u6570\u91cf\u200b\u4e0a\u200b\u6269\u5c55\u200b\u5b83\u200b\uff08\u200b\u65b9\u7a0b\u200b 1\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torch.cat()</code> \u200b\u5c06\u200b\u7b2c\u200b 13 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u7c7b\u522b\u200b\u4ee4\u724c\u200b\u5d4c\u5165\u200b\u8fde\u63a5\u200b\u5230\u200b\u7b2c\u200b 12 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5206\u5757\u200b\u5d4c\u5165\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b\u7ef4\u5ea6\u200b\uff08\u200b\u65b9\u7a0b\u200b 1\uff09\u3002</li> <li>\u200b\u5c06\u200b\u7b2c\u200b 6 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u6dfb\u52a0\u200b\u5230\u200b\u7b2c\u200b 14 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5206\u5757\u200b\u548c\u200b\u7c7b\u522b\u200b\u4ee4\u724c\u200b\u5d4c\u5165\u200b\uff08\u200b\u65b9\u7a0b\u200b 1\uff09\u3002</li> <li>\u200b\u5c06\u200b\u7b2c\u200b 16 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5206\u5757\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u901a\u8fc7\u200b\u7b2c\u200b 7 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u4e22\u5f03\u200b\u5c42\u200b\u3002</li> <li>\u200b\u5c06\u200b\u7b2c\u200b 16 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5206\u5757\u200b\u548c\u200b\u4f4d\u7f6e\u200b\u5d4c\u5165\u200b\u901a\u8fc7\u200b\u7b2c\u200b 9 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u5c42\u200b\u5806\u6808\u200b\uff08\u200b\u65b9\u7a0b\u200b 2 \u200b\u548c\u200b 3\uff09\u3002</li> <li>\u200b\u5c06\u200b\u7b2c\u200b 17 \u200b\u6b65\u200b Transformer \u200b\u7f16\u7801\u5668\u200b\u5c42\u200b\u5806\u6808\u200b\u8f93\u51fa\u200b\u7684\u200b\u7b2c\u200b 0 \u200b\u4e2a\u200b\u7d22\u5f15\u200b\u901a\u8fc7\u200b\u7b2c\u200b 10 \u200b\u6b65\u200b\u521b\u5efa\u200b\u7684\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\uff08\u200b\u65b9\u7a0b\u200b 4\uff09\u3002</li> <li>\u200b\u8df3\u821e\u200b\u5e76\u200b\u9ad8\u547c\u200b woohoo!!! \u200b\u6211\u4eec\u200b\u521a\u521a\u200b\u6784\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u89c6\u89c9\u200b Transformer\uff01</li> </ol> <p>\u200b\u51c6\u5907\u200b\u597d\u4e86\u5417\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u5427\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#81-vit","title":"8.1 \u200b\u83b7\u53d6\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u6458\u8981\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u624b\u5de5\u200b\u6253\u9020\u200b\u4e86\u200b\u81ea\u5df1\u200b\u7684\u200bViT\u200b\u67b6\u6784\u200b\u7248\u672c\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\u968f\u673a\u200b\u56fe\u50cf\u200b\u5f20\u91cf\u200b\u53ef\u4ee5\u200b\u5b8c\u5168\u200b\u6d41\u7ecf\u200b\u8be5\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchinfo.summary()</code> \u200b\u6765\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u4e2d\u200b\u6240\u6709\u200b\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u6982\u89c8\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6ce8\u610f\u200b\uff1a ViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u63d0\u5230\u200b\u4f7f\u7528\u200b4096\u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u200b\u9700\u8981\u200b\u76f8\u5f53\u200b\u5927\u200b\u7684\u200bCPU/GPU\u200b\u8ba1\u7b97\u200b\u5185\u5b58\u200b\u6765\u200b\u5904\u7406\u200b\uff08\u200b\u6279\u91cf\u200b\u8d8a\u5927\u200b\uff0c\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5185\u5b58\u200b\u8d8a\u200b\u591a\u200b\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u907f\u514d\u200b\u5185\u5b58\u200b\u9519\u8bef\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u575a\u6301\u200b\u4f7f\u7528\u200b32\u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u5185\u5b58\u200b\u7684\u200b\u786c\u4ef6\u200b\u8bbf\u95ee\u200b\u6743\u9650\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u968f\u65f6\u200b\u589e\u52a0\u200b\u8fd9\u4e2a\u200b\u503c\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#9-vit","title":"9. \u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u7b80\u5355\u200b\u90e8\u5206\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\uff01</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u7b80\u5355\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u5927\u90e8\u5206\u200b\u6240\u200b\u9700\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4ece\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff08<code>vit</code>\uff09\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b DataLoader\uff08<code>train_dataloader</code>\u3001<code>test_dataloader</code>\uff09\uff0c\u200b\u518d\u200b\u5230\u200b\u6211\u4eec\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u7b2c\u200b 4 \u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b <code>going_modular.going_modular.engine</code> \u200b\u5bfc\u5165\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u7684\u200b\u53ea\u662f\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#91","title":"9.1 \u200b\u521b\u5efa\u200b\u4f18\u5316\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u5728\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u641c\u7d22\u200b \"optimizer\"\uff0c\u200b\u7b2c\u200b 4.1 \u200b\u8282\u200b\u5173\u4e8e\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u5fae\u8c03\u200b\u7684\u200b\u5185\u5bb9\u200b\u5982\u4e0b\u200b\uff1a</p> <p>\u200b\u8bad\u7ec3\u200b\u4e0e\u200b\u5fae\u8c03\u200b\u3002 \u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b Adam \u200b\u4f18\u5316\u200b\u5668\u200b\uff08Kingma &amp; Ba, 2015\uff09\u200b\u8bad\u7ec3\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\uff0c\u200b\u5305\u62ec\u200b ResNets\uff0c\u200b\u8bbe\u7f6e\u200b $\\beta_{1}=0.9, \\beta_{2}=0.999$\uff0c\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b 4096\uff0c\u200b\u5e76\u200b\u5e94\u7528\u200b\u9ad8\u200b\u6743\u91cd\u200b\u8870\u51cf\u200b $0.1$\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u8fd9\u200b\u5bf9\u4e8e\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u7684\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\uff08\u200b\u9644\u5f55\u200b D.1 \u200b\u8868\u660e\u200b\uff0c\u200b\u4e0e\u200b\u5e38\u89c1\u200b\u505a\u6cd5\u200b\u76f8\u53cd\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u4e2d\u200b\uff0cAdam \u200b\u7565\u200b\u4f18\u4e8e\u200b SGD \u200b\u7528\u4e8e\u200b ResNets\uff09\u3002</p> <p>\u200b\u7531\u6b64\u53ef\u89c1\u200b\uff0c\u200b\u4ed6\u4eec\u200b\u9009\u62e9\u200b\u4f7f\u7528\u200b \"Adam\" \u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>torch.optim.Adam()</code>\uff09\u200b\u800c\u200b\u4e0d\u662f\u200b SGD\uff08\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff0c<code>torch.optim.SGD()</code>\uff09\u3002</p> <p>\u200b\u4f5c\u8005\u200b\u5c06\u200b Adam \u200b\u7684\u200b $\\beta$ \u200b\u503c\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b $\\beta_{1}=0.9, \\beta_{2}=0.999$\uff0c\u200b\u8fd9\u4e9b\u200b\u662f\u200b <code>torch.optim.Adam(betas=(0.9, 0.999))</code> \u200b\u4e2d\u200b <code>betas</code> \u200b\u53c2\u6570\u200b\u7684\u200b\u9ed8\u8ba4\u503c\u200b\u3002</p> <p>\u200b\u4ed6\u4eec\u200b\u8fd8\u200b\u63d0\u5230\u200b\u4e86\u200b\u4f7f\u7528\u6743\u200b\u91cd\u200b\u8870\u51cf\u200b\uff08\u200b\u5728\u200b\u4f18\u5316\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u9010\u6e10\u200b\u51cf\u5c0f\u200b\u6743\u91cd\u200b\u503c\u4ee5\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>torch.optim.Adam(weight_decay=0.3)</code> \u200b\u4e2d\u200b\u7684\u200b <code>weight_decay</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u8bbe\u7f6e\u200b\uff08\u200b\u6839\u636e\u200b ViT-* \u200b\u5728\u200b ImageNet-1k \u200b\u4e0a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u6839\u636e\u200b\u8868\u200b 3 \u200b\u5c06\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b 0.003\uff08\u200b\u6839\u636e\u200b ViT-* \u200b\u5728\u200b ImageNet-1k \u200b\u4e0a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\uff09\u3002</p> <p>\u200b\u6b63\u5982\u200b\u4e4b\u524d\u200b\u8ba8\u8bba\u200b\u7684\u200b\uff0c\u200b\u7531\u4e8e\u200b\u786c\u4ef6\u200b\u9650\u5236\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5c0f\u4e8e\u200b 4096 \u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\uff08\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b\u5927\u578b\u200b GPU\uff0c\u200b\u53ef\u4ee5\u200b\u968f\u610f\u200b\u589e\u52a0\u200b\u8fd9\u4e2a\u200b\u503c\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#92","title":"9.2 \u200b\u521b\u5efa\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u5947\u602a\u7684\u662f\u200b\uff0c\u200b\u5728\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u641c\u7d22\u200b \"loss\"\u3001\"loss function\" \u200b\u6216\u200b \"criterion\" \u200b\u90fd\u200b\u6ca1\u6709\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5904\u7406\u200b\u7684\u200b\u76ee\u6807\u200b\u95ee\u9898\u200b\u662f\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\uff08\u200b\u4e0e\u200b ViT \u200b\u8bba\u6587\u200b\u76f8\u540c\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.nn.CrossEntropyLoss()</code>\u3002</p>"},{"location":"08_pytorch_paper_replicating/#93-vit","title":"9.3 \u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u5c06\u8981\u200b\u4f7f\u7528\u200b\u4ec0\u4e48\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b\u6765\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u4ece\u200b <code>going_modular.going_modular</code> \u200b\u5bfc\u5165\u200b <code>engine.py</code> \u200b\u811a\u672c\u200b\uff0c\u200b\u7136\u540e\u200b\u8bbe\u7f6e\u200b\u4f18\u5316\u200b\u5668\u200b\u548c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u6700\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>engine.py</code> \u200b\u4e2d\u200b\u7684\u200b <code>train()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b 10 \u200b\u4e2a\u200b\u5468\u671f\u200b\uff08\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u5468\u671f\u200b\u6570\u6bd4\u200b ViT \u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u5c11\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u4e00\u5207\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\uff09\u3002</p>"},{"location":"08_pytorch_paper_replicating/#94","title":"9.4 \u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\u7f3a\u5c11\u200b\u4ec0\u4e48\u200b\u00b6","text":"<p>\u200b\u539f\u59cb\u200b\u7684\u200b ViT \u200b\u67b6\u6784\u200b\u5728\u200b\u591a\u4e2a\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u57fa\u51c6\u200b\u6d4b\u8bd5\u200b\u4e2d\u200b\u53d6\u5f97\u200b\u4e86\u200b\u826f\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\uff08\u200b\u5728\u200b\u53d1\u5e03\u200b\u65f6\u200b\u4e0e\u200b\u8bb8\u591a\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u7ed3\u679c\u200b\u76f8\u5f53\u200b\u6216\u200b\u66f4\u597d\u200b\uff09\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u7ed3\u679c\u200b\uff08\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff09\u200b\u5e76\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u597d\u200b\u3002</p> <p>\u200b\u6709\u200b\u51e0\u4e2a\u200b\u539f\u56e0\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\uff0c\u200b\u4f46\u200b\u4e3b\u8981\u200b\u539f\u56e0\u200b\u662f\u200b\u89c4\u6a21\u200b\u3002</p> <p>\u200b\u539f\u59cb\u200b ViT \u200b\u8bba\u6587\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u91cf\u200b\u8fdc\u5927\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u91cf\u200b\uff08\u200b\u5728\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u901a\u5e38\u200b\u603b\u662f\u200b\u4e00\u4ef6\u200b\u597d\u4e8b\u200b\uff09\uff0c\u200b\u5e76\u4e14\u200b\u8bad\u7ec3\u200b\u5468\u671f\u200b\u66f4\u957f\u200b\uff08\u200b\u89c1\u8868\u200b 3\uff09\u3002</p> \u200b\u8d85\u200b\u53c2\u6570\u503c\u200b ViT \u200b\u8bba\u6587\u200b \u200b\u6211\u4eec\u200b\u7684\u200b\u5b9e\u73b0\u200b \u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b 1.3M\uff08ImageNet-1k\uff09\uff0c14M\uff08ImageNet-21k\uff09\uff0c303M\uff08JFT\uff09 225 \u200b\u8bad\u7ec3\u200b\u5468\u671f\u200b 7\uff08\u200b\u6700\u5927\u200b\u6570\u636e\u200b\u96c6\u200b\uff09\uff0c90\uff0c300\uff08ImageNet\uff09 10 \u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b 4096 32 \u200b\u5b66\u4e60\u200b\u7387\u200b\u9884\u70ed\u200b 10k \u200b\u6b65\u200b\uff08\u200b\u8868\u200b 3\uff09 \u200b\u65e0\u200b \u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b \u200b\u7ebf\u6027\u200b/\u200b\u4f59\u5f26\u200b\uff08\u200b\u8868\u200b 3\uff09 \u200b\u65e0\u200b \u200b\u68af\u5ea6\u200b\u88c1\u526a\u200b \u200b\u5168\u5c40\u200b\u8303\u6570\u200b 1\uff08\u200b\u8868\u200b 3\uff09 \u200b\u65e0\u200b <p>\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u67b6\u6784\u200b\u4e0e\u200b\u8bba\u6587\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b ViT \u200b\u8bba\u6587\u200b\u7684\u200b\u7ed3\u679c\u200b\u662f\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u548c\u200b\u6bd4\u200b\u6211\u4eec\u200b\u66f4\u200b\u7cbe\u7ec6\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65b9\u6848\u200b\u5b9e\u73b0\u200b\u7684\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b ViT \u200b\u67b6\u6784\u200b\u7684\u200b\u89c4\u6a21\u200b\u53ca\u5176\u200b\u9ad8\u200b\u6570\u91cf\u200b\u7684\u200b\u53c2\u6570\u200b\uff08\u200b\u589e\u52a0\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\uff09\uff0c\u200b\u4ee5\u53ca\u200b\u5b83\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u91cf\u200b\uff08\u200b\u589e\u52a0\u200b\u5b66\u4e60\u200b\u673a\u4f1a\u200b\uff09\uff0cViT \u200b\u8bba\u6587\u200b\u8bad\u7ec3\u200b\u65b9\u6848\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u8bb8\u591a\u200b\u6280\u672f\u200b\uff0c\u200b\u5982\u200b\u5b66\u4e60\u200b\u7387\u200b\u9884\u70ed\u200b\u3001\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u548c\u200b\u68af\u5ea6\u200b\u88c1\u526a\u200b\uff0c\u200b\u90fd\u200b\u662f\u200b\u4e13\u95e8\u200b\u8bbe\u8ba1\u200b\u6765\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\uff08\u200b\u6b63\u5219\u200b\u5316\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5bf9\u4e8e\u200b\u4efb\u4f55\u200b\u4f60\u200b\u4e0d\u200b\u786e\u5b9a\u200b\u7684\u200b\u6280\u672f\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u641c\u7d22\u200b \"pytorch \u200b\u6280\u672f\u200b\u540d\u79f0\u200b\" \u200b\u5feb\u901f\u200b\u627e\u5230\u200b\u793a\u4f8b\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u4e86\u89e3\u200b\u5b66\u4e60\u200b\u7387\u200b\u9884\u70ed\u200b\u53ca\u5176\u200b\u4f5c\u7528\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u641c\u7d22\u200b \"pytorch learning rate warmup\"\u3002</p> <p>\u200b\u597d\u6d88\u606f\u200b\u662f\u200b\uff0c\u200b\u6709\u200b\u8bb8\u591a\u200b\u4f7f\u7528\u200b\u5927\u91cf\u200b\u6570\u636e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u5728\u7ebf\u200b\u53ef\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u7b2c\u200b 10 \u200b\u8282\u4e2d\u200b\u770b\u5230\u200b\u4e00\u4e2a\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#95-vit","title":"9.5 \u200b\u7ed8\u5236\u200b\u6211\u4eec\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u4e86\u200bViT\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u770b\u5230\u200b\u4e86\u200b\u9875\u9762\u200b\u4e0a\u200b\u5448\u73b0\u200b\u7684\u200b\u6570\u5b57\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9075\u5faa\u200b\u6570\u636e\u200b\u63a2\u7d22\u8005\u200b\u7684\u200b\u5ea7\u53f3\u94ed\u200b\uff1a\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u53ef\u89c6\u5316\u200b\uff0c\u200b\u518d\u200b\u53ef\u89c6\u5316\u200b\uff01</p> <p>\u200b\u800c\u200b\u5bf9\u4e8e\u200b\u6a21\u578b\u200b\u6765\u8bf4\u200b\uff0c\u200b\u6700\u597d\u200b\u7684\u200b\u53ef\u89c6\u5316\u200b\u4e4b\u4e00\u200b\u5c31\u662f\u200b\u5b83\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u3002</p> <p>\u200b\u8981\u200b\u67e5\u770b\u200b\u6211\u4eec\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b04. PyTorch\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b\u90e8\u5206\u200b7.8\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>helper_functions.py</code>\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b<code>plot_loss_curves</code>\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#10-torchvisionmodels-vit","title":"10. \u200b\u5728\u200b\u540c\u4e00\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u4f7f\u7528\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b ViT\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b 06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b \u200b\u4e2d\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u4f18\u52bf\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u5df2\u7ecf\u200b\u4ece\u5934\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u81ea\u5df1\u200b\u7684\u200b ViT\uff0c\u200b\u5e76\u4e14\u200b\u53d6\u5f97\u200b\u4e86\u200b\u4e0d\u200b\u592a\u200b\u7406\u60f3\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u56e0\u6b64\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\uff08\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff09\u200b\u7684\u200b\u4f18\u70b9\u200b\u771f\u6b63\u200b\u663e\u73b0\u51fa\u6765\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#101","title":"10.1 \u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff1f\u00b6","text":"<p>\u200b\u8bb8\u591a\u200b\u73b0\u4ee3\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u8bf4\u660e\u200b\u662f\u200b\uff0c\u200b\u5f88\u591a\u200b\u7ed3\u679c\u200b\u662f\u200b\u901a\u8fc7\u200b\u5927\u91cf\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u5e9e\u5927\u200b\u7684\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u83b7\u5f97\u200b\u7684\u200b\u3002</p> <p>\u200b\u800c\u200b\u5728\u200b\u73b0\u4ee3\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u539f\u59cb\u200b\u7684\u200b\u5b8c\u5168\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b ViT \u200b\u5f88\u200b\u53ef\u80fd\u200b\u4e0d\u4f1a\u200b\u88ab\u200b\u8ba4\u4e3a\u200b\u662f\u200b\u4e00\u4e2a\u200b\u201c\u200b\u8d85\u7ea7\u200b\u5927\u578b\u200b\u201d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u8bbe\u7f6e\u200b\uff08\u200b\u6a21\u578b\u200b\u4e0d\u65ad\u200b\u53d8\u5f97\u200b\u8d8a\u6765\u8d8a\u200b\u5927\u200b\uff09\u3002</p> <p>\u200b\u9605\u8bfb\u200b ViT \u200b\u8bba\u6587\u200b\u7b2c\u200b 4.2 \u200b\u8282\u200b\uff1a</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u5728\u200b\u516c\u5171\u200b ImageNet-21k \u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT-L/16 \u200b\u6a21\u578b\u200b\u5728\u200b\u5927\u591a\u6570\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8868\u73b0\u200b\u4e5f\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u540c\u65f6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u8d44\u6e90\u200b\u66f4\u200b\u5c11\u200b\uff1a\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5728\u200b\u5927\u7ea6\u200b 30 \u200b\u5929\u200b\u5185\u200b\u4f7f\u7528\u200b\u6807\u51c6\u200b\u7684\u200b\u4e91\u200b TPUv3 8 \u200b\u6838\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u622a\u81f3\u200b 2022 \u200b\u5e74\u200b 7 \u200b\u6708\u200b\uff0c\u200b\u79df\u7528\u200b TPUv3\uff08Tensor Processing Unit \u200b\u7248\u672c\u200b 3\uff09\u200b\u5728\u200b Google Cloud \u200b\u4e0a\u200b\u7684\u200b 8 \u200b\u6838\u200b\u4ef7\u683c\u200b\u4e3a\u200b\u6bcf\u200b\u5c0f\u65f6\u200b 8 \u200b\u7f8e\u5143\u200b\u3002</p> <p>\u200b\u8fde\u7eed\u200b\u79df\u7528\u200b 30 \u200b\u5929\u200b\u7684\u200b\u8d39\u7528\u200b\u4e3a\u200b 5,760 \u200b\u7f8e\u5143\u200b\u3002</p> <p>\u200b\u8fd9\u79cd\u200b\u6210\u672c\u200b\uff08\u200b\u91d1\u94b1\u200b\u548c\u200b\u65f6\u95f4\u200b\uff09\u200b\u5bf9\u4e8e\u200b\u4e00\u4e9b\u200b\u5927\u578b\u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u6216\u200b\u4f01\u4e1a\u200b\u53ef\u80fd\u200b\u662f\u200b\u53ef\u884c\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u8bb8\u591a\u200b\u4eba\u200b\u6765\u8bf4\u200b\u5219\u200b\u4e0d\u7136\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u901a\u8fc7\u200b <code>torchvision.models</code>\u3001<code>timm</code>\uff08Torch Image Models \u200b\u5e93\u200b\uff09\u3001HuggingFace Hub \u200b\u6216\u200b\u751a\u81f3\u200b\u4ece\u200b\u8bba\u6587\u200b\u4f5c\u8005\u200b\u672c\u4eba\u200b\u90a3\u91cc\u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff08\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u4eba\u5458\u200b\u53d1\u5e03\u200b\u5176\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u7684\u200b\u4ee3\u7801\u200b\u548c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u8d8b\u52bf\u200b\u6b63\u5728\u200b\u589e\u957f\u200b\uff0c\u200b\u6211\u200b\u975e\u5e38\u200b\u559c\u6b22\u200b\u8fd9\u4e00\u200b\u8d8b\u52bf\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u8d44\u6e90\u200b\u53ef\u4ee5\u200b\u5728\u200b Paperswithcode.com \u200b\u4e0a\u200b\u627e\u5230\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u5229\u7528\u200b\u7279\u5b9a\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u7684\u200b\u4f18\u52bf\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u521b\u5efa\u200b\u81ea\u5b9a\u4e49\u200b\u67b6\u6784\u200b\uff0c\u200b\u6211\u200b\u5f3a\u70c8\u5efa\u8bae\u200b\u4f7f\u7528\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#102-vit","title":"10.2 \u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u5e76\u200b\u521b\u5efa\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u5934\u5f00\u59cb\u200b\uff0c\u200b\u9996\u5148\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u5b89\u88c5\u200b\u4e86\u200b\u6b63\u786e\u200b\u7248\u672c\u200b\u7684\u200b <code>torch</code> \u200b\u548c\u200b <code>torchvision</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u9700\u8981\u200b <code>torch</code> v0.12+ \u200b\u548c\u200b <code>torchvision</code> v0.13+ \u200b\u624d\u80fd\u200b\u4f7f\u7528\u200b\u6700\u65b0\u200b\u7684\u200b <code>torchvision</code> \u200b\u6a21\u578b\u200b\u6743\u91cd\u200b API\u3002</p>"},{"location":"08_pytorch_paper_replicating/#103-vit","title":"10.3 \u200b\u4e3a\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b2\u200b\u8282\u4e2d\u200b\u5df2\u7ecf\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u521b\u5efa\u200b\u4e86\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200bDataLoader\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u9700\u8981\u200b\u518d\u6b21\u200b\u8fdb\u884c\u200b\u8fd9\u4e00\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u4e00\u4e9b\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\uff08\u200b\u4e3a\u200bFood Vision Mini\u200b\u51c6\u5907\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\uff09\uff0c\u200b\u8bbe\u7f6e\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5f20\u91cf\u200b\u548c\u200bDataLoader\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ece\u200b\u8bfe\u7a0b\u200bGitHub\u200b\u4e0b\u8f7d\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b07. PyTorch\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u7b2c\u200b1\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>download_data()</code>\u200b\u51fd\u6570\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#104-vit","title":"10.4 \u200b\u8bad\u7ec3\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b ViT \u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0cDataLoaders \u200b\u4e5f\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u4e86\u200b\uff01</p> <p>\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Adam \u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>torch.optim.Adam()</code>\uff09\uff0c\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b <code>1e-3</code>\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torch.nn.CrossEntropyLoss()</code> \u200b\u4f5c\u4e3a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u7b2c\u200b 4 \u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>engine.train()</code> \u200b\u51fd\u6570\u200b\u5c06\u200b\u5904\u7406\u200b\u5176\u4f59\u90e8\u5206\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#105-vit","title":"10.5 \u200b\u7ed8\u5236\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bViT\u200b\u7279\u5f81\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b\u8868\u73b0\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\uff0c\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\u5462\u200b\uff1f</p>"},{"location":"08_pytorch_paper_replicating/#106-vit","title":"10.6 \u200b\u4fdd\u5b58\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200bViT\u200b\u6a21\u578b\u200b\u5e76\u200b\u68c0\u67e5\u200b\u6587\u4ef6\u5927\u5c0f\u200b\u00b6","text":"<p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u5728\u200bFood Vision Mini\u200b\u95ee\u9898\u200b\u4e0a\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u6216\u8bb8\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u60f3\u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u5176\u200b\u90e8\u7f72\u200b\u5230\u200b\u751f\u4ea7\u200b\u73af\u5883\u200b\u4e2d\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff08\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u90e8\u7f72\u200b\u610f\u5473\u7740\u200b\u5c06\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u4e2d\u200b\uff0c\u200b\u6bd4\u5982\u200b\u8ba9\u200b\u7528\u6237\u200b\u4f7f\u7528\u200b\u667a\u80fd\u624b\u673a\u200b\u62cd\u6444\u200b\u98df\u7269\u200b\u7167\u7247\u200b\uff0c\u200b\u5e76\u200b\u67e5\u770b\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u5426\u200b\u8ba4\u4e3a\u200b\u5b83\u200b\u662f\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b05. PyTorch Going Modular\u200b\u7b2c\u200b5\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>utils.save_model()</code>\u200b\u51fd\u6570\u200b\u6765\u200b\u4fdd\u5b58\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/#11","title":"11. \u200b\u5bf9\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u7ec8\u6781\u200b\u6d4b\u8bd5\u200b\uff0c\u200b\u5bf9\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u62ab\u8428\u200b\u7238\u7238\u200b\u7684\u200b\u56fe\u50cf\u200b\uff08\u200b\u4e00\u5f20\u200b\u6211\u200b\u7238\u7238\u200b\u5403\u200b\u62ab\u8428\u200b\u7684\u200b\u7167\u7247\u200b\uff09\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b06. PyTorch\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b6\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>pred_and_plot()</code>\u200b\u51fd\u6570\u200b\u3002\u200b\u4e3a\u4e86\u200b\u65b9\u4fbf\u200b\u8d77\u200b\u89c1\u200b\uff0c\u200b\u6211\u200b\u5c06\u200b\u6b64\u200b\u51fd\u6570\u200b\u4fdd\u5b58\u200b\u5230\u200b\u4e86\u200b\u8bfe\u7a0b\u200bGitHub\u200b\u4e0a\u200b\u7684\u200b<code>going_modular.going_modular.predictions.py</code>\u200b\u6587\u4ef6\u200b\u4e2d\u200b\u3002</p>"},{"location":"08_pytorch_paper_replicating/","title":"\u4e3b\u8981\u200b\u6536\u83b7\u200b\u00b6","text":"<ul> <li>\u200b\u968f\u7740\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u7206\u53d1\u200b\uff0c\u200b\u6bcf\u5929\u200b\u90fd\u200b\u6709\u200b\u65b0\u200b\u7684\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u8be6\u7ec6\u200b\u4ecb\u7ecd\u200b\u8fdb\u5c55\u200b\u3002\u200b\u8981\u200b\u8ddf\u4e0a\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u4e0d\u200b\u53ef\u80fd\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u7f29\u5c0f\u200b\u5230\u200b\u81ea\u5df1\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff0c\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u6240\u200b\u505a\u200b\u7684\u200b\uff0c\u200b\u4e3a\u200bFoodVision Mini\u200b\u590d\u73b0\u200b\u4e00\u7bc7\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u8bba\u6587\u200b\u3002</li> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u901a\u5e38\u200b\u5305\u542b\u200b\u7531\u200b\u806a\u660e\u4eba\u200b\u7ec4\u6210\u200b\u7684\u200b\u56e2\u961f\u200b\u6570\u6708\u200b\u7684\u200b\u7814\u7a76\u6210\u679c\u200b\uff0c\u200b\u538b\u7f29\u200b\u5728\u200b\u51e0\u9875\u200b\u7eb8\u4e0a\u200b\uff08\u200b\u56e0\u6b64\u200b\u63d0\u53d6\u200b\u6240\u6709\u200b\u7ec6\u8282\u200b\u5e76\u200b\u5b8c\u5168\u200b\u590d\u73b0\u200b\u8bba\u6587\u200b\u53ef\u80fd\u200b\u6709\u70b9\u200b\u6311\u6218\u200b\uff09\u3002</li> <li>\u200b\u590d\u73b0\u200b\u8bba\u6587\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u5c06\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\uff08\u200b\u6587\u672c\u200b\u548c\u200b\u6570\u5b66\u200b\uff09\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u53ef\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002<ul> <li>\u200b\u5c3d\u7ba1\u5982\u6b64\u200b\uff0c\u200b\u8bb8\u591a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u56e2\u961f\u200b\u5f00\u59cb\u200b\u5728\u200b\u8bba\u6587\u200b\u4e2d\u200b\u53d1\u5e03\u200b\u4ee3\u7801\u200b\uff0c\u200b\u67e5\u770b\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u7684\u200b\u6700\u4f73\u200b\u5730\u70b9\u200b\u4e4b\u4e00\u200b\u662f\u200bPaperswithcode.com</li> </ul> </li> <li>\u200b\u5c06\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7814\u7a76\u200b\u8bba\u6587\u200b\u5206\u89e3\u200b\u4e3a\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u5c42\u200b/\u200b\u5757\u200b/\u200b\u6a21\u578b\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\uff09\u200b\u548c\u200b\u5c42\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u5c42\u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u8f93\u5165\u200b\uff1f\uff09\u200b\u4ee5\u53ca\u200b\u5757\u200b\uff08\u200b\u5c42\u200b\u7684\u200b\u96c6\u5408\u200b\uff09\uff0c\u200b\u5e76\u200b\u9010\u6b65\u200b\u590d\u73b0\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff09\uff0c\u200b\u5bf9\u4e8e\u200b\u7406\u89e3\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002</li> <li>\u200b\u8bb8\u591a\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u90fd\u200b\u6709\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u501f\u52a9\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u529b\u91cf\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u6a21\u578b\u200b\u901a\u5e38\u200b\u5728\u200b\u5c0f\u200b\u6570\u636e\u200b\u4e0a\u200b\u8868\u73b0\u200b\u975e\u5e38\u200b\u597d\u200b\u3002</li> <li>\u200b\u8f83\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u901a\u5e38\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\uff0c\u200b\u4f46\u200b\u5360\u7528\u200b\u7a7a\u95f4\u200b\u4e5f\u200b\u66f4\u200b\u5927\u200b\uff08\u200b\u5b83\u4eec\u200b\u5360\u7528\u200b\u66f4\u200b\u591a\u200b\u5b58\u50a8\u7a7a\u95f4\u200b\uff0c\u200b\u63a8\u7406\u200b\u65f6\u95f4\u200b\u53ef\u80fd\u200b\u66f4\u957f\u200b\uff09\u3002<ul> <li>\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u95ee\u9898\u200b\u662f\u200b\uff1a\u200b\u4ece\u200b\u90e8\u7f72\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0c\u200b\u8f83\u5927\u200b\u6a21\u578b\u200b\u7684\u200b\u989d\u5916\u200b\u6027\u80fd\u200b\u662f\u5426\u200b\u503c\u5f97\u200b/\u200b\u4e0e\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u76f8\u7b26\u200b\uff1f</li> </ul> </li> </ul>"},{"location":"08_pytorch_paper_replicating/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e9b\u200b\u7ec3\u4e60\u200b\u671f\u671b\u200b\u4f7f\u7528\u200b <code>torchvision</code> v0.13+\uff082022\u200b\u5e74\u200b7\u200b\u6708\u200b\u53d1\u5e03\u200b\uff09\uff0c\u200b\u4e4b\u524d\u200b\u7684\u200b\u7248\u672c\u200b\u53ef\u80fd\u200b\u4e5f\u200b\u80fd\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u4f46\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u7ec3\u4e60\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u6216\u200b\u9075\u5faa\u200b\u6240\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u5e94\u8be5\u200b\u4f7f\u7528\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u5b8c\u6210\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>\u200b\u7b2c\u200b08\u200b\u8bb2\u200b\u7684\u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b\u3002</li> <li>\u200b\u7b2c\u200b08\u200b\u8bb2\u200b\u7684\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b\uff08\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\u4e4b\u524d\u200b\u67e5\u770b\u200b\u8fd9\u4e2a\u200b\uff09\u3002<ul> <li>\u200b\u5728\u200bYouTube\u200b\u4e0a\u200b\u89c2\u770b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7684\u200b\u89c6\u9891\u200b\u8bb2\u89e3\u200b\uff08\u200b\u5305\u62ec\u200b\u6240\u6709\u200b\u9519\u8bef\u200b\uff09\u3002</li> </ul> </li> </ul> <ol> <li>\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u7684\u200bPyTorch transformer\u200b\u5c42\u200b\u590d\u5236\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200bViT\u200b\u67b6\u6784\u200b\u3002<ul> <li>\u200b\u4f60\u200b\u9700\u8981\u200b\u7814\u7a76\u200b\u5982\u4f55\u200b\u7528\u200b<code>torch.nn.TransformerEncoderLayer()</code>\u200b\u66ff\u6362\u200b\u6211\u4eec\u200b\u7684\u200b<code>TransformerEncoderBlock()</code>\u200b\u7c7b\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u5305\u542b\u200b\u4e0e\u200b\u6211\u4eec\u200b\u81ea\u5b9a\u4e49\u200b\u5757\u200b\u76f8\u540c\u200b\u7684\u200b\u5c42\u200b\uff09\u3002</li> <li>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b<code>torch.nn.TransformerEncoder()</code>\u200b\u5c06\u200b<code>torch.nn.TransformerEncoderLayer()</code>\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002</li> </ul> </li> <li>\u200b\u5c06\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200bViT\u200b\u67b6\u6784\u200b\u8f6c\u6362\u200b\u4e3a\u200bPython\u200b\u811a\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c<code>vit.py</code>\u3002<ul> <li>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u7c7b\u4f3c\u200b<code>from vit import ViT</code>\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5bfc\u5165\u200b\u6574\u4e2a\u200bViT\u200b\u6a21\u578b\u200b\u3002</li> </ul> </li> <li>\u200b\u5728\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b08. PyTorch\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u7b2c\u200b10\u200b\u8282\u4e2d\u200b\u5236\u4f5c\u200b\u7684\u200b\u90a3\u4e2a\u200b\uff09\u200b\u4e0a\u200b\u8bad\u7ec3\u200b20%\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b07. PyTorch\u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u7b2c\u200b7.3\u200b\u8282\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff09\u3002<ul> <li>\u200b\u770b\u770b\u200b\u5b83\u200b\u7684\u200b\u6027\u80fd\u200b\u4e0e\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u5728\u200b08. PyTorch\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u7b2c\u200b10.6\u200b\u8282\u4e2d\u200b\u6bd4\u8f83\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u76f8\u6bd4\u200b\u5982\u4f55\u200b\u3002</li> </ul> </li> <li>\u200b\u5c1d\u8bd5\u200b\u91cd\u590d\u200b\u7ec3\u4e60\u200b3\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u4f7f\u7528\u200b<code>torchvision.models.vit_b_16()</code>\u200b\u4e2d\u200b\u7684\u200b\"<code>ViT_B_16_Weights.IMAGENET1K_SWAG_E2E_V1</code>\"\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f7f\u7528\u200bSWAG\u200b\u6743\u91cd\u200b\u7684\u200bViT\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u6709\u200b\u4e00\u4e2a\u200b\u6700\u5c0f\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b<code>(384, 384)</code>\uff08\u200b\u7ec3\u4e60\u200b3\u200b\u4e2d\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200bViT\u200b\u7684\u200b\u6700\u5c0f\u200b\u8f93\u5165\u200b\u5c3a\u5bf8\u200b\u662f\u200b<code>(224, 224)</code>\uff09\uff0c\u200b\u5c3d\u7ba1\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6743\u91cd\u200b\u7684\u200b<code>.transforms()</code>\u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u8bbf\u95ee\u200b\u3002</li> </ul> </li> <li>\u200b\u6211\u4eec\u200b\u7684\u200b\u81ea\u5b9a\u4e49\u200bViT\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u4e0e\u200bViT\u200b\u8bba\u6587\u200b\u4e2d\u200b\u7684\u200b\u67b6\u6784\u200b\u975e\u5e38\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65b9\u6848\u200b\u7f3a\u5c11\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u4e1c\u897f\u200b\u3002\u200b\u7814\u7a76\u200bViT\u200b\u8bba\u6587\u200b\u8868\u200b3\u200b\u4e2d\u200b\u6211\u4eec\u200b\u9519\u8fc7\u200b\u7684\u200b\u4e00\u4e9b\u200b\u4e3b\u9898\u200b\uff0c\u200b\u5e76\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u4e3b\u9898\u200b\u5199\u200b\u4e00\u53e5\u200b\u8bdd\u200b\uff0c\u200b\u8bf4\u660e\u200b\u5b83\u200b\u5982\u4f55\u200b\u6709\u52a9\u4e8e\u200b\u8bad\u7ec3\u200b\uff1a<ul> <li>ImageNet-21k\u200b\u9884\u200b\u8bad\u7ec3\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\uff09\u3002</li> <li>\u200b\u5b66\u4e60\u200b\u7387\u200b\u9884\u70ed\u200b\u3002</li> <li>\u200b\u5b66\u4e60\u200b\u7387\u200b\u8870\u51cf\u200b\u3002</li> <li>\u200b\u68af\u5ea6\u200b\u88c1\u526a\u200b\u3002</li> </ul> </li> </ol>"},{"location":"08_pytorch_paper_replicating/","title":"\u8bfe\u5916\u200b\u62d3\u5c55\u200b\u00b6","text":"<ul> <li>Vision Transformer\u200b\u81ea\u200b\u53d1\u5e03\u200b\u4ee5\u6765\u200b\u7ecf\u5386\u200b\u4e86\u200b\u591a\u6b21\u200b\u8fed\u4ee3\u200b\u548c\u200b\u8c03\u6574\u200b\uff0c\u200b\u622a\u81f3\u200b2022\u200b\u5e74\u200b7\u200b\u6708\u200b\uff0c\u200b\u6700\u200b\u7b80\u6d01\u200b\u4e14\u200b\u6027\u80fd\u200b\u6700\u4f73\u200b\u7684\u200b\u7248\u672c\u200b\u53ef\u200b\u5728\u200b\u300aBetter plain ViT baselines for ImageNet-1k\u300b\u200b\u4e2d\u200b\u67e5\u770b\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u6709\u200b\u8fd9\u4e9b\u200b\u5347\u7ea7\u200b\uff0c\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4ecd\u200b\u575a\u6301\u200b\u590d\u73b0\u200b\u4e00\u4e2a\u200b\u201c\u200b\u539f\u59cb\u200b\u7684\u200bVision Transformer\u201d\uff0c\u200b\u56e0\u4e3a\u200b\u7406\u89e3\u200b\u4e86\u200b\u539f\u59cb\u200b\u7ed3\u6784\u200b\uff0c\u200b\u5c31\u200b\u80fd\u200b\u66f4\u597d\u200b\u5730\u200b\u8fc7\u6e21\u200b\u5230\u200b\u4e0d\u540c\u200b\u8fed\u4ee3\u200b\u7248\u672c\u200b\u3002</li> <li>lucidrains\u200b\u5728\u200bGitHub\u200b\u4e0a\u200b\u7684\u200b<code>vit-pytorch</code>\u200b\u4ed3\u5e93\u200b\u662f\u200bPyTorch\u200b\u5b9e\u73b0\u200b\u7684\u200b\u5404\u79cd\u200bViT\u200b\u67b6\u6784\u200b\u6700\u200b\u5168\u9762\u200b\u7684\u200b\u8d44\u6e90\u200b\u4e4b\u4e00\u200b\u3002\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u6781\u597d\u200b\u7684\u200b\u53c2\u8003\u8d44\u6599\u200b\uff0c\u200b\u6211\u200b\u7ecf\u5e38\u200b\u4f7f\u7528\u200b\u5b83\u200b\u6765\u200b\u521b\u5efa\u200b\u672c\u200b\u7ae0\u8282\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</li> <li>PyTorch\u200b\u5728\u200bGitHub\u200b\u4e0a\u200b\u6709\u200b\u81ea\u5df1\u200b\u7684\u200bViT\u200b\u67b6\u6784\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b<code>torchvision.models</code>\u200b\u4e2d\u9884\u200b\u8bad\u7ec3\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u3002</li> <li>Jay Alammar\u200b\u5728\u200b\u5176\u200b\u535a\u5ba2\u200b\u4e0a\u200b\u5bf9\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\uff08Transformer\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\uff09\u200b\u548c\u200bTransformer\u200b\u6a21\u578b\u200b\u6709\u200b\u7cbe\u5f69\u200b\u7684\u200b\u63d2\u56fe\u200b\u548c\u200b\u89e3\u91ca\u200b\u3002</li> <li>Adrish Dey\u200b\u5bf9\u5c42\u200b\u5f52\u4e00\u5316\u200b\uff08ViT\u200b\u67b6\u6784\u200b\u7684\u200b\u4e3b\u8981\u200b\u7ec4\u6210\u90e8\u5206\u200b\uff09\u200b\u7684\u200b\u8be6\u7ec6\u200b\u89e3\u8bfb\u200b\u6709\u52a9\u4e8e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8bad\u7ec3\u200b\u3002</li> <li>\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff08\u200b\u53ca\u200b\u591a\u5934\u200b\u81ea\u200b\u6ce8\u610f\u529b\u200b\uff09\u200b\u673a\u5236\u200b\u662f\u200bViT\u200b\u67b6\u6784\u200b\u4ee5\u53ca\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200bTransformer\u200b\u67b6\u6784\u200b\u7684\u200b\u6838\u5fc3\u200b\uff0c\u200b\u5b83\u200b\u6700\u521d\u200b\u5728\u200b\u300aAttention is all you need\u300b\u200b\u8bba\u6587\u200b\u4e2d\u200b\u88ab\u200b\u5f15\u5165\u200b\u3002</li> <li>Yannic Kilcher\u200b\u7684\u200bYouTube\u200b\u9891\u9053\u200b\u662f\u200b\u89c6\u89c9\u200b\u5316\u200b\u8bba\u6587\u200b\u89e3\u8bfb\u200b\u7684\u200b\u7edd\u4f73\u200b\u8d44\u6e90\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u89c2\u770b\u200b\u4ed6\u200b\u5bf9\u200b\u4ee5\u4e0b\u200b\u8bba\u6587\u200b\u7684\u200b\u89c6\u9891\u200b\u89e3\u8bfb\u200b\uff1a<ul> <li>Attention is all you need\uff08\u200b\u5f15\u5165\u200bTransformer\u200b\u67b6\u6784\u200b\u7684\u200b\u8bba\u6587\u200b\uff09\u3002</li> <li>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\uff08\u200b\u5f15\u5165\u200bViT\u200b\u67b6\u6784\u200b\u7684\u200b\u8bba\u6587\u200b\uff09\u3002</li> </ul> </li> </ul>"},{"location":"08_pytorch_profiling/","title":"08: PyTorch Profiling","text":"In\u00a0[10]: Copied! <pre>import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom going_modular import data_setup, engine\n</pre> import torch import torchvision from torch import nn from torchvision import transforms, datasets from torchinfo import summary  import numpy as np import matplotlib.pyplot as plt  from going_modular import data_setup, engine In\u00a0[11]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[11]: <pre>'cuda'</pre> In\u00a0[12]: Copied! <pre>import os\nimport requests\nfrom zipfile import ZipFile\n\ndef get_food_image_data():\n    if not os.path.exists(\"data/10_whole_foods\"):\n        os.makedirs(\"data/\", exist_ok=True)\n        # Download data\n        data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"\n        print(f\"Downloading data from {data_url}...\")\n        requests.get(data_url)\n        # Unzip data\n        targ_dir = \"data/10_whole_foods\"\n        print(f\"Extracting data to {targ_dir}...\")\n        with ZipFile(\"10_whole_foods.zip\") as zip_ref:\n            zip_ref.extractall(targ_dir)\n    else:\n        print(\"data/10_whole_foods dir exists, skipping download\")\n\nget_food_image_data()\n</pre> import os import requests from zipfile import ZipFile  def get_food_image_data():     if not os.path.exists(\"data/10_whole_foods\"):         os.makedirs(\"data/\", exist_ok=True)         # Download data         data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"         print(f\"Downloading data from {data_url}...\")         requests.get(data_url)         # Unzip data         targ_dir = \"data/10_whole_foods\"         print(f\"Extracting data to {targ_dir}...\")         with ZipFile(\"10_whole_foods.zip\") as zip_ref:             zip_ref.extractall(targ_dir)     else:         print(\"data/10_whole_foods dir exists, skipping download\")  get_food_image_data() <pre>data/10_whole_foods dir exists, skipping download\n</pre> In\u00a0[38]: Copied! <pre># Setup dirs\ntrain_dir = \"data/10_whole_foods/train\"\ntest_dir = \"data/10_whole_foods/test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create starter transform\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=32,\n    num_workers=8\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = \"data/10_whole_foods/train\" test_dir = \"data/10_whole_foods/test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create starter transform simple_transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])             # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=simple_transform,     batch_size=32,     num_workers=8 )  train_dataloader, test_dataloader, class_names Out[38]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f052ab26e20&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f05299eed00&gt;,\n ['apple',\n  'banana',\n  'beef',\n  'blueberries',\n  'carrots',\n  'chicken_wings',\n  'egg',\n  'honey',\n  'mushrooms',\n  'strawberries'])</pre> In\u00a0[66]: Copied! <pre>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n# model\n</pre> model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # model In\u00a0[67]: Copied! <pre># Update the classifier\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2),\n    nn.Linear(1280, len(class_names)).to(device))\n\n# Freeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Update the classifier model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2),     nn.Linear(1280, len(class_names)).to(device))  # Freeze all base layers  for param in model.features.parameters():     param.requires_grad = False In\u00a0[68]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[69]: Copied! <pre>model.name = \"EfficietNetB0\"\nmodel.name\n</pre> model.name = \"EfficietNetB0\" model.name Out[69]: <pre>'EfficietNetB0'</pre> In\u00a0[70]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\nfrom going_modular.engine import train_step, test_step\nfrom tqdm import tqdm\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter from going_modular.engine import train_step, test_step from tqdm import tqdm writer = SummaryWriter() <p>Update the <code>train_step()</code> function to include the PyTorch profiler.</p> In\u00a0[71]: Copied! <pre>def train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    train_loss, train_acc = 0, 0\n    ## NEW: Add PyTorch profiler\n\n    dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))\n    with torch.profiler.profile(\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),\n        # with_stack=True # this adds a lot of overhead to training (tracing all the stack)\n    ):\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to GPU\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            \n            # Turn on mixed precision if available\n            with torch.autocast(device_type=device, enabled=True):\n                # 1. Forward pass\n                y_pred = model(X)\n\n                # 2. Calculate loss\n                loss = loss_fn(y_pred, y)\n\n            # 3. Optimizer zero grad\n            optimizer.zero_grad()\n\n            # 4. Loss backward\n            loss.backward()\n\n            # 5. Optimizer step\n            optimizer.step()\n\n            # 6. Calculate metrics\n            train_loss += loss.item()\n            y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)\n            # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")\n            # print(f\"y argmax: {y_pred.argmax(dim=1)}\")\n            # print(f\"Equal: {(y_pred_class == y)}\")\n            train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n            # print(f\"batch: {batch} train_acc: {train_acc}\")\n\n    # Adjust returned metrics\n    return train_loss / len(dataloader), train_acc / len(dataloader)\n</pre> def train_step(model, dataloader, loss_fn, optimizer):     model.train()     train_loss, train_acc = 0, 0     ## NEW: Add PyTorch profiler      dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))     with torch.profiler.profile(         on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),         # with_stack=True # this adds a lot of overhead to training (tracing all the stack)     ):         for batch, (X, y) in enumerate(dataloader):             # Send data to GPU             X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)                          # Turn on mixed precision if available             with torch.autocast(device_type=device, enabled=True):                 # 1. Forward pass                 y_pred = model(X)                  # 2. Calculate loss                 loss = loss_fn(y_pred, y)              # 3. Optimizer zero grad             optimizer.zero_grad()              # 4. Loss backward             loss.backward()              # 5. Optimizer step             optimizer.step()              # 6. Calculate metrics             train_loss += loss.item()             y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)             # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")             # print(f\"y argmax: {y_pred.argmax(dim=1)}\")             # print(f\"Equal: {(y_pred_class == y)}\")             train_acc += (y_pred_class == y).sum().item() / len(y_pred)             # print(f\"batch: {batch} train_acc: {train_acc}\")      # Adjust returned metrics     return train_loss / len(dataloader), train_acc / len(dataloader) <p>TK - Now to use the writer, we've got to adjust the <code>train()</code> function...</p> In\u00a0[72]: Copied! <pre>def train(\n    model,\n    train_dataloader,\n    test_dataloader,\n    optimizer,\n    loss_fn=nn.CrossEntropyLoss(),\n    epochs=5,\n):\n\n    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n        )\n        test_loss, test_acc = test_step(\n            model=model, dataloader=test_dataloader, loss_fn=loss_fn\n        )\n\n        # Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        # Add results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n    \n    # Close the writer\n    writer.close()\n\n    return results\n</pre> def train(     model,     train_dataloader,     test_dataloader,     optimizer,     loss_fn=nn.CrossEntropyLoss(),     epochs=5, ):      results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}      for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(             model=model,             dataloader=train_dataloader,             loss_fn=loss_fn,             optimizer=optimizer,         )         test_loss, test_acc = test_step(             model=model, dataloader=test_dataloader, loss_fn=loss_fn         )          # Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # Update results         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          # Add results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)          # Close the writer     writer.close()      return results In\u00a0[73]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:05&lt;00:21,  5.27s/it]</pre> <pre>Epoch: 1 | train_loss: 1.9644 | train_acc: 0.4386 | test_loss: 1.5205 | test_acc: 0.7865\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:09&lt;00:14,  4.94s/it]</pre> <pre>Epoch: 2 | train_loss: 1.2589 | train_acc: 0.7878 | test_loss: 1.1589 | test_acc: 0.7604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:14&lt;00:09,  4.72s/it]</pre> <pre>Epoch: 3 | train_loss: 0.8642 | train_acc: 0.8776 | test_loss: 0.9347 | test_acc: 0.7917\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.6827 | train_acc: 0.8856 | test_loss: 0.6637 | test_acc: 0.8750\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:23&lt;00:00,  4.65s/it]</pre> <pre>Epoch: 5 | train_loss: 0.5688 | train_acc: 0.9069 | test_loss: 0.6175 | test_acc: 0.8854\n</pre> <pre>\n</pre> <p>Looks like mixed precision doesn't offer much benefit for smaller feature extraction models...</p> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750\n\n# # With mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906\n</pre> # # Without mixed precision #  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750  # # With mixed precision #  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906 In\u00a0[74]: Copied! <pre># Unfreeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = True\n\n# for param in model.features.parameters():\n#     print(param.requires_grad)\n</pre> # Unfreeze all base layers  for param in model.features.parameters():     param.requires_grad = True  # for param in model.features.parameters(): #     print(param.requires_grad) In\u00a0[75]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]</pre> <pre>Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]</pre> <pre>Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]</pre> <pre>Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]</pre> <pre>Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385\n\n# # With mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> # # Without mixed precision... #  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385  # # With mixed precision... #  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125 <p>Checking the PyTorch profiler, it seems that mixed precision utilises some Tensor Cores, however, these aren't large numbers.</p> <p>E.g. it uses 9-12% Tensor Cores. Perhaps the slow down when using mixed precision is because the tensors have to get altered and converted when there isn't very many of them. For example only 9-12% of tensors get converted so the speed up gains aren't realised on these tensors because they get cancelled out by the conversion time.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"08_pytorch_profiling/#08-pytorch-profiling","title":"08: PyTorch Profiling\u00b6","text":"<p>This notebook is an experiment to try out the PyTorch profiler.</p> <p>See here for more:</p> <ul> <li>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</li> <li>https://pytorch.org/docs/stable/profiler.html</li> </ul>"},{"location":"08_pytorch_profiling/#setup-device","title":"Setup device\u00b6","text":""},{"location":"08_pytorch_profiling/#get-and-load-data","title":"Get and load data\u00b6","text":""},{"location":"08_pytorch_profiling/#load-model","title":"Load model\u00b6","text":""},{"location":"08_pytorch_profiling/#train-model-and-track-results","title":"Train model and track results\u00b6","text":""},{"location":"08_pytorch_profiling/#adjust-training-function-to-track-results-with-summarywriter","title":"Adjust training function to track results with <code>SummaryWriter</code>\u00b6","text":""},{"location":"08_pytorch_profiling/#try-mixed-precision-with-larger-model","title":"Try mixed precision with larger model\u00b6","text":"<p>Now we'll try turn on mixed precision with a larger model (e.g. EffifientNetB0 with all layers tuneable).</p>"},{"location":"08_pytorch_profiling/#extensions","title":"Extensions\u00b6","text":"<ul> <li>Does changing the data input size to EfficientNetB4 change its results? E.g. input image size of (380, 380) instead of (224, 224)?</li> </ul>"},{"location":"09_pytorch_model_deployment/","title":"09. PyTorch \u200b\u6a21\u578b\u200b\u90e8\u7f72","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b | \u200b\u67e5\u770b\u200b\u5e7b\u706f\u7247\u200b</p> In\u00a0[1]: Copied! <pre># For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\ntry:\n    import torch\n    import torchvision\n    assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"\n    assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\nexcept:\n    print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")\n    !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113\n    import torch\n    import torchvision\n    print(f\"torch version: {torch.__version__}\")\n    print(f\"torchvision version: {torchvision.__version__}\")\n</pre> # For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+ try:     import torch     import torchvision     assert int(torch.__version__.split(\".\")[1]) &gt;= 12, \"torch version should be 1.12+\"     assert int(torchvision.__version__.split(\".\")[1]) &gt;= 13, \"torchvision version should be 0.13+\"     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") except:     print(f\"[INFO] torch/torchvision versions not as required, installing nightly versions.\")     !pip3 install -U torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113     import torch     import torchvision     print(f\"torch version: {torch.__version__}\")     print(f\"torchvision version: {torchvision.__version__}\") <pre>torch version: 1.13.0.dev20220824+cu113\ntorchvision version: 0.14.0.dev20220824+cu113\n</pre> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b\u4f7f\u7528\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\u5f00\u59cb\u200b\u5b89\u88c5\u200b\u5404\u79cd\u200b\u8f6f\u4ef6\u5305\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u5728\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\u540e\u200b\u91cd\u542f\u200b\u8fd0\u884c\u200b\u65f6\u200b\u3002\u200b\u91cd\u542f\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u8fd0\u884c\u200b\u8be5\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u5e76\u200b\u9a8c\u8bc1\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u4e86\u200b\u6b63\u786e\u200b\u7248\u672c\u200b\u7684\u200b <code>torch</code> \u200b\u548c\u200b <code>torchvision</code>\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u8fdb\u884c\u200b\u5e38\u89c4\u200b\u7684\u200b\u5bfc\u5165\u200b\uff0c\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u4ece\u200b GitHub \u200b\u83b7\u53d6\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u3002</p> <p><code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u5305\u542b\u200b\u6211\u4eec\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b\u7ae0\u8282\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u51e0\u4e2a\u200b\u51fd\u6570\u200b\uff1a</p> <ul> <li><code>set_seeds()</code> \u200b\u7528\u4e8e\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff08\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u7b2c\u200b0\u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff09\u3002</li> <li><code>download_data()</code> \u200b\u7528\u4e8e\u200b\u6839\u636e\u200b\u94fe\u63a5\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u6e90\u200b\uff08\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u7b2c\u200b1\u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff09\u3002</li> <li><code>plot_loss_curves()</code> \u200b\u7528\u4e8e\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ed3\u679c\u200b\uff08\u200b\u5728\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b7.8\u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff09\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5c06\u200b <code>helper_functions.py</code> \u200b\u811a\u672c\u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u51fd\u6570\u200b\u5408\u5e76\u200b\u5230\u200b <code>going_modular/going_modular/utils.py</code> \u200b\u4e2d\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u66f4\u597d\u200b\u7684\u200b\u4e3b\u610f\u200b\uff0c\u200b\u4e5f\u8bb8\u200b\u8fd9\u200b\u662f\u200b\u4f60\u200b\u60f3\u200b\u5c1d\u8bd5\u200b\u7684\u200b\u6269\u5c55\u200b\u3002</p> In\u00a0[2]: Copied! <pre># Continue with regular imports\nimport matplotlib.pyplot as plt\nimport torch\nimport torchvision\n\nfrom torch import nn\nfrom torchvision import transforms\n\n# Try to get torchinfo, install it if it doesn't work\ntry:\n    from torchinfo import summary\nexcept:\n    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n    !pip install -q torchinfo\n    from torchinfo import summary\n\n# Try to import the going_modular directory, download it from GitHub if it doesn't work\ntry:\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\nexcept:\n    # Get the going_modular scripts\n    print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")\n    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n    !mv pytorch-deep-learning/going_modular .\n    !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script\n    !rm -rf pytorch-deep-learning\n    from going_modular.going_modular import data_setup, engine\n    from helper_functions import download_data, set_seeds, plot_loss_curves\n</pre> # Continue with regular imports import matplotlib.pyplot as plt import torch import torchvision  from torch import nn from torchvision import transforms  # Try to get torchinfo, install it if it doesn't work try:     from torchinfo import summary except:     print(\"[INFO] Couldn't find torchinfo... installing it.\")     !pip install -q torchinfo     from torchinfo import summary  # Try to import the going_modular directory, download it from GitHub if it doesn't work try:     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves except:     # Get the going_modular scripts     print(\"[INFO] Couldn't find going_modular or helper_functions scripts... downloading them from GitHub.\")     !git clone https://github.com/mrdbourke/pytorch-deep-learning     !mv pytorch-deep-learning/going_modular .     !mv pytorch-deep-learning/helper_functions.py . # get the helper_functions.py script     !rm -rf pytorch-deep-learning     from going_modular.going_modular import data_setup, engine     from helper_functions import download_data, set_seeds, plot_loss_curves <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u3002</p> In\u00a0[3]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[3]: <pre>'cuda'</pre> In\u00a0[4]: Copied! <pre># Download pizza, steak, sushi images from GitHub\ndata_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",\n                                     destination=\"pizza_steak_sushi_20_percent\")\n\ndata_20_percent_path\n</pre> # Download pizza, steak, sushi images from GitHub data_20_percent_path = download_data(source=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi_20_percent.zip\",                                      destination=\"pizza_steak_sushi_20_percent\")  data_20_percent_path <pre>[INFO] data/pizza_steak_sushi_20_percent directory exists, skipping download.\n</pre> Out[4]: <pre>PosixPath('data/pizza_steak_sushi_20_percent')</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u8def\u5f84\u200b\u5427\u200b\u3002</p> In\u00a0[5]: Copied! <pre># Setup directory paths to train and test images\ntrain_dir = data_20_percent_path / \"train\"\ntest_dir = data_20_percent_path / \"test\"\n</pre> # Setup directory paths to train and test images train_dir = data_20_percent_path / \"train\" test_dir = data_20_percent_path / \"test\" In\u00a0[6]: Copied! <pre># 1. Setup pretrained EffNetB2 weights\neffnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n\n# 2. Get EffNetB2 transforms\neffnetb2_transforms = effnetb2_weights.transforms()\n\n# 3. Setup pretrained model\neffnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"\n\n# 4. Freeze the base layers in the model (this will freeze all layers to begin with)\nfor param in effnetb2.parameters():\n    param.requires_grad = False\n</pre> # 1. Setup pretrained EffNetB2 weights effnetb2_weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT  # 2. Get EffNetB2 transforms effnetb2_transforms = effnetb2_weights.transforms()  # 3. Setup pretrained model effnetb2 = torchvision.models.efficientnet_b2(weights=effnetb2_weights) # could also use weights=\"DEFAULT\"  # 4. Freeze the base layers in the model (this will freeze all layers to begin with) for param in effnetb2.parameters():     param.requires_grad = False <p>\u200b\u73b0\u5728\u200b\u8981\u200b\u4fee\u6539\u200b\u5206\u7c7b\u5668\u200b\u5934\u90e8\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5148\u200b\u4f7f\u7528\u200b\u6a21\u578b\u200b\u7684\u200b <code>classifier</code> \u200b\u5c5e\u6027\u200b\u6765\u200b\u68c0\u67e5\u200b\u5b83\u200b\u3002</p> In\u00a0[7]: Copied! <pre># Check out EffNetB2 classifier head\neffnetb2.classifier\n</pre> # Check out EffNetB2 classifier head effnetb2.classifier Out[7]: <pre>Sequential(\n  (0): Dropout(p=0.3, inplace=True)\n  (1): Linear(in_features=1408, out_features=1000, bias=True)\n)</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u4e3a\u4e86\u200b\u4f7f\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b <code>out_features</code> \u200b\u53d8\u91cf\u200b\u66ff\u6362\u200b\u4e3a\u200b\u6211\u4eec\u200b\u62e5\u6709\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\uff08\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c<code>out_features=3</code>\uff0c\u200b\u5206\u522b\u200b\u5bf9\u5e94\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u4e2a\u200b\u66f4\u6539\u200b\u8f93\u51fa\u200b\u5c42\u200b/\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u5c06\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u95ee\u9898\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u8981\u200b\u4e0d\u540c\u200b\u6570\u91cf\u200b\u7684\u200b\u8f93\u51fa\u200b\u6216\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u76f8\u5e94\u200b\u5730\u200b\u66f4\u6539\u200b\u8f93\u51fa\u200b\u5c42\u200b\u3002</p> In\u00a0[8]: Copied! <pre># 5. Update the classifier head\neffnetb2.classifier = nn.Sequential(\n    nn.Dropout(p=0.3, inplace=True), # keep dropout layer same\n    nn.Linear(in_features=1408, # keep in_features same \n              out_features=3)) # change out_features to suit our number of classes\n</pre> # 5. Update the classifier head effnetb2.classifier = nn.Sequential(     nn.Dropout(p=0.3, inplace=True), # keep dropout layer same     nn.Linear(in_features=1408, # keep in_features same                out_features=3)) # change out_features to suit our number of classes <p>\u200b\u592a\u7f8e\u200b\u4e86\u200b\uff01</p> In\u00a0[9]: Copied! <pre>def create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # 4. Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # 5. Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # 1, 2, 3. Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # 4. Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # 5. Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <p>\u200b\u54c7\u200b\u54e6\u200b\uff01\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u8bd5\u8bd5\u200b\u5427\u200b\u3002</p> In\u00a0[10]: Copied! <pre>effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,\n                                                      seed=42)\n</pre> effnetb2, effnetb2_transforms = create_effnetb2_model(num_classes=3,                                                       seed=42) <p>\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\uff0c\u200b\u5f88\u200b\u597d\u200b\uff0c\u200b\u73b0\u5728\u200b\u6765\u200b\u771f\u6b63\u200b\u5c1d\u8bd5\u200b\u4e00\u4e0b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b <code>torchinfo.summary()</code> \u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u603b\u7ed3\u200b\u3002</p> In\u00a0[11]: Copied! <pre>from torchinfo import summary\n\n# # Print EffNetB2 model summary (uncomment for full output) \n# summary(effnetb2, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Print EffNetB2 model summary (uncomment for full output)  # summary(effnetb2,  #         input_size=(1, 3, 224, 224), #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p>\u200b\u57fa\u7840\u200b\u5c42\u200b\u51bb\u7ed3\u200b\uff0c\u200b\u9876\u5c42\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u4e14\u200b\u81ea\u5b9a\u4e49\u200b\uff01</p> In\u00a0[12]: Copied! <pre># Setup DataLoaders\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                                 test_dir=test_dir,\n                                                                                                 transform=effnetb2_transforms,\n                                                                                                 batch_size=32)\n</pre> # Setup DataLoaders from going_modular.going_modular import data_setup train_dataloader_effnetb2, test_dataloader_effnetb2, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                                  test_dir=test_dir,                                                                                                  transform=effnetb2_transforms,                                                                                                  batch_size=32) In\u00a0[13]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=effnetb2.parameters(),\n                             lr=1e-3)\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Set seeds for reproducibility and train the model\nset_seeds()\neffnetb2_results = engine.train(model=effnetb2,\n                                train_dataloader=train_dataloader_effnetb2,\n                                test_dataloader=test_dataloader_effnetb2,\n                                epochs=10,\n                                optimizer=optimizer,\n                                loss_fn=loss_fn,\n                                device=device)\n</pre> from going_modular.going_modular import engine  # Setup optimizer optimizer = torch.optim.Adam(params=effnetb2.parameters(),                              lr=1e-3) # Setup loss function loss_fn = torch.nn.CrossEntropyLoss()  # Set seeds for reproducibility and train the model set_seeds() effnetb2_results = engine.train(model=effnetb2,                                 train_dataloader=train_dataloader_effnetb2,                                 test_dataloader=test_dataloader_effnetb2,                                 epochs=10,                                 optimizer=optimizer,                                 loss_fn=loss_fn,                                 device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.9856 | train_acc: 0.5604 | test_loss: 0.7408 | test_acc: 0.9347\nEpoch: 2 | train_loss: 0.7175 | train_acc: 0.8438 | test_loss: 0.5869 | test_acc: 0.9409\nEpoch: 3 | train_loss: 0.5876 | train_acc: 0.8917 | test_loss: 0.4909 | test_acc: 0.9500\nEpoch: 4 | train_loss: 0.4474 | train_acc: 0.9062 | test_loss: 0.4355 | test_acc: 0.9409\nEpoch: 5 | train_loss: 0.4290 | train_acc: 0.9104 | test_loss: 0.3915 | test_acc: 0.9443\nEpoch: 6 | train_loss: 0.4381 | train_acc: 0.8896 | test_loss: 0.3512 | test_acc: 0.9688\nEpoch: 7 | train_loss: 0.4245 | train_acc: 0.8771 | test_loss: 0.3268 | test_acc: 0.9563\nEpoch: 8 | train_loss: 0.3897 | train_acc: 0.8958 | test_loss: 0.3457 | test_acc: 0.9381\nEpoch: 9 | train_loss: 0.3749 | train_acc: 0.8812 | test_loss: 0.3129 | test_acc: 0.9131\nEpoch: 10 | train_loss: 0.3757 | train_acc: 0.8604 | test_loss: 0.2813 | test_acc: 0.9688\n</pre> In\u00a0[14]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(effnetb2_results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(effnetb2_results) <p>\u200b\u54c7\u200b\uff01</p> <p>\u200b\u8fd9\u4e9b\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u597d\u200b\uff0c\u200b\u6216\u8bb8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7a0d\u200b\u957f\u65f6\u95f4\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff08\u200b\u4ee5\u200b\u5e2e\u52a9\u200b\u9632\u6b62\u200b\u56e0\u200b\u957f\u65f6\u95f4\u200b\u8bad\u7ec3\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u7684\u200b\u8fc7\u200b\u62df\u5408\u200b\uff09\u200b\u6765\u200b\u8fdb\u4e00\u6b65\u200b\u63d0\u5347\u200b\u6027\u80fd\u200b\u3002</p> In\u00a0[15]: Copied! <pre>from going_modular.going_modular import utils\n\n# Save the model\nutils.save_model(model=effnetb2,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n</pre> from going_modular.going_modular import utils  # Save the model utils.save_model(model=effnetb2,                  target_dir=\"models\",                  model_name=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\") <pre>[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n</pre> In\u00a0[16]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_effnetb2_model_size = Path(\"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained EffNetB2 feature extractor model size: {pretrained_effnetb2_model_size} MB\") <pre>Pretrained EffNetB2 feature extractor model size: 29 MB\n</pre> In\u00a0[17]: Copied! <pre># Count number of parameters in EffNetB2\neffnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters())\neffnetb2_total_params\n</pre> # Count number of parameters in EffNetB2 effnetb2_total_params = sum(torch.numel(param) for param in effnetb2.parameters()) effnetb2_total_params Out[17]: <pre>7705221</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u4e86\u200b\u3002</p> In\u00a0[18]: Copied! <pre># Create a dictionary with EffNetB2 statistics\neffnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],\n                  \"test_acc\": effnetb2_results[\"test_acc\"][-1],\n                  \"number_of_parameters\": effnetb2_total_params,\n                  \"model_size (MB)\": pretrained_effnetb2_model_size}\neffnetb2_stats\n</pre> # Create a dictionary with EffNetB2 statistics effnetb2_stats = {\"test_loss\": effnetb2_results[\"test_loss\"][-1],                   \"test_acc\": effnetb2_results[\"test_acc\"][-1],                   \"number_of_parameters\": effnetb2_total_params,                   \"model_size (MB)\": pretrained_effnetb2_model_size} effnetb2_stats Out[18]: <pre>{'test_loss': 0.28128674924373626,\n 'test_acc': 0.96875,\n 'number_of_parameters': 7705221,\n 'model_size (MB)': 29}</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u51c6\u786e\u7387\u200b\u8d85\u8fc7\u200b\u4e86\u200b95%\uff01</p> <p>\u200b\u7b2c\u4e00\u6761\u200b\u6807\u51c6\u200b\uff1a\u200b\u8fbe\u5230\u200b95%\u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u8fbe\u6807\u200b\uff01</p> In\u00a0[19]: Copied! <pre># Check out ViT heads layer\nvit = torchvision.models.vit_b_16()\nvit.heads\n</pre> # Check out ViT heads layer vit = torchvision.models.vit_b_16() vit.heads Out[19]: <pre>Sequential(\n  (head): Linear(in_features=768, out_features=1000, bias=True)\n)</pre> <p>\u200b\u4e86\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u62e5\u6709\u200b\u4e86\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u62fc\u56fe\u200b\u788e\u7247\u200b\u3002</p> In\u00a0[20]: Copied! <pre>def create_vit_model(num_classes:int=3, \n                     seed:int=42):\n    \"\"\"Creates a ViT-B/16 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of target classes. Defaults to 3.\n        seed (int, optional): random seed value for output layer. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): ViT-B/16 feature extractor model. \n        transforms (torchvision.transforms): ViT-B/16 image transforms.\n    \"\"\"\n    # Create ViT_B_16 pretrained weights, transforms and model\n    weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.vit_b_16(weights=weights)\n\n    # Freeze all layers in model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head to suit our needs (this will be trainable)\n    torch.manual_seed(seed)\n    model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model\n                                          out_features=num_classes)) # update to reflect target number of classes\n    \n    return model, transforms\n</pre> def create_vit_model(num_classes:int=3,                       seed:int=42):     \"\"\"Creates a ViT-B/16 feature extractor model and transforms.      Args:         num_classes (int, optional): number of target classes. Defaults to 3.         seed (int, optional): random seed value for output layer. Defaults to 42.      Returns:         model (torch.nn.Module): ViT-B/16 feature extractor model.          transforms (torchvision.transforms): ViT-B/16 image transforms.     \"\"\"     # Create ViT_B_16 pretrained weights, transforms and model     weights = torchvision.models.ViT_B_16_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.vit_b_16(weights=weights)      # Freeze all layers in model     for param in model.parameters():         param.requires_grad = False      # Change classifier head to suit our needs (this will be trainable)     torch.manual_seed(seed)     model.heads = nn.Sequential(nn.Linear(in_features=768, # keep this the same as original model                                           out_features=num_classes)) # update to reflect target number of classes          return model, transforms <p>ViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\u51fd\u6570\u200b\u5df2\u200b\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p> In\u00a0[21]: Copied! <pre># Create ViT model and transforms\nvit, vit_transforms = create_vit_model(num_classes=3,\n                                       seed=42)\n</pre> # Create ViT model and transforms vit, vit_transforms = create_vit_model(num_classes=3,                                        seed=42) <p>\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\uff0c\u200b\u771f\u662f\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torchinfo.summary()</code> \u200b\u6765\u200b\u83b7\u53d6\u200b\u4e00\u4e2a\u200b\u6f02\u4eae\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u603b\u7ed3\u200b\u3002</p> In\u00a0[22]: Copied! <pre>from torchinfo import summary\n\n# # Print ViT feature extractor model summary (uncomment for full output)\n# summary(vit, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Print ViT feature extractor model summary (uncomment for full output) # summary(vit,  #         input_size=(1, 3, 224, 224), #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p></p> <p>\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\u662f\u200b\u51bb\u7ed3\u200b\u7684\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5c42\u5219\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u7684\u200b\u9700\u6c42\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5b9a\u5236\u200b\uff01</p> <p>\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u4f60\u200b\u662f\u5426\u200b\u6ce8\u610f\u200b\u5230\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u5de8\u5927\u200b\u7684\u200b\u5dee\u5f02\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u6bd4\u200bEffNetB2\u200b\u6a21\u578b\u200b\u62e5\u6709\u200b\u591a\u5f97\u591a\u200b\u7684\u200b\u53c2\u6570\u200b\u3002\u200b\u4e5f\u8bb8\u200b\u8fd9\u200b\u5728\u200b\u7a0d\u540e\u200b\u6211\u4eec\u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u5728\u200b\u901f\u5ea6\u200b\u548c\u200b\u6027\u80fd\u200b\u65b9\u9762\u200b\u7684\u200b\u8868\u73b0\u200b\u65f6\u4f1a\u200b\u8d77\u5230\u200b\u4f5c\u7528\u200b\u3002</p> In\u00a0[23]: Copied! <pre># Setup ViT DataLoaders\nfrom going_modular.going_modular import data_setup\ntrain_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,\n                                                                                       test_dir=test_dir,\n                                                                                       transform=vit_transforms,\n                                                                                       batch_size=32)\n</pre> # Setup ViT DataLoaders from going_modular.going_modular import data_setup train_dataloader_vit, test_dataloader_vit, class_names = data_setup.create_dataloaders(train_dir=train_dir,                                                                                        test_dir=test_dir,                                                                                        transform=vit_transforms,                                                                                        batch_size=32) In\u00a0[24]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=vit.parameters(),\n                             lr=1e-3)\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train ViT model with seeds set for reproducibility\nset_seeds()\nvit_results = engine.train(model=vit,\n                           train_dataloader=train_dataloader_vit,\n                           test_dataloader=test_dataloader_vit,\n                           epochs=10,\n                           optimizer=optimizer,\n                           loss_fn=loss_fn,\n                           device=device)\n</pre> from going_modular.going_modular import engine  # Setup optimizer optimizer = torch.optim.Adam(params=vit.parameters(),                              lr=1e-3) # Setup loss function loss_fn = torch.nn.CrossEntropyLoss()  # Train ViT model with seeds set for reproducibility set_seeds() vit_results = engine.train(model=vit,                            train_dataloader=train_dataloader_vit,                            test_dataloader=test_dataloader_vit,                            epochs=10,                            optimizer=optimizer,                            loss_fn=loss_fn,                            device=device) <pre>  0%|          | 0/10 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7023 | train_acc: 0.7500 | test_loss: 0.2714 | test_acc: 0.9290\nEpoch: 2 | train_loss: 0.2531 | train_acc: 0.9104 | test_loss: 0.1669 | test_acc: 0.9602\nEpoch: 3 | train_loss: 0.1766 | train_acc: 0.9542 | test_loss: 0.1270 | test_acc: 0.9693\nEpoch: 4 | train_loss: 0.1277 | train_acc: 0.9625 | test_loss: 0.1072 | test_acc: 0.9722\nEpoch: 5 | train_loss: 0.1163 | train_acc: 0.9646 | test_loss: 0.0950 | test_acc: 0.9784\nEpoch: 6 | train_loss: 0.1270 | train_acc: 0.9375 | test_loss: 0.0830 | test_acc: 0.9722\nEpoch: 7 | train_loss: 0.0899 | train_acc: 0.9771 | test_loss: 0.0844 | test_acc: 0.9784\nEpoch: 8 | train_loss: 0.0928 | train_acc: 0.9812 | test_loss: 0.0759 | test_acc: 0.9722\nEpoch: 9 | train_loss: 0.0933 | train_acc: 0.9792 | test_loss: 0.0729 | test_acc: 0.9784\nEpoch: 10 | train_loss: 0.0662 | train_acc: 0.9833 | test_loss: 0.0642 | test_acc: 0.9847\n</pre> In\u00a0[25]: Copied! <pre>from helper_functions import plot_loss_curves\n\nplot_loss_curves(vit_results)\n</pre> from helper_functions import plot_loss_curves  plot_loss_curves(vit_results) <p>\u200b\u54e6\u200b\u8036\u200b\uff01</p> <p>\u200b\u8fd9\u4e9b\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u770b\u8d77\u6765\u200b\u771f\u4e0d\u9519\u200b\u3002\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u6a21\u578b\u200b\u4e00\u6837\u200b\uff0c\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u4e5f\u200b\u4f1a\u200b\u4ece\u200b\u7a0d\u200b\u957f\u65f6\u95f4\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff08\u200b\u6709\u52a9\u4e8e\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\uff09\u200b\u4e2d\u200b\u83b7\u76ca\u200b\u3002</p> In\u00a0[26]: Copied! <pre># Save the model\nfrom going_modular.going_modular import utils\n\nutils.save_model(model=vit,\n                 target_dir=\"models\",\n                 model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\")\n</pre> # Save the model from going_modular.going_modular import utils  utils.save_model(model=vit,                  target_dir=\"models\",                  model_name=\"09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\") <pre>[INFO] Saving model to: models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\n</pre> In\u00a0[27]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_vit_model_size = Path(\"models/09_pretrained_vit_feature_extractor_pizza_steak_sushi_20_percent.pth\").stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained ViT feature extractor model size: {pretrained_vit_model_size} MB\") <pre>Pretrained ViT feature extractor model size: 327 MB\n</pre> <p>\u200b\u55ef\u200b\uff0cViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u5c3a\u5bf8\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u5c3a\u5bf8\u200b\u76f8\u6bd4\u200b\u5982\u4f55\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u901a\u8fc7\u200b\u6bd4\u8f83\u200b\u6240\u6709\u200b\u6a21\u578b\u200b\u7684\u200b\u7279\u6027\u200b\u6765\u200b\u627e\u51fa\u200b\u7b54\u6848\u200b\u3002</p> In\u00a0[28]: Copied! <pre># Count number of parameters in ViT\nvit_total_params = sum(torch.numel(param) for param in vit.parameters())\nvit_total_params\n</pre> # Count number of parameters in ViT vit_total_params = sum(torch.numel(param) for param in vit.parameters()) vit_total_params Out[28]: <pre>85800963</pre> <p>\u200b\u54c7\u200b\uff0c\u200b\u8fd9\u200b\u770b\u8d77\u6765\u200b\u6bd4\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u591a\u591a\u200b\u4e86\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u53c2\u6570\u200b\uff08\u200b\u6216\u200b\u6743\u91cd\u200b/\u200b\u6a21\u5f0f\u200b\uff09\u200b\u6570\u91cf\u200b\u8f83\u200b\u591a\u200b\u901a\u5e38\u200b\u610f\u5473\u7740\u200b\u6a21\u578b\u200b\u5177\u6709\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\uff0c\u200b\u81f3\u4e8e\u200b\u5b83\u200b\u662f\u5426\u200b\u771f\u7684\u200b\u5229\u7528\u200b\u4e86\u200b\u8fd9\u79cd\u200b\u989d\u5916\u200b\u7684\u200b\u80fd\u529b\u200b\u5219\u200b\u662f\u200b\u53e6\u4e00\u56de\u4e8b\u200b\u3002\u200b\u8003\u8651\u200b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u6709\u200b7,705,221\u200b\u4e2a\u200b\u53c2\u6570\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u6709\u200b85,800,963\u200b\u4e2a\u200b\u53c2\u6570\u200b\uff08\u200b\u591a\u200b11.1\u200b\u500d\u200b\uff09\uff0c\u200b\u56e0\u6b64\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5047\u8bbe\u200bViT\u200b\u6a21\u578b\u200b\u5728\u200b\u62e5\u6709\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u5b66\u4e60\u200b\u673a\u4f1a\u200b\uff09\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5177\u6709\u200b\u66f4\u5927\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u66f4\u5927\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\u5f80\u5f80\u200b\u4f34\u968f\u200b\u7740\u200b\u6a21\u578b\u200b\u6587\u4ef6\u5927\u5c0f\u200b\u7684\u200b\u589e\u52a0\u200b\u548c\u200b\u63a8\u7406\u200b\u65f6\u95f4\u200b\u7684\u200b\u5ef6\u957f\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200bViT\u200b\u6a21\u578b\u200b\u4e00\u4e9b\u200b\u91cd\u8981\u200b\u7279\u5f81\u200b\u7684\u200b\u5b57\u5178\u200b\u3002</p> In\u00a0[29]: Copied! <pre># Create ViT statistics dictionary\nvit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],\n             \"test_acc\": vit_results[\"test_acc\"][-1],\n             \"number_of_parameters\": vit_total_params,\n             \"model_size (MB)\": pretrained_vit_model_size}\n\nvit_stats\n</pre> # Create ViT statistics dictionary vit_stats = {\"test_loss\": vit_results[\"test_loss\"][-1],              \"test_acc\": vit_results[\"test_acc\"][-1],              \"number_of_parameters\": vit_total_params,              \"model_size (MB)\": pretrained_vit_model_size}  vit_stats Out[29]: <pre>{'test_loss': 0.06418210905976593,\n 'test_acc': 0.984659090909091,\n 'number_of_parameters': 85800963,\n 'model_size (MB)': 327}</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u4e5f\u200b\u8fbe\u5230\u200b\u4e86\u200b\u8d85\u8fc7\u200b95%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002</p> In\u00a0[30]: Copied! <pre>from pathlib import Path\n\n# Get all test data paths\nprint(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\")\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\ntest_data_paths[:5]\n</pre> from pathlib import Path  # Get all test data paths print(f\"[INFO] Finding all filepaths ending with '.jpg' in directory: {test_dir}\") test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\")) test_data_paths[:5] <pre>[INFO] Finding all filepaths ending with '.jpg' in directory: data/pizza_steak_sushi_20_percent/test\n</pre> Out[30]: <pre>[PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/2752603.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/39461.jpg'),\n PosixPath('data/pizza_steak_sushi_20_percent/test/steak/730464.jpg')]</pre> In\u00a0[31]: Copied! <pre>import pathlib\nimport torch\n\nfrom PIL import Image\nfrom timeit import default_timer as timer \nfrom tqdm.auto import tqdm\nfrom typing import List, Dict\n\n# 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time\ndef pred_and_store(paths: List[pathlib.Path], \n                   model: torch.nn.Module,\n                   transform: torchvision.transforms, \n                   class_names: List[str], \n                   device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:\n    \n    # 2. Create an empty list to store prediction dictionaires\n    pred_list = []\n    \n    # 3. Loop through target paths\n    for path in tqdm(paths):\n        \n        # 4. Create empty dictionary to store prediction information for each sample\n        pred_dict = {}\n\n        # 5. Get the sample path and ground truth class name\n        pred_dict[\"image_path\"] = path\n        class_name = path.parent.stem\n        pred_dict[\"class_name\"] = class_name\n        \n        # 6. Start the prediction timer\n        start_time = timer()\n        \n        # 7. Open image path\n        img = Image.open(path)\n        \n        # 8. Transform the image, add batch dimension and put image on target device\n        transformed_image = transform(img).unsqueeze(0).to(device) \n        \n        # 9. Prepare model for inference by sending it to target device and turning on eval() mode\n        model.to(device)\n        model.eval()\n        \n        # 10. Get prediction probability, predicition label and prediction class\n        with torch.inference_mode():\n            pred_logit = model(transformed_image) # perform inference on target sample \n            pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities\n            pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label\n            pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU\n\n            # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on) \n            pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)\n            pred_dict[\"pred_class\"] = pred_class\n            \n            # 12. End the timer and calculate time per pred\n            end_time = timer()\n            pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)\n\n        # 13. Does the pred match the true label?\n        pred_dict[\"correct\"] = class_name == pred_class\n\n        # 14. Add the dictionary to the list of preds\n        pred_list.append(pred_dict)\n    \n    # 15. Return list of prediction dictionaries\n    return pred_list\n</pre> import pathlib import torch  from PIL import Image from timeit import default_timer as timer  from tqdm.auto import tqdm from typing import List, Dict  # 1. Create a function to return a list of dictionaries with sample, truth label, prediction, prediction probability and prediction time def pred_and_store(paths: List[pathlib.Path],                     model: torch.nn.Module,                    transform: torchvision.transforms,                     class_names: List[str],                     device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\") -&gt; List[Dict]:          # 2. Create an empty list to store prediction dictionaires     pred_list = []          # 3. Loop through target paths     for path in tqdm(paths):                  # 4. Create empty dictionary to store prediction information for each sample         pred_dict = {}          # 5. Get the sample path and ground truth class name         pred_dict[\"image_path\"] = path         class_name = path.parent.stem         pred_dict[\"class_name\"] = class_name                  # 6. Start the prediction timer         start_time = timer()                  # 7. Open image path         img = Image.open(path)                  # 8. Transform the image, add batch dimension and put image on target device         transformed_image = transform(img).unsqueeze(0).to(device)                   # 9. Prepare model for inference by sending it to target device and turning on eval() mode         model.to(device)         model.eval()                  # 10. Get prediction probability, predicition label and prediction class         with torch.inference_mode():             pred_logit = model(transformed_image) # perform inference on target sample              pred_prob = torch.softmax(pred_logit, dim=1) # turn logits into prediction probabilities             pred_label = torch.argmax(pred_prob, dim=1) # turn prediction probabilities into prediction label             pred_class = class_names[pred_label.cpu()] # hardcode prediction class to be on CPU              # 11. Make sure things in the dictionary are on CPU (required for inspecting predictions later on)              pred_dict[\"pred_prob\"] = round(pred_prob.unsqueeze(0).max().cpu().item(), 4)             pred_dict[\"pred_class\"] = pred_class                          # 12. End the timer and calculate time per pred             end_time = timer()             pred_dict[\"time_for_pred\"] = round(end_time-start_time, 4)          # 13. Does the pred match the true label?         pred_dict[\"correct\"] = class_name == pred_class          # 14. Add the dictionary to the list of preds         pred_list.append(pred_dict)          # 15. Return list of prediction dictionaries     return pred_list <p>\u200b\u56af\u200b\uff0c\u200b\u56af\u200b\uff01</p> <p>\u200b\u591a\u4e48\u200b\u7f8e\u89c2\u200b\u7684\u200b\u51fd\u6570\u200b\u554a\u200b\uff01</p> <p>\u200b\u800c\u4e14\u200b\u4f60\u200b\u77e5\u9053\u200b\u5417\u200b\uff0c\u200b\u65e2\u7136\u200b\u6211\u4eec\u200b\u7684\u200b <code>pred_and_store()</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u7684\u200b\u5b9e\u7528\u200b\u51fd\u6570\u200b\uff0c\u200b\u7528\u4e8e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u5b58\u50a8\u200b\u7ed3\u679c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u5b58\u653e\u200b\u5230\u200b <code>going_modular.going_modular.predictions.py</code> \u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u5907\u200b\u540e\u7eed\u200b\u4f7f\u7528\u200b\u3002\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u4f60\u200b\u613f\u610f\u200b\u5c1d\u8bd5\u200b\u7684\u200b\u6269\u5c55\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b 05. PyTorch Going Modular \u200b\u83b7\u53d6\u200b\u66f4\u200b\u591a\u200b\u60f3\u6cd5\u200b\u3002</p> In\u00a0[32]: Copied! <pre># Make predictions across test dataset with EffNetB2\neffnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                          model=effnetb2,\n                                          transform=effnetb2_transforms,\n                                          class_names=class_names,\n                                          device=\"cpu\") # make predictions on CPU\n</pre> # Make predictions across test dataset with EffNetB2 effnetb2_test_pred_dicts = pred_and_store(paths=test_data_paths,                                           model=effnetb2,                                           transform=effnetb2_transforms,                                           class_names=class_names,                                           device=\"cpu\") # make predictions on CPU  <pre>  0%|          | 0/150 [00:00&lt;?, ?it/s]</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u770b\u770b\u200b\u90a3\u4e9b\u200b\u9884\u6d4b\u200b\u98de\u901f\u200b\u8fd0\u884c\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u200b\u524d\u200b\u51e0\u4e2a\u200b\uff0c\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p> In\u00a0[33]: Copied! <pre># Inspect the first 2 prediction dictionaries\neffnetb2_test_pred_dicts[:2]\n</pre> # Inspect the first 2 prediction dictionaries effnetb2_test_pred_dicts[:2] Out[33]: <pre>[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9293,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0494,\n  'correct': True},\n {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9534,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0264,\n  'correct': True}]</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b <code>pred_and_store()</code> \u200b\u51fd\u6570\u200b\u8fd0\u884c\u200b\u5f97\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u591a\u4e8f\u200b\u4e86\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\u7684\u200b\u6570\u636e\u7ed3\u6784\u200b\uff0c\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u62e5\u6709\u200b\u4e86\u200b\u8bb8\u591a\u200b\u53ef\u4ee5\u200b\u8fdb\u4e00\u6b65\u200b\u68c0\u67e5\u200b\u7684\u200b\u6709\u7528\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\u8f6c\u6362\u200b\u4e3a\u200b pandas DataFrame\u3002</p> In\u00a0[34]: Copied! <pre># Turn the test_pred_dicts into a DataFrame\nimport pandas as pd\neffnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts)\neffnetb2_test_pred_df.head()\n</pre> # Turn the test_pred_dicts into a DataFrame import pandas as pd effnetb2_test_pred_df = pd.DataFrame(effnetb2_test_pred_dicts) effnetb2_test_pred_df.head() Out[34]: image_path class_name pred_prob pred_class time_for_pred correct 0 data/pizza_steak_sushi_20_percent/test/steak/8... steak 0.9293 steak 0.0494 True 1 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.9534 steak 0.0264 True 2 data/pizza_steak_sushi_20_percent/test/steak/2... steak 0.7532 steak 0.0256 True 3 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.5935 steak 0.0263 True 4 data/pizza_steak_sushi_20_percent/test/steak/7... steak 0.8959 steak 0.0269 True <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u770b\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\u662f\u200b\u5982\u4f55\u200b\u8f7b\u677e\u200b\u5730\u200b\u8f6c\u6362\u6210\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8fdb\u884c\u200b\u5206\u6790\u200b\u7684\u200b\u7ed3\u6784\u5316\u200b\u683c\u5f0f\u200b\u7684\u200b\u3002</p> <p>\u200b\u6bd4\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u627e\u51fa\u200bEffNetB2\u200b\u6a21\u578b\u200b\u6709\u200b\u591a\u5c11\u200b\u9884\u6d4b\u200b\u662f\u200b\u9519\u8bef\u200b\u7684\u200b...</p> In\u00a0[35]: Copied! <pre># Check number of correct predictions\neffnetb2_test_pred_df.correct.value_counts()\n</pre> # Check number of correct predictions effnetb2_test_pred_df.correct.value_counts() Out[35]: <pre>True     145\nFalse      5\nName: correct, dtype: int64</pre> <p>150\u200b\u4e2a\u200b\u9884\u6d4b\u200b\u4e2d\u200b\u53ea\u6709\u200b5\u200b\u4e2a\u200b\u9519\u8bef\u200b\uff0c\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u90a3\u4e48\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u5462\u200b\uff1f</p> In\u00a0[36]: Copied! <pre># Find the average time per prediction \neffnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\")\n</pre> # Find the average time per prediction  effnetb2_average_time_per_pred = round(effnetb2_test_pred_df.time_for_pred.mean(), 4) print(f\"EffNetB2 average time per prediction: {effnetb2_average_time_per_pred} seconds\") <pre>EffNetB2 average time per prediction: 0.0269 seconds\n</pre> <p>\u200b\u6ce8\u200b\uff1a \u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u4f1a\u56e0\u200b\u4e0d\u540c\u200b\u786c\u4ef6\u200b\u7c7b\u578b\u200b\u800c\u5f02\u200b\uff08\u200b\u4f8b\u5982\u200b\u672c\u5730\u200b Intel i9 \u200b\u4e0e\u200b Google Colab CPU\uff09\u3002\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u786c\u4ef6\u200b\u8d8a\u597d\u200b\u8d8a\u200b\u5feb\u200b\uff0c\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u4e5f\u200b\u8d8a\u200b\u5feb\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u672c\u5730\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b PC \u200b\u4e0a\u200b\uff0c\u200b\u914d\u5907\u200b Intel i9 \u200b\u82af\u7247\u200b\uff0c\u200b\u4f7f\u7528\u200b EffNetB2 \u200b\u7684\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u7ea6\u200b\u4e3a\u200b 0.031 \u200b\u79d2\u200b\uff08\u200b\u63a5\u8fd1\u200b\u5b9e\u65f6\u200b\uff09\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u5728\u200b Google Colab \u200b\u4e0a\u200b\uff08\u200b\u6211\u200b\u4e0d\u200b\u786e\u5b9a\u200b Colab \u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u4ec0\u4e48\u200b CPU \u200b\u786c\u4ef6\u200b\uff0c\u200b\u4f46\u200b\u770b\u8d77\u6765\u200b\u53ef\u80fd\u200b\u662f\u200b Intel(R) Xeon(R)\uff09\uff0c\u200b\u4f7f\u7528\u200b EffNetB2 \u200b\u7684\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u7ea6\u200b\u4e3a\u200b 0.1396 \u200b\u79d2\u200b\uff08\u200b\u6162\u200b 3-4 \u200b\u500d\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b EffNetB2 \u200b\u7684\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b <code>effnetb2_stats</code> \u200b\u5b57\u5178\u200b\u4e2d\u200b\u3002</p> In\u00a0[37]: Copied! <pre># Add EffNetB2 average prediction time to stats dictionary \neffnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred\neffnetb2_stats\n</pre> # Add EffNetB2 average prediction time to stats dictionary  effnetb2_stats[\"time_per_pred_cpu\"] = effnetb2_average_time_per_pred effnetb2_stats Out[37]: <pre>{'test_loss': 0.28128674924373626,\n 'test_acc': 0.96875,\n 'number_of_parameters': 7705221,\n 'model_size (MB)': 29,\n 'time_per_pred_cpu': 0.0269}</pre> In\u00a0[38]: Copied! <pre># Make list of prediction dictionaries with ViT feature extractor model on test images\nvit_test_pred_dicts = pred_and_store(paths=test_data_paths,\n                                     model=vit,\n                                     transform=vit_transforms,\n                                     class_names=class_names,\n                                     device=\"cpu\")\n</pre> # Make list of prediction dictionaries with ViT feature extractor model on test images vit_test_pred_dicts = pred_and_store(paths=test_data_paths,                                      model=vit,                                      transform=vit_transforms,                                      class_names=class_names,                                      device=\"cpu\") <pre>  0%|          | 0/150 [00:00&lt;?, ?it/s]</pre> <p>\u200b\u9884\u6d4b\u200b\u5df2\u200b\u51fa\u7089\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u524d\u200b\u51e0\u5bf9\u200b\u3002</p> In\u00a0[39]: Copied! <pre># Check the first couple of ViT predictions on the test dataset\nvit_test_pred_dicts[:2]\n</pre> # Check the first couple of ViT predictions on the test dataset vit_test_pred_dicts[:2] Out[39]: <pre>[{'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/831681.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9933,\n  'pred_class': 'steak',\n  'time_for_pred': 0.1313,\n  'correct': True},\n {'image_path': PosixPath('data/pizza_steak_sushi_20_percent/test/steak/3100563.jpg'),\n  'class_name': 'steak',\n  'pred_prob': 0.9893,\n  'pred_class': 'steak',\n  'time_for_pred': 0.0638,\n  'correct': True}]</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u5c31\u200b\u50cf\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u662f\u200b\u4ee5\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\u7684\u200b\u5f62\u5f0f\u200b\u5448\u73b0\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u5730\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u6362\u200b\u4e3a\u200bpandas DataFrame\uff0c\u200b\u4ee5\u4fbf\u200b\u8fdb\u4e00\u6b65\u200b\u68c0\u67e5\u200b\u3002</p> In\u00a0[40]: Copied! <pre># Turn vit_test_pred_dicts into a DataFrame\nimport pandas as pd\nvit_test_pred_df = pd.DataFrame(vit_test_pred_dicts)\nvit_test_pred_df.head()\n</pre> # Turn vit_test_pred_dicts into a DataFrame import pandas as pd vit_test_pred_df = pd.DataFrame(vit_test_pred_dicts) vit_test_pred_df.head() Out[40]: image_path class_name pred_prob pred_class time_for_pred correct 0 data/pizza_steak_sushi_20_percent/test/steak/8... steak 0.9933 steak 0.1313 True 1 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.9893 steak 0.0638 True 2 data/pizza_steak_sushi_20_percent/test/steak/2... steak 0.9971 steak 0.0627 True 3 data/pizza_steak_sushi_20_percent/test/steak/3... steak 0.7685 steak 0.0632 True 4 data/pizza_steak_sushi_20_percent/test/steak/7... steak 0.9499 steak 0.0641 True <p>\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u4e86\u200b\u591a\u5c11\u200b\u6b21\u200b\uff1f</p> In\u00a0[41]: Copied! <pre># Count the number of correct predictions\nvit_test_pred_df.correct.value_counts()\n</pre> # Count the number of correct predictions vit_test_pred_df.correct.value_counts() Out[41]: <pre>True     148\nFalse      2\nName: correct, dtype: int64</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u5728\u200b\u6b63\u786e\u200b\u9884\u6d4b\u200b\u65b9\u9762\u200b\u7565\u80dc\u4e00\u7b79\u200b\uff0c\u200b\u6574\u4e2a\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u4ec5\u9519\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u6837\u672c\u200b\u3002</p> <p>\u200b\u4f5c\u4e3a\u200b\u6269\u5c55\u200b\uff0c\u200b\u4f60\u200b\u6216\u8bb8\u200b\u60f3\u200b\u53ef\u89c6\u5316\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u9519\u8bef\u200b\u9884\u6d4b\u200b\uff0c\u200b\u770b\u770b\u200b\u662f\u5426\u200b\u6709\u200b\u4efb\u4f55\u200b\u539f\u56e0\u200b\u5bfc\u81f4\u200b\u5b83\u200b\u51fa\u9519\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u8ba1\u7b97\u200b\u4e00\u4e0b\u200bViT\u200b\u6a21\u578b\u200b\u6bcf\u6761\u200b\u9884\u6d4b\u200b\u6240\u200b\u82b1\u8d39\u200b\u7684\u200b\u65f6\u95f4\u200b\u5462\u200b\uff1f</p> In\u00a0[42]: Copied! <pre># Calculate average time per prediction for ViT model\nvit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4)\nprint(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\")\n</pre> # Calculate average time per prediction for ViT model vit_average_time_per_pred = round(vit_test_pred_df.time_for_pred.mean(), 4) print(f\"ViT average time per prediction: {vit_average_time_per_pred} seconds\") <pre>ViT average time per prediction: 0.0641 seconds\n</pre> <p>\u200b\u55ef\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6bd4\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u7684\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u7a0d\u6162\u200b\u4e00\u4e9b\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e8c\u9879\u200b\u6807\u51c6\u200b\u2014\u2014\u200b\u901f\u5ea6\u200b\u65b9\u9762\u200b\uff0c\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u5462\u200b\uff1f</p> <p>\u200b\u76ee\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u6570\u503c\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b<code>vit_stats</code>\u200b\u5b57\u5178\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4e0e\u200bEffNetB2\u200b\u6a21\u578b\u200b\u7684\u200b\u7edf\u8ba1\u6570\u636e\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u503c\u200b\u5c06\u200b\u9ad8\u5ea6\u200b\u4f9d\u8d56\u4e8e\u200b\u8fd0\u884c\u200b\u5b83\u4eec\u200b\u7684\u200b\u786c\u4ef6\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5bf9\u4e8e\u200bViT\u200b\u6a21\u578b\u200b\uff0c\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u672c\u5730\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200bPC\uff08\u200b\u642d\u8f7d\u200bIntel i9 CPU\uff09\u200b\u4e0a\u200b\uff0c\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\uff08\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\uff09\u200b\u4e3a\u200b0.0693-0.0777\u200b\u79d2\u200b\u3002\u200b\u800c\u200b\u5728\u200bGoogle Colab\u200b\u4e0a\u200b\uff0c\u200b\u4f7f\u7528\u200bViT\u200b\u6a21\u578b\u200b\u7684\u200b\u5e73\u5747\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u4e3a\u200b0.6766-0.7113\u200b\u79d2\u200b\u3002</p> In\u00a0[43]: Copied! <pre># Add average prediction time for ViT model on CPU\nvit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred\nvit_stats\n</pre> # Add average prediction time for ViT model on CPU vit_stats[\"time_per_pred_cpu\"] = vit_average_time_per_pred vit_stats Out[43]: <pre>{'test_loss': 0.06418210905976593,\n 'test_acc': 0.984659090909091,\n 'number_of_parameters': 85800963,\n 'model_size (MB)': 327,\n 'time_per_pred_cpu': 0.0641}</pre> In\u00a0[44]: Copied! <pre># Turn stat dictionaries into DataFrame\ndf = pd.DataFrame([effnetb2_stats, vit_stats])\n\n# Add column for model names\ndf[\"model\"] = [\"EffNetB2\", \"ViT\"]\n\n# Convert accuracy to percentages\ndf[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)\n\ndf\n</pre> # Turn stat dictionaries into DataFrame df = pd.DataFrame([effnetb2_stats, vit_stats])  # Add column for model names df[\"model\"] = [\"EffNetB2\", \"ViT\"]  # Convert accuracy to percentages df[\"test_acc\"] = round(df[\"test_acc\"] * 100, 2)  df Out[44]: test_loss test_acc number_of_parameters model_size (MB) time_per_pred_cpu model 0 0.281287 96.88 7705221 29 0.0269 EffNetB2 1 0.064182 98.47 85800963 327 0.0641 ViT <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u6574\u4f53\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\u65b9\u9762\u200b\u76f8\u5f53\u200b\u63a5\u8fd1\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u5728\u200b\u5176\u4ed6\u200b\u9886\u57df\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u662f\u200b\u8ba1\u7b97\u200bViT\u200b\u6a21\u578b\u200b\u7edf\u8ba1\u6570\u636e\u200b\u4e0e\u200bEffNetB2\u200b\u6a21\u578b\u200b\u7edf\u8ba1\u6570\u636e\u200b\u7684\u200b\u6bd4\u503c\u200b\uff0c\u200b\u4ee5\u200b\u627e\u51fa\u200b\u6a21\u578b\u200b\u4e4b\u95f4\u200b\u7684\u200b\u4e0d\u540c\u200b\u6bd4\u7387\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u53e6\u200b\u4e00\u4e2a\u200bDataFrame\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> In\u00a0[45]: Copied! <pre># Compare ViT to EffNetB2 across different characteristics\npd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics\n             columns=[\"ViT to EffNetB2 ratios\"]).T\n</pre> # Compare ViT to EffNetB2 across different characteristics pd.DataFrame(data=(df.set_index(\"model\").loc[\"ViT\"] / df.set_index(\"model\").loc[\"EffNetB2\"]), # divide ViT statistics by EffNetB2 statistics              columns=[\"ViT to EffNetB2 ratios\"]).T Out[45]: test_loss test_acc number_of_parameters model_size (MB) time_per_pred_cpu ViT to EffNetB2 ratios 0.228173 1.016412 11.135432 11.275862 2.3829 <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u5728\u200b\u5404\u9879\u200b\u6027\u80fd\u6307\u6807\u200b\uff08\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\uff0c\u200b\u6570\u503c\u200b\u8d8a\u4f4e\u200b\u8d8a\u200b\u597d\u200b\uff1b\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\uff0c\u200b\u6570\u503c\u200b\u8d8a\u9ad8\u8d8a\u200b\u597d\u200b\uff09\u200b\u4e0a\u5747\u200b\u4f18\u4e8e\u200bEffNetB2\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u662f\u200b\u4ee5\u200b\u4ee5\u4e0b\u200b\u4ee3\u4ef7\u200b\u4e3a\u200b\u524d\u63d0\u200b\u7684\u200b\uff1a</p> <ul> <li>\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u591a\u51fa\u200b11\u200b\u500d\u200b\u4ee5\u4e0a\u200b\u3002</li> <li>\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u5927\u51fa\u200b11\u200b\u500d\u200b\u4ee5\u4e0a\u200b\u3002</li> <li>\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u957f\u51fa\u200b2.5\u200b\u500d\u200b\u4ee5\u4e0a\u200b\u3002</li> </ul> <p>\u200b\u8fd9\u4e9b\u200b\u6743\u8861\u200b\u662f\u5426\u200b\u503c\u5f97\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6216\u8bb8\u200b\u5728\u200b\u62e5\u6709\u200b\u65e0\u9650\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u662f\u200b\u503c\u5f97\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u90e8\u7f72\u200bFoodVision Mini\u200b\u6a21\u578b\u200b\u5230\u200b\u8f83\u200b\u5c0f\u200b\u8bbe\u5907\u200b\uff08\u200b\u4f8b\u5982\u200b\u667a\u80fd\u624b\u673a\u200b\uff09\u200b\u7684\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4f18\u5148\u9009\u62e9\u200bEffNetB2\u200b\u6a21\u578b\u200b\uff0c\u200b\u4ee5\u200b\u5b9e\u73b0\u200b\u66f4\u5feb\u200b\u7684\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\uff0c\u200b\u5c3d\u7ba1\u200b\u6027\u80fd\u200b\u7565\u6709\u200b\u964d\u4f4e\u200b\uff0c\u200b\u4f46\u200b\u6a21\u578b\u200b\u5c3a\u5bf8\u200b\u663e\u8457\u200b\u51cf\u5c0f\u200b\u3002</p> In\u00a0[46]: Copied! <pre># 1. Create a plot from model comparison DataFrame\nfig, ax = plt.subplots(figsize=(12, 8))\nscatter = ax.scatter(data=df, \n                     x=\"time_per_pred_cpu\", \n                     y=\"test_acc\", \n                     c=[\"blue\", \"orange\"], # what colours to use?\n                     s=\"model_size (MB)\") # size the dots by the model sizes\n\n# 2. Add titles, labels and customize fontsize for aesthetics\nax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18)\nax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14)\nax.set_ylabel(\"Test accuracy (%)\", fontsize=14)\nax.tick_params(axis='both', labelsize=12)\nax.grid(True)\n\n# 3. Annotate with model names\nfor index, row in df.iterrows():\n    ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270 \n                xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),\n                size=12)\n\n# 4. Create a legend based on model sizes\nhandles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5)\nmodel_size_legend = ax.legend(handles, \n                              labels, \n                              loc=\"lower right\", \n                              title=\"Model size (MB)\",\n                              fontsize=12)\n\n# Save the figure\n!mdkir images/\nplt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")\n\n# Show the figure\nplt.show()\n</pre> # 1. Create a plot from model comparison DataFrame fig, ax = plt.subplots(figsize=(12, 8)) scatter = ax.scatter(data=df,                       x=\"time_per_pred_cpu\",                       y=\"test_acc\",                       c=[\"blue\", \"orange\"], # what colours to use?                      s=\"model_size (MB)\") # size the dots by the model sizes  # 2. Add titles, labels and customize fontsize for aesthetics ax.set_title(\"FoodVision Mini Inference Speed vs Performance\", fontsize=18) ax.set_xlabel(\"Prediction time per image (seconds)\", fontsize=14) ax.set_ylabel(\"Test accuracy (%)\", fontsize=14) ax.tick_params(axis='both', labelsize=12) ax.grid(True)  # 3. Annotate with model names for index, row in df.iterrows():     ax.annotate(text=row[\"model\"], # note: depending on your version of Matplotlib, you may need to use \"s=...\" or \"text=...\", see: https://github.com/faustomorales/keras-ocr/issues/183#issuecomment-977733270                  xy=(row[\"time_per_pred_cpu\"]+0.0006, row[\"test_acc\"]+0.03),                 size=12)  # 4. Create a legend based on model sizes handles, labels = scatter.legend_elements(prop=\"sizes\", alpha=0.5) model_size_legend = ax.legend(handles,                                labels,                                loc=\"lower right\",                                title=\"Model size (MB)\",                               fontsize=12)  # Save the figure !mdkir images/ plt.savefig(\"images/09-foodvision-mini-inference-speed-vs-performance.jpg\")  # Show the figure plt.show() <p>\u200b\u54c7\u200b\uff01</p> <p>\u200b\u8fd9\u4e2a\u200b\u56fe\u8868\u200b\u786e\u5b9e\u200b\u5c55\u793a\u200b\u4e86\u200b\u901f\u5ea6\u200b\u4e0e\u200b\u6027\u80fd\u200b\u7684\u200b\u6743\u8861\u200b\uff0c\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u62e5\u6709\u200b\u4e00\u4e2a\u200b\u66f4\u5927\u200b\u3001\u200b\u6027\u80fd\u200b\u66f4\u597d\u200b\u7684\u200b\u6df1\u5ea6\u200b\u6a21\u578b\u200b\uff08\u200b\u6bd4\u5982\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\uff09\u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u66f4\u957f\u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u200b\u6267\u884c\u200b\u63a8\u7406\u200b\uff08\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u5ef6\u8fdf\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u89c4\u5f8b\u200b\u4e5f\u200b\u6709\u200b\u4f8b\u5916\u200b\uff0c\u200b\u800c\u4e14\u200b\u4e0d\u65ad\u200b\u6709\u200b\u65b0\u200b\u7684\u200b\u7814\u7a76\u200b\u53d1\u8868\u200b\uff0c\u200b\u65e8\u5728\u200b\u5e2e\u52a9\u200b\u5927\u578b\u200b\u6a21\u578b\u200b\u66f4\u5feb\u200b\u5730\u200b\u6267\u884c\u200b\u3002</p> <p>\u200b\u90e8\u7f72\u200b\u6700\u4f73\u200b\u6027\u80fd\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5f88\u200b\u8bf1\u4eba\u200b\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u8981\u200b\u8003\u8651\u200b\u6a21\u578b\u200b\u5c06\u200b\u5728\u200b\u4f55\u5904\u200b\u6267\u884c\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6848\u4f8b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u6027\u200b\u65b9\u9762\u200b\u7684\u200b\u6027\u80fd\u200b\u5dee\u5f02\u200b\u5e76\u200b\u4e0d\u592a\u5927\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6700\u521d\u200b\u5f3a\u8c03\u200b\u901f\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u575a\u6301\u200b\u90e8\u7f72\u200bEffNetB2\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u66f4\u200b\u5feb\u4e14\u200b\u5360\u7528\u200b\u7a7a\u95f4\u200b\u66f4\u200b\u5c0f\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u5728\u200b\u4e0d\u540c\u200b\u786c\u4ef6\u200b\u7c7b\u578b\u200b\u4e0a\u200b\u4f1a\u200b\u6709\u6240\u4e0d\u540c\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cIntel i9\u200b\u4e0e\u200bGoogle Colab CPU\u200b\u4e0e\u200bGPU\uff09\uff0c\u200b\u56e0\u6b64\u200b\u8003\u8651\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\u6700\u7ec8\u200b\u5c06\u200b\u5728\u200b\u4f55\u5904\u200b\u8fd0\u884c\u200b\u975e\u5e38\u200b\u91cd\u8981\u200b\u3002\u200b\u63d0\u51fa\u200b\u8bf8\u5982\u200b\u201c\u200b\u6a21\u578b\u200b\u5c06\u200b\u5728\u200b\u4f55\u5904\u200b\u8fd0\u884c\u200b\uff1f\u201d\u200b\u6216\u200b\u201c\u200b\u8fd0\u884c\u200b\u6a21\u578b\u200b\u7684\u200b\u7406\u60f3\u200b\u573a\u666f\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\u201d\u200b\u7b49\u200b\u95ee\u9898\u200b\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b\u5b9e\u9a8c\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u90e8\u7f72\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u63d0\u4f9b\u200b\u7b54\u6848\u200b\uff0c\u200b\u8fd9\u662f\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u3002</p> In\u00a0[47]: Copied! <pre># Import/install Gradio \ntry:\n    import gradio as gr\nexcept: \n    !pip -q install gradio\n    import gradio as gr\n    \nprint(f\"Gradio version: {gr.__version__}\")\n</pre> # Import/install Gradio  try:     import gradio as gr except:      !pip -q install gradio     import gradio as gr      print(f\"Gradio version: {gr.__version__}\") <pre>Gradio version: 3.1.4\n</pre> <p>Gradio \u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b FoodVision Mini \u200b\u8f6c\u53d8\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u6f14\u793a\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u3002</p> In\u00a0[48]: Copied! <pre># Put EffNetB2 on CPU\neffnetb2.to(\"cpu\") \n\n# Check the device\nnext(iter(effnetb2.parameters())).device\n</pre> # Put EffNetB2 on CPU effnetb2.to(\"cpu\")   # Check the device next(iter(effnetb2.parameters())).device Out[48]: <pre>device(type='cpu')</pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>predict()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u6765\u200b\u590d\u5236\u200b\u4e0a\u8ff0\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002</p> In\u00a0[49]: Copied! <pre>from typing import Tuple, Dict\n\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n</pre> from typing import Tuple, Dict  def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u6765\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u7684\u200b\u51fd\u6570\u200b\u5b9e\u9645\u200b\u8fd0\u884c\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u6d4b\u8bd5\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u7136\u540e\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u3002</p> <p>\u200b\u63a5\u7740\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>PIL.Image.open()</code> \u200b\u6253\u5f00\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u56fe\u50cf\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6211\u4eec\u200b\u7684\u200b <code>predict()</code> \u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[50]: Copied! <pre>import random\nfrom PIL import Image\n\n# Get a list of all test image filepaths\ntest_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))\n\n# Randomly select a test image path\nrandom_image_path = random.sample(test_data_paths, k=1)[0]\n\n# Open the target image\nimage = Image.open(random_image_path)\nprint(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")\n\n# Predict on the target image and print out the outputs\npred_dict, pred_time = predict(img=image)\nprint(f\"Prediction label and probability dictionary: \\n{pred_dict}\")\nprint(f\"Prediction time: {pred_time} seconds\")\n</pre> import random from PIL import Image  # Get a list of all test image filepaths test_data_paths = list(Path(test_dir).glob(\"*/*.jpg\"))  # Randomly select a test image path random_image_path = random.sample(test_data_paths, k=1)[0]  # Open the target image image = Image.open(random_image_path) print(f\"[INFO] Predicting on image at path: {random_image_path}\\n\")  # Predict on the target image and print out the outputs pred_dict, pred_time = predict(img=image) print(f\"Prediction label and probability dictionary: \\n{pred_dict}\") print(f\"Prediction time: {pred_time} seconds\") <pre>[INFO] Predicting on image at path: data/pizza_steak_sushi_20_percent/test/pizza/3770514.jpg\n\nPrediction label and probability dictionary: \n{'pizza': 0.9785208702087402, 'steak': 0.01169557310640812, 'sushi': 0.009783552028238773}\nPrediction time: 0.027 seconds\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u4e0a\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200bEffNetB2\u200b\u6a21\u578b\u200b\u5bf9\u200b\u6bcf\u4e2a\u200b\u6807\u7b7e\u200b\u7684\u200b\u4e0d\u540c\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u6bcf\u6b21\u200b\u9884\u6d4b\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u3002</p> In\u00a0[51]: Copied! <pre># Create a list of example inputs to our Gradio demo\nexample_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)]\nexample_list\n</pre> # Create a list of example inputs to our Gradio demo example_list = [[str(filepath)] for filepath in random.sample(test_data_paths, k=3)] example_list Out[51]: <pre>[['data/pizza_steak_sushi_20_percent/test/sushi/804460.jpg'],\n ['data/pizza_steak_sushi_20_percent/test/steak/746921.jpg'],\n ['data/pizza_steak_sushi_20_percent/test/steak/2117351.jpg']]</pre> <p>\u200b\u5b8c\u7f8e\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\u5c06\u200b\u5c55\u793a\u200b\u8fd9\u4e9b\u200b\u4f5c\u4e3a\u200b\u793a\u4f8b\u200b\u8f93\u5165\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4eba\u4eec\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u5e76\u200b\u4e86\u89e3\u200b\u5176\u200b\u529f\u80fd\u200b\uff0c\u200b\u800c\u200b\u65e0\u9700\u200b\u4e0a\u4f20\u200b\u4efb\u4f55\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> In\u00a0[52]: Copied! <pre>import gradio as gr\n\n# Create title, description and article strings\ntitle = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch(debug=False, # print errors locally?\n            share=True) # generate a publically shareable URL?\n</pre> import gradio as gr  # Create title, description and article strings title = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Create the Gradio demo demo = gr.Interface(fn=predict, # mapping function from input to output                     inputs=gr.Image(type=\"pil\"), # what are the inputs?                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?                              gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs                     examples=example_list,                      title=title,                     description=description,                     article=article)  # Launch the demo! demo.launch(debug=False, # print errors locally?             share=True) # generate a publically shareable URL? <pre>Running on local URL:  http://127.0.0.1:7860/\nRunning on public URL: https://27541.gradio.app\n\nThis share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n</pre> Out[52]: <pre>(&lt;gradio.routes.App at 0x7f122dd0f0d0&gt;,\n 'http://127.0.0.1:7860/',\n 'https://27541.gradio.app')</pre> <p>FoodVision Mini Gradio \u200b\u6f14\u793a\u200b\u5728\u200b Google Colab \u200b\u548c\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\u8fd0\u884c\u200b\uff08\u200b\u4ece\u200b Google Colab \u200b\u8fd0\u884c\u200b\u7684\u200b\u94fe\u63a5\u200b\u4ec5\u200b\u6301\u7eed\u200b 72 \u200b\u5c0f\u65f6\u200b\uff09\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u7684\u200b\u6c38\u4e45\u200b\u5728\u7ebf\u200b\u6f14\u793a\u200b\u3002</p> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01\uff01\uff01\u200b\u591a\u4e48\u200b\u7cbe\u5f69\u200b\u7684\u200b\u6f14\u793a\u200b\uff01\uff01\uff01</p> <p>FoodVision Mini \u200b\u5df2\u7ecf\u200b\u6b63\u5f0f\u200b\u5728\u200b\u4e00\u4e2a\u200b\u4eba\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u548c\u200b\u5c1d\u8bd5\u200b\u7684\u200b\u754c\u9762\u200b\u4e0a\u6d3b\u200b\u4e86\u200b\u8d77\u6765\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b <code>launch()</code> \u200b\u65b9\u6cd5\u200b\u4e2d\u200b\u8bbe\u7f6e\u200b\u53c2\u6570\u200b <code>share=True</code>\uff0cGradio \u200b\u8fd8\u4f1a\u200b\u4e3a\u200b\u4f60\u200b\u63d0\u4f9b\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u5206\u4eab\u200b\u7684\u200b\u94fe\u63a5\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>https://123XYZ.gradio.app</code>\uff08\u200b\u6b64\u200b\u94fe\u63a5\u200b\u4ec5\u4e3a\u200b\u793a\u4f8b\u200b\uff0c\u200b\u53ef\u80fd\u200b\u5df2\u8fc7\u671f\u200b\uff09\uff0c\u200b\u8be5\u200b\u94fe\u63a5\u200b\u6709\u6548\u671f\u200b\u4e3a\u200b 72 \u200b\u5c0f\u65f6\u200b\u3002</p> <p>\u200b\u8be5\u200b\u94fe\u63a5\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8fd4\u56de\u200b\u4f60\u200b\u542f\u52a8\u200b\u7684\u200b Gradio \u200b\u754c\u9762\u200b\u7684\u200b\u4ee3\u7406\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u66f4\u200b\u957f\u671f\u200b\u7684\u200b\u6258\u7ba1\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b Gradio \u200b\u5e94\u7528\u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face Spaces \u200b\u6216\u200b\u5176\u4ed6\u200b\u4efb\u4f55\u200b\u53ef\u4ee5\u200b\u8fd0\u884c\u200b Python \u200b\u4ee3\u7801\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p> In\u00a0[53]: Copied! <pre>import shutil\nfrom pathlib import Path\n\n# Create FoodVision mini demo path\nfoodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")\n\n# Remove files that might already exist there and create new directory\nif foodvision_mini_demo_path.exists():\n    shutil.rmtree(foodvision_mini_demo_path)\n    foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?\n                                    exist_ok=True) # create it even if it already exists?\nelse:\n    # If the file doesn't exist, create it anyway\n    foodvision_mini_demo_path.mkdir(parents=True, \n                                    exist_ok=True)\n    \n# Check what's in the folder\n!ls demos/foodvision_mini/\n</pre> import shutil from pathlib import Path  # Create FoodVision mini demo path foodvision_mini_demo_path = Path(\"demos/foodvision_mini/\")  # Remove files that might already exist there and create new directory if foodvision_mini_demo_path.exists():     shutil.rmtree(foodvision_mini_demo_path)     foodvision_mini_demo_path.mkdir(parents=True, # make the parent folders?                                     exist_ok=True) # create it even if it already exists? else:     # If the file doesn't exist, create it anyway     foodvision_mini_demo_path.mkdir(parents=True,                                      exist_ok=True)      # Check what's in the folder !ls demos/foodvision_mini/ In\u00a0[54]: Copied! <pre>import shutil\nfrom pathlib import Path\n\n# 1. Create an examples directory\nfoodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\"\nfoodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)\n\n# 2. Collect three random test dataset image paths\nfoodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),\n                            Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]\n\n# 3. Copy the three random images to the examples directory\nfor example in foodvision_mini_examples:\n    destination = foodvision_mini_examples_path / example.name\n    print(f\"[INFO] Copying {example} to {destination}\")\n    shutil.copy2(src=example, dst=destination)\n</pre> import shutil from pathlib import Path  # 1. Create an examples directory foodvision_mini_examples_path = foodvision_mini_demo_path / \"examples\" foodvision_mini_examples_path.mkdir(parents=True, exist_ok=True)  # 2. Collect three random test dataset image paths foodvision_mini_examples = [Path('data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg'),                             Path('data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg'),                             Path('data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg')]  # 3. Copy the three random images to the examples directory for example in foodvision_mini_examples:     destination = foodvision_mini_examples_path / example.name     print(f\"[INFO] Copying {example} to {destination}\")     shutil.copy2(src=example, dst=destination) <pre>[INFO] Copying data/pizza_steak_sushi_20_percent/test/sushi/592799.jpg to demos/foodvision_mini/examples/592799.jpg\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/steak/3622237.jpg to demos/foodvision_mini/examples/3622237.jpg\n[INFO] Copying data/pizza_steak_sushi_20_percent/test/pizza/2582289.jpg to demos/foodvision_mini/examples/2582289.jpg\n</pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u786e\u8ba4\u200b\u6211\u4eec\u200b\u7684\u200b\u793a\u4f8b\u200b\u6587\u4ef6\u200b\u5df2\u200b\u5b58\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b<code>os.listdir()</code>\u200b\u5217\u51fa\u200b<code>demos/foodvision_mini/examples/</code>\u200b\u76ee\u5f55\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u683c\u5f0f\u5316\u200b\u4e3a\u200b\u5217\u8868\u200b\u7684\u200b\u5217\u8868\u200b\uff08\u200b\u4ee5\u4fbf\u200b\u4e0e\u200bGradio\u200b\u7684\u200b<code>gradio.Interface()</code>\u200b\u7684\u200b<code>example</code>\u200b\u53c2\u6570\u200b\u517c\u5bb9\u200b\uff09\u3002</p> In\u00a0[55]: Copied! <pre>import os\n\n# Get example filepaths in a list of lists\nexample_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)]\nexample_list\n</pre> import os  # Get example filepaths in a list of lists example_list = [[\"examples/\" + example] for example in os.listdir(foodvision_mini_examples_path)] example_list Out[55]: <pre>[['examples/3622237.jpg'], ['examples/592799.jpg'], ['examples/2582289.jpg']]</pre> In\u00a0[56]: Copied! <pre>import shutil\n\n# Create a source path for our target model\neffnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"\n\n# Create a destination path for our target model \neffnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]\n\n# Try to move the file\ntry:\n    print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")\n    \n    # Move the model\n    shutil.move(src=effnetb2_foodvision_mini_model_path, \n                dst=effnetb2_foodvision_mini_model_destination)\n    \n    print(f\"[INFO] Model move complete.\")\n\n# If the model has already been moved, check if it exists\nexcept:\n    print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")\n    print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\")\n</pre> import shutil  # Create a source path for our target model effnetb2_foodvision_mini_model_path = \"models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"  # Create a destination path for our target model  effnetb2_foodvision_mini_model_destination = foodvision_mini_demo_path / effnetb2_foodvision_mini_model_path.split(\"/\")[1]  # Try to move the file try:     print(f\"[INFO] Attempting to move {effnetb2_foodvision_mini_model_path} to {effnetb2_foodvision_mini_model_destination}\")          # Move the model     shutil.move(src=effnetb2_foodvision_mini_model_path,                  dst=effnetb2_foodvision_mini_model_destination)          print(f\"[INFO] Model move complete.\")  # If the model has already been moved, check if it exists except:     print(f\"[INFO] No model found at {effnetb2_foodvision_mini_model_path}, perhaps its already been moved?\")     print(f\"[INFO] Model exists at {effnetb2_foodvision_mini_model_destination}: {effnetb2_foodvision_mini_model_destination.exists()}\") <pre>[INFO] Attempting to move models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth to demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n[INFO] Model move complete.\n</pre> In\u00a0[57]: Copied! <pre>%%writefile demos/foodvision_mini/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> %%writefile demos/foodvision_mini/model.py import torch import torchvision  from torch import nn   def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <pre>Writing demos/foodvision_mini/model.py\n</pre> In\u00a0[58]: Copied! <pre>%%writefile demos/foodvision_mini/app.py\n### 1. Imports and class names setup ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Setup class names\nclass_names = [\"pizza\", \"steak\", \"sushi\"]\n\n### 2. Model and transforms preparation ###\n\n# Create EffNetB2 model\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=3, # len(class_names) would also work\n)\n\n# Load saved weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n### 3. Predict function ###\n\n# Create predict function\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article strings\ntitle = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create examples list from \"examples/\" directory\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create the Gradio demo\ndemo = gr.Interface(fn=predict, # mapping function from input to output\n                    inputs=gr.Image(type=\"pil\"), # what are the inputs?\n                    outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?\n                             gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs\n                    # Create examples list from \"examples/\" directory\n                    examples=example_list, \n                    title=title,\n                    description=description,\n                    article=article)\n\n# Launch the demo!\ndemo.launch()\n</pre> %%writefile demos/foodvision_mini/app.py ### 1. Imports and class names setup ###  import gradio as gr import os import torch  from model import create_effnetb2_model from timeit import default_timer as timer from typing import Tuple, Dict  # Setup class names class_names = [\"pizza\", \"steak\", \"sushi\"]  ### 2. Model and transforms preparation ###  # Create EffNetB2 model effnetb2, effnetb2_transforms = create_effnetb2_model(     num_classes=3, # len(class_names) would also work )  # Load saved weights effnetb2.load_state_dict(     torch.load(         f=\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\",         map_location=torch.device(\"cpu\"),  # load to CPU     ) )  ### 3. Predict function ###  # Create predict function def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time  ### 4. Gradio app ###  # Create title, description and article strings title = \"FoodVision Mini \ud83c\udf55\ud83e\udd69\ud83c\udf63\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food as pizza, steak or sushi.\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Create examples list from \"examples/\" directory example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]  # Create the Gradio demo demo = gr.Interface(fn=predict, # mapping function from input to output                     inputs=gr.Image(type=\"pil\"), # what are the inputs?                     outputs=[gr.Label(num_top_classes=3, label=\"Predictions\"), # what are the outputs?                              gr.Number(label=\"Prediction time (s)\")], # our fn has two outputs, therefore we have two outputs                     # Create examples list from \"examples/\" directory                     examples=example_list,                      title=title,                     description=description,                     article=article)  # Launch the demo! demo.launch() <pre>Writing demos/foodvision_mini/app.py\n</pre> In\u00a0[59]: Copied! <pre>%%writefile demos/foodvision_mini/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n</pre> %%writefile demos/foodvision_mini/requirements.txt torch==1.12.0 torchvision==0.13.0 gradio==3.1.4 <pre>Writing demos/foodvision_mini/requirements.txt\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6b63\u5f0f\u200b\u96c6\u9f50\u200b\u4e86\u200b\u90e8\u7f72\u200bFoodVision Mini\u200b\u6f14\u793a\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u6587\u4ef6\u200b\uff01</p> In\u00a0[60]: Copied! <pre>!ls demos/foodvision_mini\n</pre> !ls demos/foodvision_mini <pre>09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\napp.py\nexamples\nmodel.py\nrequirements.txt\n</pre> <p>\u200b\u8fd9\u4e9b\u200b\u662f\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u6240\u6709\u200b\u6587\u4ef6\u200b\uff01</p> <p>\u200b\u8981\u200b\u5f00\u59cb\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u6587\u4ef6\u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u4ece\u200b Google Colab\uff08\u200b\u6216\u200b\u60a8\u200b\u8fd0\u884c\u200b\u6b64\u200b\u7b14\u8bb0\u672c\u200b\u7684\u200b\u4efb\u4f55\u200b\u5730\u65b9\u200b\uff09\u200b\u4e0b\u8f7d\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u5c06\u200b\u6587\u4ef6\u200b\u538b\u7f29\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b zip \u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\uff1a</p> <pre><code>zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>zip</code> \u200b\u8868\u793a\u200b\u201c\u200b\u538b\u7f29\u200b\u201d\uff0c\u200b\u5373\u200b\u201c\u200b\u8bf7\u200b\u5c06\u200b\u4ee5\u4e0b\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u6587\u4ef6\u200b\u538b\u7f29\u200b\u5728\u200b\u4e00\u8d77\u200b\u201d\u3002</li> <li><code>-r</code> \u200b\u8868\u793a\u200b\u201c\u200b\u9012\u5f52\u200b\u201d\uff0c\u200b\u5373\u200b\u201c\u200b\u904d\u5386\u200b\u76ee\u6807\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u6587\u4ef6\u200b\u201d\u3002</li> <li><code>../foodvision_mini.zip</code> \u200b\u662f\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6587\u4ef6\u200b\u88ab\u200b\u538b\u7f29\u200b\u5230\u200b\u7684\u200b\u76ee\u6807\u76ee\u5f55\u200b\u3002</li> <li><code>*</code> \u200b\u8868\u793a\u200b\u201c\u200b\u5f53\u524d\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u6587\u4ef6\u200b\u201d\u3002</li> <li><code>-x</code> \u200b\u8868\u793a\u200b\u201c\u200b\u6392\u9664\u200b\u8fd9\u4e9b\u200b\u6587\u4ef6\u200b\u201d\u3002</li> </ul> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>google.colab.files.download(\"demos/foodvision_mini.zip\")</code> \u200b\u4ece\u200b Google Colab \u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u7684\u200b zip \u200b\u6587\u4ef6\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b\u8fd9\u4e2a\u200b\u653e\u5728\u200b <code>try</code> \u200b\u548c\u200b <code>except</code> \u200b\u5757\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u9632\u200b\u6211\u4eec\u200b\u4e0d\u200b\u5728\u200b Google Colab \u200b\u4e2d\u200b\u8fd0\u884c\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5982\u679c\u200b\u662f\u200b\u8fd9\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6253\u5370\u200b\u4e00\u6761\u200b\u6d88\u606f\u200b\uff0c\u200b\u63d0\u793a\u200b\u624b\u52a8\u200b\u4e0b\u8f7d\u200b\u6587\u4ef6\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u770b\u200b\uff01</p> In\u00a0[61]: Copied! <pre># Change into and then zip the foodvision_mini folder but exclude certain files\n!cd demos/foodvision_mini &amp;&amp; zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Download the zipped FoodVision Mini app (if running in Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_mini.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\")\n</pre> # Change into and then zip the foodvision_mini folder but exclude certain files !cd demos/foodvision_mini &amp;&amp; zip -r ../foodvision_mini.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"  # Download the zipped FoodVision Mini app (if running in Google Colab) try:     from google.colab import files     files.download(\"demos/foodvision_mini.zip\") except:     print(\"Not running in Google Colab, can't use google.colab.files.download(), please manually download.\") <pre>updating: 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth (deflated 8%)\nupdating: app.py (deflated 57%)\nupdating: examples/ (stored 0%)\nupdating: examples/3622237.jpg (deflated 0%)\nupdating: examples/592799.jpg (deflated 1%)\nupdating: examples/2582289.jpg (deflated 17%)\nupdating: model.py (deflated 56%)\nupdating: requirements.txt (deflated 4%)\nNot running in Google Colab, can't use google.colab.files.download(), please manually download.\n</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b <code>zip</code> \u200b\u547d\u4ee4\u200b\u6267\u884c\u200b\u6210\u529f\u200b\u4e86\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u662f\u200b\u5728\u200b Google Colab \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u5728\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\u770b\u5230\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u200b\u5f00\u59cb\u200b\u4e0b\u8f7d\u200b\u3002</p> <p>\u200b\u5426\u5219\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b \u200b\u8bfe\u7a0b\u200b GitHub \u200b\u7684\u200b <code>demos/</code> \u200b\u76ee\u5f55\u200b\u4e0b\u200b \u200b\u627e\u5230\u200b <code>foodvision_mini.zip</code> \u200b\u6587\u4ef6\u5939\u200b\uff08\u200b\u4ee5\u53ca\u200b\u66f4\u200b\u591a\u200b\u5185\u5bb9\u200b\uff09\u3002</p> In\u00a0[62]: Copied! <pre># IPython is a library to help make Python interactive\nfrom IPython.display import IFrame\n\n# Embed FoodVision Mini Gradio demo\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750)\n</pre> # IPython is a library to help make Python interactive from IPython.display import IFrame  # Embed FoodVision Mini Gradio demo IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_mini/+\", width=900, height=750) Out[62]: In\u00a0[63]: Copied! <pre># Create EffNetB2 model capable of fitting to 101 classes for Food101\neffnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n</pre> # Create EffNetB2 model capable of fitting to 101 classes for Food101 effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u7684\u200b\u603b\u7ed3\u200b\u4fe1\u606f\u200b\u3002</p> In\u00a0[64]: Copied! <pre>from torchinfo import summary\n\n# # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output)\n# summary(effnetb2_food101, \n#         input_size=(1, 3, 224, 224),\n#         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n#         col_width=20,\n#         row_settings=[\"var_names\"])\n</pre> from torchinfo import summary  # # Get a summary of EffNetB2 feature extractor for Food101 with 101 output classes (uncomment for full output) # summary(effnetb2_food101,  #         input_size=(1, 3, 224, 224), #         col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"], #         col_width=20, #         row_settings=[\"var_names\"]) <p></p> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u770b\u200b\u6211\u4eec\u200b\u7684\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u662f\u200b\u5982\u4f55\u200b\u4e0e\u200b FoodVision Mini \u200b\u76f8\u4f3c\u200b\u7684\u200b\uff0c\u200b\u57fa\u7840\u200b\u5c42\u200b\u88ab\u200b\u51bb\u7ed3\u200b\uff08\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u5728\u200b ImageNet \u200b\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u9884\u200b\u8bad\u7ec3\u200b\uff09\uff0c\u200b\u800c\u200b\u5916\u5c42\u200b\uff08<code>\u200b\u5206\u7c7b\u5668\u200b</code>\u200b\u5c42\u200b\uff09\u200b\u662f\u200b\u53ef\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[batch_size, 101]</code>\uff08<code>101</code> \u200b\u4ee3\u8868\u200b Food101 \u200b\u4e2d\u200b\u7684\u200b 101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5c06\u200b\u5904\u7406\u200b\u6bd4\u200b\u5e73\u65f6\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u59a8\u200b\u5728\u200b\u53d8\u6362\u200b\uff08<code>effnetb2_transforms</code>\uff09\u200b\u4e2d\u200b\u589e\u52a0\u200b\u4e00\u4e9b\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff0c\u200b\u4ee5\u200b\u589e\u5f3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u662f\u200b\u4e00\u79cd\u200b\u6280\u672f\u200b\uff0c\u200b\u7528\u4e8e\u200b\u6539\u53d8\u200b\u8f93\u5165\u200b\u8bad\u7ec3\u6837\u672c\u200b\u7684\u200b\u5916\u89c2\u200b\uff08\u200b\u4f8b\u5982\u200b\u65cb\u8f6c\u200b\u56fe\u50cf\u200b\u6216\u200b\u8f7b\u5fae\u200b\u626d\u66f2\u200b\uff09\uff0c\u200b\u4ee5\u200b\u4eba\u5de5\u200b\u589e\u52a0\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u591a\u6837\u6027\u200b\uff0c\u200b\u5e0c\u671b\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b6\u200b\u8282\u200b \u200b\u4e2d\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ec4\u5408\u200b\u4e00\u4e2a\u200b <code>torchvision.transforms</code> \u200b\u7ba1\u9053\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torchvision.transforms.TrivialAugmentWide()</code>\uff08\u200b\u4e0e\u200b PyTorch \u200b\u56e2\u961f\u200b\u5728\u200b\u5176\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u98df\u8c31\u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\uff09\u200b\u4ee5\u53ca\u200b <code>effnetb2_transforms</code> \u200b\u6765\u200b\u53d8\u6362\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u3002</p> In\u00a0[65]: Copied! <pre># Create Food101 training data transforms (only perform data augmentation on the training images)\nfood101_train_transforms = torchvision.transforms.Compose([\n    torchvision.transforms.TrivialAugmentWide(),\n    effnetb2_transforms,\n])\n</pre> # Create Food101 training data transforms (only perform data augmentation on the training images) food101_train_transforms = torchvision.transforms.Compose([     torchvision.transforms.TrivialAugmentWide(),     effnetb2_transforms, ]) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6765\u200b\u6bd4\u8f83\u200b\u4e00\u4e0b\u200b <code>food101_train_transforms</code>\uff08\u200b\u7528\u4e8e\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\uff09\u200b\u548c\u200b <code>effnetb2_transforms</code>\uff08\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b/\u200b\u63a8\u7406\u200b\u6570\u636e\u200b\uff09\u3002</p> In\u00a0[66]: Copied! <pre>print(f\"Training transforms:\\n{food101_train_transforms}\\n\") \nprint(f\"Testing transforms:\\n{effnetb2_transforms}\")\n</pre> print(f\"Training transforms:\\n{food101_train_transforms}\\n\")  print(f\"Testing transforms:\\n{effnetb2_transforms}\") <pre>Training transforms:\nCompose(\n    TrivialAugmentWide(num_magnitude_bins=31, interpolation=InterpolationMode.NEAREST, fill=None)\n    ImageClassification(\n    crop_size=[288]\n    resize_size=[288]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n)\n\nTesting transforms:\nImageClassification(\n    crop_size=[288]\n    resize_size=[288]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n</pre> In\u00a0[67]: Copied! <pre>from torchvision import datasets\n\n# Setup data directory\nfrom pathlib import Path\ndata_dir = Path(\"data\")\n\n# Get training data (~750 images x 101 food classes)\ntrain_data = datasets.Food101(root=data_dir, # path to download data to\n                              split=\"train\", # dataset split to get\n                              transform=food101_train_transforms, # perform data augmentation on training data\n                              download=True) # want to download?\n\n# Get testing data (~250 images x 101 food classes)\ntest_data = datasets.Food101(root=data_dir,\n                             split=\"test\",\n                             transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data\n                             download=True)\n</pre> from torchvision import datasets  # Setup data directory from pathlib import Path data_dir = Path(\"data\")  # Get training data (~750 images x 101 food classes) train_data = datasets.Food101(root=data_dir, # path to download data to                               split=\"train\", # dataset split to get                               transform=food101_train_transforms, # perform data augmentation on training data                               download=True) # want to download?  # Get testing data (~250 images x 101 food classes) test_data = datasets.Food101(root=data_dir,                              split=\"test\",                              transform=effnetb2_transforms, # perform normal EffNetB2 transforms on test data                              download=True) <p>\u200b\u6570\u636e\u200b\u5df2\u200b\u4e0b\u8f7d\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>train_data.classes</code> \u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u7684\u200b\u5217\u8868\u200b\u3002</p> In\u00a0[68]: Copied! <pre># Get Food101 class names\nfood101_class_names = train_data.classes\n\n# View the first 10\nfood101_class_names[:10]\n</pre> # Get Food101 class names food101_class_names = train_data.classes  # View the first 10 food101_class_names[:10] Out[68]: <pre>['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']</pre> <p>\u200b\u5475\u5475\u200b\uff01\u200b\u90a3\u4e9b\u200b\u542c\u200b\u8d77\u6765\u200b\u771f\u662f\u200b\u7f8e\u5473\u200b\u7684\u200b\u98df\u7269\u200b\uff08\u200b\u867d\u7136\u200b\u6211\u200b\u4ece\u672a\u200b\u542c\u8bf4\u200b\u8fc7\u200b\u201c\u200b\u8d1d\u200b\u5948\u7279\u200b\u997c\u200b\u201d...\u200b\u66f4\u65b0\u200b\uff1a\u200b\u7ecf\u8fc7\u200b\u5feb\u901f\u200b\u8c37\u6b4c\u200b\u641c\u7d22\u200b\u540e\u200b\uff0c\u200b\u8d1d\u200b\u5948\u7279\u200b\u997c\u200b\u770b\u8d77\u6765\u200b\u4e5f\u200b\u5f88\u200b\u7f8e\u5473\u200b\uff09\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200bGitHub\u200b\u4e0a\u200b\u67e5\u770b\u200bFood101\u200b\u7c7b\u200b\u7684\u200b\u5b8c\u6574\u200b\u540d\u79f0\u200b\u5217\u8868\u200b\uff0c\u200b\u4f4d\u4e8e\u200b <code>extras/food101_class_names.txt</code>\u3002</p> In\u00a0[69]: Copied! <pre>def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):\n    \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.\n\n    Args:\n        dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.\n        split_size (float, optional): How much of the dataset should be split? \n            E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.\n        seed (int, optional): Seed for random generator. Defaults to 42.\n\n    Returns:\n        tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and \n            random_split_2 is of size (1-split_size)*len(dataset).\n    \"\"\"\n    # Create split lengths based on original dataset length\n    length_1 = int(len(dataset) * split_size) # desired length\n    length_2 = len(dataset) - length_1 # remaining length\n        \n    # Print out info\n    print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")\n    \n    # Create splits with given random seed\n    random_split_1, random_split_2 = torch.utils.data.random_split(dataset, \n                                                                   lengths=[length_1, length_2],\n                                                                   generator=torch.manual_seed(seed)) # set the random seed for reproducible splits\n    return random_split_1, random_split_2\n</pre> def split_dataset(dataset:torchvision.datasets, split_size:float=0.2, seed:int=42):     \"\"\"Randomly splits a given dataset into two proportions based on split_size and seed.      Args:         dataset (torchvision.datasets): A PyTorch Dataset, typically one from torchvision.datasets.         split_size (float, optional): How much of the dataset should be split?              E.g. split_size=0.2 means there will be a 20% split and an 80% split. Defaults to 0.2.         seed (int, optional): Seed for random generator. Defaults to 42.      Returns:         tuple: (random_split_1, random_split_2) where random_split_1 is of size split_size*len(dataset) and              random_split_2 is of size (1-split_size)*len(dataset).     \"\"\"     # Create split lengths based on original dataset length     length_1 = int(len(dataset) * split_size) # desired length     length_2 = len(dataset) - length_1 # remaining length              # Print out info     print(f\"[INFO] Splitting dataset of length {len(dataset)} into splits of size: {length_1} ({int(split_size*100)}%), {length_2} ({int((1-split_size)*100)}%)\")          # Create splits with given random seed     random_split_1, random_split_2 = torch.utils.data.random_split(dataset,                                                                     lengths=[length_1, length_2],                                                                    generator=torch.manual_seed(seed)) # set the random seed for reproducible splits     return random_split_1, random_split_2 <p>\u200b\u6570\u636e\u200b\u96c6\u200b\u5206\u5272\u200b\u51fd\u6570\u200b\u5df2\u200b\u521b\u5efa\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b20%\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u5206\u5272\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> In\u00a0[70]: Copied! <pre># Create training 20% split of Food101\ntrain_data_food101_20_percent, _ = split_dataset(dataset=train_data,\n                                                 split_size=0.2)\n\n# Create testing 20% split of Food101\ntest_data_food101_20_percent, _ = split_dataset(dataset=test_data,\n                                                split_size=0.2)\n\nlen(train_data_food101_20_percent), len(test_data_food101_20_percent)\n</pre> # Create training 20% split of Food101 train_data_food101_20_percent, _ = split_dataset(dataset=train_data,                                                  split_size=0.2)  # Create testing 20% split of Food101 test_data_food101_20_percent, _ = split_dataset(dataset=test_data,                                                 split_size=0.2)  len(train_data_food101_20_percent), len(test_data_food101_20_percent) <pre>[INFO] Splitting dataset of length 75750 into splits of size: 15150 (20%), 60600 (80%)\n[INFO] Splitting dataset of length 25250 into splits of size: 5050 (20%), 20200 (80%)\n</pre> Out[70]: <pre>(15150, 5050)</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> In\u00a0[71]: Copied! <pre>import os\nimport torch\n\nBATCH_SIZE = 32\nNUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs\n\n# Create Food101 20 percent training DataLoader\ntrain_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,\n                                                                  batch_size=BATCH_SIZE,\n                                                                  shuffle=True,\n                                                                  num_workers=NUM_WORKERS)\n# Create Food101 20 percent testing DataLoader\ntest_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,\n                                                                 batch_size=BATCH_SIZE,\n                                                                 shuffle=False,\n                                                                 num_workers=NUM_WORKERS)\n</pre> import os import torch  BATCH_SIZE = 32 NUM_WORKERS = 2 if os.cpu_count() &lt;= 4 else 4 # this value is very experimental and will depend on the hardware you have available, Google Colab generally provides 2x CPUs  # Create Food101 20 percent training DataLoader train_dataloader_food101_20_percent = torch.utils.data.DataLoader(train_data_food101_20_percent,                                                                   batch_size=BATCH_SIZE,                                                                   shuffle=True,                                                                   num_workers=NUM_WORKERS) # Create Food101 20 percent testing DataLoader test_dataloader_food101_20_percent = torch.utils.data.DataLoader(test_data_food101_20_percent,                                                                  batch_size=BATCH_SIZE,                                                                  shuffle=False,                                                                  num_workers=NUM_WORKERS) In\u00a0[72]: Copied! <pre>from going_modular.going_modular import engine\n\n# Setup optimizer\noptimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),\n                             lr=1e-3)\n\n# Setup loss function\nloss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes\n\n# Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset\nset_seeds()    \neffnetb2_food101_results = engine.train(model=effnetb2_food101,\n                                        train_dataloader=train_dataloader_food101_20_percent,\n                                        test_dataloader=test_dataloader_food101_20_percent,\n                                        optimizer=optimizer,\n                                        loss_fn=loss_fn,\n                                        epochs=5,\n                                        device=device)\n</pre> from going_modular.going_modular import engine  # Setup optimizer optimizer = torch.optim.Adam(params=effnetb2_food101.parameters(),                              lr=1e-3)  # Setup loss function loss_fn = torch.nn.CrossEntropyLoss(label_smoothing=0.1) # throw in a little label smoothing because so many classes  # Want to beat original Food101 paper with 20% of data, need 56.4%+ acc on test dataset set_seeds()     effnetb2_food101_results = engine.train(model=effnetb2_food101,                                         train_dataloader=train_dataloader_food101_20_percent,                                         test_dataloader=test_dataloader_food101_20_percent,                                         optimizer=optimizer,                                         loss_fn=loss_fn,                                         epochs=5,                                         device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 3.6317 | train_acc: 0.2869 | test_loss: 2.7670 | test_acc: 0.4937\nEpoch: 2 | train_loss: 2.8615 | train_acc: 0.4388 | test_loss: 2.4653 | test_acc: 0.5387\nEpoch: 3 | train_loss: 2.6585 | train_acc: 0.4844 | test_loss: 2.3547 | test_acc: 0.5649\nEpoch: 4 | train_loss: 2.5494 | train_acc: 0.5116 | test_loss: 2.3038 | test_acc: 0.5755\nEpoch: 5 | train_loss: 2.5006 | train_acc: 0.5239 | test_loss: 2.2805 | test_acc: 0.5810\n</pre> <p>\u200b\u54c7\u547c\u200b\uff01\uff01\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u4ec5\u7528\u200b20%\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u5c31\u200b\u8d85\u8d8a\u200b\u4e86\u200b\u539f\u59cb\u200bFood101\u200b\u8bba\u6587\u200b\u4e2d\u200b56.4%\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ea\u200b\u8bc4\u4f30\u200b\u4e86\u200b20%\u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u5b8c\u5168\u200b\u590d\u73b0\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8bc4\u4f30\u200b100%\u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7684\u200b\u5a01\u529b\u200b\uff01</p> In\u00a0[73]: Copied! <pre>from helper_functions import plot_loss_curves\n\n# Check out the loss curves for FoodVision Big\nplot_loss_curves(effnetb2_food101_results)\n</pre> from helper_functions import plot_loss_curves  # Check out the loss curves for FoodVision Big plot_loss_curves(effnetb2_food101_results) <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6b63\u5219\u200b\u5316\u200b\u6280\u672f\u200b\uff08\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u548c\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\uff09\u200b\u6709\u52a9\u4e8e\u200b\u9632\u6b62\u200b\u6a21\u578b\u200b\u8fc7\u200b\u62df\u5408\u200b\uff08\u200b\u8bad\u7ec3\u200b\u635f\u5931\u200b\u4ecd\u7136\u200b\u9ad8\u4e8e\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\uff09\uff0c\u200b\u8fd9\u200b\u8868\u660e\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8fdb\u4e00\u6b65\u200b\u8bad\u7ec3\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u6240\u200b\u63d0\u5347\u200b\u3002</p> In\u00a0[74]: Copied! <pre>from going_modular.going_modular import utils\n\n# Create a model path\neffnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\" \n\n# Save FoodVision Big model\nutils.save_model(model=effnetb2_food101,\n                 target_dir=\"models\",\n                 model_name=effnetb2_food101_model_path)\n</pre> from going_modular.going_modular import utils  # Create a model path effnetb2_food101_model_path = \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"   # Save FoodVision Big model utils.save_model(model=effnetb2_food101,                  target_dir=\"models\",                  model_name=effnetb2_food101_model_path) <pre>[INFO] Saving model to: models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n</pre> <p>\u200b\u6a21\u578b\u200b\u5df2\u200b\u4fdd\u5b58\u200b\uff01</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u786e\u4fdd\u200b\u53ef\u4ee5\u200b\u91cd\u65b0\u200b\u52a0\u8f7d\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u901a\u8fc7\u200b <code>create_effnetb2_model(num_classes=101)</code> \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\uff08101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\u5bf9\u5e94\u200b Food101 \u200b\u7684\u200b\u6240\u6709\u200b\u7c7b\u522b\u200b\uff09\u3002</p> <p>\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>torch.nn.Module.load_state_dict()</code> \u200b\u548c\u200b <code>torch.load()</code> \u200b\u52a0\u8f7d\u200b\u4fdd\u5b58\u200b\u7684\u200b <code>state_dict()</code>\u3002</p> In\u00a0[75]: Copied! <pre># Create Food101 compatible EffNetB2 instance\nloaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)\n\n# Load the saved model's state_dict()\nloaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"))\n</pre> # Create Food101 compatible EffNetB2 instance loaded_effnetb2_food101, effnetb2_transforms = create_effnetb2_model(num_classes=101)  # Load the saved model's state_dict() loaded_effnetb2_food101.load_state_dict(torch.load(\"models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\")) Out[75]: <pre>&lt;All keys matched successfully&gt;</pre> In\u00a0[76]: Copied! <pre>from pathlib import Path\n\n# Get the model size in bytes then convert to megabytes\npretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly) \nprint(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\")\n</pre> from pathlib import Path  # Get the model size in bytes then convert to megabytes pretrained_effnetb2_food101_model_size = Path(\"models\", effnetb2_food101_model_path).stat().st_size // (1024*1024) # division converts bytes to megabytes (roughly)  print(f\"Pretrained EffNetB2 feature extractor Food101 model size: {pretrained_effnetb2_food101_model_size} MB\") <pre>Pretrained EffNetB2 feature extractor Food101 model size: 30 MB\n</pre> <p>\u200b\u770b\u8d77\u6765\u200b\u6a21\u578b\u200b\u7684\u200b\u5c3a\u5bf8\u200b\u57fa\u672c\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff08FoodVision Big \u200b\u4e3a\u200b 30 MB\uff0cFoodVision Mini \u200b\u4e3a\u200b 29 MB\uff09\uff0c\u200b\u5c3d\u7ba1\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\u5927\u5e45\u200b\u589e\u52a0\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b FoodVision Big \u200b\u7684\u200b\u989d\u5916\u200b\u53c2\u6570\u200b\u4ec5\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b\uff08\u200b\u5206\u7c7b\u5668\u200b\u5934\u200b\uff09\u200b\u4e2d\u200b\u3002</p> <p>FoodVision Big \u200b\u548c\u200b FoodVision Mini \u200b\u7684\u200b\u6240\u6709\u200b\u57fa\u7840\u200b\u5c42\u200b\u90fd\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\u3002</p> <p>\u200b\u56de\u5230\u200b\u4e0a\u9762\u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u6458\u8981\u200b\u53ef\u4ee5\u200b\u63d0\u4f9b\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u3002</p> \u200b\u6a21\u578b\u200b \u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\uff08\u200b\u7c7b\u522b\u200b\u6570\u200b\uff09 \u200b\u53ef\u200b\u8bad\u7ec3\u200b\u53c2\u6570\u200b \u200b\u603b\u200b\u53c2\u6570\u200b \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\uff08MB\uff09 FoodVision Mini\uff08EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff09 3 4,227 7,705,221 29 FoodVision Big\uff08EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff09 101 142,309 7,843,303 30 In\u00a0[77]: Copied! <pre>from pathlib import Path\n\n# Create FoodVision Big demo path\nfoodvision_big_demo_path = Path(\"demos/foodvision_big/\")\n\n# Make FoodVision Big demo directory\nfoodvision_big_demo_path.mkdir(parents=True, exist_ok=True)\n\n# Make FoodVision Big demo examples directory\n(foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True)\n</pre> from pathlib import Path  # Create FoodVision Big demo path foodvision_big_demo_path = Path(\"demos/foodvision_big/\")  # Make FoodVision Big demo directory foodvision_big_demo_path.mkdir(parents=True, exist_ok=True)  # Make FoodVision Big demo examples directory (foodvision_big_demo_path / \"examples\").mkdir(parents=True, exist_ok=True) In\u00a0[78]: Copied! <pre># Download and move an example image\n!wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg \n!mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg\n\n# Move trained model to FoodVision Big demo folder (will error if model is already moved)\n!mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big\n</pre> # Download and move an example image !wget https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg  !mv 04-pizza-dad.jpeg demos/foodvision_big/examples/04-pizza-dad.jpg  # Move trained model to FoodVision Big demo folder (will error if model is already moved) !mv models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth demos/foodvision_big <pre>--2022-08-25 14:24:41--  https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 2874848 (2.7M) [image/jpeg]\nSaving to: '04-pizza-dad.jpeg\u2019\n\n04-pizza-dad.jpeg   100%[===================&gt;]   2.74M  7.85MB/s    in 0.3s    \n\n2022-08-25 14:24:43 (7.85 MB/s) - '04-pizza-dad.jpeg\u2019 saved [2874848/2874848]\n\n</pre> In\u00a0[79]: Copied! <pre># Check out the first 10 Food101 class names\nfood101_class_names[:10]\n</pre> # Check out the first 10 Food101 class names food101_class_names[:10] Out[79]: <pre>['apple_pie',\n 'baby_back_ribs',\n 'baklava',\n 'beef_carpaccio',\n 'beef_tartare',\n 'beet_salad',\n 'beignets',\n 'bibimbap',\n 'bread_pudding',\n 'breakfast_burrito']</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u5199\u5165\u200b\u4e00\u4e2a\u200b\u6587\u672c\u6587\u4ef6\u200b\uff1a\u200b\u9996\u5148\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6307\u5411\u200b <code>demos/foodvision_big/class_names.txt</code> \u200b\u7684\u200b\u8def\u5f84\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>open()</code> \u200b\u51fd\u6570\u200b\u6253\u5f00\u200b\u6587\u4ef6\u200b\uff0c\u200b\u5e76\u200b\u9010\u884c\u200b\u5199\u5165\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\uff0c\u200b\u6bcf\u884c\u200b\u4e00\u4e2a\u200b\u7c7b\u522b\u200b\u3002</p> <p>\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u4fdd\u5b58\u200b\u6210\u200b\u5982\u4e0b\u200b\u683c\u5f0f\u200b\uff1a</p> <pre><code>apple_pie\nbaby_back_ribs\nbaklava\nbeef_carpaccio\nbeef_tartare\n...\n</code></pre> In\u00a0[80]: Copied! <pre># Create path to Food101 class names\nfoodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"\n\n# Write Food101 class names list to file\nwith open(foodvision_big_class_names_path, \"w\") as f:\n    print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")\n    f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class\n</pre> # Create path to Food101 class names foodvision_big_class_names_path = foodvision_big_demo_path / \"class_names.txt\"  # Write Food101 class names list to file with open(foodvision_big_class_names_path, \"w\") as f:     print(f\"[INFO] Saving Food101 class names to {foodvision_big_class_names_path}\")     f.write(\"\\n\".join(food101_class_names)) # leave a new line between each class <pre>[INFO] Saving Food101 class names to demos/foodvision_big/class_names.txt\n</pre> <p>\u200b\u975e\u5e38\u200b\u597d\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6765\u200b\u786e\u4fdd\u200b\u80fd\u591f\u200b\u8bfb\u53d6\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>open()</code> \u200b\u51fd\u6570\u200b\u4ee5\u200b\u8bfb\u53d6\u200b\u6a21\u5f0f\u200b\uff08<code>\"r\"</code>\uff09\u200b\u6253\u5f00\u200b\u6587\u4ef6\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>readlines()</code> \u200b\u65b9\u6cd5\u200b\u8bfb\u53d6\u200b <code>class_names.txt</code> \u200b\u6587\u4ef6\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u884c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5217\u8868\u200b\u63a8\u5bfc\u200b\u5f0f\u200b\u548c\u200b <code>strip()</code> \u200b\u65b9\u6cd5\u200b\u53bb\u9664\u200b\u6bcf\u4e2a\u200b\u7c7b\u540d\u200b\u4e2d\u200b\u7684\u200b\u6362\u884c\u7b26\u200b\uff0c\u200b\u4ece\u800c\u200b\u5c06\u200b\u7c7b\u540d\u200b\u4fdd\u5b58\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5217\u8868\u200b\u4e2d\u200b\u3002</p> In\u00a0[81]: Copied! <pre># Open Food101 class names file and read each line into a list\nwith open(foodvision_big_class_names_path, \"r\") as f:\n    food101_class_names_loaded = [food.strip() for food in  f.readlines()]\n    \n# View the first 5 class names loaded back in\nfood101_class_names_loaded[:5]\n</pre> # Open Food101 class names file and read each line into a list with open(foodvision_big_class_names_path, \"r\") as f:     food101_class_names_loaded = [food.strip() for food in  f.readlines()]      # View the first 5 class names loaded back in food101_class_names_loaded[:5] Out[81]: <pre>['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare']</pre> In\u00a0[82]: Copied! <pre>%%writefile demos/foodvision_big/model.py\nimport torch\nimport torchvision\n\nfrom torch import nn\n\n\ndef create_effnetb2_model(num_classes:int=3, \n                          seed:int=42):\n    \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.\n\n    Args:\n        num_classes (int, optional): number of classes in the classifier head. \n            Defaults to 3.\n        seed (int, optional): random seed value. Defaults to 42.\n\n    Returns:\n        model (torch.nn.Module): EffNetB2 feature extractor model. \n        transforms (torchvision.transforms): EffNetB2 image transforms.\n    \"\"\"\n    # Create EffNetB2 pretrained weights, transforms and model\n    weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT\n    transforms = weights.transforms()\n    model = torchvision.models.efficientnet_b2(weights=weights)\n\n    # Freeze all layers in base model\n    for param in model.parameters():\n        param.requires_grad = False\n\n    # Change classifier head with random seed for reproducibility\n    torch.manual_seed(seed)\n    model.classifier = nn.Sequential(\n        nn.Dropout(p=0.3, inplace=True),\n        nn.Linear(in_features=1408, out_features=num_classes),\n    )\n    \n    return model, transforms\n</pre> %%writefile demos/foodvision_big/model.py import torch import torchvision  from torch import nn   def create_effnetb2_model(num_classes:int=3,                            seed:int=42):     \"\"\"Creates an EfficientNetB2 feature extractor model and transforms.      Args:         num_classes (int, optional): number of classes in the classifier head.              Defaults to 3.         seed (int, optional): random seed value. Defaults to 42.      Returns:         model (torch.nn.Module): EffNetB2 feature extractor model.          transforms (torchvision.transforms): EffNetB2 image transforms.     \"\"\"     # Create EffNetB2 pretrained weights, transforms and model     weights = torchvision.models.EfficientNet_B2_Weights.DEFAULT     transforms = weights.transforms()     model = torchvision.models.efficientnet_b2(weights=weights)      # Freeze all layers in base model     for param in model.parameters():         param.requires_grad = False      # Change classifier head with random seed for reproducibility     torch.manual_seed(seed)     model.classifier = nn.Sequential(         nn.Dropout(p=0.3, inplace=True),         nn.Linear(in_features=1408, out_features=num_classes),     )          return model, transforms <pre>Overwriting demos/foodvision_big/model.py\n</pre> In\u00a0[83]: Copied! <pre>%%writefile demos/foodvision_big/app.py\n### 1. Imports and class names setup ### \nimport gradio as gr\nimport os\nimport torch\n\nfrom model import create_effnetb2_model\nfrom timeit import default_timer as timer\nfrom typing import Tuple, Dict\n\n# Setup class names\nwith open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt\n    class_names = [food_name.strip() for food_name in  f.readlines()]\n    \n### 2. Model and transforms preparation ###    \n\n# Create model\neffnetb2, effnetb2_transforms = create_effnetb2_model(\n    num_classes=101, # could also use len(class_names)\n)\n\n# Load saved weights\neffnetb2.load_state_dict(\n    torch.load(\n        f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",\n        map_location=torch.device(\"cpu\"),  # load to CPU\n    )\n)\n\n### 3. Predict function ###\n\n# Create predict function\ndef predict(img) -&gt; Tuple[Dict, float]:\n    \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.\n    \"\"\"\n    # Start the timer\n    start_time = timer()\n    \n    # Transform the target image and add a batch dimension\n    img = effnetb2_transforms(img).unsqueeze(0)\n    \n    # Put model into evaluation mode and turn on inference mode\n    effnetb2.eval()\n    with torch.inference_mode():\n        # Pass the transformed image through the model and turn the prediction logits into prediction probabilities\n        pred_probs = torch.softmax(effnetb2(img), dim=1)\n    \n    # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)\n    pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}\n    \n    # Calculate the prediction time\n    pred_time = round(timer() - start_time, 5)\n    \n    # Return the prediction dictionary and prediction time \n    return pred_labels_and_probs, pred_time\n\n### 4. Gradio app ###\n\n# Create title, description and article strings\ntitle = \"FoodVision Big \ud83c\udf54\ud83d\udc41\"\ndescription = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\"\narticle = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"\n\n# Create examples list from \"examples/\" directory\nexample_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]\n\n# Create Gradio interface \ndemo = gr.Interface(\n    fn=predict,\n    inputs=gr.Image(type=\"pil\"),\n    outputs=[\n        gr.Label(num_top_classes=5, label=\"Predictions\"),\n        gr.Number(label=\"Prediction time (s)\"),\n    ],\n    examples=example_list,\n    title=title,\n    description=description,\n    article=article,\n)\n\n# Launch the app!\ndemo.launch()\n</pre> %%writefile demos/foodvision_big/app.py ### 1. Imports and class names setup ###  import gradio as gr import os import torch  from model import create_effnetb2_model from timeit import default_timer as timer from typing import Tuple, Dict  # Setup class names with open(\"class_names.txt\", \"r\") as f: # reading them in from class_names.txt     class_names = [food_name.strip() for food_name in  f.readlines()]      ### 2. Model and transforms preparation ###      # Create model effnetb2, effnetb2_transforms = create_effnetb2_model(     num_classes=101, # could also use len(class_names) )  # Load saved weights effnetb2.load_state_dict(     torch.load(         f=\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\",         map_location=torch.device(\"cpu\"),  # load to CPU     ) )  ### 3. Predict function ###  # Create predict function def predict(img) -&gt; Tuple[Dict, float]:     \"\"\"Transforms and performs a prediction on img and returns prediction and time taken.     \"\"\"     # Start the timer     start_time = timer()          # Transform the target image and add a batch dimension     img = effnetb2_transforms(img).unsqueeze(0)          # Put model into evaluation mode and turn on inference mode     effnetb2.eval()     with torch.inference_mode():         # Pass the transformed image through the model and turn the prediction logits into prediction probabilities         pred_probs = torch.softmax(effnetb2(img), dim=1)          # Create a prediction label and prediction probability dictionary for each prediction class (this is the required format for Gradio's output parameter)     pred_labels_and_probs = {class_names[i]: float(pred_probs[0][i]) for i in range(len(class_names))}          # Calculate the prediction time     pred_time = round(timer() - start_time, 5)          # Return the prediction dictionary and prediction time      return pred_labels_and_probs, pred_time  ### 4. Gradio app ###  # Create title, description and article strings title = \"FoodVision Big \ud83c\udf54\ud83d\udc41\" description = \"An EfficientNetB2 feature extractor computer vision model to classify images of food into [101 different classes](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/food101_class_names.txt).\" article = \"Created at [09. PyTorch Model Deployment](https://www.learnpytorch.io/09_pytorch_model_deployment/).\"  # Create examples list from \"examples/\" directory example_list = [[\"examples/\" + example] for example in os.listdir(\"examples\")]  # Create Gradio interface  demo = gr.Interface(     fn=predict,     inputs=gr.Image(type=\"pil\"),     outputs=[         gr.Label(num_top_classes=5, label=\"Predictions\"),         gr.Number(label=\"Prediction time (s)\"),     ],     examples=example_list,     title=title,     description=description,     article=article, )  # Launch the app! demo.launch() <pre>Overwriting demos/foodvision_big/app.py\n</pre> In\u00a0[84]: Copied! <pre>%%writefile demos/foodvision_big/requirements.txt\ntorch==1.12.0\ntorchvision==0.13.0\ngradio==3.1.4\n</pre> %%writefile demos/foodvision_big/requirements.txt torch==1.12.0 torchvision==0.13.0 gradio==3.1.4 <pre>Overwriting demos/foodvision_big/requirements.txt\n</pre> In\u00a0[85]: Copied! <pre># Zip foodvision_big folder but exclude certain files\n!cd demos/foodvision_big &amp;&amp; zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"\n\n# Download the zipped FoodVision Big app (if running in Google Colab)\ntry:\n    from google.colab import files\n    files.download(\"demos/foodvision_big.zip\")\nexcept:\n    print(\"Not running in Google Colab, can't use google.colab.files.download()\")\n</pre> # Zip foodvision_big folder but exclude certain files !cd demos/foodvision_big &amp;&amp; zip -r ../foodvision_big.zip * -x \"*.pyc\" \"*.ipynb\" \"*__pycache__*\" \"*ipynb_checkpoints*\"  # Download the zipped FoodVision Big app (if running in Google Colab) try:     from google.colab import files     files.download(\"demos/foodvision_big.zip\") except:     print(\"Not running in Google Colab, can't use google.colab.files.download()\") <pre>updating: 09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth (deflated 8%)\nupdating: app.py (deflated 54%)\nupdating: class_names.txt (deflated 48%)\nupdating: examples/ (stored 0%)\nupdating: flagged/ (stored 0%)\nupdating: model.py (deflated 56%)\nupdating: requirements.txt (deflated 4%)\nupdating: examples/04-pizza-dad.jpg (deflated 0%)\nNot running in Google Colab, can't use google.colab.files.download()\n</pre> In\u00a0[86]: Copied! <pre># IPython is a library to help work with Python iteractively \nfrom IPython.display import IFrame\n\n# Embed FoodVision Big Gradio demo as an iFrame\nIFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750)\n</pre> # IPython is a library to help work with Python iteractively  from IPython.display import IFrame  # Embed FoodVision Big Gradio demo as an iFrame IFrame(src=\"https://hf.space/embed/mrdbourke/foodvision_big/+\", width=900, height=750) Out[86]: <p>\u200b\u771f\u662f\u200b\u592a\u9177\u200b\u4e86\u200b\uff01\uff1f</p> <p>\u200b\u4ece\u200b\u6784\u5efa\u200b\u9884\u6d4b\u200b\u76f4\u7ebf\u200b\u7684\u200bPyTorch\u200b\u6a21\u578b\u200b\u5230\u200b\u73b0\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8d70\u8fc7\u200b\u4e86\u200b\u6f2b\u957f\u200b\u7684\u200b\u9053\u8def\u200b...\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u6784\u5efa\u200b\u9762\u5411\u200b\u5168\u7403\u200b\u4eba\u6c11\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\uff01</p>"},{"location":"09_pytorch_model_deployment/#09-pytorch","title":"09. PyTorch \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u00b6","text":"<p>\u200b\u6b22\u8fce\u200b\u6765\u5230\u200b\u7b2c\u4e09\u9636\u6bb5\u200b\u9879\u76ee\u200b\uff1aPyTorch \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u9879\u76ee\u200b\u5df2\u7ecf\u200b\u53d6\u5f97\u200b\u4e86\u200b\u957f\u8db3\u200b\u7684\u200b\u8fdb\u5c55\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5230\u200b\u76ee\u524d\u4e3a\u6b62\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u8fd8\u200b\u53ea\u80fd\u200b\u4f9b\u200b\u6211\u4eec\u200b\u4e2a\u4eba\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u4e0d\u5982\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b FoodVision Mini \u200b\u53d8\u4e3a\u200b\u73b0\u5b9e\u200b\uff0c\u200b\u8ba9\u200b\u5b83\u200b\u516c\u5f00\u200b\u53ef\u7528\u200b\uff1f</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5230\u200b\u4e92\u8054\u7f51\u200b\u4e0a\u200b\uff0c\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u53ef\u7528\u200b\u7684\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\uff01</p> <p>\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u5348\u9910\u200b\u4e0a\u200b\u5c1d\u8bd5\u200b\u5df2\u200b\u90e8\u7f72\u200b\u7684\u200b FoodVision Mini \u200b\u7248\u672c\u200b\uff08\u200b\u6211\u4eec\u200b\u5373\u5c06\u200b\u6784\u5efa\u200b\u7684\u200b\u5185\u5bb9\u200b\uff09\u3002\u200b\u6a21\u578b\u200b\u4e5f\u200b\u9884\u6d4b\u200b\u5bf9\u200b\u4e86\u200b \ud83c\udf63\uff01</p>"},{"location":"09_pytorch_model_deployment/","title":"\u4ec0\u4e48\u200b\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\uff1f\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b \u200b\u662f\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u63d0\u4f9b\u200b\u7ed9\u200b\u5176\u4ed6\u4eba\u200b\u6216\u200b\u7cfb\u7edf\u200b\u4f7f\u7528\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u5176\u4ed6\u4eba\u200b\u53ef\u4ee5\u200b\u662f\u200b\u80fd\u591f\u200b\u4ee5\u200b\u67d0\u79cd\u200b\u65b9\u5f0f\u200b\u4e0e\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u4e92\u52a8\u200b\u7684\u200b\u4e2a\u4eba\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u67d0\u4eba\u200b\u4f7f\u7528\u200b\u667a\u80fd\u624b\u673a\u200b\u62cd\u6444\u200b\u98df\u7269\u200b\u7167\u7247\u200b\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u5c06\u200b\u5176\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u3002</p> <p>\u200b\u5176\u4ed6\u200b\u7cfb\u7edf\u200b\u53ef\u80fd\u200b\u662f\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7a0b\u5e8f\u200b\u3001\u200b\u5e94\u7528\u200b\uff0c\u200b\u751a\u81f3\u200b\u662f\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u4eec\u200b\u4e0e\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4ea4\u4e92\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u94f6\u884c\u200b\u6570\u636e\u5e93\u200b\u53ef\u80fd\u200b\u4f9d\u8d56\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5728\u200b\u8f6c\u8d26\u200b\u524d\u200b\u9884\u6d4b\u200b\u4ea4\u6613\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u6b3a\u8bc8\u200b\u884c\u4e3a\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u64cd\u4f5c\u7cfb\u7edf\u200b\u53ef\u80fd\u200b\u6839\u636e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5bf9\u200b\u67d0\u4eba\u200b\u5728\u200b\u7279\u5b9a\u200b\u65f6\u95f4\u6bb5\u200b\u5185\u200b\u901a\u5e38\u200b\u4f7f\u7528\u200b\u7684\u200b\u7535\u91cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4ece\u800c\u200b\u964d\u4f4e\u200b\u8d44\u6e90\u200b\u6d88\u8017\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u53ef\u4ee5\u200b\u6df7\u5408\u200b\u642d\u914d\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7279\u65af\u62c9\u200b\u6c7d\u8f66\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7cfb\u7edf\u200b\u5c06\u200b\u4e0e\u200b\u8f66\u8f86\u200b\u7684\u200b\u8def\u7ebf\u200b\u89c4\u5212\u200b\u7a0b\u5e8f\u200b\uff08\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7cfb\u7edf\u200b\uff09\u200b\u4ea4\u4e92\u200b\uff0c\u200b\u7136\u540e\u200b\u8def\u7ebf\u200b\u89c4\u5212\u200b\u7a0b\u5e8f\u200b\u4f1a\u200b\u4ece\u200b\u9a7e\u9a76\u5458\u200b\uff08\u200b\u5176\u4ed6\u4eba\u200b\uff09\u200b\u90a3\u91cc\u200b\u83b7\u53d6\u200b\u8f93\u5165\u200b\u548c\u200b\u53cd\u9988\u200b\u3002</p> <p></p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u6d89\u53ca\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u63d0\u4f9b\u200b\u7ed9\u200b\u5176\u4ed6\u4eba\u200b\u6216\u200b\u7cfb\u7edf\u200b\u4f7f\u7528\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u67d0\u4eba\u200b\u53ef\u80fd\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u4f5c\u4e3a\u200b\u98df\u7269\u200b\u8bc6\u522b\u200b\u5e94\u7528\u200b\uff08\u200b\u5982\u200b FoodVision Mini \u200b\u6216\u200b Nutrify\uff09\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u3002\u200b\u800c\u200b\u5176\u4ed6\u200b\u7cfb\u7edf\u200b\u53ef\u80fd\u200b\u662f\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6216\u200b\u7a0b\u5e8f\u200b\u4f7f\u7528\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f8b\u5982\u200b\u94f6\u884c\u200b\u7cfb\u7edf\u200b\u4f7f\u7528\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u6765\u200b\u68c0\u6d4b\u200b\u4ea4\u6613\u200b\u662f\u5426\u200b\u5b58\u5728\u200b\u6b3a\u8bc8\u200b\u884c\u4e3a\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u4e3a\u4ec0\u4e48\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff1f\u00b6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e2d\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b\u662f\u200b\uff1a</p> <p>\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u540c\u6837\u200b\u91cd\u8981\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u5c3d\u7ba1\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7cbe\u5fc3\u8bbe\u8ba1\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u8bc4\u4f30\u200b\u6216\u200b\u53ef\u89c6\u5316\u200b\u7ed3\u679c\u200b\u6765\u200b\u5927\u81f4\u200b\u4e86\u89e3\u200b\u6a21\u578b\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u6c38\u8fdc\u200b\u65e0\u6cd5\u200b\u771f\u6b63\u200b\u77e5\u9053\u200b\u5b83\u200b\u5728\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u4e2d\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff0c\u200b\u76f4\u5230\u200b\u4f60\u200b\u5c06\u200b\u5176\u200b\u53d1\u5e03\u200b\u5230\u200b\u771f\u5b9e\u200b\u73af\u5883\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u4ece\u672a\u200b\u4f7f\u7528\u200b\u8fc7\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b\u4eba\u200b\u4e0e\u200b\u4e4b\u200b\u4ea4\u4e92\u200b\uff0c\u200b\u5f80\u5f80\u200b\u4f1a\u200b\u63ed\u793a\u200b\u4f60\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4ece\u672a\u200b\u8003\u8651\u200b\u8fc7\u200b\u7684\u200b\u8fb9\u7f18\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u4eba\u200b\u5411\u200b\u6211\u4eec\u200b\u7684\u200bFoodVision Mini\u200b\u6a21\u578b\u200b\u4e0a\u4f20\u200b\u4e86\u200b\u4e00\u5f20\u200b\u4e0d\u662f\u200b\u98df\u7269\u200b\u7684\u200b\u7167\u7247\u200b\uff0c\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u4e00\u4e2a\u200b\u89e3\u51b3\u65b9\u6848\u200b\u662f\u200b\u521b\u5efa\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff0c\u200b\u9996\u5148\u200b\u5c06\u200b\u56fe\u50cf\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u201c\u200b\u98df\u7269\u200b\u201d\u200b\u6216\u200b\u201c\u200b\u975e\u200b\u98df\u7269\u200b\u201d\uff0c\u200b\u5e76\u200b\u5c06\u200b\u76ee\u6807\u200b\u56fe\u50cf\u200b\u9996\u5148\u200b\u901a\u8fc7\u200b\u8be5\u200b\u6a21\u578b\u200b\uff08\u200b\u8fd9\u200b\u5c31\u662f\u200bNutrify\u200b\u6240\u200b\u505a\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u5982\u679c\u200b\u56fe\u50cf\u200b\u662f\u200b\u201c\u200b\u98df\u7269\u200b\u201d\uff0c\u200b\u5b83\u200b\u5c06\u200b\u8fdb\u5165\u200b\u6211\u4eec\u200b\u7684\u200bFoodVision Mini\u200b\u6a21\u578b\u200b\u5e76\u200b\u88ab\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u662f\u200b\u201c\u200b\u975e\u200b\u98df\u7269\u200b\u201d\uff0c\u200b\u5219\u200b\u663e\u793a\u200b\u4e00\u6761\u200b\u6d88\u606f\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u662f\u200b\u9519\u8bef\u200b\u7684\u200b\u5462\u200b\uff1f</p> <p>\u200b\u90a3\u65f6\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6301\u7eed\u200b\u4e0b\u53bb\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8fd9\u200b\u51f8\u663e\u200b\u4e86\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u7684\u200b\u91cd\u8981\u6027\u200b\uff1a\u200b\u5b83\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u53d1\u73b0\u200b\u6a21\u578b\u200b\u5728\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u4e0d\u200b\u660e\u663e\u200b\u7684\u200b\u9519\u8bef\u200b\u3002</p> <p></p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b01. PyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200bPyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002\u200b\u4f46\u200b\u4e00\u65e6\u200b\u4f60\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u90e8\u7f72\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u4e0b\u200b\u4e00\u6b65\u200b\u3002\u200b\u76d1\u63a7\u200b\u6d89\u53ca\u200b\u5230\u200b\u5982\u4f55\u200b\u5904\u7406\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u6570\u636e\u200b\u5206\u5272\u200b\uff1a\u200b\u6765\u81ea\u200b\u73b0\u5b9e\u200b\u4e16\u754c\u200b\u7684\u200b\u6570\u636e\u200b\u3002\u200b\u6709\u5173\u200b\u90e8\u7f72\u200b\u548c\u200b\u76d1\u63a7\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u8d44\u6e90\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200bPyTorch\u200b\u989d\u5916\u200b\u8d44\u6e90\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u00b6","text":"<p>\u200b\u5173\u4e8e\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5199\u6210\u200b\u6574\u672c\u4e66\u200b\uff08\u200b\u5b9e\u9645\u4e0a\u200b\uff0cPyTorch \u200b\u989d\u5916\u200b\u8d44\u6e90\u200b\u4e2d\u200b\u5217\u51fa\u200b\u4e86\u200b\u8bb8\u591a\u200b\u4f18\u79c0\u200b\u7684\u200b\u4e66\u7c4d\u200b\uff09\u3002</p> <p>\u200b\u800c\u4e14\u200b\u8fd9\u4e2a\u200b\u9886\u57df\u200b\u5728\u200b\u6700\u4f73\u200b\u5b9e\u8df5\u200b\u65b9\u9762\u200b\u4ecd\u200b\u5728\u200b\u4e0d\u65ad\u200b\u53d1\u5c55\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u200b\u559c\u6b22\u200b\u4ece\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5f00\u59cb\u200b\uff1a</p> <p>\"\u200b\u6211\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u6700\u200b\u7406\u60f3\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f\"</p> <p>\u200b\u7136\u540e\u200b\u4ece\u200b\u90a3\u91cc\u200b\u9006\u5411\u200b\u601d\u8003\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4e8b\u5148\u200b\u5e76\u4e0d\u77e5\u9053\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002\u200b\u4f46\u200b\u4f60\u200b\u8db3\u591f\u200b\u806a\u660e\u200b\uff0c\u200b\u80fd\u591f\u200b\u60f3\u8c61\u200b\u8fd9\u4e9b\u200b\u60c5\u666f\u200b\u3002</p> <p>\u200b\u4ee5\u200b FoodVision Mini \u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u7406\u60f3\u200b\u7684\u200b\u573a\u666f\u200b\u53ef\u80fd\u200b\u662f\u200b\uff1a</p> <ul> <li>\u200b\u67d0\u4eba\u200b\u5728\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u62cd\u7167\u200b\uff08\u200b\u901a\u8fc7\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u6216\u200b\u7f51\u9875\u200b\u6d4f\u89c8\u5668\u200b\uff09\u3002</li> <li>\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u8fc5\u901f\u200b\u8fd4\u56de\u200b\u3002</li> </ul> <p>\u200b\u5f88\u200b\u7b80\u5355\u200b\u3002</p> <p>\u200b\u6240\u4ee5\u200b\u6211\u4eec\u200b\u6709\u200b\u4e24\u4e2a\u200b\u4e3b\u8981\u200b\u6807\u51c6\u200b\uff1a</p> <ol> <li>\u200b\u6a21\u578b\u200b\u5e94\u8be5\u200b\u80fd\u200b\u5728\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff08\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4f1a\u200b\u6709\u200b\u4e00\u4e9b\u200b\u8ba1\u7b97\u200b\u9650\u5236\u200b\uff09\u3002</li> <li>\u200b\u6a21\u578b\u200b\u5e94\u8be5\u200b\u80fd\u200b\u5feb\u901f\u200b\u505a\u51fa\u200b\u9884\u6d4b\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6162\u901f\u200b\u7684\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u662f\u200b\u65e0\u8da3\u200b\u7684\u200b\uff09\u3002</li> </ol> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u6839\u636e\u200b\u4f60\u200b\u7684\u200b\u4f7f\u7528\u200b\u573a\u666f\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u9700\u6c42\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u6240\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u4e0a\u8ff0\u200b\u4e24\u70b9\u200b\u53ef\u4ee5\u200b\u5206\u89e3\u200b\u4e3a\u200b\u53e6\u5916\u200b\u4e24\u4e2a\u200b\u95ee\u9898\u200b\uff1a</p> <ol> <li>\u200b\u5b83\u200b\u5c06\u200b\u90e8\u7f72\u200b\u5728\u200b\u54ea\u91cc\u200b\uff1f - \u200b\u5373\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u5b58\u50a8\u200b\u5728\u200b\u54ea\u91cc\u200b\uff1f</li> <li>\u200b\u5b83\u200b\u5c06\u200b\u5982\u4f55\u200b\u8fd0\u884c\u200b\uff1f - \u200b\u5373\u200b\uff0c\u200b\u5b83\u200b\u662f\u200b\u7acb\u5373\u200b\u8fd4\u56de\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u8fd8\u662f\u200b\u7a0d\u540e\u200b\u8fd4\u56de\u200b\uff1f</li> </ol> <p>\u200b\u5f00\u59cb\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u4ece\u200b\u8be2\u95ee\u200b\u6700\u200b\u7406\u60f3\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\u5f00\u59cb\u200b\uff0c\u200b\u7136\u540e\u200b\u9006\u5411\u200b\u601d\u8003\u200b\uff0c\u200b\u8be2\u95ee\u200b\u6a21\u578b\u200b\u5c06\u200b\u90e8\u7f72\u200b\u5728\u200b\u54ea\u91cc\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5b83\u200b\u5c06\u200b\u5982\u4f55\u200b\u8fd0\u884c\u200b\uff0c\u200b\u8fd9\u200b\u662f\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u5b83\u200b\u5c06\u200b\u4f55\u53bb\u4f55\u4ece\u200b\uff1f\u00b6","text":"<p>\u200b\u5f53\u200b\u4f60\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u4f4d\u4e8e\u200b\u4f55\u5904\u200b\uff1f</p> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u4e3b\u8981\u200b\u4e89\u8bae\u200b\u901a\u5e38\u200b\u5728\u4e8e\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u8fb9\u7f18\u200b/\u200b\u5728\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\uff09\u200b\u6216\u200b\u4e91\u7aef\u200b\uff08\u200b\u4e00\u53f0\u200b\u8ba1\u7b97\u673a\u200b/\u200b\u670d\u52a1\u5668\u200b\uff0c\u200b\u5e76\u975e\u200b\u5b9e\u9645\u200b\u8c03\u7528\u200b\u6a21\u578b\u200b\u7684\u200b\u8bbe\u5907\u200b\uff09\u3002</p> <p>\u200b\u4e24\u8005\u200b\u5404\u6709\u200b\u4f18\u7f3a\u70b9\u200b\u3002</p> \u200b\u90e8\u7f72\u200b\u4f4d\u7f6e\u200b \u200b\u4f18\u70b9\u200b \u200b\u7f3a\u70b9\u200b \u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff08\u200b\u8fb9\u7f18\u200b/\u200b\u5728\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\uff09 \u200b\u901f\u5ea6\u200b\u975e\u5e38\u200b\u5feb\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6570\u636e\u200b\u4e0d\u200b\u79bb\u5f00\u200b\u8bbe\u5907\u200b\uff09 \u200b\u8ba1\u7b97\u80fd\u529b\u200b\u6709\u9650\u200b\uff08\u200b\u8f83\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u8fd0\u884c\u200b\u65f6\u95f4\u200b\u8f83\u957f\u200b\uff09 \u200b\u4fdd\u62a4\u200b\u9690\u79c1\u200b\uff08\u200b\u6570\u636e\u200b\u65e0\u9700\u200b\u79bb\u5f00\u200b\u8bbe\u5907\u200b\uff09 \u200b\u5b58\u50a8\u7a7a\u95f4\u200b\u6709\u9650\u200b\uff08\u200b\u9700\u8981\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\u5c3a\u5bf8\u200b\uff09 \u200b\u65e0\u9700\u200b\u4e92\u8054\u7f51\u200b\u8fde\u63a5\u200b\uff08\u200b\u6709\u65f6\u200b\uff09 \u200b\u901a\u5e38\u200b\u9700\u8981\u200b\u8bbe\u5907\u200b\u7279\u5b9a\u200b\u7684\u200b\u6280\u80fd\u200b \u200b\u4e91\u7aef\u200b \u200b\u8fd1\u4e4e\u200b\u65e0\u9650\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\uff08\u200b\u53ef\u200b\u6839\u636e\u200b\u9700\u8981\u200b\u6269\u5c55\u200b\uff09 \u200b\u6210\u672c\u200b\u53ef\u80fd\u200b\u5931\u63a7\u200b\uff08\u200b\u5982\u679c\u200b\u672a\u200b\u5b9e\u65bd\u200b\u9002\u5f53\u200b\u7684\u200b\u6269\u5c55\u200b\u9650\u5236\u200b\uff09 \u200b\u90e8\u7f72\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5e76\u200b\u5728\u200b\u5404\u5904\u200b\u4f7f\u7528\u200b\uff08\u200b\u901a\u8fc7\u200bAPI\uff09 \u200b\u9884\u6d4b\u200b\u53ef\u80fd\u200b\u8f83\u6162\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6570\u636e\u200b\u9700\u8981\u200b\u79bb\u5f00\u200b\u8bbe\u5907\u200b\u5e76\u200b\u8fd4\u56de\u200b\uff08\u200b\u7f51\u7edc\u200b\u5ef6\u8fdf\u200b\uff09 \u200b\u4e0e\u200b\u73b0\u6709\u200b\u4e91\u200b\u751f\u6001\u7cfb\u7edf\u200b\u94fe\u63a5\u200b \u200b\u6570\u636e\u200b\u5fc5\u987b\u200b\u79bb\u5f00\u200b\u8bbe\u5907\u200b\uff08\u200b\u53ef\u80fd\u200b\u5f15\u53d1\u200b\u9690\u79c1\u200b\u95ee\u9898\u200b\uff09 <p>\u200b\u8fd9\u4e9b\u200b\u7ec6\u8282\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u4f46\u200b\u6211\u200b\u5728\u200b\u989d\u5916\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u7559\u4e0b\u200b\u4e86\u200b\u8d44\u6e90\u200b\uff0c\u200b\u4f9b\u200b\u4f60\u200b\u6df1\u5165\u200b\u4e86\u89e3\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3e\u4e2a\u200b\u4f8b\u5b50\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5c06\u200bFoodVision Mini\u200b\u90e8\u7f72\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5b83\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u4e14\u200b\u5feb\u901f\u200b\u3002</p> <p>\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u66f4\u200b\u559c\u6b22\u200b\u54ea\u200b\u79cd\u200b\u6a21\u578b\u200b\uff1f</p> <ol> <li>\u200b\u4e00\u4e2a\u200b\u5728\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fd0\u884c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u51c6\u786e\u7387\u200b\u4e3a\u200b95%\uff0c\u200b\u6bcf\u6b21\u200b\u9884\u6d4b\u200b\u7684\u200b\u63a8\u7406\u200b\u65f6\u95f4\u200b\uff08\u200b\u5ef6\u8fdf\u200b\uff09\u200b\u4e3a\u200b\u4e00\u79d2\u200b\u3002</li> <li>\u200b\u4e00\u4e2a\u200b\u5728\u200b\u4e91\u7aef\u200b\u8fd0\u884c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u51c6\u786e\u7387\u200b\u4e3a\u200b98%\uff0c\u200b\u6bcf\u6b21\u200b\u9884\u6d4b\u200b\u7684\u200b\u63a8\u7406\u200b\u65f6\u95f4\u200b\u4e3a\u200b10\u200b\u79d2\u200b\uff08\u200b\u66f4\u5927\u200b\u3001\u200b\u66f4\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f46\u200b\u8ba1\u7b97\u200b\u65f6\u95f4\u200b\u66f4\u957f\u200b\uff09\u3002</li> </ol> <p>\u200b\u8fd9\u4e9b\u200b\u6570\u5b57\u200b\u662f\u200b\u6211\u200b\u7f16\u9020\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u5c55\u793a\u200b\u4e86\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u548c\u200b\u4e91\u7aef\u200b\u4e4b\u95f4\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u7684\u200b\u5dee\u5f02\u200b\u3002</p> <p>\u200b\u9009\u9879\u200b1\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8f83\u200b\u5c0f\u200b\u3001\u200b\u6027\u80fd\u200b\u8f83\u200b\u4f4e\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u80fd\u591f\u200b\u9002\u5e94\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8fd0\u884c\u200b\u901f\u5ea6\u200b\u5feb\u200b\u3002</p> <p>\u200b\u9009\u9879\u200b2\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u66f4\u5927\u200b\u3001\u200b\u6027\u80fd\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u200b\u9700\u8981\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u8ba1\u7b97\u200b\u548c\u200b\u5b58\u50a8\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5fc5\u987b\u200b\u5c06\u200b\u6570\u636e\u200b\u53d1\u9001\u200b\u51fa\u200b\u8bbe\u5907\u200b\u5e76\u200b\u8fd4\u56de\u200b\uff08\u200b\u56e0\u6b64\u200b\u5c3d\u7ba1\u200b\u5b9e\u9645\u200b\u9884\u6d4b\u200b\u53ef\u80fd\u200b\u5f88\u5feb\u200b\uff0c\u200b\u4f46\u200b\u7f51\u7edc\u200b\u65f6\u95f4\u200b\u548c\u200b\u6570\u636e\u4f20\u8f93\u200b\u5fc5\u987b\u200b\u8003\u8651\u200b\u5728\u5185\u200b\uff09\uff0c\u200b\u6240\u4ee5\u200b\u8fd0\u884c\u200b\u65f6\u95f4\u200b\u7a0d\u957f\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200bFoodVision Mini\uff0c\u200b\u6211\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u503e\u5411\u200b\u4e8e\u200b\u9009\u62e9\u200b\u9009\u9879\u200b1\uff0c\u200b\u56e0\u4e3a\u200b\u6027\u80fd\u200b\u7684\u200b\u5c0f\u5e45\u200b\u4e0b\u964d\u200b\u8fdc\u200b\u4e0d\u53ca\u200b\u66f4\u5feb\u200b\u7684\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u91cd\u8981\u200b\u3002</p> <p></p> <p>\u200b\u4ee5\u200b\u7279\u65af\u62c9\u200b\u6c7d\u8f66\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7cfb\u7edf\u200b\u4e3a\u4f8b\u200b\uff0c\u200b\u54ea\u200b\u79cd\u200b\u66f4\u597d\u200b\uff1f\u200b\u4e00\u4e2a\u200b\u5728\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u8f83\u200b\u5c0f\u200b\u6a21\u578b\u200b\uff08\u200b\u6a21\u578b\u200b\u5728\u200b\u6c7d\u8f66\u200b\u4e0a\u200b\uff09\uff0c\u200b\u8fd8\u662f\u200b\u4e00\u4e2a\u200b\u5728\u200b\u4e91\u7aef\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\u7684\u200b\u8f83\u5927\u200b\u6a21\u578b\u200b\uff1f\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u4f60\u200b\u80af\u5b9a\u200b\u4f1a\u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b\u6a21\u578b\u200b\u5728\u200b\u6c7d\u8f66\u200b\u4e0a\u200b\u3002\u200b\u6570\u636e\u200b\u4ece\u200b\u6c7d\u8f66\u200b\u5230\u200b\u4e91\u7aef\u200b\u518d\u200b\u8fd4\u56de\u200b\u6c7d\u8f66\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u989d\u5916\u200b\u7f51\u7edc\u200b\u65f6\u95f4\u200b\u6839\u672c\u200b\u4e0d\u200b\u503c\u5f97\u200b\uff08\u200b\u6216\u8005\u200b\u5728\u200b\u4fe1\u53f7\u200b\u5dee\u200b\u7684\u200b\u5730\u533a\u200b\u751a\u81f3\u200b\u4e0d\u200b\u53ef\u80fd\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8981\u200b\u5b8c\u6574\u200b\u4f53\u9a8c\u200b\u5c06\u200bPyTorch\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5230\u200b\u8fb9\u7f18\u200b\u8bbe\u5907\u200b\u7684\u200b\u60c5\u666f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200bPyTorch\u200b\u6559\u7a0b\u200b\uff0c\u200b\u5b9e\u73b0\u200b\u6811\u8393\u200b\u6d3e\u200b\u4e0a\u200b\u7684\u200b\u5b9e\u65f6\u200b\u63a8\u7406\u200b\uff0830fps+\uff09\uff0c\u200b\u4f7f\u7528\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u529f\u80fd\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\uff1f\u00b6","text":"<p>\u200b\u56de\u5230\u200b\u7406\u60f3\u200b\u7684\u200b\u7528\u4f8b\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u5e94\u8be5\u200b\u5982\u4f55\u200b\u5de5\u4f5c\u200b\uff1f</p> <p>\u200b\u6bd4\u5982\u8bf4\u200b\uff0c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u7acb\u5373\u200b\u5f97\u5230\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u5417\u200b\uff1f</p> <p>\u200b\u8fd8\u662f\u200b\u8bf4\u200b\uff0c\u200b\u7a0d\u540e\u200b\u5f97\u5230\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u60c5\u51b5\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\uff1a</p> <ul> <li>\u200b\u5728\u7ebf\u200b\uff08\u200b\u5b9e\u65f6\u200b\uff09 - \u200b\u9884\u6d4b\u200b/\u200b\u63a8\u7406\u200b\u7acb\u5373\u200b\u53d1\u751f\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u67d0\u4eba\u200b\u4e0a\u4f20\u200b\u4e00\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u56fe\u7247\u200b\u88ab\u200b\u5904\u7406\u200b\u5e76\u200b\u8fd4\u56de\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6216\u8005\u200b\u67d0\u4eba\u200b\u8fdb\u884c\u200b\u8d2d\u4e70\u200b\uff0c\u200b\u6a21\u578b\u200b\u9a8c\u8bc1\u200b\u4ea4\u6613\u200b\u975e\u200b\u6b3a\u8bc8\u200b\uff0c\u200b\u8d2d\u4e70\u200b\u5f97\u4ee5\u200b\u5b8c\u6210\u200b\u3002</li> <li>\u200b\u79bb\u7ebf\u200b\uff08\u200b\u6279\u5904\u7406\u200b\uff09 - \u200b\u9884\u6d4b\u200b/\u200b\u63a8\u7406\u200b\u5b9a\u671f\u200b\u53d1\u751f\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7167\u7247\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u5728\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u5145\u7535\u200b\u65f6\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u56fe\u7247\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u4e0d\u540c\u200b\u7684\u200b\u7c7b\u522b\u200b\uff08\u200b\u5982\u200b\u6d77\u6ee9\u200b\u3001\u200b\u7528\u9910\u200b\u65f6\u95f4\u200b\u3001\u200b\u5bb6\u5ead\u200b\u3001\u200b\u670b\u53cb\u200b\uff09\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u201c\u200b\u6279\u5904\u7406\u200b\u201d\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u4e00\u6b21\u200b\u5bf9\u200b\u591a\u4e2a\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u589e\u52a0\u200b\u4e00\u70b9\u200b\u6df7\u6dc6\u200b\uff0c\u200b\u6279\u5904\u7406\u200b\u65e2\u200b\u53ef\u4ee5\u200b\u7acb\u5373\u200b/\u200b\u5728\u7ebf\u200b\u53d1\u751f\u200b\uff08\u200b\u540c\u65f6\u200b\u5bf9\u591a\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff09\uff0c\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u79bb\u7ebf\u200b\u53d1\u751f\u200b\uff08\u200b\u540c\u65f6\u200b\u5bf9\u591a\u5f20\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b/\u200b\u8bad\u7ec3\u200b\uff09\u3002</p> <p>\u200b\u4e3b\u8981\u200b\u533a\u522b\u200b\u5728\u4e8e\u200b\uff1a\u200b\u9884\u6d4b\u200b\u662f\u200b\u7acb\u5373\u200b\u8fdb\u884c\u200b\u8fd8\u662f\u200b\u5b9a\u671f\u200b\u8fdb\u884c\u200b\u3002</p> <p>\u200b\u5b9a\u671f\u200b\u7684\u200b\u65f6\u95f4\u5c3a\u5ea6\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4e0d\u540c\u200b\uff0c\u200b\u4ece\u200b\u6bcf\u9694\u200b\u51e0\u79d2\u200b\u5230\u200b\u6bcf\u9694\u200b\u51e0\u5c0f\u65f6\u200b\u6216\u200b\u51e0\u5929\u200b\u3002</p> <p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u6df7\u5408\u200b\u4f7f\u7528\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\u3002</p> <p>\u200b\u5728\u200b FoodVision Mini \u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u63a8\u7406\u200b\u7ba1\u9053\u200b\u5728\u7ebf\u200b\uff08\u200b\u5b9e\u65f6\u200b\uff09\u200b\u8fdb\u884c\u200b\uff0c\u200b\u8fd9\u6837\u200b\u5f53\u200b\u6709\u4eba\u200b\u4e0a\u4f20\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\u7684\u200b\u56fe\u7247\u200b\u65f6\u200b\uff0c\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u4f1a\u200b\u7acb\u5373\u200b\u8fd4\u56de\u200b\uff08\u200b\u5982\u679c\u200b\u6bd4\u200b\u5b9e\u65f6\u200b\u6162\u200b\uff0c\u200b\u4f53\u9a8c\u200b\u4f1a\u200b\u53d8\u5f97\u200b\u4e4f\u5473\u200b\uff09\u3002</p> <p>\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u7ba1\u9053\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u91c7\u7528\u200b\u6279\u5904\u7406\u200b\uff08\u200b\u79bb\u7ebf\u200b\uff09\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u6211\u4eec\u200b\u5728\u200b\u524d\u200b\u51e0\u7ae0\u200b\u4e2d\u200b\u4e00\u76f4\u200b\u5728\u200b\u505a\u200b\u7684\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u65b9\u6cd5\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8ba8\u8bba\u200b\u4e86\u200b\u51e0\u79cd\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u9009\u9879\u200b\uff08\u200b\u8bbe\u5907\u200b\u7aef\u200b\u548c\u200b\u4e91\u7aef\u200b\uff09\u3002</p> <p>\u200b\u6bcf\u79cd\u200b\u9009\u9879\u200b\u90fd\u200b\u6709\u200b\u5176\u200b\u7279\u5b9a\u200b\u7684\u200b\u8981\u6c42\u200b\uff1a</p> <p>| \u200b\u5de5\u5177\u200b/\u200b\u8d44\u6e90\u200b | \u200b\u90e8\u7f72\u200b\u7c7b\u578b\u200b | | ----- | ----- | | Google \u200b\u7684\u200b ML Kit | \u200b\u8bbe\u5907\u200b\u7aef\u200b\uff08Android \u200b\u548c\u200b iOS\uff09 | | Apple \u200b\u7684\u200b Core ML \u200b\u548c\u200b <code>coremltools</code> Python \u200b\u5305\u200b | \u200b\u8bbe\u5907\u200b\u7aef\u200b\uff08\u200b\u6240\u6709\u200b Apple \u200b\u8bbe\u5907\u200b\uff09 | | Amazon Web Service (AWS) \u200b\u7684\u200b Sagemaker | \u200b\u4e91\u7aef\u200b | | Google Cloud \u200b\u7684\u200b Vertex AI | \u200b\u4e91\u7aef\u200b | | Microsoft \u200b\u7684\u200b Azure Machine Learning | \u200b\u4e91\u7aef\u200b | | Hugging Face Spaces | \u200b\u4e91\u7aef\u200b | | \u200b\u4f7f\u7528\u200b FastAPI \u200b\u7684\u200b API | \u200b\u4e91\u7aef\u200b/\u200b\u81ea\u200b\u6258\u7ba1\u200b\u670d\u52a1\u5668\u200b | | \u200b\u4f7f\u7528\u200b TorchServe \u200b\u7684\u200b API | \u200b\u4e91\u7aef\u200b/\u200b\u81ea\u200b\u6258\u7ba1\u200b\u670d\u52a1\u5668\u200b | | ONNX (Open Neural Network Exchange) | \u200b\u591a\u79cd\u200b/\u200b\u901a\u7528\u200b | | \u200b\u66f4\u200b\u591a\u200b... |</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u7f16\u7a0b\u200b\u63a5\u53e3\u200b\uff08API\uff09 \u200b\u662f\u200b\u4e24\u4e2a\u200b\uff08\u200b\u6216\u200b\u66f4\u200b\u591a\u200b\uff09\u200b\u8ba1\u7b97\u673a\u7a0b\u5e8f\u200b\u76f8\u4e92\u200b\u4ea4\u4e92\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u4e3a\u200b API\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u7a0b\u5e8f\u200b\u5411\u200b\u5176\u200b\u53d1\u9001\u6570\u636e\u200b\u5e76\u200b\u63a5\u6536\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4f60\u200b\u9009\u62e9\u200b\u7684\u200b\u9009\u9879\u200b\u5c06\u200b\u9ad8\u5ea6\u200b\u4f9d\u8d56\u4e8e\u200b\u4f60\u200b\u6b63\u5728\u200b\u6784\u5efa\u200b\u7684\u200b\u5185\u5bb9\u200b/\u200b\u4f60\u200b\u6b63\u5728\u200b\u4e0e\u200b\u8c01\u200b\u5408\u4f5c\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6709\u200b\u8fd9\u4e48\u200b\u591a\u200b\u7684\u200b\u9009\u9879\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u975e\u5e38\u200b\u4ee4\u4eba\u200b\u754f\u60e7\u200b\u3002</p> <p>\u200b\u6240\u4ee5\u200b\u6700\u597d\u200b\u4ece\u5c0f\u200b\u89c4\u6a21\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4fdd\u6301\u200b\u7b80\u5355\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u4e00\u79cd\u200b\u6700\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u662f\u200b\u4f7f\u7528\u200b Gradio \u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u6f14\u793a\u200b\u5e94\u7528\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u5176\u200b\u90e8\u7f72\u200b\u5728\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u5c06\u200b\u901a\u8fc7\u200b FoodVision Mini \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u4e00\u4e9b\u200b\u6258\u7ba1\u200b\u548c\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u5de5\u5177\u200b\u548c\u200b\u5730\u70b9\u200b\u3002\u200b\u8fd8\u6709\u200b\u5f88\u591a\u200b\u6211\u200b\u9057\u6f0f\u200b\u7684\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u6dfb\u52a0\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b GitHub \u200b\u8ba8\u8bba\u200b \u200b\u4e2d\u200b\u7559\u8a00\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u6211\u4eec\u200b\u5c06\u8981\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u5173\u4e8e\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u8bdd\u9898\u200b\u5df2\u7ecf\u200b\u8c08\u5f97\u591f\u200b\u591a\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6210\u4e3a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u5e08\u200b\uff0c\u200b\u5e76\u200b\u5b9e\u9645\u200b\u90e8\u7f72\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u901a\u8fc7\u200b\u4e00\u4e2a\u200b\u6f14\u793a\u200b\u6027\u200b\u7684\u200b Gradio \u200b\u5e94\u7528\u200b\u6765\u200b\u90e8\u7f72\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision \u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u8fbe\u5230\u200b\u4ee5\u4e0b\u200b\u6307\u6807\u200b\uff1a</p> <ol> <li>\u200b\u6027\u80fd\u200b\uff1a 95% \u200b\u4ee5\u4e0a\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002</li> <li>\u200b\u901f\u5ea6\u200b\uff1a \u200b\u5b9e\u65f6\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u8fbe\u5230\u200b 30FPS \u200b\u4ee5\u4e0a\u200b\uff08\u200b\u6bcf\u4e2a\u200b\u9884\u6d4b\u200b\u7684\u200b\u5ef6\u8fdf\u200b\u4f4e\u4e8e\u200b\u7ea6\u200b 0.03 \u200b\u79d2\u200b\uff09\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u6bd4\u8f83\u200b\u6211\u4eec\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u597d\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\uff1aEffNetB2 \u200b\u548c\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u90e8\u7f72\u200b\u6700\u200b\u63a5\u8fd1\u200b\u6211\u4eec\u200b\u76ee\u6807\u200b\u6307\u6807\u200b\u7684\u200b\u90a3\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u4e00\u4e2a\u200b\uff08\u200b\u5927\u200b\uff09\u200b\u60ca\u559c\u200b\u4f5c\u4e3a\u200b\u7ed3\u5c3e\u200b\u3002</p> \u200b\u4e3b\u9898\u200b \u200b\u5185\u5bb9\u200b 0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b \u200b\u6211\u4eec\u200b\u5728\u200b\u8fc7\u53bb\u200b\u51e0\u8282\u200b\u4e2d\u200b\u7f16\u5199\u200b\u4e86\u200b\u4e0d\u5c11\u200b\u6709\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u5b83\u200b\u5e76\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u518d\u6b21\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002 1. \u200b\u83b7\u53d6\u6570\u636e\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b <code>pizza_steak_sushi_20_percent.zip</code> \u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5728\u200b\u540c\u4e00\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3002 2. FoodVision Mini \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5b9e\u9a8c\u200b\u6982\u8ff0\u200b \u200b\u5373\u4f7f\u200b\u5728\u200b\u7b2c\u4e09\u4e2a\u200b\u91cc\u7a0b\u7891\u200b\u9879\u76ee\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4ecd\u7136\u200b\u4f1a\u200b\u8fd0\u884c\u200b\u591a\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4ee5\u200b\u67e5\u770b\u200b\u54ea\u4e2a\u200b\u6a21\u578b\u200b\uff08EffNetB2 \u200b\u6216\u200b ViT\uff09\u200b\u6700\u200b\u63a5\u8fd1\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u6307\u6807\u200b\u3002 3. \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b \u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u4e2d\u200b\uff0cEfficientNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u5b83\u200b\u4f5c\u4e3a\u200b\u90e8\u7f72\u200b\u7684\u200b\u5019\u9009\u200b\u6a21\u578b\u200b\u3002 4. \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b \u200b\u5728\u200b 08. PyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b \u200b\u4e2d\u200b\uff0cViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u5728\u200b\u6211\u4eec\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u5b83\u200b\u4f5c\u4e3a\u200b\u4e0e\u200b EffNetB2 \u200b\u5e76\u5217\u200b\u7684\u200b\u90e8\u7f72\u200b\u5019\u9009\u200b\u6a21\u578b\u200b\u3002 5. \u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8ba1\u65f6\u200b \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6784\u5efa\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8ddf\u8e2a\u200b\u7ed3\u679c\u200b\u3002 6. \u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u3001\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u548c\u200b\u5927\u5c0f\u200b \u200b\u8ba9\u200b\u6211\u4eec\u200b\u6bd4\u8f83\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u770b\u770b\u200b\u54ea\u4e2a\u200b\u5728\u200b\u76ee\u6807\u200b\u4e0a\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u3002 7. \u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b Gradio \u200b\u6f14\u793a\u200b\u5c06\u200b FoodVision Mini \u200b\u53d8\u4e3a\u200b\u73b0\u5b9e\u200b \u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e2d\u6709\u200b\u4e00\u4e2a\u200b\u5728\u200b\u76ee\u6807\u200b\u4e0a\u200b\u8868\u73b0\u200b\u66f4\u597d\u200b\uff0c\u200b\u6240\u4ee5\u200b\u8ba9\u200b\u6211\u4eec\u200b\u628a\u200b\u5b83\u200b\u53d8\u6210\u200b\u4e00\u4e2a\u200b\u53ef\u7528\u200b\u7684\u200b\u5e94\u7528\u200b\u6f14\u793a\u200b\uff01 8. \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini Gradio \u200b\u6f14\u793a\u200b\u53d8\u6210\u200b\u4e00\u4e2a\u200b\u53ef\u200b\u90e8\u7f72\u200b\u7684\u200b\u5e94\u7528\u200b \u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u5e94\u7528\u200b\u6f14\u793a\u200b\u5728\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\u826f\u597d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b\u90e8\u7f72\u200b\u505a\u597d\u200b\u51c6\u5907\u200b\uff01 9. \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\u90e8\u7f72\u200b\u5230\u200b HuggingFace Spaces \u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b FoodVision Mini \u200b\u5e26\u5230\u200b\u7f51\u7edc\u200b\u4e0a\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u200b\u5176\u200b\u516c\u5f00\u200b\u53ef\u200b\u8bbf\u95ee\u200b\uff01 10. \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5927\u200b\u60ca\u559c\u200b \u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6784\u5efa\u200b\u4e86\u200b FoodVision Mini\uff0c\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u63d0\u5347\u200b\u4e00\u4e2a\u200b\u6863\u6b21\u200b\u4e86\u200b\u3002 11. \u200b\u90e8\u7f72\u200b\u6211\u4eec\u200b\u7684\u200b BIG \u200b\u60ca\u559c\u200b \u200b\u90e8\u7f72\u200b\u4e00\u4e2a\u200b\u5e94\u7528\u200b\u5f88\u200b\u6709\u8da3\u200b\uff0c\u200b\u6211\u4eec\u200b\u518d\u6765\u4e00\u4e2a\u200b\u600e\u4e48\u6837\u200b\uff1f"},{"location":"09_pytorch_model_deployment/","title":"\u5728\u200b\u54ea\u91cc\u200b\u53ef\u4ee5\u200b\u83b7\u5f97\u200b\u5e2e\u52a9\u200b\uff1f\u00b6","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u6240\u6709\u200b\u8d44\u6599\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u5728\u200bGitHub\u200b\u4e0a\u200b\u627e\u5230\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u95ee\u9898\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u7684\u200bGitHub\u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b\u4e0a\u200b\u63d0\u95ee\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd8\u6709\u200bPyTorch\u200b\u6587\u6863\u200b\u548c\u200bPyTorch\u200b\u5f00\u53d1\u8005\u200b\u8bba\u575b\u200b\uff0c\u200b\u8fd9\u662f\u200b\u6240\u6709\u200bPyTorch\u200b\u76f8\u5173\u200b\u95ee\u9898\u200b\u7684\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u7684\u200b\u5730\u65b9\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#0","title":"0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u00b6","text":"<p>\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u5df2\u200b\u5b89\u88c5\u200b\u4e86\u200b\u672c\u8282\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u6a21\u5757\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bfc\u5165\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b Python \u200b\u811a\u672c\u200b\uff08\u200b\u4f8b\u5982\u200b <code>data_setup.py</code> \u200b\u548c\u200b <code>engine.py</code>\uff09\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>pytorch-deep-learning</code> \u200b\u4ed3\u5e93\u200b \u200b\u4e0b\u8f7d\u200b <code>going_modular</code> \u200b\u76ee\u5f55\u200b\uff08\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u4e0b\u8f7d\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u5c1a\u672a\u200b\u5b89\u88c5\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u83b7\u53d6\u200b <code>torchinfo</code> \u200b\u5305\u200b\u3002</p> <p><code>torchinfo</code> \u200b\u5c06\u200b\u5728\u200b\u540e\u7eed\u200b\u5e2e\u52a9\u200b\u6211\u4eec\u200b\u76f4\u89c2\u200b\u5730\u200b\u5c55\u793a\u200b\u6a21\u578b\u200b\u7ed3\u6784\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u540e\u7eed\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torchvision</code> v0.13 \u200b\u7248\u672c\u200b\uff08\u200b\u81ea\u200b 2022 \u200b\u5e74\u200b 7 \u200b\u6708\u200b\u8d77\u200b\u53ef\u7528\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u786e\u4fdd\u200b\u5df2\u200b\u5b89\u88c5\u200b\u6700\u65b0\u200b\u7248\u672c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u5e76\u4e14\u200b\u5c1a\u672a\u200b\u542f\u7528\u200b GPU\uff0c\u200b\u73b0\u5728\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>Runtime -&gt; Change runtime type -&gt; Hardware accelerator -&gt; GPU</code> \u200b\u6765\u200b\u542f\u7528\u200b GPU\u3002</p>"},{"location":"09_pytorch_model_deployment/#1","title":"1. \u200b\u83b7\u53d6\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b08. PyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u4e2d\u200b\u6bd4\u8f83\u200b\u4e86\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u6784\u5efa\u200b\u7684\u200b Vision Transformer (ViT) \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u4e0e\u200b\u5728\u200b07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b EfficientNetB2 (EffNetB2) \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b\u4e24\u8005\u200b\u5728\u200b\u6bd4\u8f83\u200b\u4e2d\u200b\u5b58\u5728\u200b\u7ec6\u5fae\u200b\u5dee\u5f02\u200b\u3002</p> <p>EffNetB2 \u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b 20% \u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u800c\u200b ViT \u200b\u6a21\u578b\u200b\u5219\u200b\u662f\u200b\u5728\u200b 10% \u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u90e8\u7f72\u200b\u6700\u200b\u9002\u5408\u200b FoodVision Mini \u200b\u95ee\u9898\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u4e0b\u8f7d\u200b20% \u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5176\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u548c\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff0c\u200b\u7136\u540e\u200b\u6bd4\u8f83\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u80fd\u200b\u8fdb\u884c\u200b\u82f9\u679c\u200b\u5bf9\u200b\u82f9\u679c\u200b\u7684\u200b\u6bd4\u8f83\u200b\uff08\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5728\u200b\u67d0\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u5728\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u6574\u4e2a\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\uff08101 \u200b\u79cd\u200b\u98df\u7269\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6bcf\u200b\u7c7b\u200b 1,000 \u200b\u5f20\u200b\u56fe\u7247\u200b\uff09\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6837\u672c\u200b\u3002\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c20% \u200b\u6307\u200b\u7684\u200b\u662f\u4ece\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u7c7b\u522b\u200b\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u7684\u200b 20% \u200b\u7684\u200b\u56fe\u7247\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b <code>extras/04_custom_data_creation.ipynb</code> \u200b\u4e2d\u200b\u67e5\u770b\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u5982\u4f55\u200b\u521b\u5efa\u200b\u7684\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u5728\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u7b2c\u200b 1 \u200b\u8282\u4e2d\u200b\u67e5\u770b\u200b\u66f4\u200b\u591a\u200b\u7ec6\u8282\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u7b2c\u200b 1 \u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>helper_functions.py</code> \u200b\u4e2d\u200b\u7684\u200b <code>download_data()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#2-foodvision-mini","title":"2. FoodVision Mini \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5b9e\u9a8c\u200b\u6982\u8ff0\u200b\u00b6","text":"<p>\u200b\u7406\u60f3\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u4e0d\u4ec5\u200b\u8868\u73b0\u51fa\u8272\u200b\uff0c\u200b\u800c\u4e14\u200b\u8fd0\u884c\u200b\u8fc5\u901f\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6a21\u578b\u200b\u5c3d\u53ef\u80fd\u200b\u63a5\u8fd1\u200b\u5b9e\u65f6\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5b9e\u65f6\u200b\u6027\u80fd\u200b\u6307\u200b\u7684\u200b\u662f\u200b\u7ea6\u200b 30 FPS\uff08\u200b\u6bcf\u79d2\u200b\u5e27\u200b\u6570\u200b\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u8fd9\u662f\u200b\u4eba\u7c7b\u200b\u773c\u775b\u200b\u80fd\u200b\u770b\u5230\u200b\u7684\u200b\u901f\u5ea6\u200b\uff08\u200b\u5bf9\u6b64\u200b\u6709\u200b\u4e89\u8bae\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5c06\u200b 30 FPS \u200b\u4f5c\u4e3a\u200b\u57fa\u51c6\u200b\uff09\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u5206\u7c7b\u200b\u4e09\u79cd\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\uff08\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6a21\u578b\u200b\u8fbe\u5230\u200b 95% \u200b\u4ee5\u4e0a\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u66f4\u200b\u9ad8\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u4f1a\u200b\u66f4\u597d\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u727a\u7272\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\uff1a</p> <ol> <li>\u200b\u6027\u80fd\u200b - \u200b\u4e00\u4e2a\u200b\u51c6\u786e\u7387\u200b\u8d85\u8fc7\u200b 95% \u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u901f\u5ea6\u200b - \u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u4ee5\u7ea6\u200b 30 FPS \u200b\u7684\u200b\u901f\u5ea6\u200b\u5206\u7c7b\u200b\u56fe\u50cf\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u6bcf\u5f20\u200b\u56fe\u50cf\u200b\u7684\u200b\u63a8\u7406\u200b\u65f6\u95f4\u200b\u4e3a\u200b 0.03 \u200b\u79d2\u200b\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u5ef6\u8fdf\u200b\uff09\u3002</li> </ol> <p>FoodVision Mini \u200b\u90e8\u7f72\u200b\u76ee\u6807\u200b\u3002\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5feb\u901f\u200b\u4e14\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u9884\u6d4b\u200b\u6a21\u578b\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6162\u901f\u200b\u7684\u200b\u5e94\u7528\u200b\u662f\u200b\u65e0\u8da3\u200b\u7684\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u70b9\u200b\u653e\u5728\u200b\u901f\u5ea6\u200b\u4e0a\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\uff0c\u200b\u6211\u4eec\u200b\u66f4\u200b\u503e\u5411\u200b\u4e8e\u200b\u4e00\u4e2a\u200b\u5728\u200b 30 FPS \u200b\u4e0b\u200b\u8fbe\u5230\u200b 90% \u200b\u4ee5\u4e0a\u200b\u51c6\u786e\u7387\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4e00\u4e2a\u200b\u5728\u200b 10 FPS \u200b\u4e0b\u200b\u8fbe\u5230\u200b 95% \u200b\u4ee5\u4e0a\u200b\u51c6\u786e\u7387\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5c1d\u8bd5\u200b\u5b9e\u73b0\u200b\u8fd9\u4e9b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f15\u5165\u200b\u4e4b\u524d\u200b\u90e8\u5206\u200b\u4e2d\u200b\u8868\u73b0\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <ol> <li>EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff08\u200b\u7b80\u79f0\u200b EffNetB2\uff09- \u200b\u6700\u521d\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b\u90e8\u5206\u200b 7.5 \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torchvision.models.efficientnet_b2()</code> \u200b\u5e76\u200b\u8c03\u6574\u200b\u4e86\u200b <code>classifier</code> \u200b\u5c42\u200b\u3002</li> <li>ViT-B/16 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff08\u200b\u7b80\u79f0\u200b ViT\uff09- \u200b\u6700\u521d\u200b\u5728\u200b 08. PyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u90e8\u5206\u200b 10 \u200b\u4e2d\u200b\u521b\u5efa\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torchvision.models.vit_b_16()</code> \u200b\u5e76\u200b\u8c03\u6574\u200b\u4e86\u200b <code>head</code> \u200b\u5c42\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b ViT-B/16 \u200b\u4ee3\u8868\u200b\u201cVision Transformer Base\uff0cpatch size 16\u201d\u3002</li> </ul> </li> </ol> <p></p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u201c\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u201d\u200b\u901a\u5e38\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u5728\u200b\u7c7b\u4f3c\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u8fc7\u200b\u7684\u200b\u6a21\u578b\u200b\u5f00\u59cb\u200b\u3002\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\u901a\u5e38\u200b\u4fdd\u6301\u200b\u51bb\u7ed3\u200b\uff08\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6a21\u5f0f\u200b/\u200b\u6743\u91cd\u200b\u4fdd\u6301\u200b\u4e0d\u53d8\u200b\uff09\uff0c\u200b\u800c\u200b\u4e00\u4e9b\u200b\u9876\u5c42\u200b\uff08\u200b\u6216\u200b\u5206\u7c7b\u5668\u200b/\u200b\u5206\u7c7b\u200b\u5934\u200b\uff09\u200b\u4f1a\u200b\u6839\u636e\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5b9a\u5236\u200b\u8bad\u7ec3\u200b\u3002\u200b\u6211\u4eec\u200b\u5728\u200b 06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u90e8\u5206\u200b 3.4 \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u6982\u5ff5\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#3-effnetb2","title":"3. \u200b\u521b\u5efa\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u7b2c\u200b 7.5 \u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u5728\u200b\u8be5\u8282\u200b\u7684\u200b\u672b\u5c3e\u200b\uff0c\u200b\u6211\u4eec\u200b\u770b\u5230\u200b\u5b83\u200b\u7684\u200b\u8868\u73b0\u200b\u975e\u5e38\u200b\u51fa\u8272\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u5b83\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5c06\u200b\u5176\u200b\u7ed3\u679c\u200b\u4e0e\u200b\u5728\u200b\u76f8\u540c\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\uff1a</p> <ol> <li>\u200b\u5c06\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6743\u91cd\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>weights=torchvision.models.EfficientNet_B2_Weights.DEFAULT</code>\uff0c\u200b\u5176\u4e2d\u200b \"<code>DEFAULT</code>\" \u200b\u8868\u793a\u200b \"\u200b\u5f53\u524d\u200b\u6700\u4f73\u200b\u53ef\u7528\u200b\"\uff08\u200b\u6216\u8005\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>weights=\"DEFAULT\"</code>\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>transforms()</code> \u200b\u65b9\u6cd5\u200b\u4ece\u200b\u6743\u91cd\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u56fe\u50cf\u200b\u53d8\u6362\u200b\uff08\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u8fd9\u4e9b\u200b\u53d8\u6362\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e0e\u200b\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b EffNetB2 \u200b\u8bad\u7ec3\u200b\u65f6\u200b\u76f8\u540c\u200b\u7684\u200b\u683c\u5f0f\u200b\uff09\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5c06\u200b\u6743\u91cd\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>torchvision.models.efficientnet_b2</code> \u200b\u7684\u200b\u5b9e\u4f8b\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\u3002</li> <li>\u200b\u51bb\u7ed3\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u57fa\u7840\u200b\u5c42\u200b\u3002</li> <li>\u200b\u66f4\u65b0\u200b\u5206\u7c7b\u5668\u200b\u5934\u4ee5\u200b\u9002\u5e94\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u3002</li> </ol>"},{"location":"09_pytorch_model_deployment/#31-effnetb2","title":"3.1 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u751f\u6210\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u8fd9\u91cc\u200b\u6d89\u53ca\u200b\u7684\u200b\u6b65\u9aa4\u200b\u8f83\u200b\u591a\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u59a8\u200b\u5c06\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u5c01\u88c5\u200b\u6210\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u540e\u7eed\u200b\u590d\u7528\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u547d\u540d\u200b\u4e3a\u200b <code>create_effnetb2_model()</code>\uff0c\u200b\u5e76\u200b\u5141\u8bb8\u200b\u7528\u6237\u200b\u81ea\u5b9a\u4e49\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\u548c\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u53c2\u6570\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u53ef\u91cd\u590d\u6027\u200b\u3002</p> <p>\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u5c06\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u53ca\u5176\u200b\u5173\u8054\u200b\u7684\u200b\u8f6c\u6362\u200b\u64cd\u4f5c\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#32-effnetb2dataloader","title":"3.2 \u200b\u4e3a\u200bEffNetB2\u200b\u521b\u5efa\u200bDataLoader\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u662f\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b<code>DataLoader</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b05. PyTorch Going Modular\u200b\u7b2c\u200b2\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>data_setup.create_dataloaders()</code>\u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>batch_size</code>\u200b\u4e3a\u200b32\uff0c\u200b\u5e76\u200b\u901a\u8fc7\u200b<code>effnetb2_transforms</code>\u200b\u5bf9\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u53d8\u6362\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u5b83\u4eec\u200b\u4e0e\u200b\u6211\u4eec\u200b\u7684\u200b<code>effnetb2</code>\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u683c\u5f0f\u200b\u4e00\u81f4\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#33-effnetb2","title":"3.3 \u200b\u8bad\u7ec3\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u6a21\u578b\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c<code>DataLoader</code> \u200b\u4e5f\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u5427\u200b\uff01</p> <p>\u200b\u5c31\u200b\u50cf\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ddf\u8e2a\u200b \u200b\u7b2c\u200b 7.6 \u200b\u8282\u200b \u200b\u4e2d\u200b\u4e00\u6837\u200b\uff0c\u200b\u5341\u4e2a\u200b epoch \u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\u83b7\u5f97\u200b\u826f\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b <code>1e-3</code> \u200b\u7684\u200b <code>torch.optim.Adam()</code>\uff09\u3001\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u9002\u7528\u200b\u4e8e\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u7684\u200b <code>torch.nn.CrossEntropyLoss()</code>\uff09\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u4ee5\u53ca\u200b\u6211\u4eec\u200b\u7684\u200b <code>DataLoader</code> \u200b\u4f20\u9012\u200b\u7ed9\u200b\u6211\u4eec\u200b\u5728\u200b 05. PyTorch \u200b\u6a21\u5757\u5316\u200b \u200b\u7b2c\u200b 4 \u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>engine.train()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#34-effnetb2","title":"3.4 \u200b\u68c0\u67e5\u200b EffNetB2 \u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u5f88\u200b\u597d\u200b\uff01</p> <p>\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u5728\u200b 07. PyTorch \u200b\u5b9e\u9a8c\u200b\u8ffd\u8e2a\u200b\u4e2d\u6240\u89c1\u200b\uff0cEffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u8868\u73b0\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u7ed3\u679c\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u8fdb\u4e00\u6b65\u200b\u68c0\u67e5\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u662f\u200b\u53ef\u89c6\u5316\u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u4e4b\u4e00\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u7b2c\u200b 8 \u200b\u8282\u200b\uff1a\u200b\u7406\u60f3\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u5e94\u8be5\u200b\u662f\u200b\u4ec0\u4e48\u6837\u200b\u7684\u200b\uff1f</p>"},{"location":"09_pytorch_model_deployment/#35-effnetb2","title":"3.5 \u200b\u4fdd\u5b58\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u7a0d\u540e\u200b\u5bfc\u5165\u200b\u548c\u200b\u4f7f\u7528\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u4fdd\u5b58\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b05. PyTorch Going Modular \u200b\u7b2c\u200b5\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>utils.save_model()</code>\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b<code>target_dir</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b<code>\"models\"</code>\uff0c\u200b\u5e76\u200b\u5c06\u200b<code>model_name</code>\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b<code>\"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"</code>\uff08\u200b\u867d\u7136\u200b\u6709\u70b9\u200b\u8be6\u7ec6\u200b\uff0c\u200b\u4f46\u200b\u81f3\u5c11\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u53d1\u751f\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#36-effnetb2","title":"3.6 \u200b\u68c0\u67e5\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7684\u200b\u5927\u5c0f\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\u4ee5\u200b\u652f\u6301\u200bFoodVision Mini\u200b\u7684\u200b\u6807\u51c6\u200b\u4e4b\u4e00\u200b\u662f\u200b\u901f\u5ea6\u200b\uff08~30FPS\u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\uff09\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u68c0\u67e5\u200b\u5927\u5c0f\u200b\uff1f</p> <p>\u200b\u867d\u7136\u200b\u5e76\u975e\u200b\u603b\u662f\u200b\u5982\u6b64\u200b\uff0c\u200b\u4f46\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\u53ef\u4ee5\u200b\u5f71\u54cd\u200b\u5176\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c\u200b\u5b83\u200b\u901a\u5e38\u200b\u4f1a\u200b\u6267\u884c\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u800c\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u4e2d\u200b\u7684\u200b\u6bcf\u200b\u4e00\u4e2a\u200b\u90fd\u200b\u9700\u8981\u200b\u4e00\u5b9a\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5728\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u6709\u9650\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u5de5\u4f5c\u200b\uff08\u200b\u4f8b\u5982\u200b\u5728\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u6216\u200b\u7f51\u9875\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\uff09\uff0c\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u6a21\u578b\u200b\u8d8a\u5c0f\u8d8a\u200b\u597d\u200b\uff08\u200b\u53ea\u8981\u200b\u5b83\u200b\u5728\u200b\u51c6\u786e\u6027\u200b\u65b9\u9762\u200b\u4ecd\u7136\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\uff09\u3002</p> <p>\u200b\u8981\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\uff08\u200b\u4ee5\u200b\u5b57\u8282\u200b\u4e3a\u200b\u5355\u4f4d\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bPython\u200b\u7684\u200b<code>pathlib.Path.stat(\"path_to_model\").st_size</code>\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u5176\u200b\u9664\u4ee5\u200b<code>(1024*1024)</code>\u200b\u6765\u200b\u7c97\u7565\u5730\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5146\u200b\u5b57\u8282\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#37-effnetb2","title":"3.7 \u200b\u6536\u96c6\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7684\u200b\u7edf\u8ba1\u200b\u4fe1\u606f\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u83b7\u5f97\u200b\u4e86\u200b\u4e00\u4e9b\u200b\u5173\u4e8e\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u7edf\u8ba1\u6570\u636e\u200b\uff0c\u200b\u4f8b\u5982\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u3001\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\u548c\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u3002\u200b\u4e3a\u4e86\u200b\u4fbf\u4e8e\u200b\u4e0e\u200b\u5373\u5c06\u200b\u63a8\u51fa\u200b\u7684\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u6536\u96c6\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u589e\u52a0\u200b\u8da3\u5473\u6027\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u4e00\u4e2a\u200b\u989d\u5916\u200b\u7684\u200b\u7edf\u8ba1\u6570\u636e\u200b\uff1a\u200b\u603b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8ba1\u7b97\u200b<code>effnetb2.parameters()</code>\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\uff08\u200b\u6216\u200b\u6a21\u5f0f\u200b/\u200b\u6743\u91cd\u200b\uff09\u200b\u6570\u91cf\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>torch.numel()</code>\uff08\"number of elements\"\u200b\u7684\u200b\u7f29\u5199\u200b\uff09\u200b\u65b9\u6cd5\u200b\u6765\u200b\u8bbf\u95ee\u200b\u6bcf\u4e2a\u200b\u53c2\u6570\u200b\u4e2d\u200b\u7684\u200b\u5143\u7d20\u200b\u6570\u91cf\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#4-vit","title":"4. \u200b\u521b\u5efa\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u63a5\u4e0b\u6765\u200b\u7ee7\u7eed\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u5b9e\u9a8c\u200b\u3002</p> <p>\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u4e0e\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u4f7f\u7528\u200b <code>torchvision.models.vit_b_16()</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>torchvision.models.efficientnet_b2()</code>\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>create_vit_model()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u4e0e\u200b <code>create_effnetb2_model()</code> \u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\uff0c\u200b\u5f53\u7136\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u548c\u200b\u53d8\u6362\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b EffNetB2\u3002</p> <p>\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7ec6\u5fae\u5dee\u522b\u200b\u662f\u200b\uff0c<code>torchvision.models.vit_b_16()</code> \u200b\u7684\u200b\u8f93\u51fa\u200b\u5c42\u200b\u79f0\u4e3a\u200b <code>heads</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>classifier</code>\u3002</p>"},{"location":"09_pytorch_model_deployment/#41-vit-dataloader","title":"4.1 \u200b\u4e3a\u200b ViT \u200b\u521b\u5efa\u200b DataLoader\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e3a\u200b\u5b83\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b <code>DataLoader</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u4e0e\u200b\u4e3a\u200b EffNetB2 \u200b\u521b\u5efa\u200b DataLoader \u200b\u76f8\u540c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u6211\u4eec\u200b\u4f1a\u200b\u4f7f\u7528\u200b <code>vit_transforms</code> \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u8f6c\u6362\u6210\u200b ViT \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u7684\u200b\u76f8\u540c\u200b\u683c\u5f0f\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#42-vit","title":"4.2 \u200b\u8bad\u7ec3\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u77e5\u9053\u200b\u73b0\u5728\u200b\u662f\u200b\u4ec0\u4e48\u200b\u65f6\u5019\u200b\u5417\u200b...</p> <p>...\u200b\u662f\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff08\u200b\u7528\u200b\u4e0e\u200b\u6b4c\u66f2\u200b\u300aClosing Time\u300b\u200b\u76f8\u540c\u200b\u7684\u200b\u65cb\u5f8b\u200b\u5531\u51fa\u6765\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>engine.train()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u7ed3\u5408\u200b <code>torch.optim.Adam()</code> \u200b\u4f18\u5316\u200b\u5668\u200b\uff08\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b <code>1e-3</code>\uff09\u200b\u548c\u200b <code>torch.nn.CrossEntropyLoss()</code> \u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u5bf9\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b 10 \u200b\u4e2a\u200b\u5468\u671f\u200b\u7684\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u5728\u200b\u8bad\u7ec3\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>set_seeds()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u200b\u5c3d\u53ef\u80fd\u200b\u786e\u4fdd\u200b\u7ed3\u679c\u200b\u7684\u200b\u53ef\u91cd\u590d\u6027\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#43-vit","title":"4.3 \u200b\u68c0\u67e5\u200b ViT \u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u597d\u200b\u4e86\u200b\uff0cViT \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u5b8c\u6210\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u76f4\u89c2\u200b\u5730\u200b\u770b\u770b\u200b\u4e00\u4e9b\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u522b\u5fd8\u4e86\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b8\u200b\u8282\u4e2d\u200b\u67e5\u770b\u200b\u4e00\u7ec4\u200b\u7406\u60f3\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u5e94\u8be5\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6837\u5b50\u200b\u7684\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#44-vit","title":"4.4 \u200b\u4fdd\u5b58\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b ViT \u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u975e\u5e38\u200b\u51fa\u8272\u200b\uff01</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u65e5\u540e\u200b\u53ef\u4ee5\u200b\u5bfc\u5165\u200b\u5e76\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u7b2c\u200b5\u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>utils.save_model()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#45-vit","title":"4.5 \u200b\u68c0\u67e5\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7684\u200b\u5927\u5c0f\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5728\u200b\u591a\u4e2a\u200b\u7279\u6027\u200b\u4e0a\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u4e0e\u200b ViT \u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u4e86\u89e3\u200b\u4e00\u4e0b\u200b\u5b83\u200b\u7684\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u8981\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u7684\u200b\u5927\u5c0f\u200b\uff08\u200b\u4ee5\u200b\u5b57\u8282\u200b\u4e3a\u200b\u5355\u4f4d\u200b\uff09\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>pathlib.Path.stat(\"path_to_model\").st_size</code>\uff0c\u200b\u7136\u540e\u200b\u901a\u8fc7\u200b\u5c06\u200b\u5176\u200b\u9664\u4ee5\u200b <code>(1024*1024)</code> \u200b\u6765\u200b\uff08\u200b\u5927\u81f4\u200b\uff09\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u5146\u200b\u5b57\u8282\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#46-vit","title":"4.6 \u200b\u6536\u96c6\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7684\u200b\u7edf\u8ba1\u200b\u4fe1\u606f\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6c47\u603b\u200b\u6240\u6709\u200bViT\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u7684\u200b\u7edf\u8ba1\u6570\u636e\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0a\u9762\u200b\u7684\u200b\u603b\u7ed3\u200b\u8f93\u51fa\u200b\u4e2d\u200b\u770b\u5230\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u5176\u200b\u603b\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#5","title":"5. \u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8ba1\u65f6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u8bad\u7ec3\u200b\u5f97\u200b\u76f8\u5f53\u200b\u4e0d\u9519\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u5b83\u4eec\u200b\u662f\u5426\u200b\u80fd\u200b\u5b8c\u6210\u200b\u6211\u4eec\u200b\u671f\u671b\u200b\u5b83\u4eec\u200b\u5b8c\u6210\u200b\u7684\u200b\u4efb\u52a1\u200b\u3002</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u4eec\u200b\u5728\u200b\u505a\u51fa\u200b\u9884\u6d4b\u200b\uff08\u200b\u6267\u884c\u200b\u63a8\u7406\u200b\uff09\u200b\u65f6\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\u5728\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u51c6\u786e\u7387\u200b\u90fd\u200b\u8d85\u8fc7\u200b\u4e86\u200b95%\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u7684\u200b\u8fd0\u884c\u200b\u901f\u5ea6\u200b\u5982\u4f55\u200b\u5462\u200b\uff1f</p> <p>\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u6253\u7b97\u200b\u5c06\u200bFoodVision Mini\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5230\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u8ba9\u200b\u7528\u6237\u200b\u62cd\u6444\u200b\u98df\u7269\u200b\u7167\u7247\u200b\u5e76\u200b\u8fdb\u884c\u200b\u8bc6\u522b\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u9884\u6d4b\u200b\u80fd\u591f\u200b\u5728\u200b\u5b9e\u65f6\u200b\uff08\u200b\u5927\u7ea6\u200b\u6bcf\u79d2\u200b30\u200b\u5e27\u200b\uff09\u200b\u4e0b\u200b\u8fdb\u884c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u4e3a\u4ec0\u4e48\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e8c\u4e2a\u200b\u6807\u51c6\u200b\u662f\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u5feb\u901f\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u627e\u51fa\u200b\u6bcf\u4e2a\u200b\u6a21\u578b\u200b\u6267\u884c\u200b\u63a8\u7406\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>pred_and_store()</code>\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u9010\u4e2a\u200b\u8fed\u4ee3\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u6bcf\u5f20\u200b\u56fe\u50cf\u200b\u5e76\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bb0\u5f55\u200b\u6bcf\u6b21\u200b\u9884\u6d4b\u200b\u7684\u200b\u65f6\u95f4\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u7ed3\u679c\u200b\u5b58\u50a8\u200b\u5728\u200b\u4e00\u4e2a\u200b\u901a\u7528\u200b\u7684\u200b\u9884\u6d4b\u200b\u683c\u5f0f\u200b\u4e2d\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\uff08\u200b\u5176\u4e2d\u200b\u5217\u8868\u200b\u7684\u200b\u6bcf\u4e2a\u200b\u5143\u7d20\u200b\u662f\u200b\u4e00\u6b21\u200b\u9884\u6d4b\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u9884\u6d4b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u9010\u4e2a\u200b\u8ba1\u65f6\u200b\u9884\u6d4b\u200b\u800c\u200b\u4e0d\u662f\u200b\u6279\u91cf\u200b\u8ba1\u65f6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u65f6\u200b\uff0c\u200b\u5b83\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4e00\u6b21\u200b\u53ea\u200b\u5bf9\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u7528\u6237\u200b\u62cd\u6444\u200b\u4e00\u5f20\u200b\u7167\u7247\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u90a3\u200b\u5f20\u200b\u5355\u4e00\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u96c6\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u9996\u5148\u200b\u8ba9\u200b\u6211\u4eec\u200b\u83b7\u53d6\u200b\u6240\u6709\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u8fed\u4ee3\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bPython\u200b\u7684\u200b<code>pathlib.Path(\"target_dir\").glob(\"*/*.jpg\"))</code>\u200b\u6765\u200b\u67e5\u627e\u200b\u76ee\u6807\u76ee\u5f55\u200b\u4e2d\u200b\u6240\u6709\u200b\u6269\u5c55\u200b\u540d\u4e3a\u200b<code>.jpg</code>\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\uff08\u200b\u6211\u4eec\u200b\u6240\u6709\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#51","title":"5.1 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u8def\u5f84\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u7f16\u5199\u200b <code>pred_and_store()</code> \u200b\u51fd\u6570\u200b\uff1a</p> <ol> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u8be5\u200b\u51fd\u6570\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u8def\u5f84\u200b\u5217\u8868\u200b\u3001\u200b\u4e00\u4e2a\u200b\u7ecf\u8fc7\u8bad\u7ec3\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u3001\u200b\u4e00\u7cfb\u5217\u200b\u8f6c\u6362\u200b\uff08\u200b\u7528\u4e8e\u200b\u51c6\u5907\u200b\u56fe\u50cf\u200b\uff09\u3001\u200b\u76ee\u6807\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u5217\u8868\u200b\u548c\u200b\u4e00\u4e2a\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7a7a\u200b\u5217\u8868\u200b\u6765\u200b\u5b58\u50a8\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\uff08\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u51fd\u6570\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5b57\u5178\u200b\u5bf9\u5e94\u200b\u4e00\u4e2a\u200b\u9884\u6d4b\u200b\uff09\u3002</li> <li>\u200b\u904d\u5386\u200b\u76ee\u6807\u200b\u8f93\u5165\u200b\u8def\u5f84\u200b\uff08\u200b\u6b65\u9aa4\u200b 4-14 \u200b\u5c06\u200b\u5728\u200b\u5faa\u73af\u200b\u5185\u90e8\u200b\u8fdb\u884c\u200b\uff09\u3002</li> <li>\u200b\u4e3a\u200b\u6bcf\u6b21\u200b\u5faa\u73af\u200b\u8fed\u4ee3\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7a7a\u200b\u5b57\u5178\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b\u6bcf\u4e2a\u200b\u6837\u672c\u200b\u7684\u200b\u9884\u6d4b\u503c\u200b\u3002</li> <li>\u200b\u83b7\u53d6\u200b\u6837\u672c\u200b\u8def\u5f84\u200b\u548c\u200b\u771f\u5b9e\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\uff08\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8def\u5f84\u200b\u63a8\u65ad\u200b\u7c7b\u522b\u200b\uff09\u3002</li> <li>\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>timeit.default_timer()</code> \u200b\u5f00\u59cb\u200b\u9884\u6d4b\u200b\u8ba1\u65f6\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>PIL.Image.open(path)</code> \u200b\u6253\u5f00\u200b\u56fe\u50cf\u200b\u3002</li> <li>\u200b\u8f6c\u6362\u200b\u56fe\u50cf\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u80fd\u591f\u200b\u88ab\u200b\u76ee\u6807\u200b\u6a21\u578b\u200b\u4f7f\u7528\u200b\uff0c\u200b\u540c\u65f6\u200b\u6dfb\u52a0\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u5e76\u200b\u5c06\u200b\u56fe\u50cf\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5c06\u200b\u6a21\u578b\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u5e76\u200b\u5f00\u542f\u200b <code>eval()</code> \u200b\u6a21\u5f0f\u200b\u6765\u200b\u51c6\u5907\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u63a8\u7406\u200b\u3002</li> <li>\u200b\u5f00\u542f\u200b <code>torch.inference_mode()</code>\uff0c\u200b\u5c06\u200b\u76ee\u6807\u200b\u8f6c\u6362\u200b\u540e\u200b\u7684\u200b\u56fe\u50cf\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torch.softmax()</code> \u200b\u8ba1\u7b97\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>torch.argmax()</code> \u200b\u8ba1\u7b97\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u3002</li> <li>\u200b\u5c06\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u548c\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6b65\u9aa4\u200b 4 \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\u4e2d\u200b\u3002\u200b\u540c\u65f6\u200b\u786e\u4fdd\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u540e\u7eed\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u975e\u200b GPU \u200b\u5e93\u200b\uff08\u200b\u5982\u200b NumPy \u200b\u548c\u200b pandas\uff09\u200b\u8fdb\u884c\u200b\u68c0\u67e5\u200b\u3002</li> <li>\u200b\u7ed3\u675f\u200b\u6b65\u9aa4\u200b 6 \u200b\u5f00\u59cb\u200b\u7684\u200b\u9884\u6d4b\u200b\u8ba1\u65f6\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u65f6\u95f4\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6b65\u9aa4\u200b 4 \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u68c0\u67e5\u200b\u9884\u6d4b\u200b\u7c7b\u522b\u200b\u662f\u5426\u200b\u4e0e\u200b\u6b65\u9aa4\u200b 5 \u200b\u4e2d\u200b\u7684\u200b\u771f\u5b9e\u200b\u7c7b\u522b\u200b\u5339\u914d\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u7ed3\u679c\u200b\u6dfb\u52a0\u200b\u5230\u200b\u6b65\u9aa4\u200b 4 \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u5c06\u200b\u66f4\u65b0\u200b\u540e\u200b\u7684\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\u8ffd\u52a0\u200b\u5230\u200b\u6b65\u9aa4\u200b 2 \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u7a7a\u200b\u9884\u6d4b\u200b\u5217\u8868\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u8fd4\u56de\u200b\u9884\u6d4b\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\u3002</li> </ol> <p>\u200b\u867d\u7136\u200b\u6b65\u9aa4\u200b\u5f88\u591a\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5b8c\u5168\u200b\u53ef\u4ee5\u200b\u5e94\u5bf9\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u5427\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#52-effnetb2","title":"5.2 \u200b\u4f7f\u7528\u200bEffNetB2\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u53ca\u200b\u8ba1\u65f6\u200b\u00b6","text":"<p>\u200b\u662f\u200b\u65f6\u5019\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200b <code>pred_and_store()</code> \u200b\u51fd\u6570\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\u5b83\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u540c\u65f6\u200b\u6ce8\u610f\u200b\u4ee5\u4e0b\u200b\u4e24\u4e2a\u200b\u7ec6\u8282\u200b\uff1a</p> <ol> <li>\u200b\u8bbe\u5907\u200b - \u200b\u6211\u4eec\u200b\u5c06\u200b <code>device</code> \u200b\u53c2\u6570\u200b\u786c\u200b\u7f16\u7801\u200b\u4e3a\u200b\u4f7f\u7528\u200b <code>\"cpu\"</code>\uff0c\u200b\u56e0\u4e3a\u200b\u5728\u200b\u90e8\u7f72\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e76\u200b\u4e0d\u200b\u603b\u662f\u200b\u80fd\u200b\u8bbf\u95ee\u200b\u5230\u200b <code>\"cuda\"</code>\uff08GPU\uff09\u200b\u8bbe\u5907\u200b\u3002<ul> <li>\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u6307\u793a\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u901a\u5e38\u200b\u5728\u200bCPU\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u7684\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u6bd4\u200bGPU\u200b\u8bbe\u5907\u200b\u6162\u200b\u3002</li> </ul> </li> <li>\u200b\u8f6c\u6362\u200b - \u200b\u6211\u4eec\u200b\u8fd8\u8981\u200b\u786e\u4fdd\u200b\u5c06\u200b <code>transform</code> \u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b <code>effnetb2_transforms</code>\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u56fe\u50cf\u200b\u4ee5\u200b\u4e0e\u200b <code>effnetb2</code> \u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u76f8\u540c\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6253\u5f00\u200b\u548c\u200b\u8f6c\u6362\u200b\u3002</li> </ol>"},{"location":"09_pytorch_model_deployment/#53-vit","title":"5.3 \u200b\u4f7f\u7528\u200bViT\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u53ca\u200b\u8ba1\u65f6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4f7f\u7528\u200bEffNetB2\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e86\u200b\u9884\u6d4b\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bf9\u200bViT\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u540c\u6837\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e0a\u9762\u200b\u521b\u5efa\u200b\u7684\u200b<code>pred_and_store()</code>\u200b\u51fd\u6570\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f20\u5165\u200b\u6211\u4eec\u200b\u7684\u200b<code>vit</code>\u200b\u6a21\u578b\u200b\u4ee5\u53ca\u200b<code>vit_transforms</code>\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u5c06\u200b\u9884\u6d4b\u200b\u4fdd\u6301\u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u901a\u8fc7\u200b<code>device=\"cpu\"</code>\uff08\u200b\u8fd9\u91cc\u200b\u7684\u200b\u4e00\u4e2a\u200b\u81ea\u7136\u200b\u6269\u5c55\u200b\u662f\u200b\u6d4b\u8bd5\u200b\u5728\u200bCPU\u200b\u548c\u200bGPU\u200b\u4e0a\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#6","title":"6. \u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u3001\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u548c\u200b\u5927\u5c0f\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b\u4e24\u4e2a\u200b\u6700\u4f73\u200b\u6a21\u578b\u200b\u5019\u9009\u200b\u5df2\u7ecf\u200b\u5b8c\u6210\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u6b63\u9762\u200b\u6bd4\u8f83\u200b\uff0c\u200b\u5e76\u200b\u5bf9\u6bd4\u200b\u5b83\u4eec\u200b\u7684\u200b\u4e0d\u540c\u200b\u7edf\u8ba1\u6570\u636e\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b <code>effnetb2_stats</code> \u200b\u548c\u200b <code>vit_stats</code> \u200b\u5b57\u5178\u200b\u8f6c\u6362\u200b\u4e3a\u200b pandas DataFrame\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u6dfb\u52a0\u200b\u4e00\u5217\u200b\u6765\u200b\u663e\u793a\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u7387\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u767e\u5206\u6bd4\u200b\u5f62\u5f0f\u200b\uff0c\u200b\u800c\u200b\u975e\u200b\u5c0f\u6570\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#61","title":"6.1 \u200b\u53ef\u89c6\u5316\u200b\u901f\u5ea6\u200b\u4e0e\u200b\u6027\u80fd\u200b\u7684\u200b\u6743\u8861\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\uff0c\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u51c6\u786e\u6027\u200b\u7b49\u200b\u6027\u80fd\u6307\u6807\u200b\u65b9\u9762\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200bViT\u200b\u6a21\u578b\u200b\u4f18\u4e8e\u200bEffNetB2\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0cEffNetB2\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6a21\u578b\u200b\u5c3a\u5bf8\u200b\u8981\u200b\u5c0f\u5f97\u591a\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6027\u80fd\u200b\u6216\u200b\u63a8\u7406\u200b\u65f6\u95f4\u200b\u901a\u5e38\u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u5ef6\u8fdf\u200b\u201d\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5c06\u200b\u8fd9\u200b\u4e00\u200b\u4e8b\u5b9e\u200b\u53ef\u89c6\u5316\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200bmatplotlib\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u56fe\u8868\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u4ece\u200b\u6bd4\u8f83\u200bDataFrame\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6563\u70b9\u56fe\u200b\uff0c\u200b\u6bd4\u8f83\u200bEffNetB2\u200b\u548c\u200bViT\u200b\u7684\u200b<code>time_per_pred_cpu</code>\u200b\u548c\u200b<code>test_acc</code>\u200b\u503c\u200b\u3002</li> <li>\u200b\u6839\u636e\u200b\u6570\u636e\u200b\u6dfb\u52a0\u200b\u6807\u9898\u200b\u548c\u200b\u6807\u7b7e\u200b\uff0c\u200b\u5e76\u200b\u8c03\u6574\u200b\u5b57\u4f53\u5927\u5c0f\u200b\u4ee5\u200b\u589e\u5f3a\u200b\u7f8e\u89c2\u200b\u6027\u200b\u3002</li> <li>\u200b\u5728\u200b\u6b65\u9aa4\u200b1\u200b\u7684\u200b\u6563\u70b9\u56fe\u200b\u4e0a\u200b\u6807\u6ce8\u200b\u6837\u672c\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u9002\u5f53\u200b\u7684\u200b\u6807\u7b7e\u200b\uff08\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b\uff09\u200b\u8fdb\u884c\u200b\u6807\u6ce8\u200b\u3002</li> <li>\u200b\u6839\u636e\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\uff08<code>model_size (MB)</code>\uff09\u200b\u521b\u5efa\u200b\u56fe\u4f8b\u200b\u3002</li> </ol>"},{"location":"09_pytorch_model_deployment/#7-gradio-foodvision-mini","title":"7. \u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b Gradio \u200b\u6f14\u793a\u200b\u8ba9\u200b FoodVision Mini \u200b\u7115\u53d1\u200b\u751f\u673a\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u51b3\u5b9a\u200b\u9996\u5148\u200b\u90e8\u7f72\u200b EffNetB2 \u200b\u6a21\u578b\u200b\uff08\u200b\u5f53\u7136\u200b\uff0c\u200b\u4ee5\u540e\u200b\u53ef\u4ee5\u200b\u968f\u65f6\u200b\u66f4\u6539\u200b\uff09\u3002</p> <p>\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6709\u200b\u591a\u79cd\u200b\u65b9\u5f0f\u200b\u53ef\u4ee5\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff0c\u200b\u6bcf\u79cd\u200b\u65b9\u5f0f\u200b\u90fd\u200b\u6709\u200b\u7279\u5b9a\u200b\u7684\u200b\u5e94\u7528\u200b\u573a\u666f\u200b\uff08\u200b\u5982\u4e0a\u6240\u8ff0\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u53ef\u80fd\u200b\u662f\u200b\u6700\u200b\u5feb\u6377\u200b\u4e14\u200b\u80af\u5b9a\u200b\u662f\u200b\u6700\u200b\u6709\u8da3\u200b\u7684\u200b\u4e00\u79cd\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5c06\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5230\u200b\u4e92\u8054\u7f51\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u90a3\u200b\u5c31\u662f\u200b\u4f7f\u7528\u200b Gradio\u3002</p> <p>\u200b\u4ec0\u4e48\u200b\u662f\u200b Gradio\uff1f</p> <p>\u200b\u5176\u200b\u4e3b\u9875\u200b\u4e0a\u200b\u7684\u200b\u63cf\u8ff0\u200b\u975e\u5e38\u200b\u7cbe\u5f69\u200b\uff1a</p> <p>Gradio \u200b\u662f\u200b\u6f14\u793a\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5e76\u200b\u63d0\u4f9b\u200b\u53cb\u597d\u200b\u7f51\u9875\u200b\u754c\u9762\u200b\u7684\u200b\u6700\u5feb\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u8ba9\u200b\u4efb\u4f55\u4eba\u200b\u90fd\u200b\u80fd\u200b\u5728\u200b\u4efb\u4f55\u200b\u5730\u65b9\u200b\u4f7f\u7528\u200b\u5b83\u200b\uff01</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u4e3a\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\u6f14\u793a\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b\u6307\u6807\u200b\u867d\u7136\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u53ea\u6709\u200b\u5728\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u4e2d\u200b\u624d\u80fd\u200b\u771f\u6b63\u200b\u4e86\u89e3\u200b\u6a21\u578b\u200b\u7684\u200b\u8868\u73b0\u200b\u3002</p> <p>\u200b\u6240\u4ee5\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u90e8\u7f72\u200b\u5427\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u4f7f\u7528\u200b\u5e38\u89c1\u200b\u7684\u200b\u522b\u540d\u200b <code>gr</code> \u200b\u5bfc\u5165\u200b Gradio\uff0c\u200b\u5982\u679c\u200b\u5b83\u200b\u5c1a\u672a\u200b\u5b89\u88c5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#71-gradio","title":"7.1 Gradio \u200b\u6982\u8ff0\u200b\u00b6","text":"<p>Gradio \u200b\u7684\u200b\u6574\u4f53\u200b\u524d\u63d0\u200b\u4e0e\u200b\u6211\u4eec\u200b\u6574\u4e2a\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u53cd\u590d\u5f3a\u8c03\u200b\u7684\u200b\u5185\u5bb9\u200b\u975e\u5e38\u200b\u76f8\u4f3c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\u8fd9\u4e00\u200b\u76ee\u6807\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6211\u4eec\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <pre><code>\u200b\u8f93\u5165\u200b -&gt; \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b -&gt; \u200b\u8f93\u51fa\u200b\n</code></pre> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u5bf9\u4e8e\u200b FoodVision Mini\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u662f\u200b\u98df\u7269\u200b\u56fe\u7247\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u662f\u200b EffNetB2\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u51fa\u200b\u662f\u200b\u98df\u7269\u200b\u7c7b\u522b\u200b\uff08\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u6216\u200b\u5bff\u53f8\u200b\uff09\u3002</p> <pre><code>\u200b\u98df\u7269\u200b\u56fe\u7247\u200b -&gt; EffNetB2 -&gt; \u200b\u8f93\u51fa\u200b\n</code></pre> <p>\u200b\u5c3d\u7ba1\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7684\u200b\u6982\u5ff5\u200b\u53ef\u4ee5\u200b\u6865\u200b\u63a5\u5230\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u4f60\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u53ef\u80fd\u200b\u662f\u200b\u4ee5\u4e0b\u200b\u4efb\u610f\u200b\u7ec4\u5408\u200b\uff1a</p> <ul> <li>\u200b\u56fe\u50cf\u200b</li> <li>\u200b\u6587\u672c\u200b</li> <li>\u200b\u89c6\u9891\u200b</li> <li>\u200b\u8868\u683c\u200b\u6570\u636e\u200b</li> <li>\u200b\u97f3\u9891\u200b</li> <li>\u200b\u6570\u5b57\u200b</li> <li>\u200b\u7b49\u7b49\u200b</li> </ul> <p>\u200b\u4f60\u200b\u6784\u5efa\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u5c06\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u3002</p> <p>Gradio \u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4ece\u200b\u8f93\u5165\u200b\u5230\u200b\u8f93\u51fa\u200b\u7684\u200b\u63a5\u53e3\u200b\uff08<code>gradio.Interface()</code>\uff09\u200b\u6765\u200b\u6a21\u62df\u200b\u8fd9\u79cd\u200b\u8303\u5f0f\u200b\u3002</p> <pre><code>gradio.Interface(fn, inputs, outputs)\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff0c<code>fn</code> \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5c06\u200b <code>\u200b\u8f93\u5165\u200b</code> \u200b\u6620\u5c04\u200b\u5230\u200b <code>\u200b\u8f93\u51fa\u200b</code> \u200b\u7684\u200b Python \u200b\u51fd\u6570\u200b\u3002</p> <p>Gradio \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u6709\u7528\u200b\u7684\u200b <code>Interface</code> \u200b\u7c7b\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u8f7b\u677e\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4ece\u200b\u8f93\u5165\u200b\u5230\u200b\u6a21\u578b\u200b/\u200b\u51fd\u6570\u200b\u518d\u200b\u5230\u200b\u8f93\u51fa\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u53ef\u4ee5\u200b\u662f\u200b\u51e0\u4e4e\u200b\u4efb\u4f55\u200b\u4f60\u200b\u60f3\u8981\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u8f93\u5165\u200b\u63a8\u6587\u200b\uff08\u200b\u6587\u672c\u200b\uff09\u200b\u6765\u200b\u5224\u65ad\u200b\u5b83\u4eec\u200b\u662f\u5426\u200b\u4e0e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u76f8\u5173\u200b\uff0c\u200b\u6216\u8005\u200b\u8f93\u5165\u200b\u6587\u672c\u200b\u63d0\u793a\u200b\u6765\u200b\u751f\u6210\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a Gradio \u200b\u6709\u200b\u5927\u91cf\u200b\u7684\u200b\u53ef\u80fd\u200b <code>\u200b\u8f93\u5165\u200b</code> \u200b\u548c\u200b <code>\u200b\u8f93\u51fa\u200b</code> \u200b\u9009\u9879\u200b\uff0c\u200b\u79f0\u4e3a\u200b\u201c\u200b\u7ec4\u4ef6\u200b\u201d\uff0c\u200b\u4ece\u200b\u56fe\u50cf\u200b\u5230\u200b\u6587\u672c\u200b\u5230\u200b\u6570\u5b57\u200b\u5230\u200b\u97f3\u9891\u200b\u5230\u200b\u89c6\u9891\u200b\u7b49\u7b49\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b Gradio \u200b\u7ec4\u4ef6\u200b\u6587\u6863\u200b \u200b\u4e2d\u200b\u67e5\u770b\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#72","title":"7.2 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u6620\u5c04\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u4f7f\u7528\u200b Gradio \u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u5c06\u200b\u8f93\u5165\u200b\u6620\u5c04\u200b\u5230\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>pred_and_store()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u7528\u4e8e\u200b\u4f7f\u7528\u200b\u7ed9\u5b9a\u200b\u6a21\u578b\u200b\u5bf9\u200b\u76ee\u6807\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u7ed3\u679c\u200b\u5b58\u50a8\u200b\u5728\u200b\u5b57\u5178\u200b\u5217\u8868\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u662f\u5426\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u7684\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u5bf9\u200b\u5355\u5f20\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff1f</p> <p>\u200b\u66f4\u200b\u5177\u4f53\u5730\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u63a5\u53d7\u200b\u56fe\u50cf\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\uff0c\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\uff08\u200b\u53d8\u6362\u200b\uff09\uff0c\u200b\u4f7f\u7528\u200b EffNetB2 \u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u7136\u540e\u200b\u8fd4\u56de\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\uff08\u200b\u7b80\u79f0\u200b\u4e3a\u200b pred \u200b\u6216\u200b pred label\uff09\u200b\u4ee5\u53ca\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\uff08pred prob\uff09\u3002</p> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u8fd4\u56de\u200b\u5b8c\u6210\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\uff1a</p> <pre><code>\u200b\u8f93\u5165\u200b\uff1a\u200b\u56fe\u50cf\u200b -&gt; \u200b\u53d8\u6362\u200b -&gt; \u200b\u4f7f\u7528\u200b EffNetB2 \u200b\u9884\u6d4b\u200b -&gt; \u200b\u8f93\u51fa\u200b\uff1a\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b, \u200b\u9884\u6d4b\u200b\u6982\u7387\u200b, \u200b\u65f6\u95f4\u200b\n</code></pre> <p>\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u63a5\u53e3\u200b\u7684\u200b <code>fn</code> \u200b\u53c2\u6570\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\uff08\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u575a\u6301\u200b\u4f7f\u7528\u200b\u4ec5\u200b CPU \u200b\u7684\u200b\u9884\u6d4b\u200b\uff0c\u200b\u4f46\u200b\u5982\u679c\u200b\u4f60\u200b\u6709\u200b GPU \u200b\u8bbf\u95ee\u200b\u6743\u9650\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u66f4\u6539\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#73","title":"7.3 \u200b\u521b\u5efa\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u5217\u8868\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>predict()</code> \u200b\u51fd\u6570\u200b\u5b9e\u73b0\u200b\u4e86\u200b\u4ece\u200b\u8f93\u5165\u200b -&gt; \u200b\u8f6c\u6362\u200b -&gt; \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b -&gt; \u200b\u8f93\u51fa\u200b\u7684\u200b\u6d41\u7a0b\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u6b63\u662f\u200b\u6211\u4eec\u200b\u4e3a\u200b Graido \u200b\u6f14\u793a\u200b\u6240\u200b\u9700\u8981\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5728\u200b\u521b\u5efa\u200b\u6f14\u793a\u200b\u4e4b\u524d\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u518d\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e1c\u897f\u200b\uff1a\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u5217\u8868\u200b\u3002</p> <p>Gradio \u200b\u7684\u200b <code>Interface</code> \u200b\u7c7b\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u53ef\u9009\u200b\u7684\u200b <code>examples</code> \u200b\u53c2\u6570\u200b\uff08<code>gradio.Interface(examples=List[Any])</code>\uff09\u3002</p> <p>\u200b\u800c\u200b <code>examples</code> \u200b\u53c2\u6570\u200b\u7684\u200b\u683c\u5f0f\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5217\u8868\u200b\u7684\u200b\u5217\u8868\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u968f\u673a\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u6587\u4ef6\u200b\u8def\u5f84\u200b\u7684\u200b\u5217\u8868\u200b\u7684\u200b\u5217\u8868\u200b\u3002</p> <p>\u200b\u4e09\u4e2a\u200b\u793a\u4f8b\u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\u4e86\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#74-gradio","title":"7.4 \u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b Gradio \u200b\u754c\u9762\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u662f\u200b\u65f6\u5019\u200b\u628a\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u6574\u5408\u200b\u8d77\u6765\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u6d3b\u200b\u8d77\u6765\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b Gradio \u200b\u754c\u9762\u200b\u6765\u200b\u590d\u5236\u200b\u4ee5\u4e0b\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff1a</p> <pre><code>\u200b\u8f93\u5165\u200b\uff1a\u200b\u56fe\u50cf\u200b -&gt; \u200b\u8f6c\u6362\u200b -&gt; \u200b\u4f7f\u7528\u200b EffNetB2 \u200b\u9884\u6d4b\u200b -&gt; \u200b\u8f93\u51fa\u200b\uff1a\u200b\u9884\u6d4b\u200b\u7ed3\u679c\u200b\u3001\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u3001\u200b\u8017\u65f6\u200b\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>gradio.Interface()</code> \u200b\u7c7b\u200b\uff0c\u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u4ee5\u4e0b\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li><code>fn</code> - \u200b\u4e00\u4e2a\u200b\u5c06\u200b <code>inputs</code> \u200b\u6620\u5c04\u200b\u5230\u200b <code>outputs</code> \u200b\u7684\u200b Python \u200b\u51fd\u6570\u200b\uff0c\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>predict()</code> \u200b\u51fd\u6570\u200b\u3002</li> <li><code>inputs</code> - \u200b\u6211\u4eec\u200b\u754c\u9762\u200b\u7684\u200b\u8f93\u5165\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4f7f\u7528\u200b <code>gradio.Image()</code> \u200b\u6216\u200b <code>\"image\"</code> \u200b\u7684\u200b\u56fe\u50cf\u200b\u3002</li> <li><code>outputs</code> - \u200b\u8f93\u5165\u200b\u7ecf\u8fc7\u200b <code>fn</code> \u200b\u5904\u7406\u200b\u540e\u200b\u7684\u200b\u8f93\u51fa\u200b\uff0c\u200b\u4f8b\u5982\u200b\u4f7f\u7528\u200b <code>gradio.Label()</code> \u200b\u7684\u200b\u6807\u7b7e\u200b\uff08\u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\uff09\u200b\u6216\u200b\u4f7f\u7528\u200b <code>gradio.Number()</code> \u200b\u7684\u200b\u6570\u5b57\u200b\uff08\u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a Gradio \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5185\u7f6e\u200b\u7684\u200b <code>inputs</code> \u200b\u548c\u200b <code>outputs</code> \u200b\u9009\u9879\u200b\uff0c\u200b\u79f0\u4e3a\u200b \"Components\"\u3002</li> </ul> </li> <li><code>examples</code> - \u200b\u5c55\u793a\u200b\u7ed9\u200b\u7528\u6237\u200b\u7684\u200b\u793a\u4f8b\u200b\u5217\u8868\u200b\u3002</li> <li><code>title</code> - \u200b\u6f14\u793a\u200b\u7684\u200b\u6807\u9898\u200b\u5b57\u7b26\u4e32\u200b\u3002</li> <li><code>description</code> - \u200b\u6f14\u793a\u200b\u7684\u200b\u63cf\u8ff0\u200b\u5b57\u7b26\u4e32\u200b\u3002</li> <li><code>article</code> - \u200b\u6f14\u793a\u200b\u5e95\u90e8\u200b\u5f15\u7528\u200b\u7684\u200b\u6ce8\u91ca\u200b\u3002</li> </ul> <p>\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e86\u200b <code>gr.Interface()</code> \u200b\u7684\u200b\u6f14\u793a\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>gradio.Interface().launch()</code> \u200b\u6216\u200b <code>demo.launch()</code> \u200b\u547d\u4ee4\u200b\u6765\u200b\u542f\u52a8\u200b\u5b83\u200b\u3002</p> <p>\u200b\u5f88\u200b\u7b80\u5355\u200b\uff01</p>"},{"location":"09_pytorch_model_deployment/#8-foodvision-mini-gradio","title":"8. \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini Gradio \u200b\u6f14\u793a\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u53ef\u200b\u90e8\u7f72\u200b\u7684\u200b\u5e94\u7528\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u89c1\u8bc1\u200b\u4e86\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u901a\u8fc7\u200b Gradio \u200b\u6f14\u793a\u200b\u53d8\u5f97\u200b\u751f\u52a8\u200b\u8d77\u6765\u200b\u3002</p> <p>\u200b\u4f46\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u60f3\u200b\u4e0e\u200b\u670b\u53cb\u200b\u4eec\u200b\u5206\u4eab\u200b\u5b83\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u63d0\u4f9b\u200b\u7684\u200b Gradio \u200b\u94fe\u63a5\u200b\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u5171\u4eab\u200b\u94fe\u63a5\u200b\u4ec5\u200b\u6301\u7eed\u200b 72 \u200b\u5c0f\u65f6\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u66f4\u52a0\u200b\u6301\u4e45\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u6253\u5305\u200b\u6210\u200b\u4e00\u4e2a\u200b\u5e94\u7528\u200b\u5e76\u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face Spaces\u3002</p>"},{"location":"09_pytorch_model_deployment/#81-hugging-face-spaces","title":"8.1 \u200b\u4ec0\u4e48\u200b\u662f\u200b Hugging Face Spaces\uff1f\u00b6","text":"<p>Hugging Face Spaces \u200b\u662f\u200b\u4e00\u4e2a\u200b\u8d44\u6e90\u200b\uff0c\u200b\u5141\u8bb8\u200b\u4f60\u200b\u6258\u7ba1\u200b\u548c\u200b\u5206\u4eab\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5e94\u7528\u200b\u3002</p> <p>\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u6f14\u793a\u200b\u662f\u200b\u5c55\u793a\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u4f60\u200b\u6240\u200b\u505a\u200b\u5de5\u4f5c\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u800c\u200b Spaces \u200b\u6b63\u662f\u200b\u4e3a\u6b64\u200b\u800c\u200b\u8bbe\u8ba1\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b Hugging Face \u200b\u89c6\u4e3a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b GitHub\u3002</p> <p>\u200b\u5982\u679c\u200b\u62e5\u6709\u200b\u4e00\u4e2a\u200b\u4f18\u79c0\u200b\u7684\u200b GitHub \u200b\u4f5c\u54c1\u96c6\u200b\u5c55\u793a\u200b\u4e86\u200b\u4f60\u200b\u7684\u200b\u7f16\u7a0b\u200b\u80fd\u529b\u200b\uff0c\u200b\u90a3\u4e48\u200b\u62e5\u6709\u200b\u4e00\u4e2a\u200b\u4f18\u79c0\u200b\u7684\u200b Hugging Face \u200b\u4f5c\u54c1\u96c6\u200b\u5219\u200b\u53ef\u4ee5\u200b\u5c55\u793a\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u5c06\u200b Gradio \u200b\u5e94\u7528\u200b\u4e0a\u4f20\u200b\u5e76\u200b\u6258\u7ba1\u200b\u5230\u200b\u5176\u4ed6\u200b\u8bb8\u591a\u200b\u5730\u65b9\u200b\uff0c\u200b\u4f8b\u5982\u200b Google Cloud\u3001AWS\uff08\u200b\u4e9a\u9a6c\u900a\u200b\u7f51\u7edc\u670d\u52a1\u200b\uff09\u200b\u6216\u200b\u5176\u4ed6\u200b\u4e91\u200b\u670d\u52a1\u63d0\u4f9b\u5546\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b Hugging Face Spaces \u200b\u7684\u200b\u4f7f\u7528\u200b\u4fbf\u6377\u6027\u200b\u548c\u200b\u5e7f\u6cdb\u200b\u88ab\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u793e\u533a\u200b\u91c7\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b Hugging Face Spaces\u3002</p>"},{"location":"09_pytorch_model_deployment/#82-gradio","title":"8.2 \u200b\u90e8\u7f72\u200b\u7684\u200b Gradio \u200b\u5e94\u7528\u200b\u7ed3\u6784\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u4e0a\u4f20\u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\u5e94\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5c06\u200b\u4e0e\u200b\u4e4b\u200b\u76f8\u5173\u200b\u7684\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u653e\u5165\u200b\u4e00\u4e2a\u200b\u5355\u72ec\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6f14\u793a\u200b\u53ef\u80fd\u200b\u4f4d\u4e8e\u200b\u8def\u5f84\u200b <code>demos/foodvision_mini/</code>\uff0c\u200b\u5176\u200b\u6587\u4ef6\u200b\u7ed3\u6784\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>demos/\n\u2514\u2500\u2500 foodvision_mini/\n    \u251c\u2500\u2500 09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\n    \u251c\u2500\u2500 app.py\n    \u251c\u2500\u2500 examples/\n    \u2502   \u251c\u2500\u2500 example_1.jpg\n    \u2502   \u251c\u2500\u2500 example_2.jpg\n    \u2502   \u2514\u2500\u2500 example_3.jpg\n    \u251c\u2500\u2500 model.py\n    \u2514\u2500\u2500 requirements.txt\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code> \u200b\u662f\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u3002</li> <li><code>app.py</code> \u200b\u5305\u542b\u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u5e94\u7528\u200b\uff08\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u542f\u52a8\u200b\u5e94\u7528\u200b\u7684\u200b\u4ee3\u7801\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a <code>app.py</code> \u200b\u662f\u200b Hugging Face Spaces \u200b\u4f7f\u7528\u200b\u7684\u200b\u9ed8\u8ba4\u200b\u6587\u4ef6\u540d\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5c06\u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u5728\u200b\u90a3\u91cc\u200b\uff0cSpaces \u200b\u9ed8\u8ba4\u200b\u4f1a\u200b\u5bfb\u627e\u200b\u540d\u4e3a\u200b <code>app.py</code> \u200b\u7684\u200b\u6587\u4ef6\u200b\u6765\u200b\u8fd0\u884c\u200b\u3002\u200b\u8fd9\u4e2a\u200b\u6587\u4ef6\u540d\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bbe\u7f6e\u200b\u4e2d\u200b\u66f4\u6539\u200b\u3002</li> </ul> </li> <li><code>examples/</code> \u200b\u5305\u542b\u200b\u7528\u4e8e\u200b Gradio \u200b\u5e94\u7528\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u3002</li> <li><code>model.py</code> \u200b\u5305\u542b\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u4ee5\u53ca\u200b\u4e0e\u200b\u6a21\u578b\u200b\u76f8\u5173\u200b\u7684\u200b\u4efb\u4f55\u200b\u53d8\u6362\u200b\u3002</li> <li><code>requirements.txt</code> \u200b\u5305\u542b\u200b\u8fd0\u884c\u200b\u5e94\u7528\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\uff0c\u200b\u5982\u200b <code>torch</code>\u3001<code>torchvision</code> \u200b\u548c\u200b <code>gradio</code>\u3002</li> </ul> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8fd9\u6837\u200b\u7ec4\u7ec7\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u5f00\u59cb\u200b\u7684\u200b\u6700\u200b\u7b80\u5355\u200b\u7684\u200b\u5e03\u5c40\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u91cd\u70b9\u200b\u662f\u200b\uff1a\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u5b9e\u9a8c\u200b\u3001\u200b\u5b9e\u9a8c\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u66f4\u5feb\u200b\u5730\u200b\u8fd0\u884c\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u7684\u200b\u8f83\u5927\u200b\u5b9e\u9a8c\u200b\u5c31\u200b\u4f1a\u200b\u66f4\u597d\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9010\u6b65\u200b\u91cd\u73b0\u200b\u4e0a\u8ff0\u200b\u7ed3\u6784\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u67e5\u770b\u200b\u6b63\u5728\u200b\u8fd0\u884c\u200b\u7684\u200b\u5b9e\u65f6\u200b\u6f14\u793a\u200b\u5e94\u7528\u200b\u4ee5\u53ca\u200b\u6587\u4ef6\u200b\u7ed3\u6784\u200b\uff1a</p> <ul> <li>FoodVision Mini \u200b\u7684\u200b\u5b9e\u65f6\u200b Gradio \u200b\u6f14\u793a\u200b \ud83c\udf55\ud83e\udd69\ud83c\udf63\u3002</li> <li>FoodVision Mini \u200b\u5728\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u7684\u200b\u6587\u4ef6\u200b\u7ed3\u6784\u200b\u3002</li> </ul>"},{"location":"09_pytorch_model_deployment/#83-demos-foodvision-mini","title":"8.3 \u200b\u521b\u5efa\u200b <code>demos</code> \u200b\u6587\u4ef6\u5939\u200b\u4ee5\u200b\u5b58\u50a8\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u6587\u4ef6\u200b\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>demos/</code> \u200b\u76ee\u5f55\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b\u6240\u6709\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>pathlib.Path(\"path_to_dir\")</code> \u200b\u6765\u200b\u5efa\u7acb\u200b\u76ee\u5f55\u200b\u8def\u5f84\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>pathlib.Path(\"path_to_dir\").mkdir()</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u8be5\u200b\u76ee\u5f55\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#84-foodvision-mini","title":"8.4 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u7684\u200b\u6587\u4ef6\u5939\u200b\uff0c\u200b\u7528\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u6587\u4ef6\u200b\u7684\u200b\u76ee\u5f55\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6dfb\u52a0\u200b\u4e00\u4e9b\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u4ece\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u9009\u62e9\u200b\u4e09\u5f20\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\uff1a</p> <ol> <li>\u200b\u5728\u200b <code>demos/foodvision_mini</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>examples/</code> \u200b\u5b50\u76ee\u5f55\u200b\u3002</li> <li>\u200b\u4ece\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u4e09\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5b83\u4eec\u200b\u7684\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u6536\u96c6\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5217\u8868\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u5c06\u200b\u8fd9\u200b\u4e09\u5f20\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u56fe\u50cf\u200b\u590d\u5236\u5230\u200b <code>demos/foodvision_mini/examples/</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</li> </ol>"},{"location":"09_pytorch_model_deployment/#85-effnetb2foodvision-mini","title":"8.5 \u200b\u5c06\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u79fb\u52a8\u200b\u5230\u200bFoodVision Mini\u200b\u6f14\u793a\u200b\u76ee\u5f55\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u5c06\u200bFoodVision Mini\u200b\u7684\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u4fdd\u5b58\u200b\u5728\u200b<code>models/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code>\u200b\u8def\u5f84\u200b\u4e0b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u907f\u514d\u200b\u91cd\u590d\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8be5\u200b\u6a21\u578b\u200b\u79fb\u52a8\u200b\u5230\u200b<code>demos/foodvision_mini</code>\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200bPython\u200b\u7684\u200b<code>shutil.move()</code>\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u5e76\u200b\u4f20\u5165\u200b<code>src</code>\uff08\u200b\u76ee\u6807\u200b\u6587\u4ef6\u200b\u7684\u200b\u6e90\u200b\u8def\u5f84\u200b\uff09\u200b\u548c\u200b<code>dst</code>\uff08\u200b\u76ee\u6807\u200b\u6587\u4ef6\u200b\u8981\u200b\u79fb\u52a8\u200b\u5230\u200b\u7684\u200b\u76ee\u6807\u200b\u8def\u5f84\u200b\uff09\u200b\u53c2\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u4e00\u200b\u64cd\u4f5c\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#86-effnetb2pythonmodelpy","title":"8.6 \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2\u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200bPython\u200b\u811a\u672c\u200b\uff08<code>model.py</code>\uff09\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u6a21\u578b\u200b\u7684\u200b<code>state_dict</code>\u200b\u5df2\u200b\u4fdd\u5b58\u200b\u5230\u200b<code>demos/foodvision_mini/09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth</code>\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u52a0\u8f7d\u200b\u5b83\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>model.load_state_dict()</code>\u200b\u4ee5\u53ca\u200b<code>torch.load()</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5173\u4e8e\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\uff08\u200b\u6216\u200bPyTorch\u200b\u4e2d\u200b\u6a21\u578b\u200b\u7684\u200b<code>state_dict</code>\uff09\u200b\u7684\u200b\u590d\u4e60\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b01. PyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b \u200b\u7b2c\u200b5\u200b\u8282\u200b\uff1a\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200bPyTorch\u200b\u6a21\u578b\u200b\uff0c\u200b\u6216\u200b\u53c2\u9605\u200bPyTorch\u200b\u7684\u200b\u6559\u7a0b\u200bPyTorch\u200b\u4e2d\u200b\u7684\u200b<code>state_dict</code>\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u4f46\u200b\u5728\u200b\u6211\u4eec\u200b\u80fd\u591f\u200b\u8fd9\u6837\u200b\u505a\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u9700\u8981\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u6765\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b<code>model</code>\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u4ee5\u200b\u6a21\u5757\u5316\u200b\u7684\u200b\u65b9\u5f0f\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b<code>model.py</code>\u200b\u7684\u200b\u811a\u672c\u200b\uff0c\u200b\u5176\u4e2d\u200b\u5305\u542b\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b3.1\u200b\u8282\u200b\uff1a\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u5236\u4f5c\u200bEffNetB2\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b<code>create_effnetb2_model()</code>\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u811a\u672c\u200b\uff08\u200b\u89c1\u200b\u4e0b\u9762\u200b\u7684\u200b<code>app.py</code>\uff09\u200b\u4e2d\u200b\u5bfc\u5165\u200b\u8be5\u200b\u51fd\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u5b83\u200b\u6765\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200bEffNetB2 <code>model</code>\u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5e76\u200b\u83b7\u53d6\u200b\u5176\u200b\u9002\u5f53\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u5c31\u200b\u50cf\u200b\u5728\u200b05. PyTorch\u200b\u6a21\u5757\u5316\u200b\u4e2d\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>%%writefile path/to/file</code>\u200b\u9b54\u6cd5\u200b\u547d\u4ee4\u200b\u5c06\u200b\u4ee3\u7801\u200b\u5355\u5143\u683c\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u6587\u4ef6\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#87-foodvision-mini-gradio-python-apppy","title":"8.7 \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini Gradio \u200b\u5e94\u7528\u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u811a\u672c\u200b (<code>app.py</code>)\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b <code>model.py</code> \u200b\u811a\u672c\u200b\u4ee5\u53ca\u200b\u4e00\u4e2a\u200b\u53ef\u4ee5\u200b\u52a0\u8f7d\u200b\u7684\u200b\u4fdd\u5b58\u200b\u6a21\u578b\u200b <code>state_dict</code> \u200b\u7684\u200b\u8def\u5f84\u200b\u3002</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u6784\u5efa\u200b <code>app.py</code> \u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u547d\u540d\u200b\u4e3a\u200b <code>app.py</code>\uff0c\u200b\u56e0\u4e3a\u200b\u9ed8\u8ba4\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b HuggingFace Space \u200b\u65f6\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u5bfb\u627e\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>app.py</code> \u200b\u7684\u200b\u6587\u4ef6\u200b\u6765\u200b\u8fd0\u884c\u200b\u548c\u200b\u6258\u7ba1\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8bbe\u7f6e\u200b\u4e2d\u200b\u66f4\u6539\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>app.py</code> \u200b\u811a\u672c\u200b\u5c06\u200b\u628a\u200b\u6240\u6709\u200b\u62fc\u56fe\u200b\u7684\u200b\u788e\u7247\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\uff0c\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u6709\u200b\u56db\u4e2a\u200b\u4e3b\u8981\u200b\u90e8\u5206\u200b\uff1a</p> <ol> <li>\u200b\u5bfc\u5165\u200b\u548c\u200b\u7c7b\u540d\u200b\u8bbe\u7f6e\u200b - \u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5bfc\u5165\u200b\u6f14\u793a\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u5404\u79cd\u200b\u4f9d\u8d56\u200b\u9879\u200b\uff0c\u200b\u5305\u62ec\u200b\u6765\u81ea\u200b <code>model.py</code> \u200b\u7684\u200b <code>create_effnetb2_model()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u8bbe\u7f6e\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u7684\u200b\u4e0d\u540c\u200b\u7c7b\u540d\u200b\u3002</li> <li>\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\u51c6\u5907\u200b - \u200b\u5728\u200b\u8fd9\u91cc\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\u4ee5\u53ca\u200b\u4e0e\u200b\u4e4b\u200b\u5bf9\u5e94\u200b\u7684\u200b\u8f6c\u6362\u200b\uff0c\u200b\u7136\u540e\u200b\u6211\u4eec\u200b\u5c06\u200b\u52a0\u8f7d\u200b\u4fdd\u5b58\u200b\u7684\u200b\u6a21\u578b\u200b\u6743\u91cd\u200b/<code>state_dict</code>\u3002\u200b\u5f53\u200b\u6211\u4eec\u200b\u52a0\u8f7d\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u4f1a\u200b\u5728\u200b <code>torch.load()</code> \u200b\u4e2d\u200b\u8bbe\u7f6e\u200b <code>map_location=torch.device(\"cpu\")</code>\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u65e0\u8bba\u200b\u5728\u200b\u54ea\u4e2a\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u90fd\u200b\u4f1a\u200b\u88ab\u200b\u52a0\u8f7d\u200b\u5230\u200b CPU \u200b\u4e0a\u200b\uff08\u200b\u6211\u4eec\u200b\u8fd9\u6837\u200b\u505a\u200b\u662f\u56e0\u4e3a\u200b\u5728\u200b\u90e8\u7f72\u200b\u65f6\u200b\u6211\u4eec\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u6709\u200b GPU\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u4f46\u200b\u5728\u200b\u6ca1\u6709\u200b\u660e\u786e\u200b\u8bf4\u660e\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u5c1d\u8bd5\u200b\u90e8\u7f72\u200b\u5230\u200b CPU \u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5f97\u5230\u200b\u4e00\u4e2a\u200b\u9519\u8bef\u200b\uff09\u3002</li> <li>\u200b\u9884\u6d4b\u200b\u51fd\u6570\u200b - Gradio \u200b\u7684\u200b <code>gradio.Interface()</code> \u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b <code>fn</code> \u200b\u53c2\u6570\u200b\u6765\u200b\u6620\u5c04\u200b\u8f93\u5165\u200b\u5230\u200b\u8f93\u51fa\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b <code>predict()</code> \u200b\u51fd\u6570\u200b\u5c06\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b \u200b\u7b2c\u200b 7.2 \u200b\u8282\u200b\uff1a\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u6620\u5c04\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b \u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u7684\u200b\u51fd\u6570\u200b\u76f8\u540c\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u63a5\u6536\u200b\u4e00\u5f20\u200b\u56fe\u50cf\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u52a0\u8f7d\u200b\u7684\u200b\u8f6c\u6362\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u9884\u5904\u7406\u200b\uff0c\u200b\u518d\u200b\u4f7f\u7528\u200b\u52a0\u8f7d\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u901a\u8fc7\u200b <code>examples</code> \u200b\u53c2\u6570\u200b\u52a8\u6001\u521b\u5efa\u200b\u793a\u4f8b\u200b\u5217\u8868\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b <code>examples/</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u6587\u4ef6\u200b\u5217\u8868\u200b\u6765\u200b\u5b9e\u73b0\u200b\uff1a<code>[[\"examples/\" + example] for example in os.listdir(\"examples\")]</code>\u3002</li> </ul> </li> <li>Gradio \u200b\u5e94\u7528\u200b - \u200b\u8fd9\u662f\u200b\u6211\u4eec\u200b\u6f14\u793a\u200b\u7684\u200b\u4e3b\u8981\u200b\u903b\u8f91\u200b\u6240\u5728\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>demo</code> \u200b\u7684\u200b <code>gradio.Interface()</code> \u200b\u5b9e\u4f8b\u200b\uff0c\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u3001<code>predict()</code> \u200b\u51fd\u6570\u200b\u548c\u200b\u8f93\u51fa\u200b\u7ec4\u5408\u200b\u5728\u200b\u4e00\u8d77\u200b\u3002\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u8c03\u7528\u200b <code>demo.launch()</code> \u200b\u6765\u200b\u542f\u52a8\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\uff01</li> </ol>"},{"location":"09_pytorch_model_deployment/#88-foodvision-mini-requirementstxt","title":"8.8 \u200b\u4e3a\u200b FoodVision Mini \u200b\u521b\u5efa\u200b\u9700\u6c42\u200b\u6587\u4ef6\u200b (<code>requirements.txt</code>)\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u4e3a\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u521b\u5efa\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u200b\u662f\u200b <code>requirements.txt</code> \u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6211\u4eec\u200b\u6f14\u793a\u200b\u6240\u200b\u9700\u200b\u6240\u6709\u200b\u4f9d\u8d56\u200b\u9879\u200b\u7684\u200b\u6587\u672c\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u5f53\u200b\u6211\u4eec\u200b\u5c06\u200b\u6f14\u793a\u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u5230\u200b Hugging Face Spaces \u200b\u65f6\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u641c\u7d22\u200b\u8fd9\u4e2a\u200b\u6587\u4ef6\u200b\u5e76\u200b\u5b89\u88c5\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u7684\u200b\u5e94\u7528\u200b\u80fd\u591f\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u597d\u6d88\u606f\u200b\u662f\u200b\uff0c\u200b\u53ea\u6709\u200b\u4e09\u4e2a\u200b\uff01</p> <ol> <li><code>torch==1.12.0</code></li> <li><code>torchvision==0.13.0</code></li> <li><code>gradio==3.1.4</code></li> </ol> <p>\"==1.12.0\" \u200b\u8868\u793a\u200b\u8981\u200b\u5b89\u88c5\u200b\u7684\u200b\u7248\u672c\u53f7\u200b\u3002</p> <p>\u200b\u5b9a\u4e49\u200b\u7248\u672c\u53f7\u200b\u5e76\u200b\u4e0d\u662f\u200b 100% \u200b\u5fc5\u9700\u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u73b0\u5728\u200b\u4f1a\u200b\u8fd9\u6837\u200b\u505a\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u5728\u200b\u672a\u6765\u200b\u53d1\u5e03\u200b\u4e2d\u200b\u51fa\u73b0\u200b\u4efb\u4f55\u200b\u7834\u574f\u6027\u200b\u66f4\u65b0\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u5e94\u7528\u200b\u4ecd\u7136\u200b\u80fd\u591f\u200b\u8fd0\u884c\u200b\uff08PS\uff1a\u200b\u5982\u679c\u200b\u4f60\u200b\u53d1\u73b0\u200b\u4efb\u4f55\u200b\u9519\u8bef\u200b\uff0c\u200b\u6b22\u8fce\u200b\u5728\u200b\u8bfe\u7a0b\u200b GitHub Issues \u200b\u4e0a\u200b\u53d1\u5e16\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#9-foodvision-mini-huggingface-spaces","title":"9. \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u5230\u200b HuggingFace Spaces\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u5305\u542b\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u7684\u200b\u6587\u4ef6\u200b\uff0c\u200b\u73b0\u5728\u200b\u5982\u4f55\u200b\u8ba9\u200b\u5b83\u200b\u5728\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face Space\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b Hugging Face \u200b\u4ed3\u5e93\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b git \u200b\u4ed3\u5e93\u200b\uff09\u200b\u4e3b\u8981\u200b\u6709\u200b\u4e24\u79cd\u200b\u65b9\u5f0f\u200b\uff1a</p> <ol> <li>\u200b\u901a\u8fc7\u200b Hugging Face \u200b\u7f51\u9875\u200b\u754c\u9762\u200b\u4e0a\u4f20\u200b\uff08\u200b\u6700\u200b\u7b80\u5355\u200b\uff09\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u547d\u4ee4\u884c\u200b\u6216\u200b\u7ec8\u7aef\u200b\u4e0a\u4f20\u200b\u3002<ul> <li>\u200b\u989d\u5916\u200b\u63d0\u793a\u200b\uff1a \u200b\u4f60\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>huggingface_hub</code> \u200b\u5e93\u200b \u200b\u4e0e\u200b Hugging Face \u200b\u8fdb\u884c\u200b\u4ea4\u4e92\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e0a\u8ff0\u200b\u4e24\u79cd\u200b\u9009\u9879\u200b\u7684\u200b\u826f\u597d\u200b\u6269\u5c55\u200b\u3002</li> </ul> </li> </ol> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u81ea\u7531\u200b\u9605\u8bfb\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u9009\u9879\u200b\u7684\u200b\u6587\u6863\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u9009\u62e9\u200b\u7b2c\u4e8c\u79cd\u200b\u65b9\u5f0f\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8981\u200b\u5728\u200b Hugging Face \u200b\u4e0a\u200b\u6258\u7ba1\u200b\u4efb\u4f55\u200b\u5185\u5bb9\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u6ce8\u518c\u200b\u4e00\u4e2a\u200b\u514d\u8d39\u200b\u7684\u200b Hugging Face \u200b\u8d26\u6237\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#91-foodvision-mini","title":"9.1 \u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u6587\u4ef6\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u67e5\u770b\u200b\u4e00\u4e0b\u200b <code>demos/foodvision_mini</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u7684\u200b\u6f14\u793a\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>!ls</code> \u200b\u547d\u4ee4\u200b\uff0c\u200b\u540e\u200b\u8ddf\u200b\u76ee\u6807\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u3002</p> <p><code>ls</code> \u200b\u4ee3\u8868\u200b\u201c\u200b\u5217\u51fa\u200b\u201d\uff0c\u200b\u800c\u200b <code>!</code> \u200b\u8868\u793a\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u5728\u200b shell \u200b\u7ea7\u522b\u200b\u6267\u884c\u200b\u8be5\u200b\u547d\u4ee4\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#92-foodvision-mini","title":"9.2 \u200b\u5728\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u4e0b\u8f7d\u200b\u4e86\u200b <code>foodvision_mini.zip</code> \u200b\u6587\u4ef6\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5728\u200b\u672c\u5730\u200b\u6d4b\u8bd5\u200b\u5b83\u200b\uff1a</p> <ol> <li>\u200b\u89e3\u200b\u538b\u7f29\u6587\u4ef6\u200b\u3002</li> <li>\u200b\u6253\u5f00\u200b\u7ec8\u7aef\u200b\u6216\u200b\u547d\u4ee4\u884c\u200b\u63d0\u793a\u7b26\u200b\u3002</li> <li>\u200b\u5207\u6362\u200b\u5230\u200b <code>foodvision_mini</code> \u200b\u76ee\u5f55\u200b\uff08<code>cd foodvision_mini</code>\uff09\u3002</li> <li>\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u73af\u5883\u200b\uff08<code>python3 -m venv env</code>\uff09\u3002</li> <li>\u200b\u6fc0\u6d3b\u200b\u73af\u5883\u200b\uff08<code>source env/bin/activate</code>\uff09\u3002</li> <li>\u200b\u5b89\u88c5\u200b\u9700\u6c42\u200b\u6587\u4ef6\u200b\uff08<code>pip install -r requirements.txt</code>\uff0c\"<code>-r</code>\" \u200b\u8868\u793a\u200b\u9012\u5f52\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd9\u200b\u4e00\u6b65\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b 5-10 \u200b\u5206\u949f\u200b\uff0c\u200b\u5177\u4f53\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u7684\u200b\u7f51\u7edc\u8fde\u63a5\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u9047\u5230\u200b\u9519\u8bef\u200b\uff0c\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u5148\u200b\u5347\u7ea7\u200b <code>pip</code>\uff1a<code>pip install --upgrade pip</code>\u3002</li> </ul> </li> <li>\u200b\u8fd0\u884c\u200b\u5e94\u7528\u200b\uff08<code>python3 app.py</code>\uff09\u3002</li> </ol> <p>\u200b\u8fd9\u200b\u5c06\u200b\u5bfc\u81f4\u200b\u4e00\u4e2a\u200b Gradio \u200b\u6f14\u793a\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u4e0a\u9762\u200b\u6784\u5efa\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u5728\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u4e0a\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\uff0cURL \u200b\u53ef\u80fd\u200b\u662f\u200b <code>http://127.0.0.1:7860/</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\u5e94\u7528\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6ce8\u610f\u200b\u5230\u200b\u4e00\u4e2a\u200b <code>flagged/</code> \u200b\u76ee\u5f55\u200b\u51fa\u73b0\u200b\uff0c\u200b\u5b83\u200b\u5305\u542b\u200b\u5df2\u7ecf\u200b\u88ab\u200b\u201c\u200b\u6807\u8bb0\u200b\u201d\u200b\u7684\u200b\u6837\u672c\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u4eba\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b\u6f14\u793a\u200b\u7a0b\u5e8f\u200b\uff0c\u200b\u800c\u200b\u6a21\u578b\u200b\u4ea7\u751f\u200b\u4e86\u200b\u9519\u8bef\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u8be5\u200b\u6837\u672c\u200b\u53ef\u4ee5\u200b\u88ab\u200b\u201c\u200b\u6807\u8bb0\u200b\u201d\u200b\u5e76\u200b\u7559\u5f85\u200b\u540e\u7eed\u200b\u5ba1\u67e5\u200b\u3002</p> <p>\u200b\u6709\u5173\u200b Gradio \u200b\u4e2d\u200b\u7684\u200b\u6807\u8bb0\u200b\u529f\u80fd\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b \u200b\u6807\u8bb0\u200b\u6587\u6863\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#93-hugging-face","title":"9.3 \u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u9a8c\u8bc1\u200b\u4e86\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u5728\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\u6b63\u5e38\u200b\uff0c\u200b\u4f46\u662f\u200b\u521b\u5efa\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6f14\u793a\u200b\u7684\u200b\u4e50\u8da3\u200b\u5728\u4e8e\u200b\u5411\u200b\u5176\u4ed6\u4eba\u200b\u5c55\u793a\u200b\u5e76\u200b\u5141\u8bb8\u200b\u4ed6\u4eec\u200b\u4f7f\u7528\u200b\u5b83\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u7cfb\u5217\u200b\u4f7f\u7528\u200b Git\uff08\u200b\u4e00\u4e2a\u200b\u6587\u4ef6\u200b\u8ddf\u8e2a\u200b\u7cfb\u7edf\u200b\uff09\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002\u200b\u8981\u200b\u4e86\u89e3\u200b Git \u200b\u7684\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\uff0c\u200b\u5efa\u8bae\u200b\u901a\u8fc7\u200b freeCodeCamp \u200b\u4e0a\u200b\u7684\u200b Git \u200b\u548c\u200b GitHub \u200b\u521d\u5b66\u8005\u200b\u6559\u7a0b\u200b\u3002</p> <ol> <li>\u200b\u6ce8\u518c\u200b \u200b\u4e00\u4e2a\u200b Hugging Face \u200b\u8d26\u6237\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u8bbf\u95ee\u200b\u60a8\u200b\u7684\u200b\u4e2a\u4eba\u8d44\u6599\u200b\u5e76\u200b\u70b9\u51fb\u200b\u201cNew Space\u201d \u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b Hugging Face Space\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a Hugging Face \u200b\u4e2d\u200b\u7684\u200b Space \u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u4ee3\u7801\u200b\u4ed3\u5e93\u200b\u201d\uff08\u200b\u5b58\u50a8\u200b\u4ee3\u7801\u200b/\u200b\u6587\u4ef6\u200b\u7684\u200b\u5730\u65b9\u200b\uff09\uff0c\u200b\u7b80\u79f0\u200b\u201crepo\u201d\u3002</li> </ul> </li> <li>\u200b\u7ed9\u200b Space \u200b\u547d\u540d\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u200b\u7684\u200b\u662f\u200b <code>mrdbourke/foodvision_mini</code>\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u770b\u5230\u200b\uff1ahttps://huggingface.co/spaces/mrdbourke/foodvision_mini</li> <li>\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u8bb8\u53ef\u8bc1\u200b\uff08\u200b\u6211\u200b\u4f7f\u7528\u200b\u4e86\u200b MIT\uff09\u3002</li> <li>\u200b\u9009\u62e9\u200b Gradio \u200b\u4f5c\u4e3a\u200b Space SDK\uff08\u200b\u8f6f\u4ef6\u5f00\u53d1\u200b\u5de5\u5177\u5305\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u60a8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200b\u9009\u9879\u200b\uff0c\u200b\u5982\u200b Streamlit\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u5e94\u7528\u200b\u662f\u200b\u57fa\u4e8e\u200b Gradio \u200b\u6784\u5efa\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u4f7f\u7528\u200b Gradio\u3002</li> </ul> </li> <li>\u200b\u9009\u62e9\u200b\u60a8\u200b\u7684\u200b Space \u200b\u662f\u200b\u516c\u5f00\u200b\u7684\u200b\u8fd8\u662f\u200b\u79c1\u6709\u200b\u7684\u200b\uff08\u200b\u6211\u200b\u9009\u62e9\u200b\u4e86\u200b\u516c\u5f00\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u200b\u5e0c\u671b\u200b\u6211\u200b\u7684\u200b Space \u200b\u5bf9\u200b\u5176\u4ed6\u4eba\u200b\u53ef\u7528\u200b\uff09\u3002</li> <li>\u200b\u70b9\u51fb\u200b\u201cCreate Space\u201d\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5728\u200b\u7ec8\u7aef\u200b\u6216\u200b\u547d\u4ee4\u63d0\u793a\u7b26\u200b\u4e2d\u200b\u8fd0\u884c\u200b\u7c7b\u4f3c\u200b <code>git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]</code> \u200b\u7684\u200b\u547d\u4ee4\u200b\uff0c\u200b\u5c06\u200b\u4ed3\u5e93\u200b\u514b\u9686\u200b\u5230\u200b\u672c\u5730\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u60a8\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5728\u200b\u201cFiles and versions\u201d\u200b\u6807\u7b7e\u200b\u4e0b\u200b\u4e0a\u4f20\u200b\u6587\u4ef6\u200b\u6765\u200b\u6dfb\u52a0\u200b\u6587\u4ef6\u200b\u3002</li> </ul> </li> <li>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u7684\u200b <code>foodvision_mini</code> \u200b\u6587\u4ef6\u5939\u200b\u7684\u200b\u5185\u5bb9\u200b\u590d\u5236\u200b/\u200b\u79fb\u52a8\u200b\u5230\u200b\u514b\u9686\u200b\u7684\u200b\u4ed3\u5e93\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u8981\u200b\u4e0a\u4f20\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u8f83\u5927\u200b\u7684\u200b\u6587\u4ef6\u200b\uff08\u200b\u4f8b\u5982\u200b\u8d85\u8fc7\u200b 10MB \u200b\u7684\u200b\u6587\u4ef6\u200b\uff0c\u200b\u6216\u8005\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0cPyTorch \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff09\uff0c\u200b\u60a8\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b Git LFS\uff08\u200b\u5373\u200b\u201cgit \u200b\u5927\u200b\u6587\u4ef6\u200b\u5b58\u50a8\u200b\u201d\uff09\u3002</li> <li>\u200b\u5b89\u88c5\u200b Git LFS \u200b\u540e\u200b\uff0c\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b <code>git lfs install</code> \u200b\u6765\u200b\u6fc0\u6d3b\u200b\u5b83\u200b\u3002</li> <li>\u200b\u5728\u200b <code>foodvision_mini</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b Git LFS \u200b\u8ddf\u8e2a\u200b\u8d85\u8fc7\u200b 10MB \u200b\u7684\u200b\u6587\u4ef6\u200b\uff0c\u200b\u547d\u4ee4\u200b\u4e3a\u200b <code>git lfs track \"*.file_extension\"</code>\u3002<ul> <li>\u200b\u8ddf\u8e2a\u200b EffNetB2 PyTorch \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u547d\u4ee4\u200b\u4e3a\u200b <code>git lfs track \"09_pretrained_effnetb2_feature_extractor_pizza_steak_sushi_20_percent.pth\"</code>\u3002</li> </ul> </li> <li>\u200b\u8ddf\u8e2a\u200b <code>.gitattributes</code> \u200b\u6587\u4ef6\u200b\uff08\u200b\u4ece\u200b HuggingFace \u200b\u514b\u9686\u200b\u65f6\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\uff0c\u200b\u8be5\u200b\u6587\u4ef6\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u5927\u200b\u6587\u4ef6\u200b\u901a\u8fc7\u200b Git LFS \u200b\u8fdb\u884c\u200b\u8ddf\u8e2a\u200b\uff09\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b FoodVision Mini Hugging Face Space \u200b\u4e0a\u200b\u770b\u5230\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b <code>.gitattributes</code> \u200b\u6587\u4ef6\u200b\u3002<ul> <li><code>git add .gitattributes</code></li> </ul> </li> <li>\u200b\u6dfb\u52a0\u200b <code>foodvision_mini</code> \u200b\u5e94\u7528\u200b\u7684\u200b\u5176\u4ed6\u200b\u6587\u4ef6\u200b\u5e76\u200b\u63d0\u4ea4\u200b\u5b83\u4eec\u200b\uff1a<ul> <li><code>git add *</code></li> <li><code>git commit -m \"first commit\"</code></li> </ul> </li> <li>\u200b\u63a8\u9001\u200b\uff08\u200b\u4e0a\u4f20\u200b\uff09\u200b\u6587\u4ef6\u200b\u5230\u200b Hugging Face\uff1a<ul> <li><code>git push</code></li> </ul> </li> <li>\u200b\u7b49\u5f85\u200b 3-5 \u200b\u5206\u949f\u200b\u8ba9\u200b\u6784\u5efa\u200b\u5b8c\u6210\u200b\uff08\u200b\u672a\u6765\u200b\u7684\u200b\u6784\u5efa\u200b\u4f1a\u200b\u66f4\u200b\u5feb\u200b\uff09\uff0c\u200b\u60a8\u200b\u7684\u200b\u5e94\u7528\u200b\u5c31\u200b\u4f1a\u200b\u4e0a\u7ebf\u200b\uff01</li> </ol> <p>\u200b\u5982\u679c\u200b\u4e00\u5207\u987a\u5229\u200b\uff0c\u200b\u60a8\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u770b\u5230\u200b\u4e00\u4e2a\u200b\u50cf\u200b\u8fd9\u91cc\u200b\u4e00\u6837\u200b\u7684\u200b FoodVision Mini Gradio \u200b\u6f14\u793a\u200b\u7684\u200b\u5b9e\u65f6\u200b\u8fd0\u884c\u200b\u793a\u4f8b\u200b\uff1ahttps://huggingface.co/spaces/mrdbourke/foodvision_mini</p> <p>\u200b\u6211\u4eec\u200b\u751a\u81f3\u200b\u53ef\u4ee5\u200b\u5c06\u200b FoodVision Mini Gradio \u200b\u6f14\u793a\u200b\u5d4c\u5165\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b iframe\uff0c\u200b\u4f7f\u7528\u200b <code>IPython.display.IFrame</code> \u200b\u548c\u200b\u94fe\u63a5\u200b\u683c\u5f0f\u200b <code>https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+</code>\u3002</p>"},{"location":"09_pytorch_model_deployment/#10-foodvision-big","title":"10. \u200b\u521b\u5efa\u200b FoodVision Big\u00b6","text":"<p>\u200b\u5728\u200b\u524d\u200b\u51e0\u8282\u200b\u548c\u200b\u7ae0\u8282\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e00\u76f4\u200b\u5728\u200b\u52aa\u529b\u200b\u5c06\u200b FoodVision Mini \u200b\u53d8\u4e3a\u200b\u73b0\u5b9e\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u770b\u5230\u200b\u5b83\u200b\u5728\u200b\u5b9e\u65f6\u200b\u6f14\u793a\u200b\u4e2d\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4e0d\u5982\u200b\u6211\u4eec\u200b\u518d\u200b\u63d0\u5347\u200b\u4e00\u4e2a\u200b\u6863\u6b21\u200b\uff1f</p> <p>\u200b\u600e\u4e48\u200b\u505a\u200b\uff1f</p> <p>FoodVision Big\uff01</p> <p>\u200b\u65e2\u7136\u200b FoodVision Mini \u200b\u662f\u200b\u57fa\u4e8e\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\uff08101 \u200b\u79cd\u200b\u98df\u7269\u200b x \u200b\u6bcf\u79cd\u200b 1000 \u200b\u5f20\u200b\u56fe\u7247\u200b\uff09\u200b\u4e2d\u200b\u7684\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u56fe\u7247\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f55\u4e0d\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6db5\u76d6\u200b\u6240\u6709\u200b 101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u6a21\u578b\u200b\u6765\u200b\u6253\u9020\u200b FoodVision Big \u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\u6269\u5c55\u200b\u5230\u200b 101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff01</p> <p>\u200b\u4ece\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u6269\u5c55\u200b\u5230\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u3001\u200b\u70ed\u72d7\u200b\u3001\u200b\u82f9\u679c\u6d3e\u200b\u3001\u200b\u80e1\u841d\u535c\u200b\u86cb\u7cd5\u200b\u3001\u200b\u5de7\u514b\u529b\u200b\u86cb\u7cd5\u200b\u3001\u200b\u85af\u6761\u200b\u3001\u200b\u849c\u84c9\u200b\u9762\u5305\u200b\u3001\u200b\u62c9\u9762\u200b\u3001\u200b\u7389\u7c73\u7247\u200b\u3001\u200b\u5854\u200b\u53ef\u200b\u7b49\u7b49\u200b\uff01</p> <p>\u200b\u600e\u4e48\u200b\u505a\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u6240\u6709\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u7a0d\u5fae\u200b\u8c03\u6574\u200b\u6211\u4eec\u200b\u7684\u200b EffNetB2 \u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u51c6\u5907\u200b\u4e00\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u5b8c\u6210\u200b\u7b2c\u4e09\u9636\u6bb5\u200b\u9879\u76ee\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u91cd\u65b0\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b FoodVision Mini\uff08\u200b\u4e09\u4e2a\u200b\u7c7b\u522b\u200b\uff09\u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u662f\u200b\u4e3a\u200b FoodVision Big\uff08101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09\u3002</p> <p>FoodVision Mini \u200b\u9002\u7528\u200b\u4e8e\u200b\u4e09\u4e2a\u200b\u98df\u7269\u200b\u7c7b\u522b\u200b\uff1a\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u548c\u200b\u5bff\u53f8\u200b\u3002\u200b\u800c\u200b FoodVision Big \u200b\u5219\u200b\u63d0\u5347\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u6863\u6b21\u200b\uff0c\u200b\u9002\u7528\u200b\u4e8e\u200b 101 \u200b\u4e2a\u200b\u98df\u7269\u200b\u7c7b\u522b\u200b\uff1a\u200b\u6240\u6709\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u7c7b\u522b\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#101-foodvision-big","title":"10.1 \u200b\u4e3a\u200b FoodVision Big \u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u521b\u5efa\u200b FoodVision Mini \u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53d1\u73b0\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u5728\u200b\u901f\u5ea6\u200b\u548c\u200b\u6027\u80fd\u200b\u4e4b\u95f4\u200b\u53d6\u5f97\u200b\u4e86\u200b\u826f\u597d\u200b\u7684\u200b\u5e73\u8861\u200b\uff08\u200b\u5b83\u200b\u5728\u200b\u6027\u80fd\u200b\u826f\u597d\u200b\u4e14\u200b\u901f\u5ea6\u200b\u5feb\u200b\uff09\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ee7\u7eed\u200b\u4e3a\u200b FoodVision Big \u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b3.1\u200b\u8282\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>create_effnetb2_model()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u200b\u4f20\u9012\u200b\u53c2\u6570\u200b <code>num_classes=101</code>\uff08\u200b\u56e0\u4e3a\u200b Food101 \u200b\u6709\u200b 101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff09\uff0c\u200b\u6765\u200b\u4e3a\u200b Food101 \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#102-foodvision-big","title":"10.2 \u200b\u83b7\u53d6\u200b FoodVision Big \u200b\u7684\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u5bf9\u4e8e\u200b FoodVision Mini\uff0c\u200b\u6211\u4eec\u200b\u81ea\u5df1\u200b\u5236\u4f5c\u200b\u4e86\u200b\u6574\u4e2a\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u5206\u5272\u200b\u3002</p> <p>\u200b\u8981\u200b\u83b7\u53d6\u200b\u6574\u4e2a\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torchvision.datasets.Food101()</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u8def\u5f84\u200b\u5230\u200b <code>data/</code> \u200b\u76ee\u5f55\u200b\u6765\u200b\u5b58\u50a8\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>food101_train_transforms</code> \u200b\u548c\u200b <code>effnetb2_transforms</code> \u200b\u5206\u522b\u200b\u8f6c\u6362\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u5206\u5272\u200b\uff0c\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u8f6c\u6362\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b Google Colab\uff0c\u200b\u4e0b\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\u5c06\u200b\u9700\u8981\u200b\u5927\u7ea6\u200b 3-5 \u200b\u5206\u949f\u200b\u6765\u200b\u5b8c\u5168\u200b\u8fd0\u884c\u200b\u5e76\u200b\u4ece\u200b PyTorch \u200b\u4e0b\u8f7d\u200b Food101 \u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u8981\u200b\u4e0b\u8f7d\u200b\u8d85\u8fc7\u200b 10 \u200b\u4e07\u5f20\u200b\u56fe\u50cf\u200b\uff08101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b x \u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b 1000 \u200b\u5f20\u200b\u56fe\u50cf\u200b\uff09\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u91cd\u542f\u200b\u4e86\u200b Google Colab \u200b\u8fd0\u884c\u200b\u65f6\u200b\u5e76\u200b\u8fd4\u56de\u200b\u5230\u200b\u8fd9\u4e2a\u200b\u5355\u5143\u683c\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5c06\u200b\u9700\u8981\u200b\u91cd\u65b0\u200b\u4e0b\u8f7d\u200b\u3002\u200b\u6216\u8005\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b\u672c\u5730\u200b\u8fd0\u884c\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\uff0c\u200b\u56fe\u50cf\u200b\u5c06\u200b\u88ab\u200b\u7f13\u5b58\u200b\u5e76\u200b\u5b58\u50a8\u200b\u5728\u200b <code>torchvision.datasets.Food101()</code> \u200b\u7684\u200b <code>root</code> \u200b\u53c2\u6570\u200b\u6307\u5b9a\u200b\u7684\u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#103-food101","title":"10.3 \u200b\u521b\u5efa\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5b50\u96c6\u200b\u4ee5\u200b\u52a0\u5feb\u200b\u5b9e\u9a8c\u200b\u901f\u5ea6\u200b\u00b6","text":"<p>\u200b\u8fd9\u200b\u4e00\u90e8\u5206\u200b\u662f\u200b\u53ef\u9009\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e76\u200b\u4e0d\u200b\u5fc5\u987b\u200b\u521b\u5efa\u200b\u53e6\u200b\u4e00\u4e2a\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5b50\u96c6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b8c\u5168\u200b\u53ef\u4ee5\u200b\u5bf9\u200b\u6574\u4e2a\u200b\u5305\u542b\u200b 101,000 \u200b\u5f20\u200b\u56fe\u7247\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u8bc4\u4f30\u200b\u3002</p> <p>\u200b\u4f46\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u8bad\u7ec3\u200b\u96c6\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u200b\u5404\u200b 20% \u200b\u7684\u200b\u5b50\u96c6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u770b\u770b\u200b\u662f\u5426\u200b\u80fd\u4ec5\u7528\u200b 20% \u200b\u7684\u200b\u6570\u636e\u200b\u5c31\u200b\u8d85\u8d8a\u200b\u539f\u59cb\u200b Food101 \u200b\u8bba\u6587\u200b \u200b\u7684\u200b\u6700\u4f73\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b/\u200b\u5c06\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u7ec6\u5206\u200b\uff1a</p> \u200b\u7b14\u8bb0\u672c\u200b\u7f16\u53f7\u200b \u200b\u9879\u76ee\u540d\u79f0\u200b \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b \u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b \u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u6570\u91cf\u200b 04, 05, 06, 07, 08 FoodVision Mini (10% \u200b\u6570\u636e\u200b) Food101 \u200b\u81ea\u5b9a\u4e49\u200b\u5206\u5272\u200b 3 (\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b) 225 75 07, 08, 09 FoodVision Mini (20% \u200b\u6570\u636e\u200b) Food101 \u200b\u81ea\u5b9a\u4e49\u200b\u5206\u5272\u200b 3 (\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b) 450 150 09 (\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b) FoodVision Big (20% \u200b\u6570\u636e\u200b) Food101 \u200b\u81ea\u5b9a\u4e49\u200b\u5206\u5272\u200b 101 (\u200b\u6240\u6709\u200b Food101 \u200b\u7c7b\u522b\u200b) 15150 5050 \u200b\u6269\u5c55\u200b FoodVision Big Food101 \u200b\u5168\u90e8\u200b\u6570\u636e\u200b 101 75750 25250 <p>\u200b\u4f60\u200b\u80fd\u200b\u770b\u51fa\u200b\u5176\u4e2d\u200b\u7684\u200b\u8d8b\u52bf\u200b\u5417\u200b\uff1f</p> <p>\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u89c4\u6a21\u200b\u9010\u6e10\u200b\u589e\u5927\u200b\u4e00\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u7528\u4e8e\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u89c4\u6a21\u200b\u4e5f\u200b\u5728\u200b\u9010\u6e10\u200b\u589e\u5927\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8981\u200b\u771f\u6b63\u200b\u7528\u200b 20% \u200b\u7684\u200b\u6570\u636e\u200b\u8d85\u8d8a\u200b\u539f\u59cb\u200b Food101 \u200b\u8bba\u6587\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u6211\u4eec\u200b\u9700\u8981\u200b\u5728\u200b 20% \u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u6574\u4e2a\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5728\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u5206\u5272\u200b\u6570\u636e\u200b\u4e0a\u200b\u3002\u200b\u6211\u200b\u5c06\u200b\u5176\u200b\u7559\u4f5c\u200b\u4e00\u4e2a\u200b\u6269\u5c55\u200b\u7ec3\u4e60\u200b\uff0c\u200b\u4f9b\u200b\u4f60\u200b\u5c1d\u8bd5\u200b\u3002\u200b\u6211\u200b\u4e5f\u200b\u9f13\u52b1\u200b\u4f60\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u6574\u4e2a\u200b Food101 \u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big (20% \u200b\u6570\u636e\u200b) \u200b\u5206\u5272\u200b\uff0c\u200b\u6211\u4eec\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u540d\u4e3a\u200b <code>split_dataset()</code> \u200b\u7684\u200b\u51fd\u6570\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5c06\u200b\u7ed9\u5b9a\u200b\u6570\u636e\u200b\u96c6\u200b\u6309\u200b\u7279\u5b9a\u200b\u6bd4\u4f8b\u200b\u5206\u5272\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.utils.data.random_split()</code> \u200b\u901a\u8fc7\u200b <code>lengths</code> \u200b\u53c2\u6570\u200b\u521b\u5efa\u200b\u6307\u5b9a\u200b\u5927\u5c0f\u200b\u7684\u200b\u5206\u5272\u200b\u3002</p> <p><code>lengths</code> \u200b\u53c2\u6570\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u671f\u671b\u200b\u5206\u5272\u200b\u957f\u5ea6\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u5217\u8868\u200b\u7684\u200b\u603b\u548c\u200b\u5fc5\u987b\u200b\u7b49\u4e8e\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u603b\u957f\u5ea6\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u4e00\u4e2a\u200b\u5927\u5c0f\u200b\u4e3a\u200b 100 \u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f20\u5165\u200b <code>lengths=[20, 80]</code> \u200b\u6765\u200b\u83b7\u5f97\u200b 20% \u200b\u548c\u200b 80% \u200b\u7684\u200b\u5206\u5272\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5e0c\u671b\u200b\u51fd\u6570\u200b\u8fd4\u56de\u200b\u4e24\u4e2a\u200b\u5206\u5272\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u76ee\u6807\u200b\u957f\u5ea6\u200b\uff08\u200b\u4f8b\u5982\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b 20%\uff09\uff0c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u5269\u4f59\u200b\u957f\u5ea6\u200b\uff08\u200b\u4f8b\u5982\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u5269\u4f59\u200b 80%\uff09\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b <code>generator</code> \u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b <code>torch.manual_seed()</code> \u200b\u503c\u200b\uff0c\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u53ef\u91cd\u590d\u6027\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#104-food101-dataloader","title":"10.4 \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader()</code> \u200b\u5c06\u200b Food101 20% \u200b\u6570\u636e\u200b\u96c6\u200b\u62c6\u5206\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>DataLoader</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ea\u200b\u4e3a\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u8bbe\u7f6e\u200b <code>shuffle=True</code>\uff0c\u200b\u5e76\u4e14\u200b\u4e24\u4e2a\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u90fd\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>32</code>\u3002</p> <p>\u200b\u5982\u679c\u200b CPU \u200b\u6570\u91cf\u200b\u53ef\u7528\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b <code>num_workers</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>4</code>\uff0c\u200b\u5426\u5219\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>2</code>\uff08\u200b\u5c3d\u7ba1\u200b <code>num_workers</code> \u200b\u7684\u200b\u503c\u200b\u975e\u5e38\u200b\u5177\u6709\u200b\u5b9e\u9a8c\u6027\u200b\uff0c\u200b\u5e76\u4e14\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u4f7f\u7528\u200b\u7684\u200b\u786c\u4ef6\u200b\uff0cPyTorch \u200b\u8bba\u575b\u200b\u4e0a\u200b\u6709\u200b\u4e00\u4e2a\u200b\u5173\u4e8e\u200b\u6b64\u200b\u7684\u200b\u6d3b\u8dc3\u200b\u8ba8\u8bba\u200b\u7ebf\u7a0b\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#105-foodvision-big","title":"10.5 \u200b\u8bad\u7ec3\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u00b6","text":"<p>FoodVision Big \u200b\u6a21\u578b\u200b\u548c\u200b <code>DataLoader</code> \u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u662f\u200b\u8bad\u7ec3\u200b\u7684\u200b\u65f6\u5019\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.optim.Adam()</code> \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u5b66\u4e60\u200b\u7387\u4e3a\u200b <code>1e-3</code>\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6709\u200b\u5982\u6b64\u200b\u591a\u200b\u7684\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.nn.CrossEntropyLoss()</code> \u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\uff0c\u200b\u5e76\u200b\u8bbe\u7f6e\u200b <code>label_smoothing=0.1</code>\uff0c\u200b\u8fd9\u200b\u4e0e\u200b <code>torchvision</code> \u200b\u7684\u200b\u6700\u65b0\u200b\u8bad\u7ec3\u65b9\u6cd5\u200b\u4e00\u81f4\u200b\u3002</p> <p>\u200b\u4ec0\u4e48\u200b\u662f\u200b \u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\uff1f</p> <p>\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u662f\u200b\u4e00\u79cd\u200b\u6b63\u5219\u200b\u5316\u200b\u6280\u672f\u200b\uff08\u200b\u6b63\u5219\u200b\u5316\u662f\u200b\u63cf\u8ff0\u200b\u9632\u6b62\u200b\u8fc7\u200b\u62df\u5408\u200b\u8fc7\u7a0b\u200b\u7684\u200b\u53e6\u200b\u4e00\u79cd\u200b\u8bf4\u6cd5\u200b\uff09\uff0c\u200b\u5b83\u200b\u51cf\u5c11\u200b\u4e86\u200b\u6a21\u578b\u200b\u5bf9\u200b\u4efb\u4f55\u200b\u5355\u4e00\u200b\u6807\u7b7e\u200b\u7684\u200b\u91cd\u89c6\u200b\u7a0b\u5ea6\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u8fd9\u79cd\u200b\u91cd\u89c6\u200b\u5206\u6563\u200b\u5230\u200b\u5176\u4ed6\u200b\u6807\u7b7e\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u4e0d\u662f\u200b\u8ba9\u200b\u6a21\u578b\u200b\u5bf9\u200b\u5355\u4e00\u200b\u6807\u7b7e\u200b\u8fc7\u4e8e\u81ea\u4fe1\u200b\uff0c\u200b\u800c\u662f\u200b\u7ed9\u200b\u5176\u4ed6\u200b\u6807\u7b7e\u200b\u8d4b\u4e88\u200b\u975e\u96f6\u503c\u200b\uff0c\u200b\u4ee5\u200b\u5e2e\u52a9\u200b\u6a21\u578b\u200b\u66f4\u597d\u200b\u5730\u200b\u6cdb\u5316\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4e00\u4e2a\u200b\u6ca1\u6709\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u7684\u200b\u6a21\u578b\u200b\u5bf9\u200b5\u200b\u4e2a\u200b\u7c7b\u522b\u200b\u7684\u200b\u8f93\u51fa\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>[0, 0, 0.99, 0.01, 0]\n</code></pre> <p>\u200b\u4e00\u4e2a\u200b\u5e26\u6709\u200b\u6807\u7b7e\u200b\u5e73\u6ed1\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u200b\u5982\u4e0b\u200b\u8f93\u51fa\u200b\uff1a</p> <pre><code>[0.01, 0.01, 0.96, 0.01, 0.01]\n</code></pre> <p>\u200b\u6a21\u578b\u200b\u4ecd\u7136\u200b\u5bf9\u200b\u5176\u200b\u9884\u6d4b\u200b\u7684\u200b\u7c7b\u522b\u200b3\u200b\u5145\u6ee1\u4fe1\u5fc3\u200b\uff0c\u200b\u4f46\u200b\u7ed9\u200b\u5176\u4ed6\u200b\u6807\u7b7e\u200b\u8d4b\u4e88\u200b\u5c0f\u503c\u200b\u4f1a\u200b\u8feb\u4f7f\u200b\u6a21\u578b\u200b\u81f3\u5c11\u200b\u8003\u8651\u200b\u5176\u4ed6\u200b\u9009\u9879\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u5feb\u901f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u7b2c\u200b4\u200b\u8282\u200b \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b <code>engine.train()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u5bf9\u6a21\u578b\u200b\u8fdb\u884c\u200b\u4e94\u4e2a\u200b\u5468\u671f\u200b\u7684\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u76ee\u6807\u200b\u662f\u200b\u51fb\u8d25\u200b\u539f\u59cb\u200b Food101 \u200b\u8bba\u6587\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u7684\u200b 56.4% \u200b\u51c6\u786e\u7387\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd0\u884c\u200b\u4e0b\u9762\u200b\u7684\u200b\u5355\u5143\u683c\u200b\u5c06\u200b\u5728\u200b Google Colab \u200b\u4e0a\u200b\u82b1\u8d39\u200b\u7ea6\u200b 15-20 \u200b\u5206\u949f\u200b\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u8bad\u7ec3\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u4f7f\u7528\u200b\u7684\u200b\u6700\u5927\u200b\u6a21\u578b\u200b\u548c\u200b\u6700\u5927\u200b\u6570\u636e\u91cf\u200b\uff0815,150 \u200b\u5f20\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\uff0c5050 \u200b\u5f20\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\uff09\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u51b3\u5b9a\u200b\u5c06\u200b\u5b8c\u6574\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b 20% \u200b\u5206\u79bb\u51fa\u6765\u200b\u7684\u200b\u539f\u56e0\u200b\uff08\u200b\u4ee5\u514d\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u8d85\u8fc7\u200b\u4e00\u200b\u5c0f\u65f6\u200b\uff09\u3002</p>"},{"location":"09_pytorch_model_deployment/#106-foodvision-big","title":"10.6 \u200b\u68c0\u67e5\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b FoodVision Big \u200b\u7684\u200b\u635f\u5931\u200b\u66f2\u7ebf\u200b\u53ef\u89c6\u5316\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>helper_functions.py</code> \u200b\u4e2d\u200b\u7684\u200b <code>plot_loss_curves()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#107-foodvision-big","title":"10.7 \u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b FoodVision Big\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u8bad\u7ec3\u200b\u4e86\u200b\u6211\u4eec\u200b\u8fc4\u4eca\u4e3a\u6b62\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5176\u200b\u4fdd\u5b58\u8d77\u6765\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u7a0d\u540e\u200b\u53ef\u4ee5\u200b\u91cd\u65b0\u200b\u52a0\u8f7d\u200b\u5b83\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#108-foodvision-big","title":"10.8 \u200b\u68c0\u67e5\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5206\u7c7b\u200b 101 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u76f8\u8f83\u200b\u4e8e\u200b FoodVision Mini \u200b\u7684\u200b 3 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u589e\u52a0\u200b\u4e86\u200b 33.6 \u200b\u500d\u200b\uff01</p> <p>\u200b\u8fd9\u200b\u5bf9\u6a21\u578b\u200b\u5927\u5c0f\u200b\u6709\u4f55\u200b\u5f71\u54cd\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e00\u63a2\u200b\u7a76\u7adf\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#11-foodvision-big","title":"11. \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u53ef\u200b\u90e8\u7f72\u200b\u5e94\u7528\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b 20% \u200b\u7684\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u5e76\u200b\u4fdd\u5b58\u200b\u4e86\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u4e0e\u5176\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e00\u76f4\u200b\u5b58\u653e\u200b\u5728\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\uff0c\u200b\u4e0d\u5982\u200b\u5c06\u200b\u5176\u200b\u90e8\u7f72\u200b\u8d77\u6765\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ee5\u200b\u4e0e\u200b\u90e8\u7f72\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u76f8\u540c\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u5c06\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u4e3a\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b <code>demos/foodvision_big/</code> \u200b\u76ee\u5f55\u200b\u6765\u200b\u5b58\u50a8\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u6f14\u793a\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4ee5\u53ca\u200b\u4e00\u4e2a\u200b <code>demos/foodvision_big/examples</code> \u200b\u76ee\u5f55\u200b\u6765\u200b\u5b58\u653e\u200b\u7528\u4e8e\u200b\u6d4b\u8bd5\u200b\u6f14\u793a\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u3002</p> <p>\u200b\u5b8c\u6210\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u62e5\u6709\u200b\u4ee5\u4e0b\u200b\u6587\u4ef6\u200b\u7ed3\u6784\u200b\uff1a</p> <pre><code>demos/\n  foodvision_big/\n    09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\n    app.py\n    class_names.txt\n    examples/\n      example_1.jpg\n    model.py\n    requirements.txt\n</code></pre> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth</code> \u200b\u662f\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\u3002</li> <li><code>app.py</code> \u200b\u5305\u542b\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big Gradio \u200b\u5e94\u7528\u200b\u3002</li> <li><code>class_names.txt</code> \u200b\u5305\u542b\u200b FoodVision Big \u200b\u7684\u200b\u6240\u6709\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u3002</li> <li><code>examples/</code> \u200b\u5305\u542b\u200b\u7528\u4e8e\u200b Gradio \u200b\u5e94\u7528\u200b\u7684\u200b\u793a\u4f8b\u200b\u56fe\u50cf\u200b\u3002</li> <li><code>model.py</code> \u200b\u5305\u542b\u200b\u6a21\u578b\u200b\u5b9a\u4e49\u200b\u4ee5\u53ca\u200b\u4e0e\u200b\u6a21\u578b\u200b\u76f8\u5173\u200b\u7684\u200b\u4efb\u4f55\u200b\u8f6c\u6362\u200b\u3002</li> <li><code>requirements.txt</code> \u200b\u5305\u542b\u200b\u8fd0\u884c\u200b\u6211\u4eec\u200b\u5e94\u7528\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\uff0c\u200b\u5982\u200b <code>torch</code>\u3001<code>torchvision</code> \u200b\u548c\u200b <code>gradio</code>\u3002</li> </ul>"},{"location":"09_pytorch_model_deployment/#111-examples","title":"11.1 \u200b\u4e0b\u8f7d\u200b\u793a\u4f8b\u200b\u56fe\u7247\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u79fb\u52a8\u200b\u5230\u200b <code>examples</code> \u200b\u76ee\u5f55\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u53ef\u9760\u200b\u7684\u200b <code>pizza-dad</code> \u200b\u56fe\u7247\u200b\uff08\u200b\u4e00\u5f20\u200b\u6211\u200b\u7238\u7238\u200b\u5403\u200b\u62ab\u8428\u200b\u7684\u200b\u7167\u7247\u200b\uff09\u200b\u4f5c\u4e3a\u200b\u793a\u4f8b\u200b\u56fe\u7247\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b <code>!wget</code> \u200b\u547d\u4ee4\u200b\u4ece\u200b\u8bfe\u7a0b\u200b\u7684\u200b GitHub \u200b\u4e0a\u200b\u4e0b\u8f7d\u200b\u8fd9\u5f20\u200b\u56fe\u7247\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b <code>!mv</code> \u200b\u547d\u4ee4\u200b\uff08\"move\" \u200b\u7684\u200b\u7b80\u5199\u200b\uff09\u200b\u5c06\u200b\u5176\u200b\u79fb\u52a8\u200b\u5230\u200b <code>demos/foodvision_big/examples</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u540c\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u4ece\u200b <code>models/09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth</code> \u200b\u79fb\u52a8\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b Food101 EffNetB2 \u200b\u6a21\u578b\u200b\u5230\u200b <code>demos/foodvision_big</code> \u200b\u76ee\u5f55\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#112-food101class_namestxt","title":"11.2 \u200b\u5c06\u200bFood101\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\uff08<code>class_names.txt</code>\uff09\u00b6","text":"<p>\u200b\u7531\u4e8e\u200bFood101\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b\u4f17\u591a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e0d\u200b\u5c06\u200b\u5176\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u4ee5\u200b\u5217\u8868\u200b\u5f62\u5f0f\u200b\u5b58\u50a8\u200b\u5728\u200b<code>app.py</code>\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u800c\u662f\u200b\u5c06\u200b\u5176\u200b\u4fdd\u5b58\u200b\u5230\u200b<code>.txt</code>\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u9700\u8981\u200b\u65f6\u200b\u8bfb\u53d6\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u67e5\u770b\u200b<code>food101_class_names</code>\u200b\u6765\u200b\u63d0\u9192\u200b\u81ea\u5df1\u200b\u5b83\u4eec\u200b\u7684\u200b\u6837\u5b50\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#113-foodvision-big-python-modelpy","title":"11.3 \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u811a\u672c\u200b (<code>model.py</code>)\u00b6","text":"<p>\u200b\u5c31\u200b\u50cf\u200b FoodVision Mini \u200b\u6f14\u793a\u200b\u4e00\u6837\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u811a\u672c\u200b\uff0c\u200b\u8be5\u200b\u811a\u672c\u200b\u80fd\u591f\u200b\u5b9e\u4f8b\u200b\u5316\u200b\u4e00\u4e2a\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u53ca\u5176\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#114-foodvision-big-gradio-python-apppy","title":"11.4 \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big Gradio \u200b\u5e94\u7528\u200b\u8f6c\u6362\u200b\u4e3a\u200b Python \u200b\u811a\u672c\u200b (<code>app.py</code>)\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b FoodVision Big \u200b\u7684\u200b <code>model.py</code> \u200b\u811a\u672c\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b FoodVision Big \u200b\u7684\u200b <code>app.py</code> \u200b\u811a\u672c\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u57fa\u672c\u4e0a\u200b\u4e0e\u200b FoodVision Mini \u200b\u7684\u200b <code>app.py</code> \u200b\u811a\u672c\u200b\u76f8\u540c\u200b\uff0c\u200b\u9664\u4e86\u200b\u4ee5\u4e0b\u51e0\u70b9\u200b\u9700\u8981\u200b\u66f4\u6539\u200b\uff1a</p> <ol> <li>\u200b\u5bfc\u5165\u200b\u548c\u200b\u7c7b\u540d\u200b\u8bbe\u7f6e\u200b - <code>class_names</code> \u200b\u53d8\u91cf\u200b\u5c06\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6240\u6709\u200b Food101 \u200b\u7c7b\u522b\u200b\u7684\u200b\u5217\u8868\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u62ab\u8428\u200b\u3001\u200b\u725b\u6392\u200b\u3001\u200b\u5bff\u53f8\u200b\u3002\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>demos/foodvision_big/class_names.txt</code> \u200b\u8bbf\u95ee\u200b\u8fd9\u4e9b\u200b\u7c7b\u522b\u200b\u3002</li> <li>\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\u51c6\u5907\u200b - <code>model</code> \u200b\u5c06\u200b\u5177\u6709\u200b <code>num_classes=101</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>num_classes=3</code>\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u786e\u4fdd\u200b\u4ece\u200b <code>\"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"</code>\uff08\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u6a21\u578b\u200b\u8def\u5f84\u200b\uff09\u200b\u52a0\u8f7d\u200b\u6743\u91cd\u200b\u3002</li> <li>\u200b\u9884\u6d4b\u200b\u51fd\u6570\u200b - \u200b\u8fd9\u200b\u5c06\u200b\u4fdd\u6301\u200b\u4e0e\u200b FoodVision Mini \u200b\u7684\u200b <code>app.py</code> \u200b\u76f8\u540c\u200b\u3002</li> <li>Gradio \u200b\u5e94\u7528\u200b - Gradio \u200b\u754c\u9762\u200b\u5c06\u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u7684\u200b <code>title</code>\u3001<code>description</code> \u200b\u548c\u200b <code>article</code> \u200b\u53c2\u6570\u200b\uff0c\u200b\u4ee5\u200b\u53cd\u6620\u200b FoodVision Big \u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\u3002</li> </ol> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u786e\u4fdd\u200b\u4f7f\u7528\u200b <code>%%writefile</code> \u200b\u9b54\u6cd5\u200b\u547d\u4ee4\u200b\u5c06\u200b\u5176\u200b\u4fdd\u5b58\u200b\u5230\u200b <code>demos/foodvision_big/app.py</code>\u3002</p>"},{"location":"09_pytorch_model_deployment/#115-foodvision-big-requirementstxt","title":"11.5 \u200b\u4e3a\u200b FoodVision Big \u200b\u521b\u5efa\u200b\u9700\u6c42\u200b\u6587\u4ef6\u200b\uff08<code>requirements.txt</code>\uff09\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4e00\u4e2a\u200b <code>requirements.txt</code> \u200b\u6587\u4ef6\u200b\uff0c\u200b\u7528\u6765\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u7684\u200b Hugging Face Space \u200b\u5e73\u53f0\u200b FoodVision Big \u200b\u5e94\u7528\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u4f9d\u8d56\u200b\u9879\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#116-foodvision-big","title":"11.6 \u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u5e94\u7528\u200b\u6587\u4ef6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u62e5\u6709\u200b\u4e86\u200b\u5728\u200b Hugging Face \u200b\u4e0a\u200b\u90e8\u7f72\u200b FoodVision Big \u200b\u5e94\u7528\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u6240\u6709\u200b\u6587\u4ef6\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u538b\u7f29\u200b\u5e76\u200b\u4e0b\u8f7d\u200b\u4e0b\u6765\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e0e\u200b\u4e0a\u8ff0\u200b \u200b\u7b2c\u200b9.1\u200b\u8282\u200b\uff1a\u200b\u4e0b\u8f7d\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u6587\u4ef6\u200b \u200b\u4e2d\u4e3a\u200b FoodVision Mini \u200b\u5e94\u7528\u200b\u6240\u7528\u200b\u7684\u200b\u76f8\u540c\u200b\u6d41\u7a0b\u200b\u3002</p>"},{"location":"09_pytorch_model_deployment/#117-foodvision-big-huggingface-spaces","title":"11.7 \u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u5230\u200b HuggingFace Spaces\u00b6","text":"<p>\u200b\u68d2\u6781\u4e86\u200b\uff01</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u5c06\u200b\u6211\u4eec\u200b\u6574\u4e2a\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u6700\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u53d8\u4e3a\u200b\u73b0\u5b9e\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b FoodVision Big \u200b\u7684\u200b Gradio \u200b\u6f14\u793a\u200b\u90e8\u7f72\u200b\u5230\u200b Hugging Face Spaces\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4ea4\u4e92\u5f0f\u200b\u5730\u200b\u6d4b\u8bd5\u200b\u5b83\u200b\uff0c\u200b\u5e76\u200b\u8ba9\u200b\u5176\u4ed6\u4eba\u200b\u4f53\u9a8c\u200b\u6211\u4eec\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u52aa\u529b\u200b\u7684\u200b\u9b54\u529b\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6709\u200b\u591a\u79cd\u200b\u65b9\u5f0f\u200b\u4e0a\u4f20\u200b\u6587\u4ef6\u200b\u5230\u200b Hugging Face Spaces\u3002\u200b\u4ee5\u4e0b\u200b\u6b65\u9aa4\u200b\u5c06\u200b Hugging Face \u200b\u89c6\u4e3a\u200b\u4e00\u4e2a\u200b git \u200b\u4ed3\u5e93\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u6587\u4ef6\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u7f51\u9875\u200b\u754c\u9762\u200b\u6216\u200b<code>huggingface_hub</code> \u200b\u5e93\u200b\u76f4\u63a5\u200b\u4e0a\u200b\u4f20\u5230\u200b Hugging Face Spaces\u3002</p> <p>\u200b\u597d\u6d88\u606f\u200b\u662f\u200b\uff0c\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u901a\u8fc7\u200b FoodVision Mini \u200b\u5b8c\u6210\u200b\u4e86\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u5c06\u200b\u5176\u200b\u5b9a\u5236\u200b\u4ee5\u200b\u9002\u5e94\u200b FoodVision Big\uff1a</p> <ol> <li>\u200b\u6ce8\u518c\u200b\u4e00\u4e2a\u200b Hugging Face \u200b\u8d26\u6237\u200b\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u8bbf\u95ee\u200b\u4f60\u200b\u7684\u200b\u4e2a\u4eba\u8d44\u6599\u200b\u5e76\u200b\u70b9\u51fb\u200b\u201cNew Space\u201d\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u7684\u200b Hugging Face Space\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a Hugging Face \u200b\u4e2d\u200b\u7684\u200b Space \u200b\u4e5f\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u4ee3\u7801\u200b\u4ed3\u5e93\u200b\u201d\uff08\u200b\u5b58\u50a8\u200b\u4ee3\u7801\u200b/\u200b\u6587\u4ef6\u200b\u7684\u200b\u5730\u65b9\u200b\uff09\uff0c\u200b\u7b80\u79f0\u200b\u201crepo\u201d\u3002</li> </ul> </li> <li>\u200b\u7ed9\u200b Space \u200b\u547d\u540d\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u6211\u200b\u7684\u200b\u662f\u200b <code>mrdbourke/foodvision_big</code>\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u770b\u5230\u200b\u5b83\u200b\uff1ahttps://huggingface.co/spaces/mrdbourke/foodvision_big</li> <li>\u200b\u9009\u62e9\u200b\u4e00\u4e2a\u200b\u8bb8\u53ef\u8bc1\u200b\uff08\u200b\u6211\u200b\u4f7f\u7528\u200b\u4e86\u200b MIT\uff09\u3002</li> <li>\u200b\u9009\u62e9\u200b Gradio \u200b\u4f5c\u4e3a\u200b Space SDK\uff08\u200b\u8f6f\u4ef6\u5f00\u53d1\u200b\u5de5\u5177\u5305\u200b\uff09\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u5176\u4ed6\u200b\u9009\u9879\u200b\uff0c\u200b\u5982\u200b Streamlit\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u7684\u200b\u5e94\u7528\u200b\u662f\u200b\u4f7f\u7528\u200b Gradio \u200b\u6784\u5efa\u200b\u7684\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u575a\u6301\u200b\u4f7f\u7528\u200b Gradio\u3002</li> </ul> </li> <li>\u200b\u9009\u62e9\u200b\u4f60\u200b\u7684\u200b Space \u200b\u662f\u200b\u516c\u5f00\u200b\u7684\u200b\u8fd8\u662f\u200b\u79c1\u6709\u200b\u7684\u200b\uff08\u200b\u6211\u200b\u9009\u62e9\u200b\u4e86\u200b\u516c\u5f00\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u200b\u5e0c\u671b\u200b\u6211\u200b\u7684\u200b Space \u200b\u5bf9\u200b\u5176\u4ed6\u4eba\u200b\u53ef\u7528\u200b\uff09\u3002</li> <li>\u200b\u70b9\u51fb\u200b\u201cCreate Space\u201d\u3002</li> <li>\u200b\u901a\u8fc7\u200b\u5728\u200b\u7ec8\u7aef\u200b\u6216\u200b\u547d\u4ee4\u63d0\u793a\u7b26\u200b\u4e2d\u200b\u8fd0\u884c\u200b <code>git clone https://huggingface.co/spaces/[YOUR_USERNAME]/[YOUR_SPACE_NAME]</code> \u200b\u6765\u200b\u672c\u5730\u200b\u514b\u9686\u200b\u4ed3\u5e93\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u5728\u200b\u201cFiles and versions\u201d\u200b\u6807\u7b7e\u200b\u4e0b\u200b\u4e0a\u4f20\u200b\u6587\u4ef6\u200b\u6765\u200b\u6dfb\u52a0\u200b\u6587\u4ef6\u200b\u3002</li> </ul> </li> <li>\u200b\u5c06\u200b\u4e0b\u8f7d\u200b\u7684\u200b <code>foodvision_big</code> \u200b\u6587\u4ef6\u5939\u200b\u7684\u200b\u5185\u5bb9\u200b\u590d\u5236\u200b/\u200b\u79fb\u52a8\u200b\u5230\u200b\u514b\u9686\u200b\u7684\u200b\u4ed3\u5e93\u200b\u6587\u4ef6\u5939\u200b\u4e2d\u200b\u3002</li> <li>\u200b\u8981\u200b\u4e0a\u4f20\u200b\u548c\u200b\u8ddf\u8e2a\u200b\u8f83\u5927\u200b\u7684\u200b\u6587\u4ef6\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8d85\u8fc7\u200b 10MB \u200b\u7684\u200b\u6587\u4ef6\u200b\uff0c\u200b\u6216\u8005\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff09\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u5b89\u88c5\u200b Git LFS\uff08\u200b\u5373\u200b\u201cgit large file storage\u201d\uff09\u3002</li> <li>\u200b\u5b89\u88c5\u200b Git LFS \u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8fd0\u884c\u200b <code>git lfs install</code> \u200b\u6765\u200b\u6fc0\u6d3b\u200b\u5b83\u200b\u3002</li> <li>\u200b\u5728\u200b <code>foodvision_big</code> \u200b\u76ee\u5f55\u200b\u4e2d\u200b\uff0c\u200b\u4f7f\u7528\u200b Git LFS \u200b\u8ddf\u8e2a\u200b\u8d85\u8fc7\u200b 10MB \u200b\u7684\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>git lfs track \"*.file_extension\"</code>\u3002<ul> <li>\u200b\u8ddf\u8e2a\u200b EffNetB2 PyTorch \u200b\u6a21\u578b\u200b\u6587\u4ef6\u200b\uff0c\u200b\u4f7f\u7528\u200b <code>git lfs track \"09_pretrained_effnetb2_feature_extractor_food101_20_percent.pth\"</code>\u3002</li> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b\u4e0a\u4f20\u200b\u56fe\u50cf\u200b\u65f6\u200b\u9047\u5230\u200b\u4efb\u4f55\u200b\u9519\u8bef\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4e5f\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b <code>git lfs</code> \u200b\u8ddf\u8e2a\u200b\u5b83\u4eec\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>git lfs track \"examples/04-pizza-dad.jpg\"</code>\u3002</li> </ul> </li> <li>\u200b\u8ddf\u8e2a\u200b <code>.gitattributes</code>\uff08\u200b\u4ece\u200b HuggingFace \u200b\u514b\u9686\u200b\u65f6\u200b\u81ea\u52a8\u200b\u521b\u5efa\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u6587\u4ef6\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u786e\u4fdd\u200b\u6211\u4eec\u200b\u7684\u200b\u5927\u200b\u6587\u4ef6\u200b\u901a\u8fc7\u200b Git LFS \u200b\u88ab\u200b\u8ddf\u8e2a\u200b\uff09\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b FoodVision Big Hugging Face Space \u200b\u4e0a\u200b\u770b\u5230\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b <code>.gitattributes</code> \u200b\u6587\u4ef6\u200b\u3002<ul> <li><code>git add .gitattributes</code></li> </ul> </li> <li>\u200b\u6dfb\u52a0\u200b\u5176\u4f59\u200b\u7684\u200b <code>foodvision_big</code> \u200b\u5e94\u7528\u200b\u6587\u4ef6\u200b\u5e76\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u547d\u4ee4\u200b\u63d0\u4ea4\u200b\u5b83\u4eec\u200b\uff1a<ul> <li><code>git add *</code></li> <li><code>git commit -m \"first commit\"</code></li> </ul> </li> <li>\u200b\u63a8\u9001\u200b\uff08\u200b\u4e0a\u4f20\u200b\uff09\u200b\u6587\u4ef6\u200b\u5230\u200b Hugging Face\uff1a<ul> <li><code>git push</code></li> </ul> </li> <li>\u200b\u7b49\u5f85\u200b 3-5 \u200b\u5206\u949f\u200b\u8ba9\u200b\u6784\u5efa\u200b\u5b8c\u6210\u200b\uff08\u200b\u672a\u6765\u200b\u7684\u200b\u6784\u5efa\u200b\u4f1a\u200b\u66f4\u200b\u5feb\u200b\uff09\uff0c\u200b\u4f60\u200b\u7684\u200b\u5e94\u7528\u200b\u5c31\u200b\u4f1a\u200b\u4e0a\u7ebf\u200b\uff01</li> </ol> <p>\u200b\u5982\u679c\u200b\u4e00\u5207\u6b63\u5e38\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big Gradio \u200b\u6f14\u793a\u200b\u5e94\u8be5\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\uff01</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u8fd9\u91cc\u200b\u770b\u5230\u200b\u6211\u200b\u7684\u200b\u7248\u672c\u200b\uff1ahttps://huggingface.co/spaces/mrdbourke/foodvision_big/</p> <p>\u200b\u6216\u8005\u200b\u6211\u4eec\u200b\u751a\u81f3\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Big Gradio \u200b\u6f14\u793a\u200b\u76f4\u63a5\u200b\u5d4c\u5165\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b iframe\uff0c\u200b\u4f7f\u7528\u200b <code>IPython.display.IFrame</code> \u200b\u548c\u200b\u4e00\u4e2a\u200b\u94fe\u63a5\u200b\uff0c\u200b\u683c\u5f0f\u200b\u4e3a\u200b <code>https://hf.space/embed/[YOUR_USERNAME]/[YOUR_SPACE_NAME]/+</code>\u3002</p>"},{"location":"09_pytorch_model_deployment/","title":"\u4e3b\u8981\u200b\u6536\u83b7\u200b\u00b6","text":"<ul> <li>\u200b\u90e8\u7f72\u200b\u4e0e\u200b\u8bad\u7ec3\u200b\u540c\u6837\u200b\u91cd\u8981\u200b\u3002 \u200b\u4e00\u65e6\u200b\u4f60\u200b\u62e5\u6709\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8868\u73b0\u200b\u826f\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u9996\u8981\u200b\u95ee\u9898\u200b\u5e94\u8be5\u200b\u662f\u200b\uff1a\u200b\u6211\u200b\u5982\u4f55\u200b\u90e8\u7f72\u200b\u8fd9\u4e2a\u200b\u6a21\u578b\u200b\u5e76\u200b\u4f7f\u200b\u5176\u200b\u5bf9\u200b\u4ed6\u4eba\u200b\u53ef\u7528\u200b\uff1f\u200b\u90e8\u7f72\u200b\u8ba9\u200b\u4f60\u200b\u80fd\u591f\u200b\u5728\u200b\u73b0\u5b9e\u200b\u4e16\u754c\u200b\u4e2d\u200b\u6d4b\u8bd5\u200b\u6a21\u578b\u200b\uff0c\u200b\u800c\u200b\u4e0d\u4ec5\u4ec5\u200b\u662f\u200b\u5728\u200b\u79c1\u6709\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u96c6\u4e0a\u200b\u3002</li> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u7684\u200b\u4e09\u4e2a\u200b\u95ee\u9898\u200b\uff1a<ol> <li>\u200b\u6a21\u578b\u200b\u6700\u200b\u7406\u60f3\u200b\u7684\u200b\u4f7f\u7528\u200b\u573a\u666f\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff08\u200b\u5b83\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff0c\u200b\u901f\u5ea6\u200b\u5982\u4f55\u200b\uff09\uff1f</li> <li>\u200b\u6a21\u578b\u200b\u5c06\u200b\u90e8\u7f72\u200b\u5728\u200b\u54ea\u91cc\u200b\uff08\u200b\u662f\u200b\u5728\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fd8\u662f\u200b\u5728\u200b\u4e91\u7aef\u200b\uff09\uff1f</li> <li>\u200b\u6a21\u578b\u200b\u7684\u200b\u529f\u80fd\u200b\u5982\u4f55\u200b\u5b9e\u73b0\u200b\uff08\u200b\u9884\u6d4b\u200b\u662f\u200b\u5728\u7ebf\u200b\u8fd8\u662f\u200b\u79bb\u7ebf\u200b\uff09\uff1f</li> </ol> </li> <li>\u200b\u90e8\u7f72\u200b\u9009\u9879\u200b\u4f17\u591a\u200b\u3002 \u200b\u4f46\u200b\u6700\u597d\u200b\u4ece\u200b\u7b80\u5355\u200b\u5f00\u59cb\u200b\u3002\u200b\u76ee\u524d\u200b\u6700\u597d\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4e4b\u4e00\u200b\uff08\u200b\u6211\u200b\u8bf4\u200b\u76ee\u524d\u200b\u662f\u56e0\u4e3a\u200b\u8fd9\u4e9b\u200b\u4e8b\u60c5\u200b\u603b\u662f\u200b\u5728\u200b\u53d8\u5316\u200b\uff09\u200b\u662f\u200b\u4f7f\u7528\u200b Gradio \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u6f14\u793a\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u6258\u7ba1\u200b\u5728\u200b Hugging Face Spaces \u200b\u4e0a\u200b\u3002\u200b\u4ece\u200b\u7b80\u5355\u200b\u5f00\u59cb\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u9700\u8981\u200b\u65f6\u200b\u8fdb\u884c\u200b\u6269\u5c55\u200b\u3002</li> <li>\u200b\u6c38\u4e0d\u200b\u505c\u6b62\u200b\u5b9e\u9a8c\u200b\u3002 \u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u9700\u6c42\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u968f\u200b\u65f6\u95f4\u200b\u53d8\u5316\u200b\uff0c\u200b\u56e0\u6b64\u200b\u90e8\u7f72\u200b\u5355\u4e2a\u200b\u6a21\u578b\u200b\u5e76\u200b\u4e0d\u662f\u200b\u6700\u540e\u200b\u4e00\u6b65\u200b\u3002\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u73b0\u200b\u6570\u636e\u200b\u96c6\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9700\u8981\u200b\u66f4\u65b0\u200b\u6a21\u578b\u200b\u3002\u200b\u6216\u8005\u200b\u65b0\u200b\u7684\u200b\u7814\u7a76\u200b\u53d1\u5e03\u200b\uff0c\u200b\u6709\u200b\u66f4\u597d\u200b\u7684\u200b\u67b6\u6784\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u3002<ul> <li>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u90e8\u7f72\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u968f\u7740\u200b\u65f6\u95f4\u200b\u7684\u200b\u63a8\u79fb\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u66f4\u65b0\u200b\u3002</li> </ul> </li> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u662f\u200b MLOps\uff08\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8fd0\u7ef4\u200b\uff09\u200b\u5de5\u7a0b\u200b\u5b9e\u8df5\u200b\u7684\u200b\u4e00\u90e8\u5206\u200b\u3002 MLOps \u200b\u662f\u200b DevOps\uff08\u200b\u5f00\u53d1\u200b\u8fd0\u7ef4\u200b\uff09\u200b\u7684\u200b\u6269\u5c55\u200b\uff0c\u200b\u6d89\u53ca\u200b\u56f4\u7ed5\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6240\u6709\u200b\u5de5\u7a0b\u200b\u90e8\u5206\u200b\uff1a\u200b\u6570\u636e\u200b\u6536\u96c6\u200b\u548c\u200b\u5b58\u50a8\u200b\u3001\u200b\u6570\u636e\u200b\u9884\u5904\u7406\u200b\u3001\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u3001\u200b\u6a21\u578b\u200b\u76d1\u63a7\u200b\u3001\u200b\u7248\u672c\u63a7\u5236\u200b\u7b49\u200b\u3002\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u5feb\u901f\u200b\u53d1\u5c55\u200b\u7684\u200b\u9886\u57df\u200b\uff0c\u200b\u4f46\u200b\u6709\u200b\u4e00\u4e9b\u200b\u53ef\u9760\u200b\u7684\u200b\u8d44\u6e90\u200b\u53ef\u4ee5\u200b\u5b66\u4e60\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bb8\u591a\u200b\u8d44\u6e90\u200b\u5728\u200b PyTorch \u200b\u989d\u5916\u200b\u8d44\u6e90\u200b\u4e2d\u200b\u3002</li> </ul>"},{"location":"09_pytorch_model_deployment/","title":"\u7ec3\u4e60\u200b\u00b6","text":"<p>\u200b\u6240\u6709\u200b\u7ec3\u4e60\u200b\u90fd\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u5b9e\u8df5\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u53c2\u8003\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u6216\u200b\u9075\u5faa\u200b\u94fe\u63a5\u200b\u7684\u200b\u8d44\u6e90\u200b\u6765\u200b\u5b8c\u6210\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a</p> <ul> <li>09 \u200b\u7ec3\u4e60\u200b\u6a21\u677f\u200b\u7b14\u8bb0\u672c\u200b\u3002</li> <li>09 \u200b\u7ec3\u4e60\u200b\u793a\u4f8b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7b14\u8bb0\u672c\u200b \u200b\u5728\u200b\u67e5\u770b\u200b\u8fd9\u4e2a\u200b\u4e4b\u524d\u200b\u5c1d\u8bd5\u200b\u7ec3\u4e60\u200b\u3002<ul> <li>\u200b\u5728\u200b YouTube \u200b\u4e0a\u200b\u89c2\u770b\u200b\u89e3\u51b3\u65b9\u6848\u200b\u7684\u200b\u89c6\u9891\u200b\u6f14\u793a\u200b\uff08\u200b\u5305\u62ec\u200b\u6240\u6709\u200b\u9519\u8bef\u200b\uff09\u3002</li> </ul> </li> </ul> <ol> <li>\u200b\u4f7f\u7528\u200b GPU (<code>device=\"cuda\"</code>) \u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u7684\u200b\u4e24\u4e2a\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8ba1\u65f6\u200b\u3002\u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u5728\u200b GPU \u200b\u548c\u200b CPU \u200b\u4e0a\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b - \u200b\u8fd9\u4f1a\u200b\u7f29\u5c0f\u200b\u5b83\u4eec\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u8ddd\u200b\u5417\u200b\uff1f\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u662f\u5426\u200b\u4f7f\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u66f4\u200b\u63a5\u8fd1\u200b EffNetB2 \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\uff1f<ul> <li>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b\u7b2c\u200b 5 \u200b\u8282\u200b\uff1a\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u8ba1\u65f6\u200b\u548c\u200b\u7b2c\u200b 6 \u200b\u8282\u200b\uff1a\u200b\u6bd4\u8f83\u200b\u6a21\u578b\u200b\u7ed3\u679c\u200b\u3001\u200b\u9884\u6d4b\u200b\u65f6\u95f4\u200b\u548c\u200b\u5927\u5c0f\u200b\u4e2d\u200b\u627e\u5230\u200b\u6267\u884c\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\u7684\u200b\u4ee3\u7801\u200b\u3002</li> </ul> </li> <li>ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u4f3c\u4e4e\u200b\u6bd4\u200b EffNetB2 \u200b\u5177\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5b66\u4e60\u200b\u80fd\u529b\u200b\uff08\u200b\u7531\u4e8e\u200b\u53c2\u6570\u200b\u66f4\u200b\u591a\u200b\uff09\uff0c\u200b\u5b83\u200b\u5728\u200b\u6574\u4e2a\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b 20% \u200b\u5206\u5272\u200b\u4e0a\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff1f<ul> <li>\u200b\u5728\u200b 20% \u200b\u7684\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6211\u4eec\u200b\u5728\u200b\u7b2c\u200b 10 \u200b\u8282\u200b\uff1a\u200b\u521b\u5efa\u200b FoodVision Big \u200b\u4e2d\u200b\u5bf9\u200b EffNetB2 \u200b\u6240\u200b\u505a\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u8fdb\u884c\u200b 5 \u200b\u4e2a\u200b epoch\u3002</li> </ul> </li> <li>\u200b\u4f7f\u7528\u200b\u7ec3\u4e60\u200b 2 \u200b\u4e2d\u200b\u7684\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u5bf9\u200b 20% \u200b\u7684\u200b Food101 \u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff0c\u200b\u5e76\u200b\u627e\u51fa\u200b\u201c\u200b\u6700\u200b\u9519\u8bef\u200b\u201d\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002<ul> <li>\u200b\u8fd9\u4e9b\u200b\u9884\u6d4b\u200b\u5c06\u200b\u662f\u200b\u5177\u6709\u200b\u6700\u9ad8\u200b\u9884\u6d4b\u200b\u6982\u7387\u200b\u4f46\u200b\u5177\u6709\u200b\u9519\u8bef\u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b\u7684\u200b\u9884\u6d4b\u200b\u3002</li> <li>\u200b\u5199\u200b\u4e00\u4e24\u53e5\u200b\u5173\u4e8e\u200b\u4e3a\u4ec0\u4e48\u200b\u4f60\u200b\u8ba4\u4e3a\u200b\u6a21\u578b\u200b\u4f1a\u200b\u505a\u51fa\u200b\u8fd9\u4e9b\u200b\u9519\u8bef\u200b\u9884\u6d4b\u200b\u7684\u200b\u539f\u56e0\u200b\u3002</li> </ul> </li> <li>\u200b\u5728\u200b\u6574\u4e2a\u200b Food101 \u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bc4\u4f30\u200b ViT \u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u4ec5\u4ec5\u200b\u5728\u200b 20% \u200b\u7684\u200b\u7248\u672c\u200b\u4e0a\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u8868\u73b0\u200b\u5982\u4f55\u200b\uff1f<ul> <li>\u200b\u5b83\u200b\u662f\u5426\u200b\u51fb\u8d25\u200b\u4e86\u200b\u539f\u59cb\u200b Food101 \u200b\u8bba\u6587\u200b\u7684\u200b\u6700\u4f73\u200b\u7ed3\u679c\u200b 56.4% \u200b\u7684\u200b\u51c6\u786e\u7387\u200b\uff1f</li> </ul> </li> <li>\u200b\u524d\u5f80\u200b Paperswithcode.com \u200b\u5e76\u200b\u627e\u5230\u200b Food101 \u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u5f53\u524d\u200b\u8868\u73b0\u200b\u6700\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u3002<ul> <li>\u200b\u5b83\u200b\u4f7f\u7528\u200b\u4e86\u200b\u4ec0\u4e48\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\uff1f</li> </ul> </li> <li>\u200b\u5199\u4e0b\u200b\u6211\u4eec\u200b\u90e8\u7f72\u200b\u7684\u200b FoodVision \u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u5b58\u5728\u200b\u7684\u200b 1-3 \u200b\u4e2a\u200b\u6f5c\u5728\u200b\u5931\u8d25\u200b\u70b9\u200b\u4ee5\u53ca\u200b\u4e00\u4e9b\u200b\u53ef\u80fd\u200b\u7684\u200b\u89e3\u51b3\u65b9\u6848\u200b\u3002<ul> <li>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u6709\u4eba\u200b\u4e0a\u4f20\u200b\u4e86\u200b\u4e00\u5f20\u200b\u4e0d\u662f\u200b\u98df\u7269\u200b\u7684\u200b\u7167\u7247\u200b\u5230\u200b\u6211\u4eec\u200b\u7684\u200b FoodVision Mini \u200b\u6a21\u578b\u200b\u4e2d\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</li> </ul> </li> <li>\u200b\u4ece\u200b <code>torchvision.datasets</code> \u200b\u4e2d\u200b\u9009\u62e9\u200b\u4efb\u610f\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torchvision.models</code> \u200b\u4e2d\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4f8b\u5982\u200b EffNetB2 \u200b\u6216\u200b ViT\uff09\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b 5 \u200b\u4e2a\u200b epoch \u200b\u7684\u200b\u7279\u5f81\u63d0\u53d6\u200b\u5668\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u7136\u540e\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u4f5c\u4e3a\u200b Gradio \u200b\u5e94\u7528\u200b\u90e8\u7f72\u200b\u5230\u200b Hugging Face Spaces\u3002<ul> <li>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u9009\u62e9\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u6216\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u5206\u5272\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u8bad\u7ec3\u200b\u4e0d\u4f1a\u200b\u82b1\u8d39\u200b\u592a\u200b\u957f\u65f6\u95f4\u200b\u3002</li> <li>\u200b\u6211\u200b\u5f88\u200b\u60f3\u200b\u770b\u5230\u200b\u4f60\u200b\u90e8\u7f72\u200b\u7684\u200b\u6a21\u578b\u200b\uff01\u200b\u6240\u4ee5\u200b\u8bf7\u200b\u52a1\u5fc5\u200b\u5728\u200b Discord \u200b\u6216\u200b\u8bfe\u7a0b\u200b GitHub \u200b\u8ba8\u8bba\u200b\u9875\u9762\u200b\u4e0a\u200b\u5206\u4eab\u200b\u5b83\u4eec\u200b\u3002</li> </ul> </li> </ol>"},{"location":"09_pytorch_model_deployment/","title":"\u8bfe\u5916\u200b\u5b66\u4e60\u200b\u00b6","text":"<ul> <li>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u90e8\u7f72\u200b\u901a\u5e38\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5de5\u7a0b\u200b\u6311\u6218\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u7eaf\u7cb9\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6311\u6218\u200b\uff0c\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u8d44\u6e90\u200b\u8bf7\u200b\u53c2\u89c1\u200b PyTorch \u200b\u989d\u5916\u200b\u8d44\u6e90\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\u90e8\u5206\u200b\u3002<ul> <li>\u200b\u5176\u4e2d\u200b\u4f60\u200b\u4f1a\u200b\u627e\u5230\u200b Chip Huyen \u200b\u7684\u200b\u4e66\u200b Designing Machine Learning Systems\uff08\u200b\u7279\u522b\u200b\u662f\u200b\u5173\u4e8e\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u7684\u200b\u7b2c\u200b7\u200b\u7ae0\u200b\uff09\u200b\u548c\u200b Goku Mohandas \u200b\u7684\u200b Made with ML MLOps \u200b\u8bfe\u7a0b\u200b \u200b\u7b49\u200b\u63a8\u8350\u200b\u8d44\u6e90\u200b\u3002</li> </ul> </li> <li>\u200b\u968f\u7740\u200b\u4f60\u200b\u5f00\u59cb\u200b\u6784\u5efa\u200b\u8d8a\u6765\u8d8a\u200b\u591a\u200b\u7684\u200b\u9879\u76ee\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9891\u7e41\u200b\u4f7f\u7528\u200b Git\uff08\u200b\u4ee5\u53ca\u200b\u53ef\u80fd\u200b\u7684\u200b GitHub\uff09\u3002\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u8fd9\u200b\u4e24\u8005\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u6211\u200b\u63a8\u8350\u200b\u89c2\u770b\u200b freeCodeCamp YouTube \u200b\u9891\u9053\u200b\u7684\u200b Git \u200b\u548c\u200b GitHub \u200b\u521d\u5b66\u8005\u200b\u901f\u6210\u200b\u8bfe\u7a0b\u200b \u200b\u89c6\u9891\u200b\u3002</li> <li>\u200b\u6211\u4eec\u200b\u4ec5\u200b\u89e6\u53ca\u200b\u4e86\u200b Gradio \u200b\u53ef\u80fd\u6027\u200b\u7684\u200b\u8868\u9762\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u67e5\u770b\u200b \u200b\u5b8c\u6574\u200b\u6587\u6863\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\uff1a<ul> <li>\u200b\u6240\u6709\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b \u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u7ec4\u4ef6\u200b\u3002</li> <li>\u200b\u7528\u4e8e\u200b\u66f4\u200b\u9ad8\u7ea7\u200b\u5de5\u4f5c\u200b\u6d41\u200b\u7684\u200b Gradio Blocks API\u3002</li> <li>Hugging Face \u200b\u8bfe\u7a0b\u200b\u7ae0\u8282\u200b\u5173\u4e8e\u200b \u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b Gradio \u200b\u4e0e\u200b Hugging Face\u3002</li> </ul> </li> <li>\u200b\u8fb9\u7f18\u200b\u8bbe\u5907\u200b\u4e0d\u4ec5\u200b\u9650\u4e8e\u200b\u624b\u673a\u200b\uff0c\u200b\u8fd8\u200b\u5305\u62ec\u200b\u50cf\u200b\u6811\u8393\u200b\u6d3e\u200b\u8fd9\u6837\u200b\u7684\u200b\u5c0f\u578b\u200b\u8ba1\u7b97\u673a\u200b\uff0cPyTorch \u200b\u56e2\u961f\u200b\u6709\u200b\u4e00\u7bc7\u200b \u200b\u7cbe\u5f69\u200b\u7684\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200b\u6559\u7a0b\u200b \u200b\u5173\u4e8e\u200b\u5c06\u200b PyTorch \u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5230\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</li> <li>\u200b\u5173\u4e8e\u200b\u5f00\u53d1\u200b AI \u200b\u548c\u200b ML \u200b\u9a71\u52a8\u200b\u7684\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u7684\u200b\u7edd\u4f73\u200b\u6307\u5357\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u89c1\u200b Google \u200b\u7684\u200b People + AI Guidebook\u3002\u200b\u6211\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\u90e8\u5206\u200b\u662f\u200b\u5173\u4e8e\u200b \u200b\u8bbe\u5b9a\u200b\u6b63\u786e\u200b\u7684\u200b\u671f\u671b\u200b\u3002<ul> <li>\u200b\u6211\u200b\u5728\u200b 2021\u200b\u5e74\u200b4\u200b\u6708\u200b\u7248\u200b Machine Learning Monthly\uff08\u200b\u6211\u200b\u6bcf\u6708\u200b\u53d1\u9001\u200b\u7684\u200b\u5305\u542b\u200b ML \u200b\u9886\u57df\u200b\u6700\u65b0\u200b\u52a8\u6001\u200b\u7684\u200b\u901a\u8baf\u200b\uff09\u200b\u4e2d\u200b\u6db5\u76d6\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u8fd9\u7c7b\u200b\u8d44\u6e90\u200b\uff0c\u200b\u5305\u62ec\u200b\u6765\u81ea\u200b Apple\u3001Microsoft \u200b\u7b49\u200b\u7684\u200b\u6307\u5357\u200b\u3002</li> </ul> </li> <li>\u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u52a0\u5feb\u200b\u6a21\u578b\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\u7684\u200b\u8fd0\u884c\u200b\u65f6\u95f4\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u4e86\u89e3\u200b TorchScript\u3001ONNX\uff08\u200b\u5f00\u653e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4ea4\u6362\u200b\uff09\u200b\u548c\u200b OpenVINO\u3002\u200b\u4ece\u7eaf\u200b PyTorch \u200b\u8f6c\u6362\u200b\u5230\u200b ONNX/OpenVINO \u200b\u6a21\u578b\u200b\uff0c\u200b\u6211\u200b\u770b\u5230\u200b\u4e86\u200b\u6027\u80fd\u200b\u63d0\u5347\u200b\u7ea6\u200b2\u200b\u500d\u200b\u4ee5\u4e0a\u200b\u3002</li> <li>\u200b\u5173\u4e8e\u200b\u5c06\u200b\u6a21\u578b\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u53ef\u200b\u90e8\u7f72\u200b\u548c\u200b\u53ef\u200b\u6269\u5c55\u200b\u7684\u200b API\uff0c\u200b\u8bf7\u200b\u53c2\u89c1\u200b TorchServe \u200b\u5e93\u200b\u3002</li> <li>\u200b\u5173\u4e8e\u200b\u4e3a\u4ec0\u4e48\u200b\u5728\u200b\u6d4f\u89c8\u5668\u200b\u4e2d\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff08\u200b\u4e00\u79cd\u200b\u8fb9\u7f18\u200b\u90e8\u7f72\u200b\u5f62\u5f0f\u200b\uff09\u200b\u63d0\u4f9b\u200b\u591a\u79cd\u200b\u4f18\u52bf\u200b\uff08\u200b\u65e0\u9700\u200b\u7f51\u7edc\u200b\u4f20\u8f93\u200b\u5ef6\u8fdf\u200b\uff09\u200b\u7684\u200b\u7cbe\u5f69\u200b\u793a\u4f8b\u200b\u548c\u200b\u7406\u7531\u200b\uff0c\u200b\u8bf7\u200b\u53c2\u89c1\u200b Jo Kristian Bergum \u200b\u7684\u200b\u6587\u7ae0\u200b Moving ML Inference from the Cloud to the Edge\u3002</li> </ul>"},{"location":"10_pytorch_profiling/","title":"08: PyTorch Profiling","text":"In\u00a0[10]: Copied! <pre>import torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import transforms, datasets\nfrom torchinfo import summary\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom going_modular import data_setup, engine\n</pre> import torch import torchvision from torch import nn from torchvision import transforms, datasets from torchinfo import summary  import numpy as np import matplotlib.pyplot as plt  from going_modular import data_setup, engine In\u00a0[11]: Copied! <pre>device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice\n</pre> device = \"cuda\" if torch.cuda.is_available() else \"cpu\" device Out[11]: <pre>'cuda'</pre> In\u00a0[12]: Copied! <pre>import os\nimport requests\nfrom zipfile import ZipFile\n\ndef get_food_image_data():\n    if not os.path.exists(\"data/10_whole_foods\"):\n        os.makedirs(\"data/\", exist_ok=True)\n        # Download data\n        data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"\n        print(f\"Downloading data from {data_url}...\")\n        requests.get(data_url)\n        # Unzip data\n        targ_dir = \"data/10_whole_foods\"\n        print(f\"Extracting data to {targ_dir}...\")\n        with ZipFile(\"10_whole_foods.zip\") as zip_ref:\n            zip_ref.extractall(targ_dir)\n    else:\n        print(\"data/10_whole_foods dir exists, skipping download\")\n\nget_food_image_data()\n</pre> import os import requests from zipfile import ZipFile  def get_food_image_data():     if not os.path.exists(\"data/10_whole_foods\"):         os.makedirs(\"data/\", exist_ok=True)         # Download data         data_url = \"https://storage.googleapis.com/food-vision-image-playground/10_whole_foods.zip\"         print(f\"Downloading data from {data_url}...\")         requests.get(data_url)         # Unzip data         targ_dir = \"data/10_whole_foods\"         print(f\"Extracting data to {targ_dir}...\")         with ZipFile(\"10_whole_foods.zip\") as zip_ref:             zip_ref.extractall(targ_dir)     else:         print(\"data/10_whole_foods dir exists, skipping download\")  get_food_image_data() <pre>data/10_whole_foods dir exists, skipping download\n</pre> In\u00a0[38]: Copied! <pre># Setup dirs\ntrain_dir = \"data/10_whole_foods/train\"\ntest_dir = \"data/10_whole_foods/test\"\n\n# Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet)\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\n# Create starter transform\nsimple_transform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    normalize\n])           \n\n# Create data loaders\ntrain_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n    train_dir=train_dir,\n    test_dir=test_dir,\n    transform=simple_transform,\n    batch_size=32,\n    num_workers=8\n)\n\ntrain_dataloader, test_dataloader, class_names\n</pre> # Setup dirs train_dir = \"data/10_whole_foods/train\" test_dir = \"data/10_whole_foods/test\"  # Setup ImageNet normalization levels (turns all images into similar distribution as ImageNet) normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],                                  std=[0.229, 0.224, 0.225])  # Create starter transform simple_transform = transforms.Compose([     transforms.Resize((224, 224)),     transforms.ToTensor(),     normalize ])             # Create data loaders train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(     train_dir=train_dir,     test_dir=test_dir,     transform=simple_transform,     batch_size=32,     num_workers=8 )  train_dataloader, test_dataloader, class_names Out[38]: <pre>(&lt;torch.utils.data.dataloader.DataLoader at 0x7f052ab26e20&gt;,\n &lt;torch.utils.data.dataloader.DataLoader at 0x7f05299eed00&gt;,\n ['apple',\n  'banana',\n  'beef',\n  'blueberries',\n  'carrots',\n  'chicken_wings',\n  'egg',\n  'honey',\n  'mushrooms',\n  'strawberries'])</pre> In\u00a0[66]: Copied! <pre>model = torchvision.models.efficientnet_b0(pretrained=True).to(device)\n# model\n</pre> model = torchvision.models.efficientnet_b0(pretrained=True).to(device) # model In\u00a0[67]: Copied! <pre># Update the classifier\nmodel.classifier = torch.nn.Sequential(\n    nn.Dropout(p=0.2),\n    nn.Linear(1280, len(class_names)).to(device))\n\n# Freeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = False\n</pre> # Update the classifier model.classifier = torch.nn.Sequential(     nn.Dropout(p=0.2),     nn.Linear(1280, len(class_names)).to(device))  # Freeze all base layers  for param in model.features.parameters():     param.requires_grad = False In\u00a0[68]: Copied! <pre># Define loss and optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n</pre> # Define loss and optimizer loss_fn = nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(), lr=0.001) In\u00a0[69]: Copied! <pre>model.name = \"EfficietNetB0\"\nmodel.name\n</pre> model.name = \"EfficietNetB0\" model.name Out[69]: <pre>'EfficietNetB0'</pre> In\u00a0[70]: Copied! <pre>from torch.utils.tensorboard import SummaryWriter\nfrom going_modular.engine import train_step, test_step\nfrom tqdm import tqdm\nwriter = SummaryWriter()\n</pre> from torch.utils.tensorboard import SummaryWriter from going_modular.engine import train_step, test_step from tqdm import tqdm writer = SummaryWriter() <p>Update the <code>train_step()</code> function to include the PyTorch profiler.</p> In\u00a0[71]: Copied! <pre>def train_step(model, dataloader, loss_fn, optimizer):\n    model.train()\n    train_loss, train_acc = 0, 0\n    ## NEW: Add PyTorch profiler\n\n    dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))\n    with torch.profiler.profile(\n        on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),\n        # with_stack=True # this adds a lot of overhead to training (tracing all the stack)\n    ):\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to GPU\n            X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)\n            \n            # Turn on mixed precision if available\n            with torch.autocast(device_type=device, enabled=True):\n                # 1. Forward pass\n                y_pred = model(X)\n\n                # 2. Calculate loss\n                loss = loss_fn(y_pred, y)\n\n            # 3. Optimizer zero grad\n            optimizer.zero_grad()\n\n            # 4. Loss backward\n            loss.backward()\n\n            # 5. Optimizer step\n            optimizer.step()\n\n            # 6. Calculate metrics\n            train_loss += loss.item()\n            y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)\n            # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")\n            # print(f\"y argmax: {y_pred.argmax(dim=1)}\")\n            # print(f\"Equal: {(y_pred_class == y)}\")\n            train_acc += (y_pred_class == y).sum().item() / len(y_pred)\n            # print(f\"batch: {batch} train_acc: {train_acc}\")\n\n    # Adjust returned metrics\n    return train_loss / len(dataloader), train_acc / len(dataloader)\n</pre> def train_step(model, dataloader, loss_fn, optimizer):     model.train()     train_loss, train_acc = 0, 0     ## NEW: Add PyTorch profiler      dir_to_save_logs = os.path.join(\"logs\", datetime.now().strftime(\"%Y-%m-%d-%H-%M\"))     with torch.profiler.profile(         on_trace_ready=torch.profiler.tensorboard_trace_handler(dir_name=dir_to_save_logs),         # with_stack=True # this adds a lot of overhead to training (tracing all the stack)     ):         for batch, (X, y) in enumerate(dataloader):             # Send data to GPU             X, y = X.to(device, non_blocking=True), y.to(device, non_blocking=True)                          # Turn on mixed precision if available             with torch.autocast(device_type=device, enabled=True):                 # 1. Forward pass                 y_pred = model(X)                  # 2. Calculate loss                 loss = loss_fn(y_pred, y)              # 3. Optimizer zero grad             optimizer.zero_grad()              # 4. Loss backward             loss.backward()              # 5. Optimizer step             optimizer.step()              # 6. Calculate metrics             train_loss += loss.item()             y_pred_class = torch.softmax(y_pred, dim=1).argmax(dim=1)             # print(f\"y: \\n{y}\\ny_pred_class:{y_pred_class}\")             # print(f\"y argmax: {y_pred.argmax(dim=1)}\")             # print(f\"Equal: {(y_pred_class == y)}\")             train_acc += (y_pred_class == y).sum().item() / len(y_pred)             # print(f\"batch: {batch} train_acc: {train_acc}\")      # Adjust returned metrics     return train_loss / len(dataloader), train_acc / len(dataloader) <p>TK - Now to use the writer, we've got to adjust the <code>train()</code> function...</p> In\u00a0[72]: Copied! <pre>def train(\n    model,\n    train_dataloader,\n    test_dataloader,\n    optimizer,\n    loss_fn=nn.CrossEntropyLoss(),\n    epochs=5,\n):\n\n    results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(\n            model=model,\n            dataloader=train_dataloader,\n            loss_fn=loss_fn,\n            optimizer=optimizer,\n        )\n        test_loss, test_acc = test_step(\n            model=model, dataloader=test_dataloader, loss_fn=loss_fn\n        )\n\n        # Print out what's happening\n        print(\n            f\"Epoch: {epoch+1} | \"\n            f\"train_loss: {train_loss:.4f} | \"\n            f\"train_acc: {train_acc:.4f} | \"\n            f\"test_loss: {test_loss:.4f} | \"\n            f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n        # Add results to SummaryWriter\n        writer.add_scalars(main_tag=\"Loss\", \n                           tag_scalar_dict={\"train_loss\": train_loss,\n                                            \"test_loss\": test_loss},\n                           global_step=epoch)\n        writer.add_scalars(main_tag=\"Accuracy\", \n                           tag_scalar_dict={\"train_acc\": train_acc,\n                                            \"test_acc\": test_acc}, \n                           global_step=epoch)\n    \n    # Close the writer\n    writer.close()\n\n    return results\n</pre> def train(     model,     train_dataloader,     test_dataloader,     optimizer,     loss_fn=nn.CrossEntropyLoss(),     epochs=5, ):      results = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}      for epoch in tqdm(range(epochs)):         train_loss, train_acc = train_step(             model=model,             dataloader=train_dataloader,             loss_fn=loss_fn,             optimizer=optimizer,         )         test_loss, test_acc = test_step(             model=model, dataloader=test_dataloader, loss_fn=loss_fn         )          # Print out what's happening         print(             f\"Epoch: {epoch+1} | \"             f\"train_loss: {train_loss:.4f} | \"             f\"train_acc: {train_acc:.4f} | \"             f\"test_loss: {test_loss:.4f} | \"             f\"test_acc: {test_acc:.4f}\"         )          # Update results         results[\"train_loss\"].append(train_loss)         results[\"train_acc\"].append(train_acc)         results[\"test_loss\"].append(test_loss)         results[\"test_acc\"].append(test_acc)          # Add results to SummaryWriter         writer.add_scalars(main_tag=\"Loss\",                             tag_scalar_dict={\"train_loss\": train_loss,                                             \"test_loss\": test_loss},                            global_step=epoch)         writer.add_scalars(main_tag=\"Accuracy\",                             tag_scalar_dict={\"train_acc\": train_acc,                                             \"test_acc\": test_acc},                             global_step=epoch)          # Close the writer     writer.close()      return results In\u00a0[73]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:05&lt;00:21,  5.27s/it]</pre> <pre>Epoch: 1 | train_loss: 1.9644 | train_acc: 0.4386 | test_loss: 1.5205 | test_acc: 0.7865\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:09&lt;00:14,  4.94s/it]</pre> <pre>Epoch: 2 | train_loss: 1.2589 | train_acc: 0.7878 | test_loss: 1.1589 | test_acc: 0.7604\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:14&lt;00:09,  4.72s/it]</pre> <pre>Epoch: 3 | train_loss: 0.8642 | train_acc: 0.8776 | test_loss: 0.9347 | test_acc: 0.7917\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.56s/it]</pre> <pre>Epoch: 4 | train_loss: 0.6827 | train_acc: 0.8856 | test_loss: 0.6637 | test_acc: 0.8750\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:23&lt;00:00,  4.65s/it]</pre> <pre>Epoch: 5 | train_loss: 0.5688 | train_acc: 0.9069 | test_loss: 0.6175 | test_acc: 0.8854\n</pre> <pre>\n</pre> <p>Looks like mixed precision doesn't offer much benefit for smaller feature extraction models...</p> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750\n\n# # With mixed precision\n#  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906\n</pre> # # Without mixed precision #  20%|\u2588\u2588        | 1/5 [00:03&lt;00:14,  3.71s/it]Epoch: 1 | train_loss: 0.5229 | train_acc: 0.9054 | test_loss: 0.5776 | test_acc: 0.8542 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:07&lt;00:11,  3.74s/it]Epoch: 2 | train_loss: 0.4699 | train_acc: 0.9001 | test_loss: 0.5160 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:10&lt;00:07,  3.63s/it]Epoch: 3 | train_loss: 0.3913 | train_acc: 0.9196 | test_loss: 0.4888 | test_acc: 0.8906 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:14&lt;00:03,  3.61s/it]Epoch: 4 | train_loss: 0.3724 | train_acc: 0.9371 | test_loss: 0.4931 | test_acc: 0.8698 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:18&lt;00:00,  3.61s/it]Epoch: 5 | train_loss: 0.3315 | train_acc: 0.9381 | test_loss: 0.4405 | test_acc: 0.8750  # # With mixed precision #  20%|\u2588\u2588        | 1/5 [00:04&lt;00:17,  4.40s/it]Epoch: 1 | train_loss: 0.3027 | train_acc: 0.9554 | test_loss: 0.4386 | test_acc: 0.8802 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:08&lt;00:13,  4.49s/it]Epoch: 2 | train_loss: 0.2826 | train_acc: 0.9539 | test_loss: 0.4080 | test_acc: 0.8802 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:13&lt;00:08,  4.48s/it]Epoch: 3 | train_loss: 0.2450 | train_acc: 0.9609 | test_loss: 0.4130 | test_acc: 0.8750 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:18&lt;00:04,  4.53s/it]Epoch: 4 | train_loss: 0.2450 | train_acc: 0.9594 | test_loss: 0.4158 | test_acc: 0.8802 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:22&lt;00:00,  4.49s/it]Epoch: 5 | train_loss: 0.2307 | train_acc: 0.9639 | test_loss: 0.4124 | test_acc: 0.8906 In\u00a0[74]: Copied! <pre># Unfreeze all base layers \nfor param in model.features.parameters():\n    param.requires_grad = True\n\n# for param in model.features.parameters():\n#     print(param.requires_grad)\n</pre> # Unfreeze all base layers  for param in model.features.parameters():     param.requires_grad = True  # for param in model.features.parameters(): #     print(param.requires_grad) In\u00a0[75]: Copied! <pre># Train model\n# Note: Not using engine.train() since the original script isn't updated\nresults = train(model=model,\n        train_dataloader=train_dataloader,\n        test_dataloader=test_dataloader,\n        optimizer=optimizer,\n        loss_fn=loss_fn,\n        epochs=5)\n</pre> # Train model # Note: Not using engine.train() since the original script isn't updated results = train(model=model,         train_dataloader=train_dataloader,         test_dataloader=test_dataloader,         optimizer=optimizer,         loss_fn=loss_fn,         epochs=5) <pre> 20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]</pre> <pre>Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n</pre> <pre> 40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]</pre> <pre>Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n</pre> <pre> 60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]</pre> <pre>Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n</pre> <pre> 80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]</pre> <pre>Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n</pre> <pre>100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]</pre> <pre>Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre># # Without mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385\n\n# # With mixed precision...\n#  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969\n#  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385\n#  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802\n#  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854\n# 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125\n</pre> # # Without mixed precision... #  20%|\u2588\u2588        | 1/5 [00:11&lt;00:46, 11.61s/it]Epoch: 1 | train_loss: 0.4507 | train_acc: 0.8648 | test_loss: 1.0603 | test_acc: 0.7604 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:24&lt;00:36, 12.21s/it]Epoch: 2 | train_loss: 0.1659 | train_acc: 0.9464 | test_loss: 0.6398 | test_acc: 0.8490 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:36&lt;00:24, 12.38s/it]Epoch: 3 | train_loss: 0.1261 | train_acc: 0.9698 | test_loss: 0.7149 | test_acc: 0.8542 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:49&lt;00:12, 12.53s/it]Epoch: 4 | train_loss: 0.1250 | train_acc: 0.9609 | test_loss: 0.7441 | test_acc: 0.7917 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:02&lt;00:00, 12.42s/it]Epoch: 5 | train_loss: 0.1282 | train_acc: 0.9564 | test_loss: 0.8701 | test_acc: 0.8385  # # With mixed precision... #  20%|\u2588\u2588        | 1/5 [00:13&lt;00:53, 13.27s/it]Epoch: 1 | train_loss: 0.4934 | train_acc: 0.8586 | test_loss: 0.6467 | test_acc: 0.7969 #  40%|\u2588\u2588\u2588\u2588      | 2/5 [00:27&lt;00:42, 14.09s/it]Epoch: 2 | train_loss: 0.1750 | train_acc: 0.9628 | test_loss: 1.1806 | test_acc: 0.8385 #  60%|\u2588\u2588\u2588\u2588\u2588\u2588    | 3/5 [00:42&lt;00:28, 14.28s/it]Epoch: 3 | train_loss: 0.1362 | train_acc: 0.9619 | test_loss: 0.5831 | test_acc: 0.8802 #  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  | 4/5 [00:57&lt;00:14, 14.47s/it]Epoch: 4 | train_loss: 0.1743 | train_acc: 0.9462 | test_loss: 0.5702 | test_acc: 0.8854 # 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [01:11&lt;00:00, 14.38s/it]Epoch: 5 | train_loss: 0.2437 | train_acc: 0.9352 | test_loss: 0.7096 | test_acc: 0.8125 <p>Checking the PyTorch profiler, it seems that mixed precision utilises some Tensor Cores, however, these aren't large numbers.</p> <p>E.g. it uses 9-12% Tensor Cores. Perhaps the slow down when using mixed precision is because the tensors have to get altered and converted when there isn't very many of them. For example only 9-12% of tensors get converted so the speed up gains aren't realised on these tensors because they get cancelled out by the conversion time.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"10_pytorch_profiling/#08-pytorch-profiling","title":"08: PyTorch Profiling\u00b6","text":"<p>This notebook is an experiment to try out the PyTorch profiler.</p> <p>See here for more:</p> <ul> <li>https://pytorch.org/blog/introducing-pytorch-profiler-the-new-and-improved-performance-tool/</li> <li>https://pytorch.org/docs/stable/profiler.html</li> </ul>"},{"location":"10_pytorch_profiling/#setup-device","title":"Setup device\u00b6","text":""},{"location":"10_pytorch_profiling/#get-and-load-data","title":"Get and load data\u00b6","text":""},{"location":"10_pytorch_profiling/#load-model","title":"Load model\u00b6","text":""},{"location":"10_pytorch_profiling/#train-model-and-track-results","title":"Train model and track results\u00b6","text":""},{"location":"10_pytorch_profiling/#adjust-training-function-to-track-results-with-summarywriter","title":"Adjust training function to track results with <code>SummaryWriter</code>\u00b6","text":""},{"location":"10_pytorch_profiling/#try-mixed-precision-with-larger-model","title":"Try mixed precision with larger model\u00b6","text":"<p>Now we'll try turn on mixed precision with a larger model (e.g. EffifientNetB0 with all layers tuneable).</p>"},{"location":"10_pytorch_profiling/#extensions","title":"Extensions\u00b6","text":"<ul> <li>Does changing the data input size to EfficientNetB4 change its results? E.g. input image size of (380, 380) instead of (224, 224)?</li> </ul>"},{"location":"pytorch_2_intro/","title":"\u5feb\u901f\u200b\u5165\u95e8\u200b PyTorch 2.0 \u200b\u6559\u7a0b","text":"<p>\u200b\u67e5\u770b\u200b\u6e90\u4ee3\u7801\u200b</p> In\u00a0[1]: Copied! <pre>import datetime\nprint(f\"Notebook last updated: {datetime.datetime.now()}\")\n</pre> import datetime print(f\"Notebook last updated: {datetime.datetime.now()}\") <pre>Notebook last updated: 2023-04-14 15:24:37.007274\n</pre> In\u00a0[2]: Copied! <pre>import torch\nimport torchvision\n\nmodel = torchvision.models.resnet50() # note: this could be any model\n\n### Train model ###\n\n### Test model ###\n</pre> import torch import torchvision  model = torchvision.models.resnet50() # note: this could be any model  ### Train model ###  ### Test model ### In\u00a0[3]: Copied! <pre>import torch\nimport torchvision\n\nmodel = torchvision.models.resnet50() # note: this could be any model\ncompiled_model = torch.compile(model) # &lt;- magic happens!\n\n### Train model ### &lt;- faster!\n\n### Test model ### &lt;- faster!\n</pre> import torch import torchvision  model = torchvision.models.resnet50() # note: this could be any model compiled_model = torch.compile(model) # &lt;- magic happens!  ### Train model ### &lt;- faster!  ### Test model ### &lt;- faster! In\u00a0[4]: Copied! <pre>import torch\n\n# Check PyTorch version\npt_version = torch.__version__\nprint(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\n\n# Install PyTorch 2.0 if necessary\nif pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1 \n    !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n    print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\\n          Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")\n    import torch\n    pt_version = torch.__version__\n    print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")\nelse:\n    print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\")\n</pre> import torch  # Check PyTorch version pt_version = torch.__version__ print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\")  # Install PyTorch 2.0 if necessary if pt_version.split(\".\")[0] == \"1\": # Check if PyTorch version begins with 1      !pip3 install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118     print(\"[INFO] PyTorch 2.x installed, if you're on Google Colab, you may need to restart your runtime.\\           Though as of April 2023, Google Colab comes with PyTorch 2.0 pre-installed.\")     import torch     pt_version = torch.__version__     print(f\"[INFO] Current PyTorch version: {pt_version} (should be 2.x+)\") else:     print(\"[INFO] PyTorch 2.x installed, you'll be able to use the new features.\") <pre>[INFO] Current PyTorch version: 2.0.0+cu118 (should be 2.x+)\n[INFO] PyTorch 2.x installed, you'll be able to use the new features.\n</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b PyTorch 2.x \u200b\u5df2\u7ecf\u200b\u5b89\u88c5\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u4f53\u9a8c\u200b\u4e00\u4e0b\u200b\u65b0\u200b\u529f\u80fd\u200b\u5427\u200b\uff01</p> In\u00a0[5]: Copied! <pre># Make sure we're using a NVIDIA GPU\nif torch.cuda.is_available():\n  gpu_info = !nvidia-smi\n  gpu_info = '\\n'.join(gpu_info)\n  if gpu_info.find(\"failed\") &gt;= 0:\n    print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n\n  # Get GPU name\n  gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv\n  gpu_name = gpu_name[1]\n  GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving\n  print(f'GPU name: {GPU_NAME}')\n\n  # Get GPU capability score\n  GPU_SCORE = torch.cuda.get_device_capability()\n  print(f\"GPU capability score: {GPU_SCORE}\")\n  if GPU_SCORE &gt;= (8, 0):\n    print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")\n  else:\n    print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")\n  \n  # Print GPU info\n  print(f\"GPU information:\\n{gpu_info}\")\n\nelse:\n  print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")\n</pre> # Make sure we're using a NVIDIA GPU if torch.cuda.is_available():   gpu_info = !nvidia-smi   gpu_info = '\\n'.join(gpu_info)   if gpu_info.find(\"failed\") &gt;= 0:     print(\"Not connected to a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\")    # Get GPU name   gpu_name = !nvidia-smi --query-gpu=gpu_name --format=csv   gpu_name = gpu_name[1]   GPU_NAME = gpu_name.replace(\" \", \"_\") # remove underscores for easier saving   print(f'GPU name: {GPU_NAME}')    # Get GPU capability score   GPU_SCORE = torch.cuda.get_device_capability()   print(f\"GPU capability score: {GPU_SCORE}\")   if GPU_SCORE &gt;= (8, 0):     print(f\"GPU score higher than or equal to (8, 0), PyTorch 2.x speedup features available.\")   else:     print(f\"GPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\")      # Print GPU info   print(f\"GPU information:\\n{gpu_info}\")  else:   print(\"PyTorch couldn't find a GPU, to leverage the best of PyTorch 2.0, you should connect to a GPU.\") <pre>GPU name: NVIDIA_TITAN_RTX\nGPU capability score: (7, 5)\nGPU score lower than (8, 0), PyTorch 2.x speedup features will be limited (PyTorch 2.x speedups happen most on newer GPUs).\nGPU information:\nFri Apr 14 15:24:38 2023       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.89.02    Driver Version: 525.89.02    CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA TITAN RTX    Off  | 00000000:01:00.0 Off |                  N/A |\n| 40%   50C    P8     9W / 280W |    260MiB / 24576MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N/A  N/A      1020      G   /usr/lib/xorg/Xorg                 53MiB |\n|    0   N/A  N/A   1415245      G   /usr/lib/xorg/Xorg                162MiB |\n|    0   N/A  N/A   1415374      G   /usr/bin/gnome-shell                8MiB |\n+-----------------------------------------------------------------------------+\n</pre> In\u00a0[6]: Copied! <pre>import torch\n\n# Set the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Set the device with context manager (requires PyTorch 2.x+)\nwith torch.device(device):\n    # All tensors created in this block will be on device\n    layer = torch.nn.Linear(20, 30)\n    print(f\"Layer weights are on device: {layer.weight.device}\")\n    print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")\n</pre> import torch  # Set the device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Set the device with context manager (requires PyTorch 2.x+) with torch.device(device):     # All tensors created in this block will be on device     layer = torch.nn.Linear(20, 30)     print(f\"Layer weights are on device: {layer.weight.device}\")     print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\") <pre>Layer weights are on device: cuda:0\nLayer creating data on device: cuda:0\n</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u8bbe\u7f6e\u200b\u5168\u5c40\u200b\u8bbe\u5907\u200b\u5427\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\uff0c\u200b\u4efb\u4f55\u200b\u5728\u200b\u6ca1\u6709\u200b\u663e\u5f0f\u200b\u6307\u5b9a\u200b\u8bbe\u5907\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u521b\u5efa\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u90fd\u200b\u5c06\u200b\u9ed8\u8ba4\u200b\u5728\u200b\u4f60\u200b\u6240\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u521b\u5efa\u200b\u3002</p> In\u00a0[7]: Copied! <pre>import torch\n\n# Set the device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Set the device globally\ntorch.set_default_device(device)\n\n# All tensors created will be on the global device by default\nlayer = torch.nn.Linear(20, 30)\nprint(f\"Layer weights are on device: {layer.weight.device}\")\nprint(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")\n</pre> import torch  # Set the device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Set the device globally torch.set_default_device(device)  # All tensors created will be on the global device by default layer = torch.nn.Linear(20, 30) print(f\"Layer weights are on device: {layer.weight.device}\") print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\") <pre>Layer weights are on device: cuda:0\nLayer creating data on device: cuda:0\n</pre> <p>\u200b\u73b0\u5728\u200b\u56de\u5230\u200b CPU\u3002</p> In\u00a0[8]: Copied! <pre>import torch \n\n# Set the device globally\ntorch.set_default_device(\"cpu\")\n\n# All tensors created will be on \"cpu\"\nlayer = torch.nn.Linear(20, 30)\nprint(f\"Layer weights are on device: {layer.weight.device}\")\nprint(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\")\n</pre> import torch   # Set the device globally torch.set_default_device(\"cpu\")  # All tensors created will be on \"cpu\" layer = torch.nn.Linear(20, 30) print(f\"Layer weights are on device: {layer.weight.device}\") print(f\"Layer creating data on device: {layer(torch.randn(128, 20)).device}\") <pre>Layer weights are on device: cpu\nLayer creating data on device: cpu\n</pre> In\u00a0[9]: Copied! <pre>import torch\nimport torchvision\n\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"TorchVision version: {torchvision.__version__}\")\n\n# Set the target device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nprint(f\"Using device: {device}\")\n</pre> import torch import torchvision  print(f\"PyTorch version: {torch.__version__}\") print(f\"TorchVision version: {torchvision.__version__}\")  # Set the target device device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  print(f\"Using device: {device}\") <pre>PyTorch version: 2.0.0+cu118\nTorchVision version: 0.15.1+cu118\nUsing device: cuda\n</pre> In\u00a0[10]: Copied! <pre># Create model weights and transforms\nmodel_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # &lt;- use the latest weights (could also use .DEFAULT)\ntransforms = model_weights.transforms()\n\n# Setup model\nmodel = torchvision.models.resnet50(weights=model_weights)\n\n# Count the number of parameters in the model \ntotal_params = sum(\n    param.numel() for param in model.parameters() # &lt;- all params\n\t# param.numel() for param in model.parameters() if param.requires_grad # &lt;- only trainable params\n)\n\nprint(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\")\nprint(f\"Model transforms:\\n{transforms}\")\n</pre> # Create model weights and transforms model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2 # &lt;- use the latest weights (could also use .DEFAULT) transforms = model_weights.transforms()  # Setup model model = torchvision.models.resnet50(weights=model_weights)  # Count the number of parameters in the model  total_params = sum(     param.numel() for param in model.parameters() # &lt;- all params \t# param.numel() for param in model.parameters() if param.requires_grad # &lt;- only trainable params )  print(f\"Total parameters of model: {total_params} (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\") print(f\"Model transforms:\\n{transforms}\") <pre>Total parameters of model: 25557032 (the more parameters, the more GPU memory the model will use, the more *relative* of a speedup you'll get)\nModel transforms:\nImageClassification(\n    crop_size=[224]\n    resize_size=[232]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u8f6c\u6362\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u4e4b\u540e\u200b\u53ef\u4ee5\u200b\u91cd\u590d\u4f7f\u7528\u200b\u3002\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u8c03\u6574\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b\uff08<code>model.fc</code>\uff09\u200b\u7684\u200b\u8f93\u51fa\u200b\u7279\u5f81\u200b\uff0c\u200b\u4f7f\u200b\u5176\u200b\u4e0e\u200bCIFAR10\u200b\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\uff0810\uff09\u200b\u76f8\u5339\u914d\u200b\u3002</p> In\u00a0[11]: Copied! <pre>def create_model(num_classes=10):\n  \"\"\"\n  Creates a ResNet50 model with the latest weights and transforms via torchvision.\n  \"\"\"\n  model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2\n  transforms = model_weights.transforms()\n  model = torchvision.models.resnet50(weights=model_weights)\n  \n  # Adjust the number of output features in model to match the number of classes in the dataset\n  model.fc = torch.nn.Linear(in_features=2048, \n                             out_features=num_classes)\n  return model, transforms\n\nmodel, transforms = create_model()\n</pre> def create_model(num_classes=10):   \"\"\"   Creates a ResNet50 model with the latest weights and transforms via torchvision.   \"\"\"   model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2   transforms = model_weights.transforms()   model = torchvision.models.resnet50(weights=model_weights)      # Adjust the number of output features in model to match the number of classes in the dataset   model.fc = torch.nn.Linear(in_features=2048,                               out_features=num_classes)   return model, transforms  model, transforms = create_model() In\u00a0[12]: Copied! <pre># Check available GPU memory and total GPU memory \ntotal_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info()\nprint(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\")\nprint(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\")\n</pre> # Check available GPU memory and total GPU memory  total_free_gpu_memory, total_gpu_memory = torch.cuda.mem_get_info() print(f\"Total free GPU memory: {round(total_free_gpu_memory * 1e-9, 3)} GB\") print(f\"Total GPU memory: {round(total_gpu_memory * 1e-9, 3)} GB\") <pre>Total free GPU memory: 24.187 GB\nTotal GPU memory: 25.386 GB\n</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u8fd9\u91cc\u200b\u7684\u200b\u8981\u70b9\u200b\u662f\u200b\uff1a</p> <ol> <li>GPU \u200b\u4e0a\u200b\u7684\u200b\u5185\u5b58\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u53ef\u4ee5\u200b\u8d8a\u5927\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u4ee5\u200b\u8d8a\u5927\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u53ef\u4ee5\u200b\u8d8a\u5927\u200b\u3002</li> <li>\u200b\u4e3a\u4e86\u200b\u52a0\u901f\u200b\uff0c\u200b\u4f60\u200b\u5e94\u8be5\u200b\u603b\u662f\u200b\u5c3d\u91cf\u200b\u5145\u5206\u5229\u7528\u200b GPU\u3002</li> </ol> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5199\u200b\u4e00\u4e9b\u200b\u4ee3\u7801\u200b\uff0c\u200b\u5982\u679c\u200b GPU \u200b\u5185\u5b58\u200b\u66f4\u200b\u591a\u200b\uff0c\u200b\u5c31\u200b\u4f7f\u7528\u200b\u66f4\u5927\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u7406\u60f3\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u5c06\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u7279\u5b9a\u200b GPU\u3001\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6a21\u578b\u200b\u3002\u200b\u4e0b\u9762\u200b\u7684\u200b\u4ee3\u7801\u200b\u4e13\u95e8\u200b\u9488\u5bf9\u200b Google Colab Pro \u200b\u4e0a\u200b\u53ef\u7528\u200b\u7684\u200b A100 GPU\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u6839\u636e\u200b\u81ea\u5df1\u200b\u7684\u200b GPU \u200b\u8fdb\u884c\u200b\u8c03\u6574\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u8fc7\u9ad8\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b CUDA \u200b\u5185\u5b58\u4e0d\u8db3\u200b\u7684\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b GPU \u200b\u4e0a\u200b\u53ef\u7528\u200b\u7684\u200b\u603b\u200b\u5185\u5b58\u200b\u8d85\u8fc7\u200b 16GB\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u4e3a\u200b 128 \u200b\u548c\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b 224\uff08\u200b\u5728\u200b\u5185\u5b58\u200b\u66f4\u200b\u591a\u200b\u7684\u200b GPU \u200b\u4e0a\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u503c\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u589e\u52a0\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b GPU \u200b\u4e0a\u200b\u53ef\u7528\u200b\u7684\u200b\u603b\u200b\u5185\u5b58\u200b\u4f4e\u4e8e\u200b 16GB\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u4e3a\u200b 32 \u200b\u548c\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u4e3a\u200b 64\uff08\u200b\u5728\u200b\u5185\u5b58\u200b\u8f83\u5c11\u200b\u7684\u200b GPU \u200b\u4e0a\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u503c\u200b\u90fd\u200b\u53ef\u4ee5\u200b\u8c03\u6574\u200b\uff09\u3002</p> In\u00a0[13]: Copied! <pre># Set batch size depending on amount of GPU memory\ntotal_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3)\nif total_free_gpu_memory_gb &gt;= 16:\n  BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.\n  IMAGE_SIZE = 224\n  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\nelse:\n  BATCH_SIZE = 32\n  IMAGE_SIZE = 128\n  print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\")\n</pre> # Set batch size depending on amount of GPU memory total_free_gpu_memory_gb = round(total_free_gpu_memory * 1e-9, 3) if total_free_gpu_memory_gb &gt;= 16:   BATCH_SIZE = 128 # Note: you could experiment with higher values here if you like.   IMAGE_SIZE = 224   print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\") else:   BATCH_SIZE = 32   IMAGE_SIZE = 128   print(f\"GPU memory available is {total_free_gpu_memory_gb} GB, using batch size of {BATCH_SIZE} and image size {IMAGE_SIZE}\") <pre>GPU memory available is 24.187 GB, using batch size of 128 and image size 224\n</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8c03\u6574\u200b <code>transforms</code> \u200b\u4ee5\u200b\u4f7f\u7528\u200b\u5404\u81ea\u200b\u7684\u200b <code>IMAGE_SIZE</code>\u3002</p> In\u00a0[14]: Copied! <pre>transforms.crop_size = IMAGE_SIZE\ntransforms.resize_size = IMAGE_SIZE \nprint(f\"Updated data transforms:\\n{transforms}\")\n</pre> transforms.crop_size = IMAGE_SIZE transforms.resize_size = IMAGE_SIZE  print(f\"Updated data transforms:\\n{transforms}\") <pre>Updated data transforms:\nImageClassification(\n    crop_size=224\n    resize_size=224\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BILINEAR\n)\n</pre> In\u00a0[15]: Copied! <pre>if GPU_SCORE &gt;= (8, 0):\n  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")\n  torch.backends.cuda.matmul.allow_tf32 = True\nelse:\n  print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score &gt;= (8, 0)\")\n  torch.backends.cuda.matmul.allow_tf32 = False\n</pre> if GPU_SCORE &gt;= (8, 0):   print(f\"[INFO] Using GPU with score: {GPU_SCORE}, enabling TensorFloat32 (TF32) computing (faster on new GPUs)\")   torch.backends.cuda.matmul.allow_tf32 = True else:   print(f\"[INFO] Using GPU with score: {GPU_SCORE}, TensorFloat32 (TF32) not available, to use it you need a GPU with score &gt;= (8, 0)\")   torch.backends.cuda.matmul.allow_tf32 = False <pre>[INFO] Using GPU with score: (7, 5), TensorFloat32 (TF32) not available, to use it you need a GPU with score &gt;= (8, 0)\n</pre> In\u00a0[16]: Copied! <pre># Create train and test datasets\ntrain_dataset = torchvision.datasets.CIFAR10(root='.', \n                                             train=True, \n                                             download=True, \n                                             transform=transforms)\n\ntest_dataset = torchvision.datasets.CIFAR10(root='.', \n                                            train=False, # want the test split\n                                            download=True, \n                                            transform=transforms)\n\n# Get the lengths of the datasets\ntrain_len = len(train_dataset)\ntest_len = len(test_dataset)\n\nprint(f\"[INFO] Train dataset length: {train_len}\")\nprint(f\"[INFO] Test dataset length: {test_len}\")\n</pre> # Create train and test datasets train_dataset = torchvision.datasets.CIFAR10(root='.',                                               train=True,                                               download=True,                                               transform=transforms)  test_dataset = torchvision.datasets.CIFAR10(root='.',                                              train=False, # want the test split                                             download=True,                                              transform=transforms)  # Get the lengths of the datasets train_len = len(train_dataset) test_len = len(test_dataset)  print(f\"[INFO] Train dataset length: {train_len}\") print(f\"[INFO] Test dataset length: {test_len}\") <pre>Files already downloaded and verified\nFiles already downloaded and verified\n[INFO] Train dataset length: 50000\n[INFO] Test dataset length: 10000\n</pre> In\u00a0[17]: Copied! <pre>from torch.utils.data import DataLoader\n\n# Create DataLoaders\nimport os\nNUM_WORKERS = os.cpu_count() # &lt;- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)\n\ntrain_dataloader = DataLoader(dataset=train_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True,\n                              num_workers=NUM_WORKERS)\n\ntest_dataloader = DataLoader(dataset=test_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=False,\n                              num_workers=NUM_WORKERS)\n\n# Print details\nprint(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\nprint(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\")\nprint(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\")\n</pre> from torch.utils.data import DataLoader  # Create DataLoaders import os NUM_WORKERS = os.cpu_count() # &lt;- use all available CPU cores (this number can be tweaked through experimentation but generally more workers means faster dataloading from CPU to GPU)  train_dataloader = DataLoader(dataset=train_dataset,                               batch_size=BATCH_SIZE,                               shuffle=True,                               num_workers=NUM_WORKERS)  test_dataloader = DataLoader(dataset=test_dataset,                               batch_size=BATCH_SIZE,                               shuffle=False,                               num_workers=NUM_WORKERS)  # Print details print(f\"Train dataloader length: {len(train_dataloader)} batches of size {BATCH_SIZE}\") print(f\"Test dataloader length: {len(test_dataloader)} batches of size {BATCH_SIZE}\") print(f\"Using number of workers: {NUM_WORKERS} (generally more workers means faster dataloading from CPU to GPU)\") <pre>Train dataloader length: 391 batches of size 128\nTest dataloader length: 79 batches of size 128\nUsing number of workers: 16 (generally more workers means faster dataloading from CPU to GPU)\n</pre> In\u00a0[18]: Copied! <pre>import time\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(epoch: int,\n               model: torch.nn.Module, \n               dataloader: torch.utils.data.DataLoader, \n               loss_fn: torch.nn.Module, \n               optimizer: torch.optim.Optimizer,\n               device: torch.device,\n               disable_progress_bar: bool = False) -&gt; Tuple[float, float]:\n  \"\"\"Trains a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to training mode and then\n  runs through all of the required training steps (forward\n  pass, loss calculation, optimizer step).\n\n  Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n  \"\"\"\n  # Put model in train mode\n  model.train()\n\n  # Setup train loss and train accuracy values\n  train_loss, train_acc = 0, 0\n\n  # Loop through data loader data batches\n  progress_bar = tqdm(\n        enumerate(dataloader), \n        desc=f\"Training Epoch {epoch}\", \n        total=len(dataloader),\n        disable=disable_progress_bar\n    )\n\n  for batch, (X, y) in progress_bar:\n      # Send data to target device\n      X, y = X.to(device), y.to(device)\n\n      # 1. Forward pass\n      y_pred = model(X)\n\n      # 2. Calculate  and accumulate loss\n      loss = loss_fn(y_pred, y)\n      train_loss += loss.item() \n\n      # 3. Optimizer zero grad\n      optimizer.zero_grad()\n\n      # 4. Loss backward\n      loss.backward()\n\n      # 5. Optimizer step\n      optimizer.step()\n\n      # Calculate and accumulate accuracy metric across all batches\n      y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n      train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n      # Update progress bar\n      progress_bar.set_postfix(\n            {\n                \"train_loss\": train_loss / (batch + 1),\n                \"train_acc\": train_acc / (batch + 1),\n            }\n        )\n\n\n  # Adjust metrics to get average loss and accuracy per batch \n  train_loss = train_loss / len(dataloader)\n  train_acc = train_acc / len(dataloader)\n  return train_loss, train_acc\n\ndef test_step(epoch: int,\n              model: torch.nn.Module, \n              dataloader: torch.utils.data.DataLoader, \n              loss_fn: torch.nn.Module,\n              device: torch.device,\n              disable_progress_bar: bool = False) -&gt; Tuple[float, float]:\n  \"\"\"Tests a PyTorch model for a single epoch.\n\n  Turns a target PyTorch model to \"eval\" mode and then performs\n  a forward pass on a testing dataset.\n\n  Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n  \"\"\"\n  # Put model in eval mode\n  model.eval() \n\n  # Setup test loss and test accuracy values\n  test_loss, test_acc = 0, 0\n\n  # Loop through data loader data batches\n  progress_bar = tqdm(\n      enumerate(dataloader), \n      desc=f\"Testing Epoch {epoch}\", \n      total=len(dataloader),\n      disable=disable_progress_bar\n  )\n\n  # Turn on inference context manager\n  with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case\n      # Loop through DataLoader batches\n      for batch, (X, y) in progress_bar:\n          # Send data to target device\n          X, y = X.to(device), y.to(device)\n\n          # 1. Forward pass\n          test_pred_logits = model(X)\n\n          # 2. Calculate and accumulate loss\n          loss = loss_fn(test_pred_logits, y)\n          test_loss += loss.item()\n\n          # Calculate and accumulate accuracy\n          test_pred_labels = test_pred_logits.argmax(dim=1)\n          test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n          # Update progress bar\n          progress_bar.set_postfix(\n              {\n                  \"test_loss\": test_loss / (batch + 1),\n                  \"test_acc\": test_acc / (batch + 1),\n              }\n          )\n\n  # Adjust metrics to get average loss and accuracy per batch \n  test_loss = test_loss / len(dataloader)\n  test_acc = test_acc / len(dataloader)\n  return test_loss, test_acc\n\ndef train(model: torch.nn.Module, \n          train_dataloader: torch.utils.data.DataLoader, \n          test_dataloader: torch.utils.data.DataLoader, \n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device,\n          disable_progress_bar: bool = False) -&gt; Dict[str, List]:\n  \"\"\"Trains and tests a PyTorch model.\n\n  Passes a target PyTorch models through train_step() and test_step()\n  functions for a number of epochs, training and testing the model\n  in the same epoch loop.\n\n  Calculates, prints and stores evaluation metrics throughout.\n\n  Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n  Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for \n    each epoch.\n    In the form: {train_loss: [...],\n                  train_acc: [...],\n                  test_loss: [...],\n                  test_acc: [...]} \n    For example if training for epochs=2: \n                 {train_loss: [2.0616, 1.0537],\n                  train_acc: [0.3945, 0.3945],\n                  test_loss: [1.2641, 1.5706],\n                  test_acc: [0.3400, 0.2973]} \n  \"\"\"\n  # Create empty results dictionary\n  results = {\"train_loss\": [],\n      \"train_acc\": [],\n      \"test_loss\": [],\n      \"test_acc\": [],\n      \"train_epoch_time\": [],\n      \"test_epoch_time\": []\n  }\n\n  # Loop through training and testing steps for a number of epochs\n  for epoch in tqdm(range(epochs), disable=disable_progress_bar):\n\n      # Perform training step and time it\n      train_epoch_start_time = time.time()\n      train_loss, train_acc = train_step(epoch=epoch, \n                                        model=model,\n                                        dataloader=train_dataloader,\n                                        loss_fn=loss_fn,\n                                        optimizer=optimizer,\n                                        device=device,\n                                        disable_progress_bar=disable_progress_bar)\n      train_epoch_end_time = time.time()\n      train_epoch_time = train_epoch_end_time - train_epoch_start_time\n      \n      # Perform testing step and time it\n      test_epoch_start_time = time.time()\n      test_loss, test_acc = test_step(epoch=epoch,\n                                      model=model,\n                                      dataloader=test_dataloader,\n                                      loss_fn=loss_fn,\n                                      device=device,\n                                      disable_progress_bar=disable_progress_bar)\n      test_epoch_end_time = time.time()\n      test_epoch_time = test_epoch_end_time - test_epoch_start_time\n\n      # Print out what's happening\n      print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f} | \"\n          f\"train_epoch_time: {train_epoch_time:.4f} | \"\n          f\"test_epoch_time: {test_epoch_time:.4f}\"\n      )\n\n      # Update results dictionary\n      results[\"train_loss\"].append(train_loss)\n      results[\"train_acc\"].append(train_acc)\n      results[\"test_loss\"].append(test_loss)\n      results[\"test_acc\"].append(test_acc)\n      results[\"train_epoch_time\"].append(train_epoch_time)\n      results[\"test_epoch_time\"].append(test_epoch_time)\n\n  # Return the filled results at the end of the epochs\n  return results\n</pre> import time from tqdm.auto import tqdm from typing import Dict, List, Tuple  def train_step(epoch: int,                model: torch.nn.Module,                 dataloader: torch.utils.data.DataLoader,                 loss_fn: torch.nn.Module,                 optimizer: torch.optim.Optimizer,                device: torch.device,                disable_progress_bar: bool = False) -&gt; Tuple[float, float]:   \"\"\"Trains a PyTorch model for a single epoch.    Turns a target PyTorch model to training mode and then   runs through all of the required training steps (forward   pass, loss calculation, optimizer step).    Args:     model: A PyTorch model to be trained.     dataloader: A DataLoader instance for the model to be trained on.     loss_fn: A PyTorch loss function to minimize.     optimizer: A PyTorch optimizer to help minimize the loss function.     device: A target device to compute on (e.g. \"cuda\" or \"cpu\").    Returns:     A tuple of training loss and training accuracy metrics.     In the form (train_loss, train_accuracy). For example:      (0.1112, 0.8743)   \"\"\"   # Put model in train mode   model.train()    # Setup train loss and train accuracy values   train_loss, train_acc = 0, 0    # Loop through data loader data batches   progress_bar = tqdm(         enumerate(dataloader),          desc=f\"Training Epoch {epoch}\",          total=len(dataloader),         disable=disable_progress_bar     )    for batch, (X, y) in progress_bar:       # Send data to target device       X, y = X.to(device), y.to(device)        # 1. Forward pass       y_pred = model(X)        # 2. Calculate  and accumulate loss       loss = loss_fn(y_pred, y)       train_loss += loss.item()         # 3. Optimizer zero grad       optimizer.zero_grad()        # 4. Loss backward       loss.backward()        # 5. Optimizer step       optimizer.step()        # Calculate and accumulate accuracy metric across all batches       y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)       train_acc += (y_pred_class == y).sum().item()/len(y_pred)        # Update progress bar       progress_bar.set_postfix(             {                 \"train_loss\": train_loss / (batch + 1),                 \"train_acc\": train_acc / (batch + 1),             }         )     # Adjust metrics to get average loss and accuracy per batch    train_loss = train_loss / len(dataloader)   train_acc = train_acc / len(dataloader)   return train_loss, train_acc  def test_step(epoch: int,               model: torch.nn.Module,                dataloader: torch.utils.data.DataLoader,                loss_fn: torch.nn.Module,               device: torch.device,               disable_progress_bar: bool = False) -&gt; Tuple[float, float]:   \"\"\"Tests a PyTorch model for a single epoch.    Turns a target PyTorch model to \"eval\" mode and then performs   a forward pass on a testing dataset.    Args:     model: A PyTorch model to be tested.     dataloader: A DataLoader instance for the model to be tested on.     loss_fn: A PyTorch loss function to calculate loss on the test data.     device: A target device to compute on (e.g. \"cuda\" or \"cpu\").    Returns:     A tuple of testing loss and testing accuracy metrics.     In the form (test_loss, test_accuracy). For example:      (0.0223, 0.8985)   \"\"\"   # Put model in eval mode   model.eval()     # Setup test loss and test accuracy values   test_loss, test_acc = 0, 0    # Loop through data loader data batches   progress_bar = tqdm(       enumerate(dataloader),        desc=f\"Testing Epoch {epoch}\",        total=len(dataloader),       disable=disable_progress_bar   )    # Turn on inference context manager   with torch.no_grad(): # no_grad() required for PyTorch 2.0, I found some errors with `torch.inference_mode()`, please let me know if this is not the case       # Loop through DataLoader batches       for batch, (X, y) in progress_bar:           # Send data to target device           X, y = X.to(device), y.to(device)            # 1. Forward pass           test_pred_logits = model(X)            # 2. Calculate and accumulate loss           loss = loss_fn(test_pred_logits, y)           test_loss += loss.item()            # Calculate and accumulate accuracy           test_pred_labels = test_pred_logits.argmax(dim=1)           test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))            # Update progress bar           progress_bar.set_postfix(               {                   \"test_loss\": test_loss / (batch + 1),                   \"test_acc\": test_acc / (batch + 1),               }           )    # Adjust metrics to get average loss and accuracy per batch    test_loss = test_loss / len(dataloader)   test_acc = test_acc / len(dataloader)   return test_loss, test_acc  def train(model: torch.nn.Module,            train_dataloader: torch.utils.data.DataLoader,            test_dataloader: torch.utils.data.DataLoader,            optimizer: torch.optim.Optimizer,           loss_fn: torch.nn.Module,           epochs: int,           device: torch.device,           disable_progress_bar: bool = False) -&gt; Dict[str, List]:   \"\"\"Trains and tests a PyTorch model.    Passes a target PyTorch models through train_step() and test_step()   functions for a number of epochs, training and testing the model   in the same epoch loop.    Calculates, prints and stores evaluation metrics throughout.    Args:     model: A PyTorch model to be trained and tested.     train_dataloader: A DataLoader instance for the model to be trained on.     test_dataloader: A DataLoader instance for the model to be tested on.     optimizer: A PyTorch optimizer to help minimize the loss function.     loss_fn: A PyTorch loss function to calculate loss on both datasets.     epochs: An integer indicating how many epochs to train for.     device: A target device to compute on (e.g. \"cuda\" or \"cpu\").    Returns:     A dictionary of training and testing loss as well as training and     testing accuracy metrics. Each metric has a value in a list for      each epoch.     In the form: {train_loss: [...],                   train_acc: [...],                   test_loss: [...],                   test_acc: [...]}      For example if training for epochs=2:                   {train_loss: [2.0616, 1.0537],                   train_acc: [0.3945, 0.3945],                   test_loss: [1.2641, 1.5706],                   test_acc: [0.3400, 0.2973]}    \"\"\"   # Create empty results dictionary   results = {\"train_loss\": [],       \"train_acc\": [],       \"test_loss\": [],       \"test_acc\": [],       \"train_epoch_time\": [],       \"test_epoch_time\": []   }    # Loop through training and testing steps for a number of epochs   for epoch in tqdm(range(epochs), disable=disable_progress_bar):        # Perform training step and time it       train_epoch_start_time = time.time()       train_loss, train_acc = train_step(epoch=epoch,                                          model=model,                                         dataloader=train_dataloader,                                         loss_fn=loss_fn,                                         optimizer=optimizer,                                         device=device,                                         disable_progress_bar=disable_progress_bar)       train_epoch_end_time = time.time()       train_epoch_time = train_epoch_end_time - train_epoch_start_time              # Perform testing step and time it       test_epoch_start_time = time.time()       test_loss, test_acc = test_step(epoch=epoch,                                       model=model,                                       dataloader=test_dataloader,                                       loss_fn=loss_fn,                                       device=device,                                       disable_progress_bar=disable_progress_bar)       test_epoch_end_time = time.time()       test_epoch_time = test_epoch_end_time - test_epoch_start_time        # Print out what's happening       print(           f\"Epoch: {epoch+1} | \"           f\"train_loss: {train_loss:.4f} | \"           f\"train_acc: {train_acc:.4f} | \"           f\"test_loss: {test_loss:.4f} | \"           f\"test_acc: {test_acc:.4f} | \"           f\"train_epoch_time: {train_epoch_time:.4f} | \"           f\"test_epoch_time: {test_epoch_time:.4f}\"       )        # Update results dictionary       results[\"train_loss\"].append(train_loss)       results[\"train_acc\"].append(train_acc)       results[\"test_loss\"].append(test_loss)       results[\"test_acc\"].append(test_acc)       results[\"train_epoch_time\"].append(train_epoch_time)       results[\"test_epoch_time\"].append(test_epoch_time)    # Return the filled results at the end of the epochs   return results In\u00a0[19]: Copied! <pre># Set the number of epochs as a constant\nNUM_EPOCHS = 5\n\n# Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time)\nLEARNING_RATE = 0.003\n</pre> # Set the number of epochs as a constant NUM_EPOCHS = 5  # Set the learning rate as a constant (this can be changed to get better results but for now we're just focused on time) LEARNING_RATE = 0.003 <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6839\u636e\u200b\u60a8\u200b\u7684\u200bGPU\u200b\u901f\u5ea6\u200b\uff0c\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\u624d\u80fd\u200b\u8fd0\u884c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u672c\u5730\u200bNVIDIA TITAN RTX\u200b\u4e0a\u200b\u5927\u7ea6\u200b\u9700\u8981\u200b16\u200b\u5206\u949f\u200b\uff0c\u200b\u800c\u200b\u5728\u200bGoogle Colab Pro\u200b\u4e0a\u200b\u7684\u200bNVIDIA A100 GPU\u200b\u4e0a\u200b\u5927\u7ea6\u200b\u9700\u8981\u200b7\u200b\u5206\u949f\u200b\u3002</p> In\u00a0[20]: Copied! <pre># Create model\nmodel, transforms = create_model()\nmodel.to(device)\n\n# Create loss function and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Train model and track results\nsingle_run_no_compile_results = train(model=model,\n                                      train_dataloader=train_dataloader,\n                                      test_dataloader=test_dataloader,\n                                      loss_fn=loss_fn,\n                                      optimizer=optimizer,\n                                      epochs=NUM_EPOCHS,\n                                      device=device)\n</pre> # Create model model, transforms = create_model() model.to(device)  # Create loss function and optimizer loss_fn = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),                              lr=LEARNING_RATE)  # Train model and track results single_run_no_compile_results = train(model=model,                                       train_dataloader=train_dataloader,                                       test_dataloader=test_dataloader,                                       loss_fn=loss_fn,                                       optimizer=optimizer,                                       epochs=NUM_EPOCHS,                                       device=device) <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Training Epoch 0:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 0:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7734 | train_acc: 0.7333 | test_loss: 0.8021 | test_acc: 0.7477 | train_epoch_time: 184.9701 | test_epoch_time: 12.9893\n</pre> <pre>Training Epoch 1:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 1:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 2 | train_loss: 0.4337 | train_acc: 0.8501 | test_loss: 0.4794 | test_acc: 0.8338 | train_epoch_time: 185.3404 | test_epoch_time: 12.9515\n</pre> <pre>Training Epoch 2:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 2:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 3 | train_loss: 0.3055 | train_acc: 0.8944 | test_loss: 0.4282 | test_acc: 0.8533 | train_epoch_time: 185.3870 | test_epoch_time: 13.0559\n</pre> <pre>Training Epoch 3:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 3:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 4 | train_loss: 0.2268 | train_acc: 0.9198 | test_loss: 0.4387 | test_acc: 0.8580 | train_epoch_time: 185.5914 | test_epoch_time: 13.0495\n</pre> <pre>Training Epoch 4:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 4:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 5 | train_loss: 0.1723 | train_acc: 0.9395 | test_loss: 0.3901 | test_acc: 0.8754 | train_epoch_time: 185.5304 | test_epoch_time: 13.0517\n</pre> In\u00a0[21]: Copied! <pre># Create model and transforms\nmodel, transforms = create_model()\nmodel.to(device)\n\n# Create loss function and optimizer\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(),\n                             lr=LEARNING_RATE)\n\n# Compile the model and time how long it takes\ncompile_start_time = time.time()\n\n### New in PyTorch 2.x ###\ncompiled_model = torch.compile(model)\n##########################\n\ncompile_end_time = time.time()\ncompile_time = compile_end_time - compile_start_time\nprint(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n\n# Train the compiled model\nsingle_run_compile_results = train(model=compiled_model,\n                                   train_dataloader=train_dataloader,\n                                   test_dataloader=test_dataloader,\n                                   loss_fn=loss_fn,\n                                   optimizer=optimizer,\n                                   epochs=NUM_EPOCHS,\n                                   device=device)\n</pre> # Create model and transforms model, transforms = create_model() model.to(device)  # Create loss function and optimizer loss_fn = torch.nn.CrossEntropyLoss() optimizer = torch.optim.Adam(model.parameters(),                              lr=LEARNING_RATE)  # Compile the model and time how long it takes compile_start_time = time.time()  ### New in PyTorch 2.x ### compiled_model = torch.compile(model) ##########################  compile_end_time = time.time() compile_time = compile_end_time - compile_start_time print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")  # Train the compiled model single_run_compile_results = train(model=compiled_model,                                    train_dataloader=train_dataloader,                                    test_dataloader=test_dataloader,                                    loss_fn=loss_fn,                                    optimizer=optimizer,                                    epochs=NUM_EPOCHS,                                    device=device) <pre>Time to compile: 0.00491642951965332 | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\n</pre> <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Training Epoch 0:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 0:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 1 | train_loss: 0.7585 | train_acc: 0.7364 | test_loss: 0.5852 | test_acc: 0.8004 | train_epoch_time: 196.4621 | test_epoch_time: 21.0730\n</pre> <pre>Training Epoch 1:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 1:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 2 | train_loss: 0.4288 | train_acc: 0.8521 | test_loss: 0.5468 | test_acc: 0.8108 | train_epoch_time: 169.9891 | test_epoch_time: 11.0555\n</pre> <pre>Training Epoch 2:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 2:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 3 | train_loss: 0.3080 | train_acc: 0.8928 | test_loss: 0.4791 | test_acc: 0.8377 | train_epoch_time: 170.4004 | test_epoch_time: 10.9841\n</pre> <pre>Training Epoch 3:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 3:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 4 | train_loss: 0.2322 | train_acc: 0.9184 | test_loss: 0.5551 | test_acc: 0.8306 | train_epoch_time: 170.1974 | test_epoch_time: 11.0482\n</pre> <pre>Training Epoch 4:   0%|          | 0/391 [00:00&lt;?, ?it/s]</pre> <pre>Testing Epoch 4:   0%|          | 0/79 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 5 | train_loss: 0.1766 | train_acc: 0.9376 | test_loss: 0.3410 | test_acc: 0.8874 | train_epoch_time: 170.0547 | test_epoch_time: 10.9239\n</pre> In\u00a0[22]: Copied! <pre># Turn experiment results into dataframes\nimport pandas as pd\nsingle_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results)\nsingle_run_compile_results_df = pd.DataFrame(single_run_compile_results)\n</pre> # Turn experiment results into dataframes import pandas as pd single_run_no_compile_results_df = pd.DataFrame(single_run_no_compile_results) single_run_compile_results_df = pd.DataFrame(single_run_compile_results) In\u00a0[23]: Copied! <pre># Check out the head of one of the results dataframes\nsingle_run_no_compile_results_df.head()\n</pre> # Check out the head of one of the results dataframes single_run_no_compile_results_df.head() Out[23]: train_loss train_acc test_loss test_acc train_epoch_time test_epoch_time 0 0.773435 0.733272 0.802100 0.747725 184.970135 12.989331 1 0.433699 0.850052 0.479412 0.833762 185.340373 12.951483 2 0.305494 0.894429 0.428212 0.853343 185.386973 13.055891 3 0.226751 0.919829 0.438668 0.857991 185.591368 13.049541 4 0.172269 0.939482 0.390148 0.875396 185.530370 13.051713 <p>\u200b\u5b9e\u9a8c\u200b1\u200b\u548c\u200b\u5b9e\u9a8c\u200b2\u200b\u7684\u200b\u7ed3\u679c\u200b\u51fa\u6765\u200b\u5566\u200b\uff01</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\uff0c\u200b\u6211\u4eec\u200b\u5199\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u63a5\u6536\u200b\u8fd9\u4e9b\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5e76\u7528\u200b\u6761\u5f62\u56fe\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4f1a\u200b\u5728\u200b\u51fd\u6570\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u4e00\u4e9b\u200b\u5143\u200b\u6570\u636e\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u663e\u793a\u200b\u6709\u5173\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u4e00\u4e9b\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u5305\u62ec\u200b\u6211\u4eec\u200b\u5b9e\u9a8c\u200b\u8bbe\u7f6e\u200b\u4e2d\u200b\u7684\u200b\u6240\u6709\u200b\u53c2\u6570\u200b\uff1a</p> <ul> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u540d\u79f0\u200b</li> <li>\u200b\u6a21\u578b\u200b\u540d\u79f0\u200b</li> <li>\u200b\u8bad\u7ec3\u200b\u5468\u671f\u200b\u6570\u200b</li> <li>\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b</li> <li>\u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b</li> </ul> In\u00a0[24]: Copied! <pre># Create filename to save the results\nDATASET_NAME = \"CIFAR10\"\nMODEL_NAME = \"ResNet50\"\n</pre> # Create filename to save the results DATASET_NAME = \"CIFAR10\" MODEL_NAME = \"ResNet50\" In\u00a0[25]: Copied! <pre>import matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_mean_epoch_times(non_compiled_results: pd.DataFrame, \n                          compiled_results: pd.DataFrame, \n                          multi_runs: bool=False, \n                          num_runs: int=0, \n                          save: bool=False, \n                          save_path: str=\"\",\n                          dataset_name: str=DATASET_NAME,\n                          model_name: str=MODEL_NAME,\n                          num_epochs: int=NUM_EPOCHS,\n                          image_size: int=IMAGE_SIZE,\n                          batch_size: int=BATCH_SIZE) -&gt; plt.figure:\n    \n    # Get the mean epoch times from the non-compiled models\n    mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()\n    mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()\n    mean_results = [mean_train_epoch_time, mean_test_epoch_time]\n\n    # Get the mean epoch times from the compiled models\n    mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()\n    mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()\n    mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]\n\n    # Calculate the percentage difference between the mean compile and non-compile train epoch times\n    train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time\n    train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100\n\n    # Calculate the percentage difference between the mean compile and non-compile test epoch times\n    test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time\n    test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100\n\n    # Print the mean difference percentages\n    print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")\n    print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")\n\n    # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models\n    plt.figure(figsize=(10, 7))\n    width = 0.3\n    x_indicies = np.arange(len(mean_results))\n\n    plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")\n    plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")\n    plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))\n    plt.ylabel(\"Mean epoch time (seconds, lower is better)\")\n\n    # Create the title based on the parameters passed to the function\n    if multi_runs:\n        plt.suptitle(\"Multiple run results\")\n        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n    else:\n        plt.suptitle(\"Single run results\")\n        plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")\n    plt.legend();\n\n    # Save the figure\n    if save:\n        assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"\n        plt.savefig(save_path)\n        print(f\"[INFO] Plot saved to {save_path}\")\n</pre> import matplotlib.pyplot as plt import numpy as np  def plot_mean_epoch_times(non_compiled_results: pd.DataFrame,                            compiled_results: pd.DataFrame,                            multi_runs: bool=False,                            num_runs: int=0,                            save: bool=False,                            save_path: str=\"\",                           dataset_name: str=DATASET_NAME,                           model_name: str=MODEL_NAME,                           num_epochs: int=NUM_EPOCHS,                           image_size: int=IMAGE_SIZE,                           batch_size: int=BATCH_SIZE) -&gt; plt.figure:          # Get the mean epoch times from the non-compiled models     mean_train_epoch_time = non_compiled_results.train_epoch_time.mean()     mean_test_epoch_time = non_compiled_results.test_epoch_time.mean()     mean_results = [mean_train_epoch_time, mean_test_epoch_time]      # Get the mean epoch times from the compiled models     mean_compile_train_epoch_time = compiled_results.train_epoch_time.mean()     mean_compile_test_epoch_time = compiled_results.test_epoch_time.mean()     mean_compile_results = [mean_compile_train_epoch_time, mean_compile_test_epoch_time]      # Calculate the percentage difference between the mean compile and non-compile train epoch times     train_epoch_time_diff = mean_compile_train_epoch_time - mean_train_epoch_time     train_epoch_time_diff_percent = (train_epoch_time_diff / mean_train_epoch_time) * 100      # Calculate the percentage difference between the mean compile and non-compile test epoch times     test_epoch_time_diff = mean_compile_test_epoch_time - mean_test_epoch_time     test_epoch_time_diff_percent = (test_epoch_time_diff / mean_test_epoch_time) * 100      # Print the mean difference percentages     print(f\"Mean train epoch time difference: {round(train_epoch_time_diff_percent, 3)}% (negative means faster)\")     print(f\"Mean test epoch time difference: {round(test_epoch_time_diff_percent, 3)}% (negative means faster)\")      # Create a bar plot of the mean train and test epoch time for both compiled and non-compiled models     plt.figure(figsize=(10, 7))     width = 0.3     x_indicies = np.arange(len(mean_results))      plt.bar(x=x_indicies, height=mean_results, width=width, label=\"non_compiled_results\")     plt.bar(x=x_indicies + width, height=mean_compile_results, width=width, label=\"compiled_results\")     plt.xticks(x_indicies + width / 2, (\"Train Epoch\", \"Test Epoch\"))     plt.ylabel(\"Mean epoch time (seconds, lower is better)\")      # Create the title based on the parameters passed to the function     if multi_runs:         plt.suptitle(\"Multiple run results\")         plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} ({num_runs} runs) | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")     else:         plt.suptitle(\"Single run results\")         plt.title(f\"GPU: {gpu_name} | Epochs: {num_epochs} | Data: {dataset_name} | Model: {model_name} | Image size: {image_size} | Batch size: {batch_size}\")     plt.legend();      # Save the figure     if save:         assert save_path != \"\", \"Please specify a save path to save the model figure to via the save_path parameter.\"         plt.savefig(save_path)         print(f\"[INFO] Plot saved to {save_path}\") <p>\u200b\u7ed8\u56fe\u200b\u51fd\u6570\u200b\u5df2\u200b\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u76ee\u5f55\u200b\u6765\u200b\u5b58\u50a8\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u8868\u200b\uff0c\u200b\u7136\u540e\u200b\u7ed8\u5236\u200b\u524d\u200b\u4e24\u4e2a\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> In\u00a0[26]: Copied! <pre># Create directory for saving figures\nimport os\ndir_to_save_figures_in = \"pytorch_2_results/figures/\" \nos.makedirs(dir_to_save_figures_in, exist_ok=True)\n\n# Create a save path for the single run results\nsave_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\nprint(f\"[INFO] Save path for single run results: {save_path_multi_run}\")\n\n# Plot the results and save the figures\nplot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df, \n                      compiled_results=single_run_compile_results_df, \n                      multi_runs=False, \n                      save_path=save_path_multi_run, \n                      save=True)\n</pre> # Create directory for saving figures import os dir_to_save_figures_in = \"pytorch_2_results/figures/\"  os.makedirs(dir_to_save_figures_in, exist_ok=True)  # Create a save path for the single run results save_path_multi_run = f\"{dir_to_save_figures_in}single_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\" print(f\"[INFO] Save path for single run results: {save_path_multi_run}\")  # Plot the results and save the figures plot_mean_epoch_times(non_compiled_results=single_run_no_compile_results_df,                        compiled_results=single_run_compile_results_df,                        multi_runs=False,                        save_path=save_path_multi_run,                        save=True) <pre>[INFO] Save path for single run results: pytorch_2_results/figures/single_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\nMean train epoch time difference: -5.364% (negative means faster)\nMean test epoch time difference: -0.02% (negative means faster)\n[INFO] Plot saved to pytorch_2_results/figures/single_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n</pre> <p>\u200b\u55ef\u200b... \u200b\u8fd9\u91cc\u200b\u53d1\u751f\u200b\u4e86\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u770b\u8d77\u6765\u200b\u4f7f\u7528\u200b <code>torch.compile()</code> \u200b\u7684\u200b\u6a21\u578b\u200b\u6bd4\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u5b83\u200b\u7684\u200b\u6a21\u578b\u200b\u82b1\u8d39\u200b\u4e86\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff08\u200b\u5728\u200b A100 \u200b\u4e0a\u200b\u786e\u5b9e\u200b\u5982\u6b64\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u672c\u5730\u200b NVIDIA TITAN RTX \u200b\u4e0a\u200b\uff0c\u200b\u7f16\u8bd1\u200b\u540e\u200b\u7684\u200b\u6a21\u578b\u200b\u7a0d\u5fae\u200b\u5feb\u200b\u4e00\u4e9b\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u4ec0\u4e48\u200b\u539f\u56e0\u200b\u5462\u200b\uff1f</p> <p>\u200b\u4ece\u200b\u6bcf\u4e2a\u200b epoch \u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u770b\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u53d1\u73b0\u200b\u5c3d\u7ba1\u200b\u5b9e\u9a8c\u200b 2\uff08\u200b\u4f7f\u7528\u200b <code>torch.compile()</code>\uff09\u200b\u5728\u200b\u7b2c\u4e00\u4e2a\u200b epoch \u200b\u65f6\u8fdc\u200b\u6bd4\u200b\u5b9e\u9a8c\u200b 1\uff08\u200b\u4e0d\u200b\u4f7f\u7528\u200b <code>torch.compile()</code>\uff09\u200b\u6162\u200b\uff0c\u200b\u4f46\u200b\u5728\u200b\u540e\u7eed\u200b\u7684\u200b epoch \u200b\u4e2d\u200b\u5b83\u200b\u5f00\u59cb\u200b\u53d8\u5f97\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b <code>torch.compile()</code> \u200b\u5728\u200b\u5e55\u540e\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fd0\u884c\u200b\u7684\u200b\u524d\u200b\u51e0\u6b65\u200b\u4e2d\u200b\u201c\u200b\u9884\u70ed\u200b\u201d\u200b\u6a21\u578b\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5e55\u540e\u200b\u6267\u884c\u200b\u4f18\u5316\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u4f18\u5316\u200b\u6b65\u9aa4\u200b\u5728\u200b\u524d\u671f\u200b\u7684\u786e\u200b\u4f1a\u200b\u82b1\u8d39\u200b\u65f6\u95f4\u200b\uff0c\u200b\u4f46\u200b\u610f\u5473\u7740\u200b\u540e\u7eed\u200b\u6b65\u9aa4\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u9a8c\u8bc1\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u5c06\u200b\u4e0a\u8ff0\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff08\u200b\u6bd4\u5982\u200b 50 \u200b\u4e2a\u200b epoch \u200b\u800c\u200b\u4e0d\u662f\u200b 5 \u200b\u4e2a\u200b\uff09\uff0c\u200b\u770b\u770b\u200b\u5e73\u5747\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\u4f1a\u200b\u662f\u200b\u591a\u5c11\u200b\u3002</p> In\u00a0[27]: Copied! <pre># Make a directory for single_run results\nimport os\npytorch_2_results_dir = \"pytorch_2_results\"\npytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\"\nos.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)\n\n# Create filenames for each of the dataframes\nsave_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\nsave_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n\n# Create filepaths to save the results to\nsingle_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\"\nsingle_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\"\nprint(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\")\nprint(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")\n\n# Save the results\nsingle_run_no_compile_results_df.to_csv(single_run_no_compile_save_path)\nsingle_run_compile_results_df.to_csv(single_run_compile_save_path)\n</pre> # Make a directory for single_run results import os pytorch_2_results_dir = \"pytorch_2_results\" pytorch_2_single_run_results_dir = f\"{pytorch_2_results_dir}/single_run_results\" os.makedirs(pytorch_2_single_run_results_dir, exist_ok=True)  # Create filenames for each of the dataframes save_name_for_non_compiled_results = f\"single_run_non_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\" save_name_for_compiled_results = f\"single_run_compiled_results_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"  # Create filepaths to save the results to single_run_no_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_non_compiled_results}\" single_run_compile_save_path = f\"{pytorch_2_single_run_results_dir}/{save_name_for_compiled_results}\" print(f\"[INFO] Saving non-compiled experiment 1 results to: {single_run_no_compile_save_path}\") print(f\"[INFO] Saving compiled experiment 2 results to: {single_run_compile_save_path}\")  # Save the results single_run_no_compile_results_df.to_csv(single_run_no_compile_save_path) single_run_compile_results_df.to_csv(single_run_compile_save_path) <pre>[INFO] Saving non-compiled experiment 1 results to: pytorch_2_results/single_run_results/single_run_non_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n[INFO] Saving compiled experiment 2 results to: pytorch_2_results/single_run_results/single_run_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n</pre> In\u00a0[28]: Copied! <pre>def create_and_train_non_compiled_model(epochs=NUM_EPOCHS, \n                                        learning_rate=LEARNING_RATE, \n                                        disable_progress_bar=False):\n    \"\"\"\n    Create and train a non-compiled PyTorch model.\n    \"\"\"\n    model, _ = create_model()\n    model.to(device)\n\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(model.parameters(),\n                                 lr=learning_rate)\n\n    results = train(model=model,\n                    train_dataloader=train_dataloader,\n                    test_dataloader=test_dataloader,\n                    loss_fn=loss_fn,\n                    optimizer=optimizer,\n                    epochs=epochs,\n                    device=device,\n                    disable_progress_bar=disable_progress_bar)\n    return results\n\ndef create_compiled_model():\n    \"\"\"\n    Create a compiled PyTorch model and return it.\n    \"\"\"\n    model, _ = create_model()\n    model.to(device)\n    \n    compile_start_time = time.time()\n    ### New in PyTorch 2.x ###\n    compiled_model = torch.compile(model)\n    ##########################\n    compile_end_time = time.time()\n\n    compile_time = compile_end_time - compile_start_time\n\n    print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")\n    return compiled_model\n\ndef train_compiled_model(model=compiled_model, \n                         epochs=NUM_EPOCHS, \n                         learning_rate=LEARNING_RATE,\n                         disable_progress_bar=False):\n    \"\"\"\n    Train a compiled model and return the results.\n    \"\"\"\n    loss_fn = torch.nn.CrossEntropyLoss()\n    optimizer = torch.optim.Adam(compiled_model.parameters(),\n                                 lr=learning_rate)\n    \n    compile_results = train(model=model,\n                            train_dataloader=train_dataloader,\n                            test_dataloader=test_dataloader,\n                            loss_fn=loss_fn,\n                            optimizer=optimizer,\n                            epochs=epochs,\n                            device=device,\n                            disable_progress_bar=disable_progress_bar)\n    \n    return compile_results\n</pre> def create_and_train_non_compiled_model(epochs=NUM_EPOCHS,                                          learning_rate=LEARNING_RATE,                                          disable_progress_bar=False):     \"\"\"     Create and train a non-compiled PyTorch model.     \"\"\"     model, _ = create_model()     model.to(device)      loss_fn = torch.nn.CrossEntropyLoss()     optimizer = torch.optim.Adam(model.parameters(),                                  lr=learning_rate)      results = train(model=model,                     train_dataloader=train_dataloader,                     test_dataloader=test_dataloader,                     loss_fn=loss_fn,                     optimizer=optimizer,                     epochs=epochs,                     device=device,                     disable_progress_bar=disable_progress_bar)     return results  def create_compiled_model():     \"\"\"     Create a compiled PyTorch model and return it.     \"\"\"     model, _ = create_model()     model.to(device)          compile_start_time = time.time()     ### New in PyTorch 2.x ###     compiled_model = torch.compile(model)     ##########################     compile_end_time = time.time()      compile_time = compile_end_time - compile_start_time      print(f\"Time to compile: {compile_time} | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\")     return compiled_model  def train_compiled_model(model=compiled_model,                           epochs=NUM_EPOCHS,                           learning_rate=LEARNING_RATE,                          disable_progress_bar=False):     \"\"\"     Train a compiled model and return the results.     \"\"\"     loss_fn = torch.nn.CrossEntropyLoss()     optimizer = torch.optim.Adam(compiled_model.parameters(),                                  lr=learning_rate)          compile_results = train(model=model,                             train_dataloader=train_dataloader,                             test_dataloader=test_dataloader,                             loss_fn=loss_fn,                             optimizer=optimizer,                             epochs=epochs,                             device=device,                             disable_progress_bar=disable_progress_bar)          return compile_results In\u00a0[29]: Copied! <pre># Run non-compiled model for multiple runs\nNUM_RUNS = 3\nNUM_EPOCHS = 5\n\n# Create an empty list to store multiple run results\nnon_compile_results_multiple_runs = []\n\n# Run non-compiled model for multiple runs\nfor i in tqdm(range(NUM_RUNS)):\n    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for non-compiled model\")\n    results = create_and_train_non_compiled_model(epochs=NUM_EPOCHS, disable_progress_bar=True)\n    non_compile_results_multiple_runs.append(results)\n</pre> # Run non-compiled model for multiple runs NUM_RUNS = 3 NUM_EPOCHS = 5  # Create an empty list to store multiple run results non_compile_results_multiple_runs = []  # Run non-compiled model for multiple runs for i in tqdm(range(NUM_RUNS)):     print(f\"[INFO] Run {i+1} of {NUM_RUNS} for non-compiled model\")     results = create_and_train_non_compiled_model(epochs=NUM_EPOCHS, disable_progress_bar=True)     non_compile_results_multiple_runs.append(results) <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>[INFO] Run 1 of 3 for non-compiled model\nEpoch: 1 | train_loss: 0.8242 | train_acc: 0.7136 | test_loss: 0.5486 | test_acc: 0.8124 | train_epoch_time: 185.1112 | test_epoch_time: 12.9925\nEpoch: 2 | train_loss: 0.4415 | train_acc: 0.8479 | test_loss: 0.6415 | test_acc: 0.7829 | train_epoch_time: 185.0138 | test_epoch_time: 12.9690\nEpoch: 3 | train_loss: 0.3229 | train_acc: 0.8882 | test_loss: 0.4486 | test_acc: 0.8488 | train_epoch_time: 185.0366 | test_epoch_time: 12.9433\nEpoch: 4 | train_loss: 0.2433 | train_acc: 0.9151 | test_loss: 0.4376 | test_acc: 0.8596 | train_epoch_time: 185.0900 | test_epoch_time: 12.9465\nEpoch: 5 | train_loss: 0.1785 | train_acc: 0.9379 | test_loss: 0.4305 | test_acc: 0.8641 | train_epoch_time: 185.0405 | test_epoch_time: 13.0102\n[INFO] Run 2 of 3 for non-compiled model\nEpoch: 1 | train_loss: 0.8304 | train_acc: 0.7101 | test_loss: 0.6132 | test_acc: 0.7884 | train_epoch_time: 185.0911 | test_epoch_time: 13.0429\nEpoch: 2 | train_loss: 0.4602 | train_acc: 0.8411 | test_loss: 0.6183 | test_acc: 0.7907 | train_epoch_time: 185.0738 | test_epoch_time: 12.9596\nEpoch: 3 | train_loss: 0.3283 | train_acc: 0.8869 | test_loss: 0.4309 | test_acc: 0.8534 | train_epoch_time: 185.0462 | test_epoch_time: 12.9877\nEpoch: 4 | train_loss: 0.2474 | train_acc: 0.9140 | test_loss: 0.4525 | test_acc: 0.8565 | train_epoch_time: 184.9521 | test_epoch_time: 12.9942\nEpoch: 5 | train_loss: 0.1860 | train_acc: 0.9360 | test_loss: 0.6284 | test_acc: 0.8195 | train_epoch_time: 184.9911 | test_epoch_time: 12.9369\n[INFO] Run 3 of 3 for non-compiled model\nEpoch: 1 | train_loss: 0.7915 | train_acc: 0.7246 | test_loss: 0.6102 | test_acc: 0.7894 | train_epoch_time: 184.9795 | test_epoch_time: 13.0175\nEpoch: 2 | train_loss: 0.4394 | train_acc: 0.8477 | test_loss: 0.5958 | test_acc: 0.7968 | train_epoch_time: 184.9266 | test_epoch_time: 12.9909\nEpoch: 3 | train_loss: 0.3156 | train_acc: 0.8893 | test_loss: 0.4299 | test_acc: 0.8547 | train_epoch_time: 185.1226 | test_epoch_time: 12.9396\nEpoch: 4 | train_loss: 0.2371 | train_acc: 0.9163 | test_loss: 0.4185 | test_acc: 0.8608 | train_epoch_time: 184.9447 | test_epoch_time: 12.9673\nEpoch: 5 | train_loss: 0.1739 | train_acc: 0.9389 | test_loss: 0.3797 | test_acc: 0.8805 | train_epoch_time: 184.9552 | test_epoch_time: 13.0328\n</pre> <p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6709\u200b\u4e86\u200b\u5b9e\u9a8c\u200b3\u200b\u7684\u200b\u7ed3\u679c\u200b\u5217\u8868\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u904d\u5386\u200b\u8fd9\u4e9b\u200b\u7ed3\u679c\u200b\u5e76\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6240\u6709\u200b\u7ed3\u679c\u200b\u7684\u200b\u6570\u636e\u200b\u6846\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u6309\u200bepoch\u200b\u7f16\u53f7\u200b\uff08\u200b\u6570\u636e\u200b\u6846\u200b\u7684\u200b\u7d22\u5f15\u200b\uff09\u200b\u5206\u7ec4\u200b\u5e76\u200b\u5bf9\u200b\u7ed3\u679c\u200b\u53d6\u200b\u5e73\u5747\u503c\u200b\uff0c\u200b\u6765\u200b\u8ba1\u7b97\u200b3\u200b\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u5e73\u5747\u200b\u7ed3\u679c\u200b\u3002</p> <pre>import pandas as pd\n\n# \u200b\u5047\u8bbe\u200b results \u200b\u662f\u200b\u5b9e\u9a8c\u200b3\u200b\u7684\u200b\u7ed3\u679c\u200b\u5217\u8868\u200b\nresults = [...]  # \u200b\u8fd9\u91cc\u200b\u586b\u5145\u200b\u5b9e\u9645\u200b\u7684\u200b\u7ed3\u679c\u200b\u6570\u636e\u200b\n\n# \u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u6240\u6709\u200b\u7ed3\u679c\u200b\u7684\u200b\u6570\u636e\u200b\u6846\u200b\ndf = pd.DataFrame(results)\n\n# \u200b\u6309\u200bepoch\u200b\u7f16\u53f7\u200b\u5206\u7ec4\u200b\u5e76\u200b\u5bf9\u200b\u7ed3\u679c\u200b\u53d6\u200b\u5e73\u5747\u503c\u200b\naveraged_results = df.groupby(df.index).mean()\n\nprint(averaged_results)\n</pre> In\u00a0[30]: Copied! <pre># Go through non_compile_results_multiple_runs and create a dataframe for each run then concatenate them together\nnon_compile_results_dfs = []\nfor result in non_compile_results_multiple_runs:\n    result_df = pd.DataFrame(result)\n    non_compile_results_dfs.append(result_df)\nnon_compile_results_multiple_runs_df = pd.concat(non_compile_results_dfs)\n\n# Get the averages across the multiple runs\nnon_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean()\nnon_compile_results_multiple_runs_df\n</pre> # Go through non_compile_results_multiple_runs and create a dataframe for each run then concatenate them together non_compile_results_dfs = [] for result in non_compile_results_multiple_runs:     result_df = pd.DataFrame(result)     non_compile_results_dfs.append(result_df) non_compile_results_multiple_runs_df = pd.concat(non_compile_results_dfs)  # Get the averages across the multiple runs non_compile_results_multiple_runs_df = non_compile_results_multiple_runs_df.groupby(non_compile_results_multiple_runs_df.index).mean() non_compile_results_multiple_runs_df Out[30]: train_loss train_acc test_loss test_acc train_epoch_time test_epoch_time 0 0.815352 0.716103 0.590690 0.796710 185.060622 13.017663 1 0.447013 0.845567 0.618526 0.790150 185.004740 12.973144 2 0.322255 0.888117 0.436471 0.852321 185.068499 12.956863 3 0.242587 0.915120 0.436207 0.858946 184.995601 12.969341 4 0.179439 0.937612 0.479547 0.854727 184.995575 12.993280 <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u518d\u200b\u6765\u200b\u68c0\u67e5\u200b\u8fd9\u4e9b\u200b\uff0c\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7ee7\u7eed\u200b\u8fdb\u884c\u200b\u5b9e\u9a8c\u200b4\u3002</p> In\u00a0[31]: Copied! <pre># Create compiled model\ncompiled_model = create_compiled_model()\n\n# Create an empty list to store compiled model results\ncompiled_results_multiple_runs = []\n\n# Run compiled model for multiple runs\nfor i in tqdm(range(NUM_RUNS)):\n    print(f\"[INFO] Run {i+1} of {NUM_RUNS} for compiled model\")\n    # Train the compiled model (note: the model will only be compiled once and then re-used for subsequent runs)\n    results = train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS, disable_progress_bar=True)\n    compiled_results_multiple_runs.append(results)\n</pre> # Create compiled model compiled_model = create_compiled_model()  # Create an empty list to store compiled model results compiled_results_multiple_runs = []  # Run compiled model for multiple runs for i in tqdm(range(NUM_RUNS)):     print(f\"[INFO] Run {i+1} of {NUM_RUNS} for compiled model\")     # Train the compiled model (note: the model will only be compiled once and then re-used for subsequent runs)     results = train_compiled_model(model=compiled_model, epochs=NUM_EPOCHS, disable_progress_bar=True)     compiled_results_multiple_runs.append(results) <pre>Time to compile: 0.001275777816772461 | Note: The first time you compile your model, the first few epochs will be slower than subsequent runs.\n</pre> <pre>  0%|          | 0/3 [00:00&lt;?, ?it/s]</pre> <pre>[INFO] Run 1 of 3 for compiled model\nEpoch: 1 | train_loss: 0.8026 | train_acc: 0.7192 | test_loss: 0.6995 | test_acc: 0.7650 | train_epoch_time: 194.3336 | test_epoch_time: 20.6106\nEpoch: 2 | train_loss: 0.4440 | train_acc: 0.8483 | test_loss: 0.5565 | test_acc: 0.8089 | train_epoch_time: 169.3882 | test_epoch_time: 10.8076\nEpoch: 3 | train_loss: 0.3208 | train_acc: 0.8896 | test_loss: 0.4164 | test_acc: 0.8620 | train_epoch_time: 169.9283 | test_epoch_time: 10.8361\nEpoch: 4 | train_loss: 0.2329 | train_acc: 0.9197 | test_loss: 0.3635 | test_acc: 0.8792 | train_epoch_time: 169.8744 | test_epoch_time: 10.9050\nEpoch: 5 | train_loss: 0.1803 | train_acc: 0.9369 | test_loss: 0.4387 | test_acc: 0.8587 | train_epoch_time: 169.6391 | test_epoch_time: 10.8240\n[INFO] Run 2 of 3 for compiled model\nEpoch: 1 | train_loss: 0.1875 | train_acc: 0.9347 | test_loss: 0.4187 | test_acc: 0.8714 | train_epoch_time: 169.4814 | test_epoch_time: 10.8180\nEpoch: 2 | train_loss: 0.1288 | train_acc: 0.9550 | test_loss: 0.4333 | test_acc: 0.8698 | train_epoch_time: 169.4503 | test_epoch_time: 10.8263\nEpoch: 3 | train_loss: 0.0950 | train_acc: 0.9672 | test_loss: 0.4867 | test_acc: 0.8650 | train_epoch_time: 169.6038 | test_epoch_time: 10.8199\nEpoch: 4 | train_loss: 0.0943 | train_acc: 0.9675 | test_loss: 0.3714 | test_acc: 0.8966 | train_epoch_time: 169.5757 | test_epoch_time: 10.8221\nEpoch: 5 | train_loss: 0.0537 | train_acc: 0.9821 | test_loss: 0.5002 | test_acc: 0.8701 | train_epoch_time: 169.5253 | test_epoch_time: 10.8426\n[INFO] Run 3 of 3 for compiled model\nEpoch: 1 | train_loss: 0.0705 | train_acc: 0.9751 | test_loss: 0.4333 | test_acc: 0.8839 | train_epoch_time: 169.4846 | test_epoch_time: 10.9057\nEpoch: 2 | train_loss: 0.0595 | train_acc: 0.9802 | test_loss: 0.4341 | test_acc: 0.8904 | train_epoch_time: 169.6055 | test_epoch_time: 10.8804\nEpoch: 3 | train_loss: 0.0405 | train_acc: 0.9859 | test_loss: 0.4478 | test_acc: 0.8901 | train_epoch_time: 169.5788 | test_epoch_time: 10.8449\nEpoch: 4 | train_loss: 0.0365 | train_acc: 0.9873 | test_loss: 0.5382 | test_acc: 0.8765 | train_epoch_time: 169.6732 | test_epoch_time: 10.9873\nEpoch: 5 | train_loss: 0.0422 | train_acc: 0.9854 | test_loss: 0.5057 | test_acc: 0.8832 | train_epoch_time: 169.6618 | test_epoch_time: 10.8969\n</pre> <p>\u200b\u5b9e\u9a8c\u200b4\u200b\u5b8c\u6210\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u7ed3\u679c\u200b\u6c47\u603b\u200b\u5230\u200b\u4e00\u4e2a\u200b\u6570\u636e\u200b\u6846\u4e2d\u200b\uff0c\u200b\u5e76\u200b\u8ba1\u7b97\u200b\u6bcf\u4e2a\u200b\u8fd0\u884c\u200b\uff08run\uff09\u200b\u7684\u200b\u5e73\u5747\u503c\u200b\uff08\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u6309\u200b\u6570\u636e\u200b\u6846\u200b\u7684\u200b\u7d22\u5f15\u200b\u53f7\u200b\uff0c\u200b\u5373\u200bepoch\u200b\u53f7\u200b\u8fdb\u884c\u200b\u5206\u7ec4\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u3002</p> In\u00a0[32]: Copied! <pre># Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together\ncompile_results_dfs = []\nfor result in compiled_results_multiple_runs:\n    result_df = pd.DataFrame(result)\n    compile_results_dfs.append(result_df)\ncompile_results_multiple_runs_df = pd.concat(compile_results_dfs)\n\n# Get the averages across the multiple runs\ncompile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number\ncompile_results_multiple_runs_df\n</pre> # Go through compile_results_multiple_runs and create a dataframe for each run then concatenate them together compile_results_dfs = [] for result in compiled_results_multiple_runs:     result_df = pd.DataFrame(result)     compile_results_dfs.append(result_df) compile_results_multiple_runs_df = pd.concat(compile_results_dfs)  # Get the averages across the multiple runs compile_results_multiple_runs_df = compile_results_multiple_runs_df.groupby(compile_results_multiple_runs_df.index).mean() # .index = groupby the epoch number compile_results_multiple_runs_df Out[32]: train_loss train_acc test_loss test_acc train_epoch_time test_epoch_time 0 0.353548 0.876332 0.517181 0.840124 177.766548 14.111428 1 0.210781 0.927845 0.474630 0.856375 169.481367 10.838063 2 0.152098 0.947577 0.450293 0.872396 169.703638 10.833619 3 0.121230 0.958177 0.424376 0.884065 169.707751 10.904810 4 0.092080 0.968116 0.481520 0.870649 169.608708 10.854486 In\u00a0[33]: Copied! <pre># Create a directory to save the multi-run figure to \nos.makedirs(\"pytorch_2_results/figures\", exist_ok=True)\n\n# Create a path to save the figure for multiple runs\nsave_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"\n\n# Plot the mean epoch times for experiment 3 and 4\nplot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df, \n                      compiled_results=compile_results_multiple_runs_df, \n                      multi_runs=True, \n                      num_runs=NUM_RUNS, \n                      save_path=save_path_multi_run, \n                      save=True)\n</pre> # Create a directory to save the multi-run figure to  os.makedirs(\"pytorch_2_results/figures\", exist_ok=True)  # Create a path to save the figure for multiple runs save_path_multi_run = f\"pytorch_2_results/figures/multi_run_{GPU_NAME}_{MODEL_NAME}_{DATASET_NAME}_{IMAGE_SIZE}_train_epoch_time.png\"  # Plot the mean epoch times for experiment 3 and 4 plot_mean_epoch_times(non_compiled_results=non_compile_results_multiple_runs_df,                        compiled_results=compile_results_multiple_runs_df,                        multi_runs=True,                        num_runs=NUM_RUNS,                        save_path=save_path_multi_run,                        save=True) <pre>Mean train epoch time difference: -7.443% (negative means faster)\nMean test epoch time difference: -11.351% (negative means faster)\n[INFO] Plot saved to pytorch_2_results/figures/multi_run_NVIDIA_TITAN_RTX_ResNet50_CIFAR10_224_train_epoch_time.png\n</pre> <p>\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u770b\u6765\u200b\u5728\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u4e2d\u200b\uff0c\u200b\u7f16\u8bd1\u200b\u540e\u200b\u7684\u200b\u6a21\u578b\u200b\u8868\u73b0\u200b\u7565\u80dc\u4e00\u7b79\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u56e0\u4e3a\u200b\u5728\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\uff08\u200b\u4f7f\u7528\u200b\u8f83\u5c11\u200b\u7684\u200b epoch \u200b\u6570\u200b\uff09\u200b\u65f6\u200b\uff0c\u200b\u7f16\u8bd1\u200b\u6a21\u578b\u200b\u9700\u8981\u200b\u82b1\u8d39\u200b\u76f8\u5f53\u200b\u591a\u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u200b\u5b8c\u6210\u200b\u7b2c\u4e00\u4e2a\u200b epoch \u200b\u7684\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u5f53\u200b\u6a21\u578b\u200b\u5df2\u7ecf\u200b\u7f16\u8bd1\u200b\u5b8c\u6bd5\u200b\u5e76\u200b\u5f00\u59cb\u200b\u8fdb\u884c\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\u7684\u200b\u8bad\u7ec3\u200b\u65f6\u200b\uff0c\u200b\u5e55\u540e\u200b\u4f18\u5316\u200b\u5e26\u6765\u200b\u7684\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u5c31\u200b\u5f00\u59cb\u200b\u663e\u73b0\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u53ef\u80fd\u200b\u7684\u200b\u6269\u5c55\u200b\u5b9e\u9a8c\u200b\u662f\u200b\u8ba9\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff0c\u200b\u6bd4\u5982\u200b 100 \u200b\u4e2a\u200b epoch\uff0c\u200b\u770b\u770b\u200b\u7ed3\u679c\u200b\u5982\u4f55\u200b\u6bd4\u8f83\u200b\u3002</p> In\u00a0[34]: Copied! <pre># Make a directory for multi_run results\nimport os\npytorch_2_results_dir = \"pytorch_2_results\"\npytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\"\nos.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)\n\n# Create filenames for each of the dataframes\nsave_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\nsave_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"\n\n# Create filepaths to save the results to\nmulti_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\"\nmulti_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\"\nprint(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\")\nprint(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")\n\n# Save the results\nnon_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path)\ncompile_results_multiple_runs_df.to_csv(multi_run_compile_save_path)\n</pre> # Make a directory for multi_run results import os pytorch_2_results_dir = \"pytorch_2_results\" pytorch_2_multi_run_results_dir = f\"{pytorch_2_results_dir}/multi_run_results\" os.makedirs(pytorch_2_multi_run_results_dir, exist_ok=True)  # Create filenames for each of the dataframes save_name_for_multi_run_non_compiled_results = f\"multi_run_non_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\" save_name_for_multi_run_compiled_results = f\"multi_run_compiled_results_{NUM_RUNS}_runs_{DATASET_NAME}_{MODEL_NAME}_{GPU_NAME}.csv\"  # Create filepaths to save the results to multi_run_no_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_non_compiled_results}\" multi_run_compile_save_path = f\"{pytorch_2_multi_run_results_dir}/{save_name_for_compiled_results}\" print(f\"[INFO] Saving experiment 3 non-compiled results to: {multi_run_no_compile_save_path}\") print(f\"[INFO] Saving experiment 4 compiled results to: {multi_run_compile_save_path}\")  # Save the results non_compile_results_multiple_runs_df.to_csv(multi_run_no_compile_save_path) compile_results_multiple_runs_df.to_csv(multi_run_compile_save_path) <pre>[INFO] Saving experiment 3 non-compiled results to: pytorch_2_results/multi_run_results/single_run_non_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n[INFO] Saving experiment 4 compiled results to: pytorch_2_results/multi_run_results/single_run_compiled_results_CIFAR10_ResNet50_NVIDIA_TITAN_RTX.csv\n</pre>"},{"location":"pytorch_2_intro/#pytorch-20","title":"\u5feb\u901f\u200b\u5165\u95e8\u200b PyTorch 2.0 \u200b\u6559\u7a0b\u200b\u00b6","text":""},{"location":"pytorch_2_intro/#30","title":"30\u200b\u79d2\u200b\u7b80\u4ecb\u200b\u00b6","text":"<p>PyTorch 2.0 \u200b\u53d1\u5e03\u200b\u4e86\u200b\uff01</p> <p>\u200b\u4e3b\u8981\u200b\u6539\u8fdb\u200b\u5728\u4e8e\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u8fd9\u4e00\u200b\u6539\u8fdb\u200b\u901a\u8fc7\u200b\u4e00\u884c\u200b\u5411\u200b\u540e\u200b\u517c\u5bb9\u200b\u7684\u200b\u4ee3\u7801\u200b\u5b9e\u73b0\u200b\u3002</p> <pre>torch.compile()\n</pre> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5728\u200b\u4f60\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5176\u200b\u4f20\u9012\u200b\u7ed9\u200b <code>torch.compile()</code>\uff0c\u200b\u4ece\u800c\u200b\u5728\u200b\u65b0\u6b3e\u200bGPU\uff08\u200b\u4f8b\u5982\u200bNVIDIA RTX 40\u200b\u7cfb\u5217\u200b\u3001A100\u3001H100\uff0cGPU\u200b\u8d8a\u65b0\u200b\uff0c\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\u8d8a\u200b\u660e\u663e\u200b\uff09\u200b\u4e0a\u200b\u671f\u5f85\u200b\u8bad\u7ec3\u200b\u548c\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u7684\u200b\u63d0\u5347\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a PyTorch 2.0 \u200b\u4e2d\u200b\u8fd8\u6709\u200b\u8bb8\u591a\u200b\u5176\u4ed6\u200b\u5347\u7ea7\u200b\uff0c\u200b\u4e0d\u4ec5\u4ec5\u200b\u662f\u200b <code>torch.compile()</code>\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u5b83\u200b\u662f\u200b\u4e3b\u8981\u200b\u7684\u200b\u6539\u8fdb\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u91cd\u70b9\u200b\u4ecb\u7ecd\u200b\u5b83\u200b\u3002\u200b\u5b8c\u6574\u200b\u53d8\u66f4\u200b\u5217\u8868\u200b\u8bf7\u53c2\u9605\u200b PyTorch 2.0 \u200b\u53d1\u5e03\u200b\u8bf4\u660e\u200b\u3002</p>"},{"location":"pytorch_2_intro/#pytorch","title":"\u6211\u200b\u7684\u200b\u65e7\u200bPyTorch\u200b\u4ee3\u7801\u200b\u8fd8\u200b\u80fd\u200b\u7528\u200b\u5417\u200b\uff1f\u00b6","text":"<p>\u200b\u662f\u200b\u7684\u200b\uff0cPyTorch 2.0 \u200b\u5411\u200b\u540e\u200b\u517c\u5bb9\u200b\u3002\u200b\u5927\u90e8\u5206\u200b\u53d8\u5316\u200b\u662f\u200b\u65b0\u589e\u200b\u529f\u80fd\u200b\uff08\u200b\u65b0\u200b\u7279\u6027\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5982\u679c\u200b\u4f60\u200b\u5df2\u7ecf\u200b\u719f\u6089\u200b PyTorch\uff0c\u200b\u6bd4\u5982\u200b\u901a\u8fc7\u200b learnpytorch.io \u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u7acb\u5373\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b PyTorch 2.0\u3002\u200b\u5e76\u4e14\u200b\u4f60\u200b\u7684\u200b\u65e7\u200b PyTorch \u200b\u4ee3\u7801\u200b\u4ecd\u7136\u200b\u6709\u6548\u200b\u3002</p>"},{"location":"pytorch_2_intro/","title":"\u5feb\u901f\u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b\u00b6","text":""},{"location":"pytorch_2_intro/#pytorch-20","title":"\u5728\u200b PyTorch 2.0 \u200b\u4e4b\u524d\u200b\u00b6","text":"<p>\u200b\u5728\u200b PyTorch 2.0 \u200b\u53d1\u5e03\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6846\u67b6\u200b\u7684\u200b\u5f00\u53d1\u200b\u548c\u200b\u4f7f\u7528\u200b\u7ecf\u5386\u200b\u4e86\u200b\u591a\u4e2a\u200b\u9636\u6bb5\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u9636\u6bb5\u200b\u90fd\u200b\u5e26\u6765\u200b\u4e86\u200b\u65b0\u200b\u7684\u200b\u7279\u6027\u200b\u548c\u200b\u6539\u8fdb\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e9b\u200b\u5173\u952e\u200b\u7684\u200b\u53d1\u5c55\u200b\u91cc\u7a0b\u7891\u200b\uff1a</p>"},{"location":"pytorch_2_intro/#pytorch-10","title":"PyTorch 1.0\u00b6","text":"<p>PyTorch 1.0 \u200b\u662f\u200b PyTorch \u200b\u53d1\u5c55\u200b\u5386\u7a0b\u200b\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u91cd\u8981\u200b\u7248\u672c\u200b\uff0c\u200b\u5b83\u200b\u5f15\u5165\u200b\u4e86\u200b\u8bb8\u591a\u200b\u5173\u952e\u200b\u7279\u6027\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u5f00\u53d1\u200b\u548c\u200b\u90e8\u7f72\u200b\u66f4\u52a0\u200b\u9ad8\u6548\u200b\u548c\u200b\u7075\u6d3b\u200b\u3002</p> <ul> <li>JIT \u200b\u7f16\u8bd1\u5668\u200b\uff1aPyTorch 1.0 \u200b\u5f15\u5165\u200b\u4e86\u200b Just-In-Time (JIT) \u200b\u7f16\u8bd1\u5668\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b Python \u200b\u4ee3\u7801\u200b\u8f6c\u6362\u200b\u4e3a\u200b TorchScript\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u79cd\u200b\u53ef\u4ee5\u200b\u5728\u200b\u6ca1\u6709\u200b Python \u200b\u4f9d\u8d56\u200b\u7684\u200b\u73af\u5883\u4e2d\u8fd0\u884c\u200b\u7684\u200b\u4e2d\u95f4\u200b\u8868\u793a\u200b\u5f62\u5f0f\u200b\u3002</li> <li>C++ \u200b\u524d\u7aef\u200b\uff1a\u200b\u9664\u4e86\u200b Python \u200b\u63a5\u53e3\u200b\uff0cPyTorch 1.0 \u200b\u8fd8\u200b\u63d0\u4f9b\u200b\u4e86\u200b C++ \u200b\u524d\u7aef\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u5728\u200b\u6027\u80fd\u200b\u654f\u611f\u200b\u7684\u200b\u5e94\u7528\u200b\u4e2d\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6210\u4e3a\u200b\u53ef\u80fd\u200b\u3002</li> <li>\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\uff1aPyTorch 1.0 \u200b\u589e\u5f3a\u200b\u4e86\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u529f\u80fd\u200b\uff0c\u200b\u652f\u6301\u200b\u591a\u200b\u8282\u70b9\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u5927\u89c4\u6a21\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6548\u7387\u200b\u3002</li> </ul>"},{"location":"pytorch_2_intro/#pytorch-04","title":"PyTorch 0.4\u00b6","text":"<p>PyTorch 0.4 \u200b\u7248\u672c\u200b\u5728\u200b\u6613\u7528\u6027\u200b\u548c\u200b\u6027\u80fd\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u4e86\u200b\u591a\u9879\u200b\u6539\u8fdb\u200b\u3002</p> <ul> <li>Tensor \u200b\u548c\u200b Variable \u200b\u5408\u5e76\u200b\uff1a\u200b\u5728\u200b\u4e4b\u524d\u200b\u7684\u200b\u7248\u672c\u200b\u4e2d\u200b\uff0cTensor \u200b\u548c\u200b Variable \u200b\u662f\u200b\u4e24\u4e2a\u200b\u4e0d\u540c\u200b\u7684\u200b\u6982\u5ff5\u200b\uff0cPyTorch 0.4 \u200b\u5c06\u200b\u5b83\u4eec\u200b\u5408\u5e76\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u7684\u200b Tensor \u200b\u7c7b\u578b\u200b\uff0c\u200b\u7b80\u5316\u200b\u4e86\u200b\u4ee3\u7801\u200b\u3002</li> <li>\u200b\u96f6\u7ef4\u200b\u5f20\u91cf\u200b\uff1a\u200b\u5f15\u5165\u200b\u4e86\u200b\u96f6\u7ef4\u200b\u5f20\u91cf\u200b\uff08\u200b\u6807\u91cf\u200b\uff09\uff0c\u200b\u4f7f\u5f97\u200b\u5904\u7406\u200b\u6807\u91cf\u200b\u503c\u200b\u66f4\u52a0\u200b\u76f4\u89c2\u200b\u3002</li> <li>Windows \u200b\u652f\u6301\u200b\uff1aPyTorch 0.4 \u200b\u5f00\u59cb\u200b\u652f\u6301\u200b Windows \u200b\u5e73\u53f0\u200b\uff0c\u200b\u6269\u5927\u200b\u4e86\u200b\u7528\u6237\u200b\u57fa\u7840\u200b\u3002</li> </ul>"},{"location":"pytorch_2_intro/#pytorch-03","title":"PyTorch 0.3\u00b6","text":"<p>PyTorch 0.3 \u200b\u7248\u672c\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u63d0\u9ad8\u200b\u6027\u80fd\u200b\u548c\u200b\u7a33\u5b9a\u6027\u200b\u3002</p> <ul> <li>\u200b\u52a8\u6001\u56fe\u200b\u4f18\u5316\u200b\uff1a\u200b\u5f15\u5165\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u52a8\u6001\u56fe\u200b\u4f18\u5316\u200b\u6280\u672f\u200b\uff0c\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u901f\u5ea6\u200b\u3002</li> <li>\u200b\u65b0\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff1a\u200b\u589e\u52a0\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\uff0c\u200b\u4e30\u5bcc\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u9009\u62e9\u200b\u3002</li> </ul>"},{"location":"pytorch_2_intro/#pytorch-02","title":"PyTorch 0.2\u00b6","text":"<p>PyTorch 0.2 \u200b\u7248\u672c\u200b\u5728\u200b\u7075\u6d3b\u6027\u200b\u548c\u200b\u6613\u7528\u6027\u200b\u65b9\u9762\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6539\u8fdb\u200b\u3002</p> <ul> <li>\u200b\u9ad8\u9636\u200b\u68af\u5ea6\u200b\uff1a\u200b\u652f\u6301\u200b\u9ad8\u9636\u200b\u68af\u5ea6\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u66f4\u200b\u590d\u6742\u200b\u7684\u200b\u4f18\u5316\u200b\u7b97\u6cd5\u200b\u6210\u4e3a\u200b\u53ef\u80fd\u200b\u3002</li> <li>\u200b\u5e7f\u64ad\u200b\u673a\u5236\u200b\uff1a\u200b\u5f15\u5165\u200b\u4e86\u200b NumPy \u200b\u98ce\u683c\u200b\u7684\u200b\u5e7f\u64ad\u200b\u673a\u5236\u200b\uff0c\u200b\u7b80\u5316\u200b\u4e86\u200b\u5f20\u91cf\u200b\u64cd\u4f5c\u200b\u3002</li> </ul>"},{"location":"pytorch_2_intro/#pytorch-01","title":"PyTorch 0.1\u00b6","text":"<p>PyTorch 0.1 \u200b\u662f\u200b PyTorch \u200b\u7684\u200b\u521d\u59cb\u200b\u7248\u672c\u200b\uff0c\u200b\u5960\u5b9a\u200b\u4e86\u200b\u6846\u67b6\u200b\u7684\u200b\u57fa\u7840\u200b\u3002</p> <ul> <li>\u200b\u52a8\u6001\u200b\u8ba1\u7b97\u200b\u56fe\u200b\uff1aPyTorch \u200b\u4ee5\u200b\u5176\u200b\u52a8\u6001\u200b\u8ba1\u7b97\u200b\u56fe\u200b\u7279\u6027\u200b\u800c\u200b\u95fb\u540d\u200b\uff0c\u200b\u8fd9\u200b\u4f7f\u5f97\u200b\u6a21\u578b\u200b\u7684\u200b\u6784\u5efa\u200b\u548c\u200b\u8c03\u8bd5\u200b\u66f4\u52a0\u200b\u76f4\u89c2\u200b\u548c\u200b\u7075\u6d3b\u200b\u3002</li> <li>\u200b\u81ea\u52a8\u200b\u5fae\u5206\u200b\uff1a\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u81ea\u52a8\u200b\u5fae\u5206\u200b\u529f\u80fd\u200b\uff0c\u200b\u7b80\u5316\u200b\u4e86\u200b\u68af\u5ea6\u200b\u8ba1\u7b97\u200b\u7684\u200b\u8fc7\u7a0b\u200b\u3002</li> </ul> <p>\u200b\u8fd9\u4e9b\u200b\u7248\u672c\u200b\u7684\u200b\u53d1\u5c55\u200b\u4e3a\u200b PyTorch 2.0 \u200b\u7684\u200b\u63a8\u51fa\u200b\u5960\u5b9a\u200b\u4e86\u200b\u575a\u5b9e\u200b\u7684\u200b\u57fa\u7840\u200b\uff0c\u200b\u4f7f\u5f97\u200b PyTorch \u200b\u6210\u4e3a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u6700\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200b\u6846\u67b6\u200b\u4e4b\u4e00\u200b\u3002</p>"},{"location":"pytorch_2_intro/#pytorch-20","title":"\u5728\u200b PyTorch 2.0 \u200b\u4e4b\u540e\u200b\u00b6","text":"<p>PyTorch 2.0 \u200b\u662f\u200b\u4e00\u6b21\u200b\u91cd\u5927\u200b\u7684\u200b\u66f4\u65b0\u200b\uff0c\u200b\u5e26\u6765\u200b\u4e86\u200b\u8bb8\u591a\u200b\u65b0\u200b\u7279\u6027\u200b\u548c\u200b\u6539\u8fdb\u200b\u3002\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e9b\u200b\u5173\u952e\u70b9\u200b\uff1a</p> <ol> <li>\u200b\u6027\u80fd\u200b\u63d0\u5347\u200b\uff1aPyTorch 2.0 \u200b\u901a\u8fc7\u200b\u5f15\u5165\u200b\u65b0\u200b\u7684\u200b\u7f16\u8bd1\u5668\u200b\u548c\u200b\u4f18\u5316\u200b\u6280\u672f\u200b\uff0c\u200b\u663e\u8457\u200b\u63d0\u9ad8\u200b\u4e86\u200b\u6a21\u578b\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u63a8\u7406\u200b\u901f\u5ea6\u200b\u3002</li> <li>\u200b\u52a8\u6001\u56fe\u200b\u4e0e\u200b\u9759\u6001\u200b\u56fe\u200b\u7684\u200b\u878d\u5408\u200b\uff1aPyTorch 2.0 \u200b\u5728\u200b\u4fdd\u6301\u200b\u52a8\u6001\u56fe\u200b\u7075\u6d3b\u6027\u200b\u7684\u200b\u540c\u65f6\u200b\uff0c\u200b\u5f15\u5165\u200b\u4e86\u200b\u9759\u6001\u200b\u56fe\u200b\u7684\u200b\u4f18\u5316\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u7528\u6237\u200b\u53ef\u4ee5\u200b\u5728\u200b\u4e0d\u200b\u727a\u7272\u200b\u6613\u7528\u6027\u200b\u7684\u200b\u524d\u63d0\u200b\u4e0b\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u6027\u80fd\u200b\u3002</li> <li>\u200b\u66f4\u200b\u7b80\u6d01\u200b\u7684\u200b API\uff1aPyTorch 2.0 \u200b\u5bf9\u200b\u4e00\u4e9b\u200b\u5e38\u7528\u200b\u529f\u80fd\u200b\u8fdb\u884c\u200b\u4e86\u200b\u91cd\u6784\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u4ee3\u7801\u200b\u66f4\u52a0\u200b\u7b80\u6d01\u200b\u548c\u200b\u6613\u8bfb\u200b\u3002</li> <li>\u200b\u66f4\u597d\u200b\u7684\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u652f\u6301\u200b\uff1aPyTorch 2.0 \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u66f4\u200b\u5f3a\u5927\u200b\u7684\u200b\u5206\u5e03\u5f0f\u200b\u8bad\u7ec3\u200b\u5de5\u5177\u200b\uff0c\u200b\u4f7f\u5f97\u200b\u5728\u200b\u591a\u200bGPU\u200b\u548c\u200b\u591a\u200b\u8282\u70b9\u200b\u73af\u5883\u200b\u4e0b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u66f4\u52a0\u200b\u5bb9\u6613\u200b\u3002</li> <li>\u200b\u589e\u5f3a\u200b\u7684\u200b\u81ea\u52a8\u200b\u5fae\u5206\u200b\u529f\u80fd\u200b\uff1aPyTorch 2.0 \u200b\u6539\u8fdb\u200b\u4e86\u200b\u81ea\u52a8\u200b\u5fae\u5206\u200b\u673a\u5236\u200b\uff0c\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u7075\u6d3b\u6027\u200b\u548c\u200b\u63a7\u5236\u80fd\u529b\u200b\u3002</li> </ol> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u793a\u4f8b\u200b\uff0c\u200b\u5c55\u793a\u200b\u4e86\u200b\u5982\u4f55\u200b\u5728\u200b PyTorch 2.0 \u200b\u4e2d\u200b\u5b9a\u4e49\u200b\u548c\u200b\u8bad\u7ec3\u200b\u4e00\u4e2a\u200b\u6a21\u578b\u200b\uff1a</p> <pre>import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\n# \u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\nclass SimpleNet(nn.Module):\n    def __init__(self):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(784, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# \u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u5b9e\u4f8b\u200b\nmodel = SimpleNet()\n\n# \u200b\u5b9a\u4e49\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.01)\n\n# \u200b\u751f\u6210\u200b\u4e00\u4e9b\u200b\u968f\u673a\u200b\u6570\u636e\u200b\ninputs = torch.randn(64, 784)\nlabels = torch.randint(0, 10, (64,))\n\n# \u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\noutputs = model(inputs)\nloss = criterion(outputs, labels)\n\n# \u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u548c\u200b\u4f18\u5316\u200b\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\nprint(f\"Loss: {loss.item()}\")\n</pre> <p>\u200b\u8fd9\u4e2a\u200b\u793a\u4f8b\u200b\u5c55\u793a\u200b\u4e86\u200b\u5982\u4f55\u200b\u5b9a\u4e49\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b PyTorch 2.0 \u200b\u8fdb\u884c\u200b\u524d\u5411\u200b\u4f20\u64ad\u200b\u3001\u200b\u8ba1\u7b97\u200b\u635f\u5931\u200b\u3001\u200b\u53cd\u5411\u200b\u4f20\u64ad\u200b\u548c\u200b\u53c2\u6570\u200b\u66f4\u65b0\u200b\u3002</p>"},{"location":"pytorch_2_intro/","title":"\u52a0\u901f\u200b\u6027\u80fd\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u7684\u200b\uff0cPyTorch 2.0 \u200b\u7684\u200b\u91cd\u70b9\u200b\u662f\u200b\u901f\u5ea6\u200b\uff0c\u200b\u5b83\u200b\u5b9e\u9645\u4e0a\u200b\u6709\u200b\u591a\u200b\u5feb\u200b\u5462\u200b\uff1f</p> <p>PyTorch \u200b\u56e2\u961f\u200b\u5728\u200b\u6765\u81ea\u200b Hugging Face Transformers\u3001timm\uff08PyTorch \u200b\u56fe\u50cf\u200b\u6a21\u578b\u200b\uff09\u200b\u548c\u200b TorchBench\uff08\u200b\u4ece\u200b GitHub \u200b\u7cbe\u9009\u200b\u7684\u200b\u4e00\u7cfb\u5217\u200b\u6d41\u884c\u200b\u4ee3\u7801\u200b\u5e93\u200b\uff09\u200b\u7684\u200b 163 \u200b\u4e2a\u200b\u5f00\u6e90\u200b\u6a21\u578b\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u4e86\u200b\u6d4b\u8bd5\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u5f88\u200b\u91cd\u8981\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u9664\u975e\u200b PyTorch 2.0 \u200b\u5728\u200b\u4eba\u4eec\u200b\u5b9e\u9645\u200b\u4f7f\u7528\u200b\u7684\u200b\u6a21\u578b\u200b\u4e0a\u200b\u901f\u5ea6\u200b\u66f4\u200b\u5feb\u200b\uff0c\u200b\u5426\u5219\u200b\u5b83\u200b\u5c31\u200b\u4e0d\u7b97\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u901a\u8fc7\u200b\u6df7\u5408\u200b\u4f7f\u7528\u200b AMP\uff08\u200b\u81ea\u52a8\u200b\u6df7\u5408\u200b\u7cbe\u5ea6\u200b\u6216\u200b float16\uff09\u200b\u8bad\u7ec3\u200b\u548c\u200b float32 \u200b\u7cbe\u5ea6\u200b\uff08\u200b\u66f4\u200b\u9ad8\u7cbe\u5ea6\u200b\u9700\u8981\u200b\u66f4\u200b\u591a\u200b\u8ba1\u7b97\u200b\uff09\uff0cPyTorch \u200b\u56e2\u961f\u200b\u53d1\u73b0\u200b <code>torch.compile()</code> \u200b\u5728\u200b NVIDIA A100 GPU \u200b\u4e0a\u200b\u7684\u200b\u8bad\u7ec3\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5e73\u5747\u200b 43% \u200b\u7684\u200b\u52a0\u901f\u200b\u3002</p> <p>\u200b\u6216\u8005\u200b\u5728\u200b timm \u200b\u4e0a\u200b\u63d0\u5347\u200b 38%\uff0c\u200b\u5728\u200b TorchBench \u200b\u4e0a\u200b\u63d0\u5347\u200b 76%\uff0c\u200b\u5728\u200b Hugging Face Transformers \u200b\u4e0a\u200b\u63d0\u5347\u200b 52%\u3002</p> <p>PyTorch 2.0 \u200b\u5728\u200b\u6765\u81ea\u200b\u4e0d\u540c\u200b\u6765\u6e90\u200b\u7684\u200b\u5404\u79cd\u200b\u6a21\u578b\u200b\u4e0a\u200b\u7684\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002\u200b\u6765\u6e90\u200b\uff1aPyTorch 2.0 \u200b\u53d1\u5e03\u516c\u544a\u200b\u3002</p>"},{"location":"pytorch_2_intro/#3","title":"3\u200b\u5206\u949f\u200b\u6982\u89c8\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\u6539\u7f16\u81ea\u200b mrdbourke.com \u200b\u4e0a\u200b\u7684\u200b A Quick Introduction to PyTorch 2.0\uff0c\u200b\u540c\u65f6\u200b\u8fd8\u6709\u200b\u914d\u5957\u200b\u7684\u200b YouTube \u200b\u89c6\u9891\u200b\u89e3\u91ca\u200b\u3002</p> <p><code>torch.compile()</code> \u200b\u80cc\u540e\u200b\u7684\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\u662f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p><code>torch.compile()</code> \u200b\u65e8\u5728\u200b\u201c\u200b\u5373\u63d2\u5373\u7528\u200b\u201d\uff0c\u200b\u4f46\u200b\u5176\u200b\u80cc\u540e\u200b\u6d89\u53ca\u200b\u51e0\u9879\u200b\u5173\u952e\u6280\u672f\u200b\uff1a</p> <ul> <li>TorchDynamo</li> <li>AOTAutograd</li> <li>PrimTorch</li> <li>TorchInductor</li> </ul> <p>PyTorch 2.0 \u200b\u5165\u95e8\u200b\u6307\u5357\u200b \u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u6280\u672f\u200b\u8fdb\u884c\u200b\u4e86\u200b\u66f4\u200b\u8be6\u7ec6\u200b\u7684\u200b\u89e3\u91ca\u200b\uff0c\u200b\u4f46\u200b\u4ece\u200b\u9ad8\u5c42\u6b21\u200b\u6765\u770b\u200b\uff0c<code>torch.compile()</code> \u200b\u63d0\u4f9b\u200b\u7684\u200b\u4e24\u5927\u200b\u4e3b\u8981\u200b\u6539\u8fdb\u200b\u662f\u200b\uff1a</p> <ul> <li>\u200b\u878d\u5408\u200b\uff08\u200b\u6216\u200b\u64cd\u4f5c\u200b\u878d\u5408\u200b\uff09</li> <li>\u200b\u56fe\u200b\u6355\u83b7\u200b\uff08\u200b\u6216\u56fe\u200b\u8ffd\u8e2a\u200b\uff09</li> </ul>"},{"location":"pytorch_2_intro/","title":"\u878d\u5408\u200b\u00b6","text":"<p>\u200b\u878d\u5408\u200b\uff0c\u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u7b97\u5b50\u200b\u878d\u5408\u200b\uff0c\u200b\u662f\u200b\u8ba9\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u52a0\u901f\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u6cd5\u200b\u4e4b\u4e00\u200b\uff08brrrrrr \u200b\u662f\u200b\u4f60\u200b\u7684\u200b GPU \u200b\u98ce\u6247\u200b\u5728\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u53d1\u51fa\u200b\u7684\u200b\u58f0\u97f3\u200b\uff09\u3002</p> <p>\u200b\u7b97\u5b50\u200b\u878d\u5408\u200b\u5c06\u200b\u591a\u4e2a\u200b\u64cd\u4f5c\u200b\u6d53\u7f29\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\uff08\u200b\u6216\u200b\u591a\u200b\u5bf9\u200b\u5c11\u200b\uff09\uff0c\u200b\u5c31\u200b\u50cf\u200b\u300a\u200b\u9f99\u73e0\u200bZ\u300b\u200b\u4e00\u6837\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u5462\u200b\uff1f</p> <p>\u200b\u73b0\u4ee3\u200b GPU \u200b\u62e5\u6709\u200b\u5982\u6b64\u200b\u5f3a\u5927\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\uff0c\u200b\u4ee5\u81f3\u4e8e\u200b\u5b83\u4eec\u200b\u5f80\u5f80\u200b\u4e0d\u662f\u200b\u8ba1\u7b97\u200b\u53d7\u9650\u200b\u7684\u200b\uff0c\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u4e3b\u8981\u200b\u74f6\u9888\u200b\u5728\u4e8e\u200b\u4f60\u200b\u80fd\u200b\u4ee5\u200b\u591a\u200b\u5feb\u200b\u7684\u200b\u901f\u5ea6\u200b\u5c06\u200b\u6570\u636e\u200b\u4ece\u200b CPU \u200b\u4f20\u8f93\u200b\u5230\u200b GPU\u3002\u200b\u8fd9\u200b\u5c31\u662f\u200b\u6240\u8c13\u200b\u7684\u200b\u5e26\u5bbd\u200b\u6216\u200b\u5185\u5b58\u200b\u5e26\u5bbd\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5c3d\u53ef\u80fd\u51cf\u5c11\u200b\u5e26\u5bbd\u200b\u6210\u672c\u200b\u3002</p> <p>\u200b\u5e76\u4e14\u200b\u5c3d\u53ef\u80fd\u200b\u591a\u5730\u200b\u5411\u200b\u9965\u997f\u200b\u7684\u200b GPU \u200b\u63d0\u4f9b\u6570\u636e\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e0e\u5176\u200b\u5bf9\u200b\u4e00\u5757\u200b\u6570\u636e\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\u7136\u540e\u200b\u5c06\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5230\u200b\u5185\u5b58\u200b\uff08\u200b\u589e\u52a0\u200b\u5e26\u5bbd\u200b\u6210\u672c\u200b\uff09\uff0c\u200b\u4e0d\u5982\u200b\u901a\u8fc7\u200b\u878d\u5408\u200b\u5c06\u200b\u5c3d\u53ef\u80fd\u200b\u591a\u200b\u7684\u200b\u64cd\u4f5c\u200b\u4e32\u8054\u200b\u8d77\u6765\u200b\u3002</p> <p>\u200b\u4e00\u4e2a\u200b\u7c97\u7565\u200b\u7684\u200b\u6bd4\u55bb\u200b\u662f\u200b\u4f7f\u7528\u200b\u6405\u62cc\u673a\u200b\u5236\u4f5c\u200b\u51b0\u6c99\u200b\u3002</p> <p>\u200b\u5927\u591a\u6570\u200b\u6405\u62cc\u673a\u200b\u64c5\u957f\u200b\u6405\u62cc\u200b\u4e1c\u897f\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b GPU \u200b\u64c5\u957f\u200b\u6267\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u4e00\u6837\u200b\uff09\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u6ca1\u6709\u200b\u7b97\u5b50\u200b\u878d\u5408\u200b\u7684\u200b\u6405\u62cc\u673a\u200b\uff0c\u200b\u5c31\u200b\u50cf\u200b\u6bcf\u6b21\u200b\u6dfb\u52a0\u200b\u4e00\u79cd\u200b\u539f\u6599\u200b\u5e76\u200b\u6bcf\u6b21\u200b\u6df7\u5408\u200b\u3002\u200b\u8fd9\u200b\u4e0d\u4ec5\u200b\u75af\u72c2\u200b\uff0c\u200b\u8fd8\u200b\u589e\u52a0\u200b\u4e86\u200b\u4f60\u200b\u7684\u200b\u5e26\u5bbd\u200b\u6210\u672c\u200b\u3002</p> <p>\u200b\u6bcf\u6b21\u200b\u5b9e\u9645\u200b\u6df7\u5408\u200b\u7684\u200b\u901f\u5ea6\u200b\u5f88\u5feb\u200b\uff08\u200b\u5c31\u200b\u50cf\u200b GPU \u200b\u8ba1\u7b97\u200b\u901a\u5e38\u200b\u5f88\u5feb\u200b\u4e00\u6837\u200b\uff09\uff0c\u200b\u4f46\u200b\u4f60\u200b\u6bcf\u6b21\u200b\u6dfb\u52a0\u200b\u539f\u6599\u200b\u65f6\u200b\u90fd\u200b\u4f1a\u200b\u635f\u5931\u200b\u5927\u91cf\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u6709\u200b\u7b97\u5b50\u200b\u878d\u5408\u200b\u7684\u200b\u6405\u62cc\u673a\u200b\uff0c\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4e00\u200b\u5f00\u59cb\u200b\u5c31\u200b\u6dfb\u52a0\u200b\u6240\u6709\u200b\u539f\u6599\u200b\uff08\u200b\u7b97\u5b50\u200b\u878d\u5408\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u4e00\u6b21\u6027\u200b\u8fdb\u884c\u200b\u6df7\u5408\u200b\u3002</p> <p>\u200b\u4e00\u200b\u5f00\u59cb\u200b\u6dfb\u52a0\u200b\u65f6\u4f1a\u200b\u635f\u5931\u200b\u4e00\u70b9\u200b\u65f6\u95f4\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u5c06\u200b\u6240\u6709\u200b\u5931\u53bb\u200b\u7684\u200b\u5185\u5b58\u200b\u5e26\u5bbd\u200b\u65f6\u95f4\u200b\u90fd\u200b\u627e\u200b\u56de\u6765\u200b\u4e86\u200b\u3002</p>"},{"location":"pytorch_2_intro/","title":"\u56fe\u200b\u6355\u6349\u200b\u00b6","text":"<p>\u200b\u56fe\u200b\u6355\u6349\u200b\u8fd9\u4e2a\u200b\u6982\u5ff5\u200b\u6211\u200b\u89e3\u91ca\u200b\u8d77\u6765\u200b\u4e0d\u200b\u90a3\u4e48\u200b\u81ea\u4fe1\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u200b\u7684\u200b\u7406\u89e3\u200b\u662f\u200b\uff0c\u200b\u56fe\u200b\u6355\u6349\u200b\u6216\u56fe\u200b\u8ffd\u8e2a\u200b\u662f\u200b\u8fd9\u6837\u200b\u7684\u200b\uff1a</p> <ul> <li>\u200b\u7ecf\u5386\u200b\u4e00\u7cfb\u5217\u200b\u9700\u8981\u200b\u53d1\u751f\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6bd4\u5982\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</li> <li>\u200b\u5e76\u4e14\u200b\u9884\u5148\u200b\u6355\u6349\u200b\u6216\u200b\u8ffd\u8e2a\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u3002</li> </ul> <p>\u200b\u4e0d\u200b\u4f7f\u7528\u200b\u56fe\u200b\u6355\u6349\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5c31\u200b\u50cf\u200b\u53bb\u200b\u4e00\u4e2a\u200b\u65b0\u200b\u5730\u65b9\u200b\uff0c\u200b\u6309\u7167\u200bGPS\u200b\u7684\u200b\u6307\u793a\u200b\u4e00\u6b65\u6b65\u200b\u8f6c\u5f2f\u200b\u3002</p> <p>\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200b\u4f18\u79c0\u200b\u7684\u200b\u4eba\u7c7b\u200b\u53f8\u673a\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5f88\u200b\u5bb9\u6613\u200b\u5730\u200b\u8ddf\u7740\u200b\u8f6c\u5f2f\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u4ecd\u7136\u200b\u9700\u8981\u200b\u601d\u8003\u200b\u6bcf\u200b\u4e00\u6b21\u200b\u8f6c\u5f2f\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u76f8\u5f53\u4e8e\u200bPyTorch\u200b\u5728\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\u65f6\u200b\u5fc5\u987b\u200b\u67e5\u627e\u200b\u6bcf\u4e2a\u200b\u64cd\u4f5c\u200b\u7684\u200b\u5177\u4f53\u5185\u5bb9\u200b\u3002</p> <p>\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u8981\u200b\u6267\u884c\u200b\u4e00\u4e2a\u200b\u52a0\u6cd5\u200b\uff0c\u200b\u5b83\u200b\u5fc5\u987b\u200b\u5148\u200b\u67e5\u627e\u200b\u52a0\u6cd5\u200b\u7684\u200b\u5b9a\u4e49\u200b\uff0c\u200b\u7136\u540e\u200b\u624d\u80fd\u200b\u6267\u884c\u200b\u3002</p> <p>\u200b\u5c3d\u7ba1\u200b\u5b83\u200b\u6267\u884c\u200b\u5f97\u200b\u5f88\u5feb\u200b\uff0c\u200b\u4f46\u200b\u4ecd\u7136\u200b\u5b58\u5728\u200b\u975e\u96f6\u200b\u7684\u200b\u5f00\u9500\u200b\u3002</p> <p></p> <p>\u200b\u56fe\u200b\u6355\u6349\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u7ed8\u5236\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u7684\u200b\u6b65\u9aa4\u200b\u5e76\u200b\u9884\u5148\u200b\u6355\u6349\u200b\u6bcf\u4e2a\u200b\u9700\u8981\u200b\u53d1\u751f\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u56fe\u200b\u6355\u6349\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5c31\u200b\u50cf\u200b\u5728\u200b\u4f60\u200b\u81ea\u5df1\u200b\u7684\u200b\u793e\u533a\u200b\u91cc\u200b\u5f00\u8f66\u200b\u3002</p> <p>\u200b\u4f60\u200b\u51e0\u4e4e\u200b\u4e0d\u200b\u9700\u8981\u200b\u601d\u8003\u200b\u8981\u200b\u5982\u4f55\u200b\u8f6c\u5f2f\u200b\u3002</p> <p>\u200b\u6709\u65f6\u5019\u200b\u4f60\u200b\u4e0b\u8f66\u200b\u540e\u200b\uff0c\u200b\u751a\u81f3\u200b\u4e0d\u200b\u8bb0\u5f97\u200b\u6700\u540e\u200b\u4e94\u5206\u949f\u200b\u7684\u200b\u9a7e\u9a76\u200b\u8fc7\u7a0b\u200b\u3002</p> <p>\u200b\u4f60\u200b\u7684\u200b\u5927\u8111\u200b\u5728\u200b\u81ea\u52a8\u200b\u9a7e\u9a76\u200b\u6a21\u5f0f\u200b\u4e0b\u200b\u8fd0\u884c\u200b\uff0c\u200b\u5f00\u9500\u200b\u6781\u5c0f\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u4f60\u200b\u5728\u200b\u4e00\u200b\u5f00\u59cb\u200b\u82b1\u200b\u4e86\u200b\u4e9b\u200b\u65f6\u95f4\u200b\u6765\u200b\u8bb0\u4f4f\u200b\u5982\u4f55\u200b\u5f00\u200b\u56de\u5bb6\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u56fe\u200b\u6355\u6349\u200b\u7684\u200b\u4e00\u4e2a\u200b\u7f3a\u70b9\u200b\uff0c\u200b\u5b83\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u65f6\u95f4\u200b\u6765\u200b\u9884\u5148\u200b\u8bb0\u4f4f\u200b\u9700\u8981\u200b\u53d1\u751f\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u4f46\u200b\u968f\u540e\u200b\u7684\u200b\u8ba1\u7b97\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u66f4\u200b\u5feb\u200b\u3002</p> <p>\u200b\u5f53\u7136\u200b\uff0c\u200b\u8fd9\u200b\u53ea\u662f\u200b\u5bf9\u200b<code>torch.compile()</code>\u200b\u80cc\u540e\u200b\u53d1\u751f\u200b\u4e8b\u60c5\u200b\u7684\u200b\u4e00\u4e2a\u200b\u5feb\u901f\u200b\u9ad8\u5c42\u200b\u6982\u8ff0\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u662f\u200b\u6211\u200b\u7406\u89e3\u200b\u7684\u200b\u65b9\u5f0f\u200b\u3002</p> <p>\u200b\u5173\u4e8e\u200b\u878d\u5408\u200b\u548c\u200b\u56fe\u200b\u8ffd\u8e2a\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u5185\u5bb9\u200b\uff0c\u200b\u6211\u200b\u63a8\u8350\u200bHorace He\u200b\u7684\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200bMaking Deep Learning Go Brrrr From First Principles\u3002</p>"},{"location":"pytorch_2_intro/","title":"\u6ce8\u610f\u4e8b\u9879\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b PyTorch 2.0 \u200b\u521a\u521a\u200b\u53d1\u5e03\u200b\uff0c\u200b\u67d0\u4e9b\u200b\u529f\u80fd\u200b\u5b58\u5728\u200b\u4e00\u4e9b\u200b\u9650\u5236\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u6700\u200b\u4e3b\u8981\u200b\u7684\u200b\u4e00\u4e2a\u200b\u662f\u200b\u6a21\u578b\u200b\u5bfc\u51fa\u200b\u7684\u200b\u9650\u5236\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b PyTorch 2.0 \u200b\u529f\u80fd\u200b\u65f6\u6709\u200b\u4e00\u4e9b\u200b\u6ce8\u610f\u4e8b\u9879\u200b\uff0c\u200b\u4f8b\u5982\u200b\u5728\u200b\u4f7f\u7528\u200b <code>torch.compile()</code> \u200b\u9ed8\u8ba4\u200b\u9009\u9879\u200b\u65f6\u200b\u65e0\u6cd5\u200b\u5bfc\u51fa\u200b\u5230\u200b\u79fb\u52a8\u200b\u8bbe\u5907\u200b\u3002\u200b\u4e0d\u8fc7\u200b\uff0c\u200b\u5bf9\u6b64\u200b\u6709\u200b\u4e00\u4e9b\u200b\u53d8\u901a\u65b9\u6cd5\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6539\u8fdb\u200b\u5bfc\u51fa\u200b\u529f\u80fd\u200b\u5df2\u200b\u5217\u5165\u200b PyTorch 2.x \u200b\u8def\u7ebf\u56fe\u200b\u3002\u200b\u6765\u6e90\u200b\uff1aPyTorch 2.0 \u200b\u53d1\u5e03\u516c\u544a\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u672a\u6765\u200b\u7684\u200b\u7248\u672c\u200b\u4e2d\u200b\u5f97\u5230\u200b\u89e3\u51b3\u200b\u3002</p> <p>\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u4e3b\u8981\u200b\u9650\u5236\u200b\u662f\u200b\uff0c\u200b\u7531\u4e8e\u200b PyTorch 2.0 \u200b\u7684\u200b\u529f\u80fd\u8bbe\u8ba1\u200b\u9488\u5bf9\u200b\u8f83\u200b\u65b0\u200b\u7684\u200b\u786c\u4ef6\u200b\uff0c\u200b\u65e7\u6b3e\u200b GPU \u200b\u548c\u200b\u53f0\u5f0f\u673a\u200b\u7ea7\u200b GPU\uff08\u200b\u4f8b\u5982\u200b NVIDIA RTX 30 \u200b\u7cfb\u5217\u200b\uff09\u200b\u53ef\u80fd\u200b\u4e0d\u4f1a\u200b\u50cf\u200b\u65b0\u200b\u786c\u4ef6\u200b\u90a3\u6837\u200b\u83b7\u5f97\u200b\u663e\u8457\u200b\u7684\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\u3002</p>"},{"location":"pytorch_2_intro/","title":"\u6211\u4eec\u200b\u5c06\u200b\u6db5\u76d6\u200b\u7684\u200b\u5185\u5bb9\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b PyTorch 2.0 \u200b\u4e2d\u200b\u7684\u200b\u8bb8\u591a\u200b\u5347\u7ea7\u200b\u90fd\u200b\u662f\u200b\u4ee5\u200b\u901f\u5ea6\u200b\u4e3a\u200b\u91cd\u70b9\u200b\uff0c\u200b\u5e76\u4e14\u200b\u5728\u200b\u5e55\u540e\u200b\u8fdb\u884c\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0cPyTorch \u200b\u4f1a\u200b\u81ea\u52a8\u200b\u5904\u7406\u200b\u5b83\u4eec\u200b\uff09\uff0c\u200b\u5728\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u4e00\u4e2a\u200b\u5bf9\u6bd4\u200b\u901f\u5ea6\u200b\u6d4b\u8bd5\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e24\u4e2a\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u200b\u7684\u200b PyTorch \u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u4f7f\u7528\u200b\u65b0\u200b\u7684\u200b <code>torch.compile()</code> \u200b\u8bbe\u7f6e\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8bad\u7ec3\u200b\u5b83\u4eec\u200b\u3002</p> <ol> <li>\u200b\u6a21\u578b\u200b1 - \u200b\u4e0d\u200b\u4f7f\u7528\u200b <code>torch.compile()</code>\u3002</li> <li>\u200b\u6a21\u578b\u200b2 - \u200b\u4f7f\u7528\u200b <code>torch.compile()</code>\u3002</li> </ol> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6bd4\u8f83\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u6a21\u578b\u200b\u5728\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u548c\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u65f6\u95f4\u200b\u3002</p> \u200b\u5b9e\u9a8c\u200b \u200b\u6a21\u578b\u200b \u200b\u6570\u636e\u200b \u200b\u5468\u671f\u200b \u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b \u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b <code>torch.compile()</code> 1 (\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b) ResNet50 CIFAR10 5 128 224 \u200b\u5426\u200b 2 (\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b) ResNet50 CIFAR10 5 128 224 \u200b\u662f\u200b 3 (\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b) ResNet50 CIFAR10 3x5 128 224 \u200b\u5426\u200b 4 (\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b) ResNet50 CIFAR10 3x5 128 224 \u200b\u662f\u200b <p>\u200b\u6211\u4eec\u200b\u5728\u200b\u8fd9\u91cc\u200b\u9009\u62e9\u200b ResNet50 \u200b\u548c\u200b CIFAR10 \u200b\u662f\u200b\u4e3a\u4e86\u200b\u4fbf\u4e8e\u200b\u8bbf\u95ee\u200b\u6216\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u66ff\u6362\u200b\u4e3a\u200b\u4f60\u200b\u559c\u6b22\u200b\u7684\u200b\u4efb\u4f55\u200b\u6a21\u578b\u200b/\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u6211\u200b\u6ce8\u610f\u200b\u5230\u200b PyTorch 2.0 \u200b\u6700\u5927\u200b\u7684\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\u662f\u200b\u5728\u200b GPU \u200b\u5c3d\u53ef\u80fd\u200b\u591a\u5730\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u65f6\u200b\uff08\u200b\u4f8b\u5982\u200b\uff0c\u200b\u66f4\u5927\u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b/\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b/\u200b\u6570\u636e\u200b\u5927\u5c0f\u200b/\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6839\u636e\u200b\u4f60\u200b\u7684\u200b GPU \u200b\u5927\u5c0f\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u964d\u4f4e\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\uff08\u200b\u6216\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\uff09\u200b\u4ee5\u200b\u9002\u5e94\u200b\u4f60\u200b\u7684\u200b GPU\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u5185\u5b58\u200b\u4e3a\u200b 8GB \u200b\u6216\u200b\u66f4\u200b\u5c11\u200b\u7684\u200b GPU\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u5c06\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u964d\u4f4e\u200b\u5230\u200b 64 \u200b\u6216\u200b 32\u3002</p>"},{"location":"pytorch_2_intro/#0","title":"0. \u200b\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u68c0\u67e5\u200b\u662f\u5426\u200b\u5df2\u200b\u5b89\u88c5\u200b PyTorch 2.x \u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\uff0c\u200b\u5982\u679c\u200b\u6ca1\u6709\u200b\u5b89\u88c5\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b PyTorch \u200b\u6587\u6863\u200b \u200b\u4e2d\u200b\u67e5\u770b\u200b\u5982\u4f55\u200b\u5728\u200b\u4f60\u200b\u7684\u200b\u7cfb\u7edf\u200b\u4e0a\u200b\u5b89\u88c5\u200b PyTorch 2.x\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u5728\u200b Google Colab \u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4f60\u200b\u9700\u8981\u200b\u8bbe\u7f6e\u200b GPU\uff1a\u200b\u8fd0\u884c\u200b\u65f6\u200b -&gt; \u200b\u66f4\u6539\u200b\u8fd0\u884c\u200b\u65f6\u200b\u7c7b\u578b\u200b -&gt; \u200b\u786c\u4ef6\u200b\u52a0\u901f\u5668\u200b\u3002\u200b\u6700\u4f73\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u5728\u200b\u65b0\u4e00\u4ee3\u200b NVIDIA/AMD GPU \u200b\u4e0a\u200b\uff08\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b PyTorch 2.0 \u200b\u5229\u7528\u200b\u4e86\u200b\u65b0\u4e00\u4ee3\u200b GPU \u200b\u786c\u4ef6\u200b\uff09\uff0c\u200b\u4f8b\u5982\u200b NVIDIA A100 \u200b\u53ca\u200b\u4ee5\u4e0a\u200b\u3002\u200b\u672c\u200b\u6559\u7a0b\u200b\u4e3b\u8981\u200b\u9488\u5bf9\u200b NVIDIA GPU\u3002</p>"},{"location":"pytorch_2_intro/#1-gpu","title":"1. \u200b\u83b7\u53d6\u200bGPU\u200b\u4fe1\u606f\u200b\u00b6","text":"<p>\u200b\u662f\u200b\u65f6\u5019\u200b\u83b7\u53d6\u200bGPU\u200b\u4fe1\u606f\u200b\u4e86\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u83b7\u53d6\u200b\uff1f</p> <p>PyTorch 2.0\u200b\u63d0\u4f9b\u200b\u7684\u200b\u8bb8\u591a\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u6700\u597d\u200b\u5728\u200b\u65b0\u4e00\u4ee3\u200bNVIDIA GPU\u200b\u4e0a\u200b\u4f53\u9a8c\u200b\uff08\u200b\u76ee\u524d\u200b\u6211\u4eec\u200b\u4e13\u6ce8\u200b\u4e8e\u200bNVIDIA GPU\uff09\u3002</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200bPyTorch 2.0\u200b\u5229\u7528\u200b\u4e86\u200b\u65b0\u4e00\u4ee3\u200bGPU\u200b\u4e0a\u200b\u7684\u200b\u65b0\u200b\u786c\u4ef6\u200b\u3002</p> <p>\u200b\u5982\u4f55\u200b\u5224\u65ad\u200bGPU\u200b\u662f\u5426\u200b\u8f83\u200b\u65b0\u200b\uff1f</p> <p>\u200b\u4e00\u822c\u6765\u8bf4\u200b\uff0c\u200b\u8f83\u200b\u65b0\u200b\u7684\u200bGPU\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b\u5c06\u200b\u8fbe\u5230\u200b8.0\u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200bNVIDIA\u200b\u5f00\u53d1\u8005\u200b\u9875\u9762\u200b\u4e0a\u200b\u67e5\u770b\u200bNVIDIA GPU\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b\u5217\u8868\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b2020\u200b\u5e74\u200b\u6216\u200b\u4e4b\u540e\u200b\u53d1\u5e03\u200b\u7684\u200b\u90e8\u5206\u200bNVIDIA GPU\u200b\u53ca\u5176\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b\uff1a</p> NVIDIA GPU \u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b GPU\u200b\u7c7b\u578b\u200b \u200b\u53d1\u5e03\u200b\u5e74\u4efd\u200b \u200b\u67b6\u6784\u200b RTX 4090 8.9 \u200b\u684c\u9762\u200b\u7ea7\u200b 2022 Ada Lovelace RTX 4080 8.9 \u200b\u684c\u9762\u200b\u7ea7\u200b 2022 Ada Lovelace RTX 4070 Ti 8.9 \u200b\u684c\u9762\u200b\u7ea7\u200b 2022 Ada Lovelace RTX 3090 8.6 \u200b\u684c\u9762\u200b\u7ea7\u200b 2020 Ampere) RTX 3080 8.6 \u200b\u684c\u9762\u200b\u7ea7\u200b 2020 Ampere RTX 3070 8.6 \u200b\u684c\u9762\u200b\u7ea7\u200b 2020 Ampere RTX 3060 Ti 8.6 \u200b\u684c\u9762\u200b\u7ea7\u200b 2020 Ampere H100 9.0 \u200b\u6570\u636e\u4e2d\u5fc3\u200b\u7ea7\u200b 2022 Hopper A100 8.0 \u200b\u6570\u636e\u4e2d\u5fc3\u200b\u7ea7\u200b 2020 Ampere A10 8.6 \u200b\u6570\u636e\u4e2d\u5fc3\u200b\u7ea7\u200b 2021 Ampere <p>\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b\u8fbe\u5230\u200b8.0\u200b\u6216\u200b\u4ee5\u4e0a\u200b\u7684\u200bGPU\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u83b7\u5f97\u200b\u6700\u5927\u200b\u7684\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u800c\u200b\u6570\u636e\u4e2d\u5fc3\u200b\u7ea7\u200bGPU\uff08\u200b\u5982\u200bA100\u3001A10\u3001H100\uff09\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6bd4\u200b\u684c\u9762\u200b\u7ea7\u200bGPU\uff08\u200b\u5982\u200bRTX 3090\u3001RTX 3080\u3001RTX 3070\u3001RTX 3060 Ti\uff09\u200b\u83b7\u5f97\u200b\u66f4\u200b\u663e\u8457\u200b\u7684\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>torch.cuda.get_device_capability()</code>\u200b\u6765\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200bGPU\u200b\u7684\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c06\u200b\u8f93\u51fa\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b<code>(\u200b\u4e3b\u8981\u200b, \u200b\u6b21\u8981\u200b)</code>\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5f97\u5206\u200b\u7684\u200b\u5143\u7ec4\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0cA100\u200b\u7684\u200b\u5f97\u5206\u200b\u662f\u200b<code>(8, 0)</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>nvidia-smi</code>\u200b\u83b7\u53d6\u200b\u5173\u4e8e\u200b\u6211\u4eec\u200bGPU\u200b\u7684\u200b\u5176\u4ed6\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\uff0c\u200b\u5982\u200b\u540d\u79f0\u200b\u548c\u200b\u5176\u4ed6\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u8d44\u6e90\u200b\uff1a \u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u6df1\u5165\u200b\u6bd4\u8f83\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b\u7684\u200bNVIDIA GPU\u200b\u53ca\u5176\u200b\u901f\u5ea6\u200b\u3001\u200b\u6210\u672c\u200b\u548c\u200b\u6743\u8861\u200b\uff0c\u200b\u6211\u200b\u63a8\u8350\u200b\u9605\u8bfb\u200bTim Dettmers\u200b\u7684\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200bWhich GPU for deep learning?\u3002</p>"},{"location":"pytorch_2_intro/#11","title":"1.1 \u200b\u5168\u5c40\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u00b6","text":"<p>PyTorch 2.x \u200b\u4e2d\u200b\u6211\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\u529f\u80fd\u200b\u4e4b\u4e00\u200b\u662f\u200b\u80fd\u591f\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u8bbe\u7f6e\u200b\u9ed8\u8ba4\u200b\u8bbe\u5907\u200b\u7c7b\u578b\u200b\uff1a</p> <ul> <li>\u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b</li> <li>\u200b\u5168\u5c40\u200b\u8bbe\u7f6e\u200b</li> </ul> <p>\u200b\u4ee5\u524d\u200b\uff0c\u200b\u4f60\u200b\u53ea\u80fd\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u8bbe\u7f6e\u200b\u9ed8\u8ba4\u200b\u8bbe\u5907\u200b\u7c7b\u578b\u200b\uff1a</p> <ul> <li><code>tensor.to(device)</code></li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u770b\u770b\u200b\u8fd9\u200b\u4e24\u79cd\u200b\u65b0\u200b\u7684\u200b\u8bbe\u5907\u200b\u8bbe\u7f6e\u200b\u65b9\u5f0f\u200b\u7684\u200b\u5b9e\u9645\u200b\u5e94\u7528\u200b\u3002</p>"},{"location":"pytorch_2_intro/#2","title":"2. \u200b\u8bbe\u7f6e\u200b\u5b9e\u9a8c\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u662f\u200b\u65f6\u5019\u200b\u6d4b\u91cf\u200b\u901f\u5ea6\u200b\u4e86\u200b\uff01</p> <p>\u200b\u4e3a\u4e86\u200b\u4fdd\u6301\u200b\u7b80\u5355\u200b\uff0c\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u8ba8\u8bba\u200b\u7684\u200b\u90a3\u6837\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u4e00\u7cfb\u5217\u200b\u56db\u4e2a\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u6240\u6709\u200b\u5b9e\u9a8c\u200b\u90fd\u200b\u4f7f\u7528\u200b\uff1a</p> <ul> <li>\u200b\u6a21\u578b\u200b\uff1a ResNet50\uff08\u200b\u6765\u81ea\u200b TorchVision\uff09</li> <li>\u200b\u6570\u636e\u200b\uff1a CIFAR10\uff08\u200b\u6765\u81ea\u200b TorchVision\uff09</li> <li>\u200b\u5468\u671f\u200b\uff1a 5\uff08\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\uff09\u200b\u548c\u200b 3x5\uff08\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\uff09</li> <li>\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\uff1a 128</li> <li>\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\uff1a 224</li> </ul> <p>\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u90fd\u200b\u5c06\u200b\u5206\u522b\u200b\u5728\u200b\u542f\u7528\u200b\u548c\u200b\u4e0d\u200b\u542f\u7528\u200b <code>torch.compile()</code> \u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u8fd0\u884c\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u8981\u200b\u8fdb\u884c\u200b\u5355\u6b21\u200b\u548c\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u901a\u8fc7\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u53ef\u4ee5\u200b\u6d4b\u91cf\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u4e5f\u200b\u9700\u8981\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u6d4b\u8bd5\u200b\u4ee5\u200b\u83b7\u5f97\u200b\u5e73\u5747\u503c\u200b\uff08\u200b\u53ea\u662f\u200b\u4e3a\u4e86\u200b\u786e\u4fdd\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0d\u662f\u200b\u5076\u7136\u200b\u7684\u200b\u6216\u200b\u51fa\u73b0\u200b\u4e86\u200b\u67d0\u4e9b\u200b\u95ee\u9898\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6839\u636e\u200b\u4f60\u200b\u7684\u200b GPU \u200b\u5185\u5b58\u5927\u5c0f\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u964d\u4f4e\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u6216\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\u3002\u200b\u672c\u200b\u6559\u7a0b\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u4f7f\u7528\u200b\u5177\u6709\u200b 40GB \u200b\u5185\u5b58\u200b\u7684\u200b NVIDIA A100 GPU\uff0c\u200b\u8fd9\u79cd\u200b GPU \u200b\u7684\u200b\u5185\u5b58\u200b\u610f\u5473\u7740\u200b\u5b83\u200b\u53ef\u4ee5\u200b\u5904\u7406\u200b\u66f4\u5927\u200b\u7684\u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b\u3002\u200b\u622a\u81f3\u200b 2023 \u200b\u5e74\u200b 4 \u200b\u6708\u200b\uff0cNVIDIA A100 GPU \u200b\u53ef\u200b\u901a\u8fc7\u200b Google Colab Pro \u200b\u83b7\u5f97\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u5bfc\u5165\u200b <code>torch</code> \u200b\u548c\u200b <code>torchvision</code> \u200b\u5e76\u200b\u8bbe\u7f6e\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u3002</p>"},{"location":"pytorch_2_intro/#21","title":"2.1 \u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u76f8\u540c\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u6765\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\uff0c\u200b\u8fd9\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b06. PyTorch \u200b\u8fc1\u79fb\u200b\u5b66\u4e60\u200b\u7b2c\u200b2.2\u200b\u8282\u4e2d\u200b\u4ecb\u7ecd\u200b\u7684\u200b\u5185\u5bb9\u200b\u76f8\u540c\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b<code>torchvision.models</code> API\u200b\u6765\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u548c\u200b\u8f6c\u6362\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u83b7\u53d6\u200bResNet50\u200b\u7684\u200b\u6743\u91cd\u200b\u548c\u200b\u8f6c\u6362\u200b\uff1a</p> <ul> <li><code>model_weights = torchvision.models.ResNet50_Weights.IMAGENET1K_V2</code>\uff08\u200b\u8fd9\u200b\u9700\u8981\u200b<code>torchvision</code> 0.14\u200b\u6216\u200b\u66f4\u200b\u9ad8\u200b\u7248\u672c\u200b\uff09\u3002</li> <li><code>transforms = model_weights.transforms()</code>\uff08\u200b\u4e00\u65e6\u200b\u6211\u4eec\u200b\u6709\u200b\u4e86\u200b\u6743\u91cd\u200b\uff0c\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u83b7\u53d6\u200b\u9002\u7528\u200b\u4e8e\u200b\u8be5\u200b\u6a21\u578b\u200b\u7684\u200b\u9002\u5f53\u200b\u8f6c\u6362\u200b\uff09\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u4eec\u200b\u5c06\u200b\u8ba1\u7b97\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u6570\u91cf\u200b\uff0c\u200b\u4ee5\u200b\u4e86\u89e3\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u662f\u200b\u591a\u200b\u5927\u89c4\u6a21\u200b\u7684\u200b\u6a21\u578b\u200b\u3002\u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u53c2\u6570\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u8bad\u7ec3\u200b\u5b83\u200b\u6240\u200b\u9700\u200b\u7684\u200bGPU\u200b\u5185\u5b58\u200b\u5c31\u200b\u8d8a\u200b\u5927\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u6a21\u578b\u200b\u7684\u200b\u53c2\u6570\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u5b83\u200b\u4f7f\u7528\u200b\u7684\u200bGPU\u200b\u5185\u5b58\u200b\u5c31\u200b\u8d8a\u200b\u591a\u200b\uff0c\u200b\u901a\u5e38\u200b\u4f1a\u200b\u83b7\u5f97\u200b\u66f4\u5927\u200b\u7684\u200b\u76f8\u5bf9\u200b\u52a0\u901f\u200b\u3002\u200b\u4e5f\u5c31\u662f\u8bf4\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u8f83\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u66f4\u957f\u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u5b83\u200b\u4f7f\u7528\u200b\u4e86\u200b\u66f4\u200b\u591a\u200b\u7684\u200bGPU\u200b\u8d44\u6e90\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u4e8e\u200b\u8f83\u200b\u5c0f\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5b83\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u66f4\u200b\u5feb\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4e00\u4e2a\u200b\u62e5\u6709\u200b1000\u200b\u4e07\u4e2a\u200b\u53c2\u6570\u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u53ea\u200b\u6bd4\u200b\u4e00\u4e2a\u200b\u62e5\u6709\u200b100\u200b\u4e07\u4e2a\u200b\u53c2\u6570\u200b\u7684\u200b\u6a21\u578b\u200b\uff0810\u200b\u500d\u200b\u5927\u5c0f\u200b\u4f46\u200b\u53ea\u200b\u589e\u52a0\u200b5\u200b\u500d\u200b\u8bad\u7ec3\u200b\u65f6\u95f4\u200b\uff09\u200b\u591a\u82b1\u200b5\u200b\u500d\u200b\u7684\u200b\u65f6\u95f4\u200b\u6765\u200b\u8bad\u7ec3\u200b\u3002</p>"},{"location":"pytorch_2_intro/#22-gpu","title":"2.2 \u200b\u5f53\u200b\u5927\u91cf\u200b\u4f7f\u7528\u200bGPU\u200b\u65f6\u200b\uff0c\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u6700\u4e3a\u200b\u663e\u8457\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u73b0\u4ee3\u200bGPU\u200b\u5728\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\u65f6\u200b\u901f\u5ea6\u200b\u6781\u5feb\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u4f1a\u200b\u53d1\u73b0\u200b\uff0c\u200b\u5f53\u200b\u5c3d\u53ef\u80fd\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u653e\u5728\u200bGPU\u200b\u4e0a\u65f6\u200b\uff0c\u200b\u76f8\u5bf9\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u6700\u4e3a\u200b\u660e\u663e\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4ee5\u4e0b\u200b\u65b9\u5f0f\u200b\u5b9e\u73b0\u200b\uff1a</p> <ul> <li>\u200b\u589e\u52a0\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b - \u200b\u6bcf\u4e2a\u200b\u6279\u6b21\u200b\u4e2d\u200b\u5305\u542b\u200b\u66f4\u200b\u591a\u6837\u200b\u672c\u200b\u610f\u5473\u7740\u200bGPU\u200b\u4e0a\u200b\u6709\u200b\u66f4\u200b\u591a\u6837\u200b\u672c\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f7f\u7528\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u4e3a\u200b256\u200b\u800c\u200b\u4e0d\u662f\u200b32\u3002</li> <li>\u200b\u589e\u52a0\u200b\u6570\u636e\u200b\u5927\u5c0f\u200b - \u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f7f\u7528\u200b\u66f4\u5927\u200b\u7684\u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b\uff0c224x224\u200b\u800c\u200b\u4e0d\u662f\u200b32x32\u3002\u200b\u66f4\u5927\u200b\u7684\u200b\u6570\u636e\u200b\u5927\u5c0f\u200b\u610f\u5473\u7740\u200b\u5728\u200bGPU\u200b\u4e0a\u4f1a\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5f20\u91cf\u200b\u64cd\u4f5c\u200b\u3002</li> <li>\u200b\u589e\u52a0\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b - \u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f7f\u7528\u200b\u66f4\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200bResNet101\u200b\u800c\u200b\u4e0d\u662f\u200bResNet50\u3002\u200b\u66f4\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u610f\u5473\u7740\u200b\u5728\u200bGPU\u200b\u4e0a\u4f1a\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u5f20\u91cf\u200b\u64cd\u4f5c\u200b\u3002</li> <li>\u200b\u51cf\u5c11\u200b\u6570\u636e\u4f20\u8f93\u200b - \u200b\u4f8b\u5982\u200b\uff0c\u200b\u5c06\u200b\u6240\u6709\u200b\u5f20\u91cf\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u4f4d\u4e8e\u200bGPU\u200b\u5185\u5b58\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u6837\u200b\u53ef\u4ee5\u200b\u6700\u5c0f\u5316\u200bCPU\u200b\u548c\u200bGPU\u200b\u4e4b\u95f4\u200b\u7684\u200b\u6570\u636e\u4f20\u8f93\u200b\u91cf\u200b\u3002</li> </ul> <p>\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u65b9\u6cd5\u200b\u90fd\u200b\u4f1a\u200b\u5bfc\u81f4\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200bGPU\u200b\u4e0a\u200b\u3002</p> <p></p> <p>\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u60f3\u200b\uff0c\u201c\u200b\u8fd9\u200b\u4e0d\u200b\u610f\u5473\u7740\u200bGPU\u200b\u4f1a\u200b\u56e0\u4e3a\u200b\u8981\u200b\u505a\u200b\u66f4\u200b\u591a\u200b\u5de5\u4f5c\u200b\u800c\u200b\u53d8\u6162\u200b\u5417\u200b\uff1f\u201d</p> <p>\u200b\u8fd9\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\uff0c\u200b\u5f53\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b\u6570\u636e\u200b\u65f6\u200b\uff0c\u200b\u64cd\u4f5c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u82b1\u8d39\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u4f1a\u200b\u4ece\u200b\u5e76\u884c\u8ba1\u7b97\u200b\u4e2d\u200b\u53d7\u76ca\u200b\uff08\u200b\u8bb8\u591a\u200b\u64cd\u4f5c\u200b\u540c\u65f6\u200b\u8fdb\u884c\u200b\uff09\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u5c3d\u7ba1\u200b\u6709\u200b\u66f4\u200b\u591a\u200b\u7684\u200b\u64cd\u4f5c\u200b\u5728\u200b\u8fdb\u884c\u200b\uff0c\u200b\u4f46\u200bGPU\u200b\u80fd\u591f\u200b\u5c3d\u53ef\u80fd\u200b\u591a\u5730\u200b\u540c\u65f6\u200b\u6267\u884c\u200b\u8fd9\u4e9b\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u867d\u7136\u200b\u4f60\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5728\u200b\u5c0f\u200b\u6570\u636e\u200b\u96c6\u200b\u3001\u200b\u6a21\u578b\u200b\u3001\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u548c\u200b\u6570\u636e\u200b\u5927\u5c0f\u200b\u4e0a\u200b\u770b\u5230\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\uff0c\u200b\u4f46\u200b\u968f\u7740\u200b\u89c4\u6a21\u200b\u7684\u200b\u589e\u52a0\u200b\uff0c\u200b\u4f60\u200b\u5f80\u5f80\u200b\u4f1a\u200b\u770b\u5230\u200b\u6700\u5927\u200b\u7684\u200b\u76f8\u5bf9\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002</p>"},{"location":"pytorch_2_intro/#23-gpu","title":"2.3 \u200b\u68c0\u67e5\u200bGPU\u200b\u7684\u200b\u5185\u5b58\u200b\u9650\u5236\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u5229\u7528\u200b\u89c4\u6a21\u5316\u200b\u5e26\u6765\u200b\u7684\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u6211\u4eec\u200b\u7684\u200bGPU\u200b\u6709\u200b\u591a\u5c11\u200b\u5185\u5b58\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200bGPU\u200b\u5185\u5b58\u200b\u8f83\u200b\u5c11\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u51cf\u5c0f\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u6216\u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b\uff08\u200b\u51cf\u5c11\u200b\u6f5c\u5728\u200b\u7684\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b<code>torch.cuda.mem_get_info()</code>\u200b\u6765\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200bGPU\u200b\u4e0a\u200b\u7684\u200b\u53ef\u7528\u5185\u5b58\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u5c06\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b<code>(total_free_gpu_memory, total_gpu_memory)</code>\u200b\u7684\u200b\u5143\u7ec4\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\uff1a</p> <ul> <li><code>total_free_gpu_memory</code> \u200b\u662f\u200bGPU\u200b\u4e0a\u200b\u5f53\u524d\u200b\u672a\u200b\u88ab\u200b\u4f7f\u7528\u200b\u7684\u200b\u5185\u5b58\u200b\u91cf\u200b\uff0c\u200b\u5355\u4f4d\u200b\u4e3a\u200b\u5b57\u8282\u200b\u3002</li> <li><code>total_gpu_memory</code> \u200b\u662f\u200bGPU\u200b\u4e0a\u200b\u53ef\u7528\u200b\u7684\u200b\u603b\u200b\u5185\u5b58\u200b\u91cf\u200b\uff0c\u200b\u5355\u4f4d\u200b\u4e3a\u200b\u5b57\u8282\u200b\u3002</li> </ul>"},{"location":"pytorch_2_intro/#24-tf32","title":"2.4 \u200b\u4f7f\u7528\u200b TF32 \u200b\u5b9e\u73b0\u200b\u66f4\u200b\u591a\u200b\u6f5c\u5728\u200b\u52a0\u901f\u200b\u00b6","text":"<p>TF32 \u200b\u4ee3\u8868\u200b TensorFloat-32\uff0c\u200b\u662f\u200b\u4e00\u79cd\u200b\u7ed3\u5408\u200b\u4e86\u200b 16 \u200b\u4f4d\u200b\u548c\u200b 32 \u200b\u4f4d\u200b\u6d6e\u70b9\u6570\u200b\u7684\u200b\u6570\u636e\u683c\u5f0f\u200b\u3002</p> <p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b NVIDIA \u200b\u7684\u200b\u535a\u5ba2\u200b\u4e0a\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u5176\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\u7684\u200b\u4fe1\u606f\u200b\u3002</p> <p>\u200b\u4f60\u200b\u5e94\u8be5\u200b\u77e5\u9053\u200b\u7684\u200b\u4e3b\u8981\u200b\u4e8b\u60c5\u200b\u662f\u200b\uff0c\u200b\u5b83\u200b\u5141\u8bb8\u200b\u4f60\u200b\u5728\u200b\u5177\u6709\u200b Ampere \u200b\u67b6\u6784\u200b\u53ca\u200b\u4ee5\u4e0a\u200b\u7684\u200b GPU\uff08\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5206\u6570\u200b\u4e3a\u200b 8.0+\uff09\u200b\u4e0a\u200b\u6267\u884c\u200b\u66f4\u5feb\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u8fd9\u200b\u5e76\u200b\u4e0d\u662f\u200b\u7279\u5b9a\u200b\u4e8e\u200b PyTorch 2.0 \u200b\u7684\u200b\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u8ba8\u8bba\u200b\u8f83\u200b\u65b0\u200b\u7684\u200b GPU\uff0c\u200b\u56e0\u6b64\u200b\u503c\u5f97\u4e00\u63d0\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5206\u6570\u200b\u4e3a\u200b 8.0 \u200b\u6216\u200b\u4ee5\u4e0a\u200b\u7684\u200b GPU\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u8bbe\u7f6e\u200b <code>torch.backends.cuda.matmul.allow_tf32 = True</code>\uff08\u200b\u9ed8\u8ba4\u200b\u4e3a\u200b <code>False</code>\uff09\u200b\u6765\u200b\u542f\u7528\u200b TF32\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7f16\u5199\u200b\u4e00\u4e2a\u200b\u68c0\u67e5\u7a0b\u5e8f\u200b\uff0c\u200b\u6839\u636e\u200b\u6211\u4eec\u200b\u7684\u200b GPU \u200b\u8ba1\u7b97\u80fd\u529b\u200b\u5206\u6570\u200b\u81ea\u52a8\u200b\u4e3a\u200b\u6211\u4eec\u200b\u8bbe\u7f6e\u200b\u5b83\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ece\u200b PyTorch 1.12 \u200b\u7248\u672c\u200b\u5f00\u59cb\u200b\uff0cTensorFloat32 \u200b\u9ed8\u8ba4\u200b\u662f\u200b\u7981\u7528\u200b\u7684\u200b\uff08\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>False</code>\uff09\u3002\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b \u200b\u5b83\u200b\u53ef\u80fd\u200b\u5bfc\u81f4\u200b\u4e0d\u540c\u200b\u8bbe\u5907\u200b\u4e4b\u95f4\u200b\u7684\u200b\u7ed3\u679c\u200b\u4e0d\u200b\u4e00\u81f4\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u5e76\u975e\u200b\u6240\u6709\u200b\u7528\u4f8b\u200b\u90fd\u200b\u4f1a\u200b\u6ce8\u610f\u200b\u5230\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f46\u200b\u4e86\u89e3\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u662f\u200b\u503c\u5f97\u200b\u7684\u200b\u3002</p>"},{"location":"pytorch_2_intro/#25","title":"2.5 \u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u8ba1\u7b97\u73af\u5883\u200b\u8bbe\u7f6e\u200b\u5b8c\u6210\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u7b80\u5316\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b CIFAR10\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5728\u200b <code>torchvision</code> \u200b\u4e2d\u200b\u53ef\u4ee5\u200b\u76f4\u63a5\u200b\u83b7\u53d6\u200b\u3002</p> <p>\u200b\u5173\u4e8e\u200b CIFAR10 \u200b\u7684\u200b\u4e00\u4e9b\u200b\u4fe1\u606f\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u53c2\u8003\u200b CIFAR10 \u200b\u5b98\u7f51\u200b\uff1a</p> <ul> <li>CIFAR10 \u200b\u662f\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b 60,000 \u200b\u5f20\u200b 32x32 \u200b\u5f69\u8272\u56fe\u50cf\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5206\u4e3a\u200b 10 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7c7b\u522b\u200b\u6709\u200b 6,000 \u200b\u5f20\u200b\u56fe\u50cf\u200b\u3002</li> <li>\u200b\u5176\u4e2d\u200b\u6709\u200b 50,000 \u200b\u5f20\u200b\u8bad\u7ec3\u200b\u56fe\u50cf\u200b\u548c\u200b 10,000 \u200b\u5f20\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u3002</li> <li>\u200b\u6570\u636e\u200b\u96c6\u200b\u5305\u542b\u200b 10 \u200b\u4e2a\u200b\u7c7b\u522b\u200b\uff1a\u200b\u98de\u673a\u200b\u3001\u200b\u6c7d\u8f66\u200b\u3001\u200b\u9e1f\u200b\u3001\u200b\u732b\u200b\u3001\u200b\u9e7f\u200b\u3001\u200b\u72d7\u200b\u3001\u200b\u9752\u86d9\u200b\u3001\u200b\u9a6c\u200b\u3001\u200b\u8239\u200b\u3001\u200b\u5361\u8f66\u200b\u3002</li> </ul> <p>\u200b\u5c3d\u7ba1\u200b\u539f\u59cb\u6570\u636e\u200b\u96c6\u7531\u200b 32x32 \u200b\u7684\u200b\u56fe\u50cf\u200b\u7ec4\u6210\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b <code>transforms</code> \u200b\u5c06\u200b\u5b83\u4eec\u200b\u8c03\u6574\u200b\u4e3a\u200b 224x224\uff08\u200b\u66f4\u5927\u200b\u7684\u200b\u56fe\u50cf\u200b\u63d0\u4f9b\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u4f1a\u200b\u5360\u7528\u200b\u66f4\u200b\u591a\u200b GPU \u200b\u5185\u5b58\u200b\uff09\u3002</p>"},{"location":"pytorch_2_intro/#26-dataloaders","title":"2.6 \u200b\u521b\u5efa\u200b DataLoaders\u00b6","text":"<p>\u200b\u901a\u5e38\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0cGPU \u200b\u5e76\u200b\u4e0d\u662f\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4ee3\u7801\u200b\u7684\u200b\u74f6\u9888\u200b\u3002</p> <p>\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u624d\u200b\u662f\u200b\u4e3b\u8981\u200b\u7684\u200b\u74f6\u9888\u200b\u3002</p> <p>\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u662f\u4ece\u200b CPU \u200b\u4f20\u8f93\u6570\u636e\u200b\u5230\u200b GPU \u200b\u7684\u200b\u901f\u5ea6\u200b\u3002</p> <p>\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u8ba8\u8bba\u200b\u7684\u200b\uff0c\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5c3d\u53ef\u80fd\u200b\u5feb\u5730\u200b\u5c06\u200b\u6570\u636e\u4f20\u8f93\u200b\u5230\u200b GPU\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torch.utils.data.DataLoader</code> \u200b\u6765\u200b\u521b\u5efa\u200b\u6211\u4eec\u200b\u7684\u200b <code>DataLoaders</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5b83\u4eec\u200b\u7684\u200b <code>batch_size</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b <code>BATCH_SIZE</code>\u3002</p> <p>\u200b\u5e76\u200b\u5c06\u200b <code>num_workers</code> \u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b\u6211\u4eec\u200b\u53ef\u7528\u200b CPU \u200b\u6838\u5fc3\u200b\u7684\u200b\u6570\u91cf\u200b\uff0c\u200b\u901a\u8fc7\u200b <code>os.cpu_count()</code> \u200b\u83b7\u53d6\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u80fd\u200b\u5e0c\u671b\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b <code>num_workers</code> \u200b\u503c\u200b\uff0c\u200b\u4ee5\u200b\u627e\u5230\u200b\u6700\u200b\u9002\u5408\u200b\u4f60\u200b\u7684\u200b\u7279\u5b9a\u200b GPU \u200b\u548c\u200b CPU \u200b\u914d\u7f6e\u200b\u7684\u200b\u8bbe\u7f6e\u200b\u3002\u200b\u6839\u636e\u200b\u6211\u200b\u7684\u200b\u7ecf\u9a8c\u200b\uff0c\u200b\u8d8a\u591a\u8d8a\u597d\u200b\uff0c\u200b\u4f46\u200b\u6709\u4e9b\u200b\u4eba\u200b\u53d1\u73b0\u200b\u8fd9\u200b\u901a\u5e38\u200b\u4f1a\u200b\u8fbe\u5230\u200b\u4e00\u4e2a\u200b\u4e0a\u9650\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c<code>num_workers = 4 * number_of_gpus_you_have</code>\uff0c\u200b\u5bf9\u4e8e\u200b 1 \u200b\u4e2a\u200b GPU \u200b\u6765\u8bf4\u200b\uff0c<code>num_workers = 4 * 1</code>\u3002</p>"},{"location":"pytorch_2_intro/#27","title":"2.7 \u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u51c6\u5907\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u5c06\u200b\u4e0e\u200b\u6211\u4eec\u200b\u5728\u200b 05. PyTorch Going Modular \u200b\u4e2d\u200b\u521b\u5efa\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u57fa\u672c\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u6709\u200b\u4e00\u4e9b\u200b\u7ec6\u5fae\u200b\u7684\u200b\u4fee\u6539\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u6d4b\u91cf\u200b\u901f\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u6bcf\u4e2a\u200b\u5faa\u73af\u200b\u4e2d\u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u8ba1\u65f6\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u4ee5\u200b\u6d4b\u91cf\u200b\u6bcf\u4e2a\u200b\u5faa\u73af\u200b\u5b8c\u6210\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b Python \u200b\u7684\u200b <code>time.time()</code> \u200b\u6d4b\u91cf\u200b\u6bcf\u4e2a\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5468\u671f\u200b\u7684\u200b\u5f00\u59cb\u200b\u548c\u200b\u7ed3\u675f\u200b\u65f6\u95f4\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u4e00\u4e2a\u200b\u5b57\u5178\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u8bb0\u5f55\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6211\u200b\u5728\u200b\u5b9e\u9a8c\u200b PyTorch 2.0 \u200b\u65f6\u200b\u53d1\u73b0\u200b\uff0c<code>torch.inference_mode()</code> \u200b\u5728\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u4e2d\u200b\u4ea7\u751f\u200b\u4e86\u200b\u9519\u8bef\u200b\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u200b\u5c06\u200b\u5176\u200b\u6539\u4e3a\u200b <code>torch.no_grad()</code>\uff0c\u200b\u5b83\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u529f\u80fd\u200b\uff0c\u200b\u4f46\u200b\u6bd4\u200b <code>torch.inference_mode()</code> \u200b\u66f4\u65e7\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002\u200b\u5982\u679c\u200b\u4f60\u200b\u53d1\u73b0\u200b <code>torch.inference_mode()</code> \u200b\u5bf9\u200b\u4f60\u200b\u6709\u6548\u200b\uff0c\u200b\u8bf7\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u544a\u8bc9\u200b\u6211\u200b\uff0c\u200b\u6211\u4f1a\u200b\u66f4\u65b0\u200b\u8fd9\u4e2a\u200b\u7b14\u8bb0\u672c\u200b\u3002</p>"},{"location":"pytorch_2_intro/#3","title":"3. \u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u65f6\u95f4\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u529f\u80fd\u200b\u5df2\u200b\u5c31\u7eea\u200b\uff01</p> <p>\u200b\u662f\u200b\u65f6\u5019\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b/\u200b\u8bc4\u4f30\u200b\u5e76\u200b\u8ba1\u65f6\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4e86\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u7b2c\u4e00\u4e2a\u200b\u5b9e\u9a8c\u200b\u5f00\u59cb\u200b\u3002</p>"},{"location":"pytorch_2_intro/#31-1-","title":"3.1 \u200b\u5b9e\u9a8c\u200b1 - \u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4e0d\u200b\u7f16\u8bd1\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u5b9e\u9a8c\u200b1\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u4ee5\u4e0b\u200b\u53c2\u6570\u200b\uff1a</p> \u200b\u5b9e\u9a8c\u200b \u200b\u6a21\u578b\u200b \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u5468\u671f\u200b\u6570\u200b \u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b \u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b <code>torch.compile()</code> 1\uff08\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\uff09 ResNet50 CIFAR10 5 128 224 \u200b\u5426\u200b <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u5468\u671f\u200b\u6570\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>5</code>\uff0c\u200b\u5e76\u200b\u5168\u7a0b\u200b\u4f7f\u7528\u200b <code>0.003</code> \u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u200b\uff08\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u4e0d\u540c\u200b\u7684\u200b\u5b66\u4e60\u200b\u7387\u4ee5\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u76ee\u524d\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u901f\u5ea6\u200b\uff09\u3002</p>"},{"location":"pytorch_2_intro/#32-2-","title":"3.2 \u200b\u5b9e\u9a8c\u200b 2 - \u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4f7f\u7528\u200b\u7f16\u8bd1\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5c06\u200b\u8fdb\u884c\u200b\u76f8\u540c\u200b\u7684\u200b\u5b9e\u9a8c\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.compile()</code>\u3002</p> \u200b\u5b9e\u9a8c\u200b \u200b\u6a21\u578b\u200b \u200b\u6570\u636e\u200b Epochs Batch size \u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b <code>torch.compile()</code> 2 (\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b) ResNet50 CIFAR10 5 128 224 \u200b\u662f\u200b <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6839\u636e\u200b\u60a8\u200b\u7684\u200b GPU \u200b\u901f\u5ea6\u200b\uff0c\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u65f6\u95f4\u200b\u6765\u200b\u8fd0\u884c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5728\u200b\u6211\u200b\u7684\u200b\u672c\u5730\u200b NVIDIA TITAN RTX \u200b\u4e0a\u200b\u5927\u7ea6\u200b\u9700\u8981\u200b 16 \u200b\u5206\u949f\u200b\uff0c\u200b\u800c\u200b\u5728\u200b Google Colab Pro \u200b\u4e0a\u200b\u7684\u200b NVIDIA A100 GPU \u200b\u4e0a\u200b\u5927\u7ea6\u200b\u9700\u8981\u200b 7 \u200b\u5206\u949f\u200b\u3002</p>"},{"location":"pytorch_2_intro/#33-12","title":"3.3 \u200b\u6bd4\u8f83\u200b\u5b9e\u9a8c\u200b1\u200b\u548c\u200b\u5b9e\u9a8c\u200b2\u200b\u7684\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u592a\u68d2\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5f97\u5230\u200b\u4e86\u200b\u4e24\u4e2a\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff1a</p> <ol> <li>\u200b\u4e00\u4e2a\u200b\u6ca1\u6709\u200b\u4f7f\u7528\u200b <code>torch.compile()</code> \u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u4e00\u4e2a\u200b\u4f7f\u7528\u200b\u4e86\u200b <code>torch.compile()</code> \u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> </ol> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u6bd4\u8f83\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u9996\u5148\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7ed3\u679c\u200b\u521b\u5efa\u200b\u6210\u200b\u6570\u636e\u200b\u6846\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u6bcf\u4e2a\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7ed3\u679c\u200b\u7ed8\u5236\u200b\u5728\u200b\u6761\u5f62\u56fe\u200b\u4e0a\u200b\u3002</p>"},{"location":"pytorch_2_intro/#34-gpu","title":"3.4 \u200b\u5c06\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u7ed3\u679c\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u5e76\u200b\u5305\u542b\u200bGPU\u200b\u8be6\u60c5\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u7ed3\u679c\u200b\u7684\u200b\u539f\u59cb\u6570\u636e\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u65b9\u6cd5\u200b\u662f\u200b\u5c06\u200b\u6570\u636e\u200b\u6846\u200b\u5bfc\u51fa\u200b\u4e3a\u200bCSV\u200b\u6587\u4ef6\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b\u7ed3\u679c\u200b\u7684\u200b\u76ee\u5f55\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u5728\u200b\u5bfc\u51fa\u200b\u6bcf\u4e2a\u200b\u76ee\u6807\u200b\u6570\u636e\u200b\u6846\u200b\u4e4b\u524d\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u6587\u4ef6\u200b\u8def\u5f84\u200b\u4ee5\u200b\u4fdd\u5b58\u200b\u5b83\u4eec\u200b\u3002</p>"},{"location":"pytorch_2_intro/#4","title":"4. \u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u65f6\u95f4\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u6d4b\u8bd5\u200b\u4e86\u200b\u5728\u200b\u542f\u7528\u200b\u548c\u200b\u7981\u7528\u200b <code>torch.compile()</code> \u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u8fdb\u884c\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5bf9\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u8fdb\u884c\u200b\u540c\u6837\u200b\u7684\u200b\u6d4b\u8bd5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u9996\u5148\u200b\u4e3a\u200b\u5b9e\u9a8c\u200b3\u200b\u548c\u200b\u5b9e\u9a8c\u200b4\u200b\u521b\u5efa\u200b\u4e09\u4e2a\u200b\u51fd\u6570\u200b\u3002</p> <ol> <li>\u200b\u5b9e\u9a8c\u200b3\uff1a <code>create_and_train_non_compiled_model()</code> - \u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u5c06\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u6211\u4eec\u200b\u7528\u4e8e\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u3002\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b\u6a21\u578b\u200b\u521b\u5efa\u200b\uff08\u200b\u901a\u8fc7\u200b <code>create_model()</code>\uff09\u200b\u548c\u200b\u8bad\u7ec3\u200b\u653e\u5728\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u591a\u6b21\u200b\u8c03\u7528\u200b\u5b83\u200b\uff08\u200b\u8fdb\u884c\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\uff09\u200b\u5e76\u200b\u6d4b\u91cf\u200b\u6bcf\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u65f6\u95f4\u200b\u3002</li> <li>\u200b\u5b9e\u9a8c\u200b4\uff1a <code>create_compiled_model()</code> - \u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u5c06\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\u4e0a\u9762\u200b\u7684\u200b <code>create_model()</code> \u200b\u51fd\u6570\u200b\uff0c\u200b\u4f46\u662f\u200b\uff0c\u200b\u5b83\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u666e\u901a\u200b\u7684\u200b PyTorch \u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u5728\u200b\u5176\u200b\u4e0a\u200b\u8c03\u7528\u200b <code>torch.compile()</code> \u200b\u5e76\u200b\u8fd4\u56de\u200b\u5b83\u200b\u3002</li> <li>\u200b\u5b9e\u9a8c\u200b4\uff1a <code>train_compiled_model()</code> - \u200b\u8fd9\u4e2a\u200b\u51fd\u6570\u200b\u5c06\u200b\u63a5\u53d7\u200b\u4e00\u4e2a\u200b\u5df2\u200b\u7f16\u8bd1\u200b\u7684\u200b\u6a21\u578b\u200b\u5e76\u200b\u4ee5\u200b\u6211\u4eec\u200b\u4e3a\u200b\u5355\u6b21\u200b\u8fd0\u884c\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u76f8\u540c\u200b\u65b9\u5f0f\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u3002</li> </ol> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u4e3a\u200b\u5b9e\u9a8c\u200b4\u200b\u5206\u79bb\u200b\u51fd\u6570\u200b2\u200b\u548c\u200b3\uff08<code>create_compiled_model()</code> \u200b\u548c\u200b <code>train_compiled_model()</code>\uff09\uff1f</p> <p>\u200b\u56e0\u4e3a\u200b\u5728\u200b\u6a21\u578b\u200b\u4e0a\u200b\u8c03\u7528\u200b <code>torch.compile()</code> \u200b\u610f\u5473\u7740\u200b\u5728\u200b\u524d\u51e0\u6b21\u200b\u8fd0\u884c\u200b\u4e2d\u200b\uff0c\u200b\u6a21\u578b\u200b\u5c06\u200b\u5904\u4e8e\u200b\u201c\u200b\u9884\u70ed\u200b\u201d\u200b\u72b6\u6001\u200b\uff0c\u200b\u56e0\u4e3a\u200b PyTorch \u200b\u5728\u200b\u540e\u53f0\u200b\u8ba1\u7b97\u200b\u4e00\u7cfb\u5217\u200b\u4f18\u5316\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\u5728\u5b9e\u8df5\u4e2d\u200b\uff0c\u200b\u4f60\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5e0c\u671b\u200b\u9884\u5148\u200b\u7f16\u8bd1\u200b\u4e00\u6b21\u200b\uff0c\u200b\u7136\u540e\u200b\u4f7f\u7528\u200b\u5df2\u7ecf\u200b\u7f16\u8bd1\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u6216\u200b\u63a8\u7406\u200b\u3002</p>"},{"location":"pytorch_2_intro/#41-3-","title":"4.1 \u200b\u5b9e\u9a8c\u200b3 - \u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\uff0c\u200b\u4e0d\u200b\u7f16\u8bd1\u200b\u00b6","text":"<p>\u200b\u51fd\u6570\u200b\u5df2\u200b\u51c6\u5907\u200b\u597d\u200b\u8fdb\u884c\u200b\u5b9e\u9a8c\u200b3\u200b\u548c\u200b4\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u5b9e\u9a8c\u200b3\u200b\u5f00\u59cb\u200b\u3002</p> \u200b\u5b9e\u9a8c\u200b \u200b\u6a21\u578b\u200b \u200b\u6570\u636e\u200b \u200b\u5468\u671f\u200b\u6570\u200b \u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b \u200b\u56fe\u50cf\u200b\u5927\u5c0f\u200b <code>torch.compile()</code> 3\uff08\u200b\u591a\u200b\u8fd0\u884c\u200b\uff09 ResNet50 CIFAR10 3x5 128 224 \u200b\u5426\u200b <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u8fd0\u884c\u200b\u6b21\u6570\u200b\u4e3a\u200b3\uff0c\u200b\u5468\u671f\u200b\u6570\u4e3a\u200b5\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7a7a\u200b\u5217\u8868\u200b\u6765\u200b\u5b58\u50a8\u200b\u7ed3\u679c\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u6bcf\u6b21\u200b\u8fd0\u884c\u200b\u540e\u200b\u5c06\u200b\u7ed3\u679c\u200b\u8ffd\u52a0\u200b\u5230\u200b\u8be5\u200b\u5217\u8868\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\uff0c\u200b\u5177\u4f53\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u7684\u200bGPU\u200b\u901f\u5ea6\u200b\u3002\u200b\u5bf9\u200b\u6211\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5728\u200bGoogle Colab Pro\u200b\u4e0a\u200b\u7684\u200bNVIDIA A100\u200b\u4e0a\u200b\u82b1\u8d39\u200b\u4e86\u200b20\u200b\u5206\u949f\u200b\uff0c\u200b\u800c\u200b\u5728\u200bNVIDIA TITAN RTX\u200b\u4e0a\u200b\u82b1\u8d39\u200b\u4e86\u200b\u5927\u7ea6\u200b49\u200b\u5206\u949f\u200b\u3002</p>"},{"location":"pytorch_2_intro/#42-4-","title":"4.2 \u200b\u5b9e\u9a8c\u200b 4 - \u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\uff0c\u200b\u5305\u542b\u200b\u7f16\u8bd1\u200b\u00b6","text":"<p>\u200b\u5b9e\u9a8c\u200b 4 \u200b\u7684\u200b\u65f6\u95f4\u200b\u5230\u200b\u4e86\u200b\u3002</p> <p>\u200b\u8fd0\u884c\u200b\u4e00\u4e2a\u200b\u5df2\u200b\u7f16\u8bd1\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u3002</p> \u200b\u5b9e\u9a8c\u200b \u200b\u6a21\u578b\u200b \u200b\u6570\u636e\u200b\u96c6\u200b \u200b\u5468\u671f\u200b\u6570\u200b \u200b\u6279\u91cf\u200b\u5927\u5c0f\u200b \u200b\u56fe\u50cf\u200b\u5c3a\u5bf8\u200b <code>torch.compile()</code> 4 (\u200b\u591a\u200b\u8fd0\u884c\u200b) ResNet50 CIFAR10 3x5 128 224 \u200b\u662f\u200b <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b <code>create_compiled_model()</code> \u200b\u548c\u200b <code>train_compiled_model()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u5148\u200b\u521b\u5efa\u200b\u7f16\u8bd1\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u5176\u200b\u8fdb\u884c\u200b 3 \u200b\u6b21\u200b\u8bad\u7ec3\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e0d\u592a\u200b\u5173\u5fc3\u200b\u6a21\u578b\u200b\u7684\u200b\u7ed3\u679c\u200b\uff08\u200b\u635f\u5931\u200b\u548c\u200b\u51c6\u786e\u6027\u200b\uff09\uff0c\u200b\u800c\u662f\u200b\u66f4\u200b\u5173\u5fc3\u200b\u5b83\u200b\u9700\u8981\u200b\u591a\u957f\u65f6\u95f4\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e4b\u6240\u4ee5\u200b\u4e00\u200b\u5f00\u59cb\u200b\u5c31\u200b\u7f16\u8bd1\u200b\u5b83\u200b\uff0c\u200b\u662f\u56e0\u4e3a\u200b PyTorch \u200b\u53ea\u200b\u9700\u8981\u200b\u8fd0\u884c\u200b\u4e00\u6b21\u200b\u4f18\u5316\u200b\u6b65\u9aa4\u200b\uff08\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u4e9b\u200b\u65f6\u95f4\u200b\uff09\uff0c\u200b\u7136\u540e\u200b\u5c31\u200b\u53ef\u4ee5\u200b\u5728\u200b\u540e\u7eed\u200b\u7684\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u91cd\u7528\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u50cf\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u7a7a\u200b\u5217\u8868\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5b58\u50a8\u200b\u6a21\u578b\u200b\u5728\u200b\u4e00\u7cfb\u5217\u200b\u8fd0\u884c\u200b\u4e2d\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8fd0\u884c\u200b\u4ee5\u4e0b\u200b\u4ee3\u7801\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u4e00\u6bb5\u65f6\u95f4\u200b\uff0c\u200b\u5177\u4f53\u200b\u53d6\u51b3\u4e8e\u200b\u60a8\u200b\u7684\u200b GPU \u200b\u901f\u5ea6\u200b\u3002\u200b\u5bf9\u200b\u6211\u200b\u6765\u8bf4\u200b\uff0c\u200b\u5728\u200b Google Colab Pro \u200b\u4e0a\u200b\u7684\u200b NVIDIA A100 \u200b\u4e0a\u200b\u9700\u8981\u200b 18 \u200b\u5206\u949f\u200b\uff0c\u200b\u800c\u200b\u5728\u200b NVIDIA TITAN RTX \u200b\u4e0a\u200b\u5927\u7ea6\u200b\u9700\u8981\u200b 45 \u200b\u5206\u949f\u200b\u3002</p>"},{"location":"pytorch_2_intro/#43-34","title":"4.3 \u200b\u6bd4\u8f83\u200b\u5b9e\u9a8c\u200b3\u200b\u548c\u200b\u5b9e\u9a8c\u200b4\u200b\u7684\u200b\u7ed3\u679c\u200b\u00b6","text":"<p>\u200b\u591a\u8f6e\u200b\u5b9e\u9a8c\u200b\u5df2\u200b\u5b8c\u6210\u200b\uff01</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6765\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b\u4e4b\u524d\u200b\u521b\u5efa\u200b\u7684\u200b <code>plot_mean_epoch_times()</code> \u200b\u51fd\u6570\u200b\u6765\u200b\u8fdb\u884c\u200b\u68c0\u67e5\u200b\u3002</p> <p>\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b <code>multi_runs</code> \u200b\u53c2\u6570\u8bbe\u7f6e\u200b\u4e3a\u200b <code>True</code>\uff0c\u200b\u4ee5\u4fbf\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u8868\u200b\u53cd\u6620\u200b\u51fa\u200b\u6211\u4eec\u200b\u6b63\u5728\u200b\u7ed8\u5236\u200b\u591a\u8f6e\u200b\u5b9e\u9a8c\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u8981\u200b\u786e\u4fdd\u200b\u6709\u200b\u4e00\u4e2a\u200b\u76ee\u5f55\u200b\u53ef\u4ee5\u200b\u4fdd\u5b58\u200b\u56fe\u8868\u200b\u3002</p>"},{"location":"pytorch_2_intro/#44-gpu","title":"4.4 \u200b\u5c06\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u7684\u200b\u7ed3\u679c\u200b\u53ca\u200bGPU\u200b\u8be6\u60c5\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u5b9e\u9a8c\u200b3\u200b\u548c\u200b\u5b9e\u9a8c\u200b4\u200b\u7684\u200b\u7ed3\u679c\u200b\u6570\u636e\u200b\u6846\u200b\u4fdd\u5b58\u200b\u5230\u200b\u6587\u4ef6\u200b\u4e2d\u200b\uff0c\u200b\u4ee5\u9632\u200b\u6211\u4eec\u200b\u65e5\u540e\u200b\u9700\u8981\u200b\u68c0\u67e5\u200b\u5b83\u4eec\u200b\u6216\u200b\u5c06\u200b\u5176\u200b\u4e0e\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u7684\u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u6bd4\u8f83\u200b\u3002</p>"},{"location":"pytorch_2_intro/#5","title":"5. \u200b\u53ef\u80fd\u200b\u7684\u200b\u6539\u8fdb\u200b\u548c\u200b\u6269\u5c55\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u63a2\u8ba8\u200b\u4e86\u200b <code>torch.compile()</code> \u200b\u7684\u200b\u57fa\u672c\u539f\u7406\u200b\uff0c\u200b\u5e76\u200b\u7f16\u5199\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u5b9e\u9a8c\u200b\u4ee3\u7801\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u5176\u200b\u6027\u80fd\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8fd8\u6709\u200b\u66f4\u200b\u591a\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u505a\u200b\u7684\u200b\u4e8b\u60c5\u200b\u3002</p> <p>\u200b\u6b63\u5982\u200b\u6211\u4eec\u200b\u6240\u200b\u8ba8\u8bba\u200b\u7684\u200b\uff0cPyTorch 2.0 \u200b\u548c\u200b <code>torch.compile()</code> \u200b\u4e2d\u200b\u8bb8\u591a\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u6765\u81ea\u200b\u4e8e\u200b\u4f7f\u7528\u200b\u66f4\u65b0\u200b\u7684\u200b GPU\uff08\u200b\u4f8b\u5982\u200b A100 \u200b\u53ca\u200b\u4ee5\u4e0a\u200b\uff09\u200b\u4ee5\u53ca\u200b\u5c3d\u53ef\u80fd\u200b\u5145\u5206\u5229\u7528\u200b GPU\uff08\u200b\u66f4\u5927\u200b\u7684\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u3001\u200b\u66f4\u5927\u200b\u7684\u200b\u6a21\u578b\u200b\u5927\u5c0f\u200b\uff09\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u8fdb\u4e00\u6b65\u200b\u52a0\u901f\u200b\uff0c\u200b\u6211\u200b\u5efa\u8bae\u200b\u7814\u7a76\u200b/\u200b\u5c1d\u8bd5\u200b\u4ee5\u4e0b\u5185\u5bb9\u200b\uff1a</p> <ul> <li>\u200b\u66f4\u200b\u5f3a\u5927\u200b\u7684\u200b CPU - \u200b\u6211\u200b\u6709\u200b\u4e00\u4e2a\u200b\u9690\u7ea6\u200b\u7684\u200b\u611f\u89c9\u200b\uff0cGoogle Colab \u200b\u5b9e\u4f8b\u200b\u53ef\u80fd\u200b\u9650\u5236\u200b\u5728\u200b 2 \u200b\u4e2a\u200b CPU \u200b\u6838\u5fc3\u200b\uff0c\u200b\u4f7f\u7528\u200b\u66f4\u200b\u591a\u200b CPU \u200b\u53ef\u80fd\u200b\u4f1a\u200b\u63d0\u9ad8\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b PyTorch Profiler\uff08\u200b\u4e00\u4e2a\u200b\u7528\u4e8e\u200b\u627e\u51fa\u200b\u5404\u4e2a\u200b\u8fdb\u7a0b\u200b\u5360\u7528\u200b\u65f6\u95f4\u200b\u7684\u200b\u5de5\u5177\u200b\uff09\u200b\u6765\u200b\u8ddf\u8e2a\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b\u6df7\u5408\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b - \u200b\u8f83\u200b\u65b0\u200b\u7684\u200b GPU \u200b\u80fd\u591f\u200b\u5904\u7406\u200b\u4e0d\u540c\u200b\u7cbe\u5ea6\u200b\u7c7b\u578b\u200b\uff08\u200b\u4f8b\u5982\u200b <code>torch.float16</code> \u200b\u548c\u200b <code>torch.bfloat16</code>\uff09\uff0c\u200b\u4ece\u800c\u200b\u5b9e\u73b0\u200b\u66f4\u5feb\u200b\u7684\u200b\u8bad\u7ec3\u200b\u548c\u200b\u63a8\u7406\u200b\u3002\u200b\u6211\u200b\u6000\u7591\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b\u6df7\u5408\u200b\u7cbe\u5ea6\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u770b\u5230\u200b\u6bd4\u200b\u6211\u4eec\u200b\u8fd9\u91cc\u200b\u770b\u5230\u200b\u7684\u200b\u66f4\u200b\u5927\u200b\u7684\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b PyTorch \u200b\u81ea\u52a8\u200b\u6df7\u5408\u200b\u7cbe\u5ea6\u200b\u7684\u200b\u6587\u6863\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b AMP\uff09\u3002</li> <li>\u200b\u57fa\u4e8e\u200b Transformer \u200b\u7684\u200b\u6a21\u578b\u200b\u53ef\u80fd\u200b\u6bd4\u200b\u5377\u79ef\u200b\u6a21\u578b\u200b\u83b7\u5f97\u200b\u66f4\u200b\u591a\u200b\u7684\u200b \u200b\u76f8\u5bf9\u200b \u200b\u52a0\u901f\u200b - PyTorch 2.0 \u200b\u5305\u542b\u200b\u4e86\u200b \u200b\u52a0\u901f\u200b Transformer \u200b\u6a21\u578b\u200b\u7684\u200b\u7a33\u5b9a\u200b\u7248\u672c\u200b\uff08\u200b\u4f7f\u7528\u200b\u6ce8\u610f\u529b\u200b\u673a\u5236\u200b\u7684\u200b\u6a21\u578b\u200b\uff09\u3002\u200b\u4e3b\u8981\u200b\u7684\u200b\u52a0\u901f\u200b\u6765\u81ea\u200b\u4e8e\u200b\u5bf9\u200b <code>scaled_dot_product_attention()</code> \u200b\u7684\u200b\u6539\u8fdb\u200b\u5b9e\u73b0\u200b\uff0c\u200b\u5b83\u4f1a\u200b\u6839\u636e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u786c\u4ef6\u200b\u81ea\u52a8\u200b\u9009\u62e9\u200b\u6700\u4f73\u200b\u7684\u200b\u6ce8\u610f\u529b\u200b\u7248\u672c\u200b\u3002\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u53ef\u4ee5\u200b\u53c2\u89c1\u200b \u200b\u4e13\u95e8\u200b\u7684\u200b PyTorch \u200b\u6559\u7a0b\u200b\u3002</li> <li>\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b - \u200b\u5982\u524d\u6240\u8ff0\u200b\uff0c<code>torch.compile()</code> \u200b\u5e26\u6765\u200b\u7684\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u5728\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u957f\u65f6\u95f4\u200b\u65f6\u200b\u53ef\u80fd\u200b\u66f4\u4e3a\u200b\u660e\u663e\u200b\u3002\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u597d\u200b\u7684\u200b\u7ec3\u4e60\u200b\u662f\u200b\u8bad\u7ec3\u200b\u66f4\u200b\u591a\u200b\u8f6e\u6b21\u200b\uff0c\u200b\u6f5c\u5728\u5730\u200b\u4f7f\u7528\u200b\u4e0d\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u4e0d\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\uff08\u200b\u4f8b\u5982\u200b Transformer\uff09\uff0c\u200b\u5e76\u200b\u6bd4\u8f83\u200b\u52a0\u901f\u200b\u6548\u679c\u200b\u3002</li> </ul>"},{"location":"pytorch_2_intro/#6","title":"6. \u200b\u8fdb\u4e00\u6b65\u200b\u5b66\u4e60\u200b\u7684\u200b\u8d44\u6e90\u200b\u00b6","text":"<p>\u200b\u6211\u200b\u53d1\u73b0\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u5bf9\u4e8e\u200b\u5b66\u4e60\u200b PyTorch 2.0 \u200b\u53ca\u5176\u200b\u5373\u5c06\u200b\u63a8\u51fa\u200b\u7684\u200b\u529f\u80fd\u200b\u975e\u5e38\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002</p> <ul> <li>PyTorch 2.0 \u200b\u53d1\u5e03\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200b\u3002</li> <li>PyTorch 2.0 \u200b\u53d1\u884c\u200b\u8bf4\u660e\u200b\uff08\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200b\uff09\u3002<ul> <li>\u200b\u4ee5\u53ca\u200b GitHub \u200b\u53d1\u884c\u200b\u8bf4\u660e\u200b\uff08\u200b\u8fd9\u91cc\u200b\u6709\u200b\u5f88\u591a\u200b\u4fe1\u606f\u200b\uff01\uff09\u3002</li> </ul> </li> <li>PyTorch \u200b\u9ed8\u8ba4\u200b\u8bbe\u5907\u200b\u4e0a\u4e0b\u6587\u200b\u7ba1\u7406\u5668\u200b\u6587\u6863\u200b\u3002</li> <li>PyTorch 2.0 YouTube \u200b\u89c6\u9891\u200b\u4ecb\u7ecd\u200b\uff08\u200b\u7531\u200b\u6211\u200b\u672c\u4eba\u200b\u521b\u5efa\u200b\uff09\u3002</li> <li>\u200b\u67e5\u770b\u200b Sebastian Raschka \u200b\u7684\u200b\u63d0\u793a\u200b\uff0c\u200b\u901a\u8fc7\u200b\u5148\u200b\u6267\u884c\u200b\u4e00\u4e2a\u200b\u793a\u4f8b\u200b\u6279\u6b21\u200b\uff08\u200b\u9884\u70ed\u200b\u6a21\u578b\u200b\uff09\u200b\u518d\u200b\u7ee7\u7eed\u200b\u8fdb\u884c\u200b\u8fdb\u4e00\u6b65\u200b\u8bad\u7ec3\u200b\u6765\u200b\u6539\u8fdb\u200b <code>torch.compile()</code>\uff08\u200b\u8fd9\u200b\u89e3\u91ca\u200b\u4e86\u200b\u591a\u6b21\u200b\u8fd0\u884c\u200b\u65f6\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\u7684\u200b\u539f\u56e0\u200b\uff09\u3002</li> </ul>"},{"location":"pytorch_cheatsheet/","title":"PyTorch \u200b\u901f\u67e5\u8868","text":"In\u00a0[1]: Copied! <pre>import torch\n\n# Check the version\nprint(f\"PyTorch version: {torch.__version__}\")\n</pre> import torch  # Check the version print(f\"PyTorch version: {torch.__version__}\") <pre>PyTorch version: 1.13.1\n</pre> In\u00a0[2]: Copied! <pre># Can also import the common abbreviation \"nn\" for \"Neural Networks\"\nfrom torch import nn\n\n# Almost everything in PyTorch is called a \"Module\" (you build neural networks by stacking together Modules)\nthis_is_a_module = nn.Linear(in_features=1,\n                             out_features=1)\nprint(type(this_is_a_module))\n</pre> # Can also import the common abbreviation \"nn\" for \"Neural Networks\" from torch import nn  # Almost everything in PyTorch is called a \"Module\" (you build neural networks by stacking together Modules) this_is_a_module = nn.Linear(in_features=1,                              out_features=1) print(type(this_is_a_module)) <pre>&lt;class 'torch.nn.modules.linear.Linear'&gt;\n</pre> In\u00a0[3]: Copied! <pre># Import PyTorch Dataset (you can store your data here) and DataLoader (you can load your data here)\nfrom torch.utils.data import Dataset, DataLoader\n</pre> # Import PyTorch Dataset (you can store your data here) and DataLoader (you can load your data here) from torch.utils.data import Dataset, DataLoader In\u00a0[4]: Copied! <pre># Create a single number tensor (scalar)\nscalar = torch.tensor(7)\n</pre> # Create a single number tensor (scalar) scalar = torch.tensor(7) In\u00a0[5]: Copied! <pre># Create a random tensor\nrandom_tensor = torch.rand(size=(3, 4)) # this will create a tensor of size 3x4 but you can manipulate the shape how you want\n</pre> # Create a random tensor random_tensor = torch.rand(size=(3, 4)) # this will create a tensor of size 3x4 but you can manipulate the shape how you want In\u00a0[6]: Copied! <pre># Multiply two random tensors\nrandom_tensor_1 = torch.rand(size=(3, 4))\nrandom_tensor_2 = torch.rand(size=(3, 4))\nrandom_tensor_3 = random_tensor_1 * random_tensor_2 # PyTorch has support for most math operators in Python (+, *, -, /)\n</pre> # Multiply two random tensors random_tensor_1 = torch.rand(size=(3, 4)) random_tensor_2 = torch.rand(size=(3, 4)) random_tensor_3 = random_tensor_1 * random_tensor_2 # PyTorch has support for most math operators in Python (+, *, -, /) In\u00a0[7]: Copied! <pre># Base computer vision library\nimport torchvision\n\n# Other components of TorchVision (premade datasets, pretrained models and image transforms)\nfrom torchvision import datasets, models, transforms\n</pre> # Base computer vision library import torchvision  # Other components of TorchVision (premade datasets, pretrained models and image transforms) from torchvision import datasets, models, transforms  In\u00a0[8]: Copied! <pre># Base text and natural language processing library\nimport torchtext\n\n# Other components of TorchText (premade datasets, pretrained models and text transforms)\nfrom torchtext import datasets, models, transforms\n</pre> # Base text and natural language processing library import torchtext  # Other components of TorchText (premade datasets, pretrained models and text transforms) from torchtext import datasets, models, transforms In\u00a0[9]: Copied! <pre># Base audio and speech processing library\nimport torchaudio\n\n# Other components of TorchAudio (premade datasets, pretrained models and text transforms)\nfrom torchaudio import datasets, models, transforms\n</pre> # Base audio and speech processing library import torchaudio  # Other components of TorchAudio (premade datasets, pretrained models and text transforms) from torchaudio import datasets, models, transforms In\u00a0[10]: Copied! <pre># # Base recommendation system library \n# import torchrec\n\n# # Other components of TorchRec\n# from torchrec import datasets, models\n</pre> # # Base recommendation system library  # import torchrec  # # Other components of TorchRec # from torchrec import datasets, models In\u00a0[11]: Copied! <pre># Setup device-agnostic code \nif torch.cuda.is_available():\n    device = \"cuda\" # NVIDIA GPU\nelif torch.backends.mps.is_available():\n    device = \"mps\" # Apple GPU\nelse:\n    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available\n\nprint(f\"Using device: {device}\")\n</pre> # Setup device-agnostic code  if torch.cuda.is_available():     device = \"cuda\" # NVIDIA GPU elif torch.backends.mps.is_available():     device = \"mps\" # Apple GPU else:     device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available  print(f\"Using device: {device}\") <pre>Using device: mps\n</pre> In\u00a0[12]: Copied! <pre># Create a tensor \nx = torch.tensor([1, 2, 3]) \nprint(x.device) # defaults to CPU \n\n# Send tensor to target device\nx = x.to(device)\nprint(x.device)\n</pre> # Create a tensor  x = torch.tensor([1, 2, 3])  print(x.device) # defaults to CPU   # Send tensor to target device x = x.to(device) print(x.device)  <pre>cpu\nmps:0\n</pre> In\u00a0[13]: Copied! <pre>import torch\n\n# Set the random seed (you can set this to any number you like, it will \"flavour\"\n# the randomness with that number.\ntorch.manual_seed(42)\n\n# Create two random tensors\nrandom_tensor_A = torch.rand(3, 4)\n\ntorch.manual_seed(42) # set the seed again (try commenting this out and see what happens)\nrandom_tensor_B = torch.rand(3, 4)\n\nprint(f\"Tensor A:\\n{random_tensor_A}\\n\")\nprint(f\"Tensor B:\\n{random_tensor_B}\\n\")\nprint(f\"Does Tensor A equal Tensor B? (anywhere)\")\nrandom_tensor_A == random_tensor_B\n</pre> import torch  # Set the random seed (you can set this to any number you like, it will \"flavour\" # the randomness with that number. torch.manual_seed(42)  # Create two random tensors random_tensor_A = torch.rand(3, 4)  torch.manual_seed(42) # set the seed again (try commenting this out and see what happens) random_tensor_B = torch.rand(3, 4)  print(f\"Tensor A:\\n{random_tensor_A}\\n\") print(f\"Tensor B:\\n{random_tensor_B}\\n\") print(f\"Does Tensor A equal Tensor B? (anywhere)\") random_tensor_A == random_tensor_B <pre>Tensor A:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nTensor B:\ntensor([[0.8823, 0.9150, 0.3829, 0.9593],\n        [0.3904, 0.6009, 0.2566, 0.7936],\n        [0.9408, 0.1332, 0.9346, 0.5936]])\n\nDoes Tensor A equal Tensor B? (anywhere)\n</pre> Out[13]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>\u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u5728\u200bGPU\uff08CUDA\u200b\u8bbe\u5907\u200b\uff09\u200b\u4e0a\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u3002</p> In\u00a0[14]: Copied! <pre># Set random seed on GPU\ntorch.cuda.manual_seed(42)\n</pre> # Set random seed on GPU torch.cuda.manual_seed(42) In\u00a0[15]: Copied! <pre>from torch import nn\n</pre> from torch import nn In\u00a0[16]: Copied! <pre># Create a linear layer with 10 in features and out features\nlinear_layer = nn.Linear(in_features=10,\n                         out_features=10)\n</pre> # Create a linear layer with 10 in features and out features linear_layer = nn.Linear(in_features=10,                          out_features=10) In\u00a0[17]: Copied! <pre># Create an Identity layer\nidentity_layer = nn.Identity()\n</pre> # Create an Identity layer identity_layer = nn.Identity() In\u00a0[18]: Copied! <pre># Create a Conv1d layer (often used for text with a singular dimension)\nconv1d = nn.Conv1d(in_channels=1,\n                   out_channels=10,\n                   kernel_size=3)\n</pre> # Create a Conv1d layer (often used for text with a singular dimension) conv1d = nn.Conv1d(in_channels=1,                    out_channels=10,                    kernel_size=3) In\u00a0[19]: Copied! <pre># Create a Conv2d layer (often used for images with Height x Width dimensions)\nconv2d = nn.Conv2d(in_channels=3, # 3 channels for color images (red, green, blue)\n                   out_channels=10,\n                   kernel_size=3)\n</pre> # Create a Conv2d layer (often used for images with Height x Width dimensions) conv2d = nn.Conv2d(in_channels=3, # 3 channels for color images (red, green, blue)                    out_channels=10,                    kernel_size=3)                    In\u00a0[20]: Copied! <pre># Create a Conv3d layer (often used for video with Height x Width x Time dimensions)\nconv3d = nn.Conv3d(in_channels=3,\n                   out_channels=10,\n                   kernel_size=3)\n</pre> # Create a Conv3d layer (often used for video with Height x Width x Time dimensions) conv3d = nn.Conv3d(in_channels=3,                    out_channels=10,                    kernel_size=3) In\u00a0[21]: Copied! <pre># Create a Transformer model (model based on the paper \"Attention Is All You Need\" - https://arxiv.org/abs/1706.03762)\ntransformer_model = nn.Transformer()\n</pre> # Create a Transformer model (model based on the paper \"Attention Is All You Need\" - https://arxiv.org/abs/1706.03762) transformer_model = nn.Transformer() In\u00a0[22]: Copied! <pre># Create a single Transformer encoder cell\ntransformer_encoder = nn.TransformerEncoderLayer(d_model=768, # embedding dimension\n                                                 nhead=12) # number of attention heads\n</pre> # Create a single Transformer encoder cell transformer_encoder = nn.TransformerEncoderLayer(d_model=768, # embedding dimension                                                  nhead=12) # number of attention heads In\u00a0[23]: Copied! <pre># Stack together Transformer encoder cells\ntransformer_encoder_stack = nn.TransformerEncoder(encoder_layer=transformer_encoder, # from above\n                                                  num_layers=6) # 6 Transformer encoders stacked on top of each other\n</pre> # Stack together Transformer encoder cells transformer_encoder_stack = nn.TransformerEncoder(encoder_layer=transformer_encoder, # from above                                                   num_layers=6) # 6 Transformer encoders stacked on top of each other In\u00a0[24]: Copied! <pre># Create a single Transformer decoder cell\ntransformer_decoder = nn.TransformerDecoderLayer(d_model=768,\n                                                 nhead=12)\n</pre> # Create a single Transformer decoder cell transformer_decoder = nn.TransformerDecoderLayer(d_model=768,                                                  nhead=12) In\u00a0[25]: Copied! <pre># Stack together Transformer decoder cells\ntransformer_decoder_stack = nn.TransformerDecoder(decoder_layer=transformer_decoder, # from above\n                                                  num_layers=6) # 6 Transformer decoders stacked on top of each other\n</pre> # Stack together Transformer decoder cells transformer_decoder_stack = nn.TransformerDecoder(decoder_layer=transformer_decoder, # from above                                                   num_layers=6) # 6 Transformer decoders stacked on top of each other In\u00a0[26]: Copied! <pre># Create a single LSTM cell\nlstm_cell = nn.LSTMCell(input_size=10, # can adjust as necessary\n                        hidden_size=10) # can adjust as necessary\n</pre> # Create a single LSTM cell lstm_cell = nn.LSTMCell(input_size=10, # can adjust as necessary                         hidden_size=10) # can adjust as necessary In\u00a0[27]: Copied! <pre># Stack together LSTM cells\nlstm_stack = nn.LSTM(input_size=10,\n                     hidden_size=10,\n                     num_layers=3) # 3 single LSTM cells stacked on top of each other\n</pre> # Stack together LSTM cells lstm_stack = nn.LSTM(input_size=10,                      hidden_size=10,                      num_layers=3) # 3 single LSTM cells stacked on top of each other In\u00a0[28]: Copied! <pre># Create a single GRU cell\ngru_cell = nn.GRUCell(input_size=10, # can adjust as necessary\n                      hidden_size=10) # can adjust as necessary\n</pre> # Create a single GRU cell gru_cell = nn.GRUCell(input_size=10, # can adjust as necessary                       hidden_size=10) # can adjust as necessary In\u00a0[29]: Copied! <pre># Stack together GRU cells\ngru_stack = nn.GRU(input_size=10, \n                   hidden_size=10,\n                   num_layers=3) # 3 single GRU cells stacked on top of each other\n</pre> # Stack together GRU cells gru_stack = nn.GRU(input_size=10,                     hidden_size=10,                    num_layers=3) # 3 single GRU cells stacked on top of each other  In\u00a0[30]: Copied! <pre># ReLU\nrelu = nn.ReLU()\n\n# Sigmoid\nsigmoid = nn.Sigmoid()\n\n# Softmax\nsoftmax = nn.Softmax()\n</pre> # ReLU relu = nn.ReLU()  # Sigmoid sigmoid = nn.Sigmoid()  # Softmax softmax = nn.Softmax() In\u00a0[31]: Copied! <pre># L1Loss\nloss_fn = nn.L1Loss() # also known as MAE or mean absolute error\n\n# MSELoss\nloss_fn = nn.MSELoss() # also known as MSE or mean squared error\n\n# Binary cross entropy (for binary classification problems)\nloss_fn = nn.BCEWithLogitsLoss()\n\n# Cross entropy (for multi-class classification problems)\nloss_fn = nn.CrossEntropyLoss()\n</pre> # L1Loss loss_fn = nn.L1Loss() # also known as MAE or mean absolute error  # MSELoss loss_fn = nn.MSELoss() # also known as MSE or mean squared error  # Binary cross entropy (for binary classification problems) loss_fn = nn.BCEWithLogitsLoss()  # Cross entropy (for multi-class classification problems) loss_fn = nn.CrossEntropyLoss() In\u00a0[32]: Copied! <pre># Create a baseline model\nmodel = nn.Transformer()\n\n# SGD (stochastic gradient descent)\noptimizer = torch.optim.SGD(lr=0.1, # set the learning rate (required)\n                            params=model.parameters()) # tell the optimizer what parameters to optimize\n</pre> # Create a baseline model model = nn.Transformer()  # SGD (stochastic gradient descent) optimizer = torch.optim.SGD(lr=0.1, # set the learning rate (required)                             params=model.parameters()) # tell the optimizer what parameters to optimize In\u00a0[33]: Copied! <pre># Create a baseline model\nmodel = nn.Transformer()\n\n# Adam optimizer\noptimizer = torch.optim.Adam(lr=0.001, # set the learning rate (required)\n                             params=model.parameters()) # tell the optimizer what parameters to optimize\n</pre> # Create a baseline model model = nn.Transformer()  # Adam optimizer optimizer = torch.optim.Adam(lr=0.001, # set the learning rate (required)                              params=model.parameters()) # tell the optimizer what parameters to optimize In\u00a0[34]: Copied! <pre># Create *known* parameters\nweight = 0.7\nbias = 0.3\n\n# Create data\nstart = 0\nend = 1\nstep = 0.02\nX = torch.arange(start, end, step).unsqueeze(dim=1) # data\ny = weight * X + bias # labels (want model to learn from data to predict these)\n\nX[:10], y[:10]\n</pre> # Create *known* parameters weight = 0.7 bias = 0.3  # Create data start = 0 end = 1 step = 0.02 X = torch.arange(start, end, step).unsqueeze(dim=1) # data y = weight * X + bias # labels (want model to learn from data to predict these)  X[:10], y[:10] Out[34]: <pre>(tensor([[0.0000],\n         [0.0200],\n         [0.0400],\n         [0.0600],\n         [0.0800],\n         [0.1000],\n         [0.1200],\n         [0.1400],\n         [0.1600],\n         [0.1800]]),\n tensor([[0.3000],\n         [0.3140],\n         [0.3280],\n         [0.3420],\n         [0.3560],\n         [0.3700],\n         [0.3840],\n         [0.3980],\n         [0.4120],\n         [0.4260]]))</pre> In\u00a0[35]: Copied! <pre># Create train/test split\ntrain_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing \nX_train, y_train = X[:train_split], y[:train_split]\nX_test, y_test = X[train_split:], y[train_split:]\n\nlen(X_train), len(y_train), len(X_test), len(y_test)\n</pre> # Create train/test split train_split = int(0.8 * len(X)) # 80% of data used for training set, 20% for testing  X_train, y_train = X[:train_split], y[:train_split] X_test, y_test = X[train_split:], y[train_split:]  len(X_train), len(y_train), len(X_test), len(y_test) Out[35]: <pre>(40, 40, 10, 10)</pre> In\u00a0[36]: Copied! <pre>from torch import nn\n\n# Option 1 - subclass torch.nn.Module\nclass LinearRegressionModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        # Use nn.Linear() for creating the model parameters\n        self.linear_layer = nn.Linear(in_features=1, \n                                      out_features=1)\n    \n    # Define the forward computation (input data x flows through nn.Linear())\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        return self.linear_layer(x)\n\nmodel_0 = LinearRegressionModel()\nmodel_0, model_0.state_dict()\n</pre> from torch import nn  # Option 1 - subclass torch.nn.Module class LinearRegressionModel(nn.Module):     def __init__(self):         super().__init__()         # Use nn.Linear() for creating the model parameters         self.linear_layer = nn.Linear(in_features=1,                                        out_features=1)          # Define the forward computation (input data x flows through nn.Linear())     def forward(self, x: torch.Tensor) -&gt; torch.Tensor:         return self.linear_layer(x)  model_0 = LinearRegressionModel() model_0, model_0.state_dict() Out[36]: <pre>(LinearRegressionModel(\n   (linear_layer): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('linear_layer.weight', tensor([[0.5025]])),\n              ('linear_layer.bias', tensor([-0.0722]))]))</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b <code>torch.nn.Sequential</code> \u200b\u521b\u5efa\u200b\u4e0e\u200b\u4e0a\u9762\u200b\u76f8\u540c\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> In\u00a0[37]: Copied! <pre>from torch import nn\n\n# Option 2 - use torch.nn.Sequential\nmodel_1 = torch.nn.Sequential(\n    nn.Linear(in_features=1,\n              out_features=1))\n\nmodel_1, model_1.state_dict()\n</pre> from torch import nn  # Option 2 - use torch.nn.Sequential model_1 = torch.nn.Sequential(     nn.Linear(in_features=1,               out_features=1))  model_1, model_1.state_dict() Out[37]: <pre>(Sequential(\n   (0): Linear(in_features=1, out_features=1, bias=True)\n ),\n OrderedDict([('0.weight', tensor([[0.9905]])), ('0.bias', tensor([0.9053]))]))</pre> In\u00a0[38]: Copied! <pre># Create loss function\nloss_fn = nn.L1Loss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters\n                            lr=0.01)\n</pre> # Create loss function loss_fn = nn.L1Loss()  # Create optimizer optimizer = torch.optim.SGD(params=model_1.parameters(), # optimize newly created model's parameters                             lr=0.01) In\u00a0[40]: Copied! <pre>torch.manual_seed(42)\n\n# Set the number of epochs \nepochs = 1000 \n\n# Put data on the available device\n# Without this, an error will happen (not all data on target device)\nX_train = X_train.to(device)\nX_test = X_test.to(device)\ny_train = y_train.to(device)\ny_test = y_test.to(device)\n\n# Put model on the available device\n# With this, an error will happen (the model is not on target device)\nmodel_1 = model_1.to(device)\n\nfor epoch in range(epochs):\n    ### Training\n    model_1.train() # train mode is on by default after construction\n\n    # 1. Forward pass\n    y_pred = model_1(X_train)\n\n    # 2. Calculate loss\n    loss = loss_fn(y_pred, y_train)\n\n    # 3. Zero grad optimizer\n    optimizer.zero_grad()\n\n    # 4. Loss backward\n    loss.backward()\n\n    # 5. Step the optimizer\n    optimizer.step()\n\n    ### Testing\n    model_1.eval() # put the model in evaluation mode for testing (inference)\n    # 1. Forward pass\n    with torch.inference_mode():\n        test_pred = model_1(X_test)\n    \n        # 2. Calculate the loss\n        test_loss = loss_fn(test_pred, y_test)\n\n    if epoch % 100 == 0:\n        print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\")\n</pre> torch.manual_seed(42)  # Set the number of epochs  epochs = 1000   # Put data on the available device # Without this, an error will happen (not all data on target device) X_train = X_train.to(device) X_test = X_test.to(device) y_train = y_train.to(device) y_test = y_test.to(device)  # Put model on the available device # With this, an error will happen (the model is not on target device) model_1 = model_1.to(device)  for epoch in range(epochs):     ### Training     model_1.train() # train mode is on by default after construction      # 1. Forward pass     y_pred = model_1(X_train)      # 2. Calculate loss     loss = loss_fn(y_pred, y_train)      # 3. Zero grad optimizer     optimizer.zero_grad()      # 4. Loss backward     loss.backward()      # 5. Step the optimizer     optimizer.step()      ### Testing     model_1.eval() # put the model in evaluation mode for testing (inference)     # 1. Forward pass     with torch.inference_mode():         test_pred = model_1(X_test)              # 2. Calculate the loss         test_loss = loss_fn(test_pred, y_test)      if epoch % 100 == 0:         print(f\"Epoch: {epoch} | Train loss: {loss} | Test loss: {test_loss}\") <pre>Epoch: 0 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 100 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 200 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 300 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 400 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 500 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 600 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 700 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 800 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\nEpoch: 900 | Train loss: 0.008362661115825176 | Test loss: 0.005596190690994263\n</pre>"},{"location":"pytorch_cheatsheet/#pytorch","title":"PyTorch \u200b\u901f\u67e5\u8868\u200b\u00b6","text":"<p>PyTorch \u200b\u4e2d\u200b\u4e00\u4e9b\u200b\u6700\u200b\u5e38\u7528\u200b\u7684\u200b\u547d\u4ee4\u200b/\u200b\u8bbe\u7f6e\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u83b7\u53d6\u200b PyTorch \u200b\u7279\u5b9a\u200b\u51fd\u6570\u200b\u548c\u200b\u7528\u4f8b\u200b\u5e2e\u52a9\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u4e4b\u4e00\u200b\u662f\u200b\u641c\u7d22\u200b\u201cpytorch \u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u201d\u200b\u6216\u200b\u201cpytorch \u200b\u53d8\u6362\u5668\u200b\u5c42\u200b\u201d\u200b\u6216\u200b\u201cpytorch \u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u201d\u3002\u200b\u6211\u200b\u7ecf\u5e38\u200b\u8fd9\u6837\u200b\u505a\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u5bfc\u5165\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200bPyTorch \u200b\u5b89\u88c5\u200b\u9875\u9762\u200b\u5728\u200b\u4e0d\u540c\u200b\u5e73\u53f0\u200b\u4e0a\u200b\u5b89\u88c5\u200b PyTorch\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u6570\u636e\u200b\u5bfc\u5165\u200b\u00b6","text":"<p>\u200b\u7531\u4e8e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u5de5\u4f5c\u200b\u662f\u200b\u53d1\u73b0\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u56e0\u6b64\u200b\u4e86\u89e3\u200b\u5982\u4f55\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u5904\u7406\u200b\u6570\u636e\u200b\u96c6\u662f\u200b\u5f88\u200b\u6709\u200b\u5fc5\u8981\u200b\u7684\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u521b\u5efa\u200b\u5f20\u91cf\u200b\u00b6","text":"<p>PyTorch \u200b\u7684\u200b\u4e3b\u8981\u7528\u9014\u200b\u4e4b\u4e00\u200b\u662f\u200b\u7528\u4e8e\u200b\u52a0\u901f\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u800c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u901a\u5e38\u200b\u6d89\u53ca\u200b\u5bf9\u200b\u5927\u578b\u200b\u5f20\u91cf\u200b\uff08\u200b\u5e9e\u5927\u200b\u3001\u200b\u591a\u7ef4\u200b\u7684\u200b\u6570\u5b57\u200b\u96c6\u5408\u200b\uff09\u200b\u7684\u200b\u64cd\u4f5c\u200b\u3002</p> <p>PyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u79cd\u200b\u521b\u5efa\u200b\u5f20\u91cf\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5173\u4e8e\u200b\u4f7f\u7528\u200b PyTorch \u200b\u521b\u5efa\u200b\u5f20\u91cf\u200b\u7684\u200b\u66f4\u200b\u5168\u9762\u200b\u6982\u8ff0\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 00. PyTorch \u200b\u57fa\u7840\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u9886\u57df\u200b\u5e93\u200b\u00b6","text":"<p>\u200b\u6839\u636e\u200b\u60a8\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u5177\u4f53\u200b\u95ee\u9898\u200b\uff0cPyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u51e0\u4e2a\u200b\u9886\u57df\u200b\u5e93\u200b\u3002</p> <ul> <li>TorchVision \u2014 PyTorch \u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b\u3002</li> <li>TorchText \u2014 PyTorch \u200b\u5185\u7f6e\u200b\u7684\u200b\u6587\u672c\u200b\u9886\u57df\u200b\u5e93\u200b\u3002</li> <li>TorchAudio \u2014 PyTorch \u200b\u7684\u200b\u97f3\u9891\u200b\u9886\u57df\u200b\u5e93\u200b\u3002</li> <li>TorchRec \u2014 PyTorch \u200b\u6700\u65b0\u200b\u7684\u200b\u5185\u7f6e\u200b\u9886\u57df\u200b\u5e93\u200b\uff0c\u200b\u7528\u4e8e\u200b\u901a\u8fc7\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9a71\u52a8\u200b\u63a8\u8350\u200b\u5f15\u64ce\u200b\u3002</li> </ul>"},{"location":"pytorch_cheatsheet/","title":"\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6709\u5173\u200b PyTorch \u200b\u4e2d\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u6df1\u5165\u200b\u6982\u8ff0\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 03. PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/#nlp","title":"\u6587\u672c\u200b\u4e0e\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff08NLP\uff09\u00b6","text":""},{"location":"pytorch_cheatsheet/","title":"\u97f3\u9891\u200b\u4e0e\u200b\u8bed\u97f3\u200b\u00b6","text":""},{"location":"pytorch_cheatsheet/","title":"\u63a8\u8350\u200b\u7cfb\u7edf\u200b\u00b6","text":"<p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8be5\u5e93\u200b\u76ee\u524d\u200b\u5904\u4e8e\u200b\u6d4b\u8bd5\u200b\u7248\u672c\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b GitHub \u200b\u9875\u9762\u200b\u8fdb\u884c\u200b\u5b89\u88c5\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/#pytorch-cpugpu-mps","title":"\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\uff08\u200b\u4f7f\u7528\u200b PyTorch \u200b\u5728\u200b CPU\u3001GPU \u200b\u6216\u200b MPS \u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff09\u00b6","text":"<p>\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u5de5\u4f5c\u200b\u6d89\u53ca\u200b\u5bf9\u200b\u5f20\u91cf\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u4e0e\u200b CPU\uff08\u200b\u4e2d\u592e\u200b\u5904\u7406\u5355\u5143\u200b\uff09\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u5728\u200b GPU\uff08\u200b\u56fe\u5f62\u200b\u5904\u7406\u5355\u5143\u200b\uff0c\u200b\u901a\u5e38\u200b\u6765\u81ea\u200b NVIDIA\uff09\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u5f20\u91cf\u200b\u8ba1\u7b97\u200b\u901a\u5e38\u200b\u4f1a\u200b\u5feb\u5f97\u591a\u200b\u3002</p> <p>MPS \u200b\u4ee3\u8868\u200b \"Metal Performance Shader\"\uff0c\u200b\u8fd9\u662f\u200b Apple \u200b\u7684\u200b GPU\uff08\u200b\u5982\u200b M1\u3001M1 Pro\u3001M2 \u200b\u7b49\u200b\uff09\u3002</p> <p>\u200b\u5efa\u8bae\u200b\u5728\u200b\u60a8\u200b\u53ef\u7528\u200b\u7684\u200b\u6700\u5feb\u200b\u7684\u200b\u786c\u4ef6\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u901a\u5e38\u200b\u7684\u200b\u4f18\u5148\u200b\u987a\u5e8f\u200b\u662f\u200b\uff1aNVIDIA GPU\uff08<code>\"cuda\"</code>\uff09&gt; MPS \u200b\u8bbe\u5907\u200b\uff08<code>\"mps\"</code>\uff09&gt; CPU\uff08<code>\"cpu\"</code>\uff09\u3002</p> <ul> <li>\u200b\u5173\u4e8e\u200b\u5982\u4f55\u200b\u8ba9\u200b PyTorch \u200b\u5728\u200b NVIDIA GPU\uff08\u200b\u4f7f\u7528\u200b CUDA\uff09\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 00. PyTorch \u200b\u57fa\u7840\u200b\u90e8\u5206\u200b 2\uff1a\u200b\u8ba9\u200b PyTorch \u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u3002</li> <li>\u200b\u5173\u4e8e\u200b\u4f7f\u7528\u200b MPS \u200b\u540e\u200b\u7aef\u200b\u8fd0\u884c\u200b PyTorch\uff08\u200b\u5728\u200b Mac GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b PyTorch\uff09\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b PyTorch \u200b\u6587\u6863\u200b\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5efa\u8bae\u200b\u5728\u200b\u5f00\u59cb\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u65f6\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\u65e0\u5173\u200b\u4ee3\u7801\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u5c06\u200b\u5f20\u91cf\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u00b6","text":"<p>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b <code>.to(\"device_name\")</code> \u200b\u65b9\u6cd5\u200b\u5c06\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b\u5bf9\u8c61\u200b\uff08\u200b\u6a21\u578b\u200b\u548c\u200b\u5f20\u91cf\u200b\uff09\u200b\u79fb\u52a8\u200b\u5230\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e2d\u200b\uff0c\u200b\u5f88\u591a\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u9700\u8981\u200b\u4ece\u200b\u5f20\u91cf\u200b\u4e2d\u200b\u83b7\u53d6\u200b\u968f\u673a\u6570\u200b\uff0c\u200b\u7136\u540e\u200b\u5bf9\u200b\u8fd9\u4e9b\u200b\u968f\u673a\u6570\u200b\u8fdb\u884c\u200b\u5904\u7406\u200b\uff0c\u200b\u4ee5\u200b\u53d1\u73b0\u200b\u6216\u200b\u8868\u793a\u200b\u771f\u5b9e\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u6709\u65f6\u200b\u4f60\u200b\u5e0c\u671b\u200b\u8fd9\u79cd\u200b\u968f\u673a\u6027\u200b\u662f\u200b\u201c\u200b\u53ef\u200b\u590d\u73b0\u200b\u201d\u200b\u7684\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8bbe\u7f6e\u200b\u968f\u673a\u200b\u79cd\u5b50\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b\u53ef\u200b\u590d\u73b0\u200b\u6027\u200b\uff08\u200b\u8bd5\u56fe\u200b\u6d88\u9664\u200b\u968f\u673a\u6027\u200b\uff09\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u795e\u7ecf\u7f51\u7edc\u200b\u00b6","text":"<p>PyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u975e\u5e38\u200b\u5168\u9762\u200b\u7684\u200b\u9884\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7ec4\u4ef6\u200b\u5e93\u200b\uff08\u200b\u5728\u200b PyTorch \u200b\u751f\u6001\u7cfb\u7edf\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u7ec4\u4ef6\u200b\u901a\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u6a21\u5757\u200b\u201d\uff09\u3002</p> <p>\u200b\u4ece\u200b\u57fa\u672c\u200b\u5c42\u9762\u200b\u6765\u770b\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u662f\u200b\u7531\u200b\u4e00\u7cfb\u5217\u200b\u5c42\u200b\u7ec4\u6210\u200b\u7684\u200b\u5806\u6808\u200b\u3002\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u5bf9\u200b\u8f93\u5165\u200b\u6267\u884c\u200b\u67d0\u79cd\u200b\u64cd\u4f5c\u200b\u5e76\u200b\u4ea7\u751f\u200b\u8f93\u51fa\u200b\u3002</p> <p>\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u5982\u4f55\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\u5c06\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u6b63\u5728\u200b\u89e3\u51b3\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u6700\u200b\u6d3b\u8dc3\u200b\u7684\u200b\u7814\u7a76\u200b\u9886\u57df\u200b\u4e4b\u4e00\u200b\u662f\u200b\u5982\u4f55\u200b\u5c06\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\u5806\u53e0\u200b\u5728\u200b\u4e00\u8d77\u200b\uff08\u200b\u5bf9\u6b64\u200b\u7684\u200b\u6700\u4f73\u7b54\u6848\u200b\u4e0d\u65ad\u200b\u53d8\u5316\u200b\uff09\u3002</p> <p>PyTorch \u200b\u4e2d\u200b\u7edd\u5927\u591a\u6570\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7ec4\u4ef6\u200b\u90fd\u200b\u5305\u542b\u200b\u5728\u200b <code>torch.nn</code> \u200b\u5305\u200b \u200b\u4e2d\u200b\uff08<code>nn</code> \u200b\u662f\u200b neural networks \u200b\u7684\u200b\u7f29\u5199\u200b\uff09\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u7ebf\u6027\u200b\u5c42\u200b\u00b6","text":"<p>PyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u51e0\u79cd\u200b\u5185\u7f6e\u200b\u7684\u200b\u7ebf\u6027\u200b\u5c42\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/#cnn","title":"\u5377\u79ef\u200b\u5c42\u200b\uff08\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6216\u200bCNN\uff09\u00b6","text":"<p>PyTorch \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u591a\u79cd\u200b\u5185\u7f6e\u200b\u7684\u200b\u5377\u79ef\u200b\u5c42\u200b\u3002</p> <p>\u200b\u5377\u79ef\u200b\u5c42\u200b\u7684\u200b\u547d\u540d\u200b\u901a\u5e38\u200b\u9075\u5faa\u200b <code>torch.nn.ConvXd</code> \u200b\u7684\u200b\u683c\u5f0f\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>X</code> \u200b\u53ef\u4ee5\u200b\u662f\u200b <code>1</code>\u3001<code>2</code> \u200b\u6216\u200b <code>3</code>\u3002</p> <p><code>X</code> \u200b\u7684\u200b\u503c\u200b\u8868\u793a\u200b\u5377\u79ef\u200b\u64cd\u4f5c\u200b\u5c06\u200b\u6d89\u53ca\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u6570\u91cf\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c<code>1</code> \u200b\u8868\u793a\u200b\u5355\u7ef4\u200b\u6587\u672c\u200b\uff0c<code>2</code> \u200b\u8868\u793a\u200b\u4e8c\u7ef4\u200b\u56fe\u50cf\u200b\uff08\u200b\u9ad8\u5ea6\u200b x \u200b\u5bbd\u5ea6\u200b\uff09\uff0c<code>3</code> \u200b\u8868\u793a\u200b\u4e09\u7ef4\u200b\u5bf9\u8c61\u200b\uff0c\u200b\u5982\u200b\u89c6\u9891\u200b\uff08\u200b\u89c6\u9891\u200b\u88ab\u200b\u89c6\u4e3a\u200b\u4e00\u7cfb\u5217\u200b\u5177\u6709\u200b\u65f6\u95f4\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u9ad8\u5ea6\u200b x \u200b\u5bbd\u5ea6\u200b x \u200b\u65f6\u95f4\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b03. PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u90e8\u5206\u200b 7.2\uff1a\u200b\u6784\u5efa\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08CNN\uff09\u200b\u4e2d\u200b\u67e5\u770b\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6784\u5efa\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5185\u5bb9\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/#transformertransformer","title":"Transformer\u200b\u5c42\u200b\uff08\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200bTransformer\u200b\u6a21\u578b\u200b\uff09\u00b6","text":"<p>PyTorch\u200b\u5185\u7f6e\u200b\u4e86\u200bTransformer\u200b\u5c42\u200b\uff0c\u200b\u8fd9\u4e9b\u200b\u5c42\u200b\u5728\u200b\u8bba\u6587\u200bAttention Is All You Need\u200b\u4e2d\u6709\u200b\u8be6\u7ec6\u63cf\u8ff0\u200b\u3002</p> <p>\u200b\u4f7f\u7528\u200b\u5185\u7f6e\u200b\u7684\u200bPyTorch Transformer\u200b\u5c42\u200b\u7684\u200b\u597d\u5904\u200b\u662f\u200b\uff0c\u200b\u5f97\u76ca\u4e8e\u200bPyTorch\u200b\u7684\u200bBetterTransformer\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u5e26\u6765\u200b\u6f5c\u5728\u200b\u7684\u200b\u901f\u5ea6\u200b\u63d0\u5347\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b08. PyTorch\u200b\u8bba\u6587\u200b\u590d\u73b0\u200b\u4e2d\u200b\u770b\u5230\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200bPyTorch\u200b\u7684\u200b\u5185\u7f6e\u200bTransformer\u200b\u5c42\u200b\u6784\u5efa\u200bVision Transformer\u3002</p>"},{"location":"pytorch_cheatsheet/#rnn","title":"\u5faa\u73af\u200b\u5c42\u200b\uff08\u200b\u7528\u4e8e\u200b\u6784\u5efa\u200b\u5faa\u73af\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6216\u200bRNN\uff09\u00b6","text":"<p>PyTorch\u200b\u5185\u7f6e\u200b\u652f\u6301\u200b\u5faa\u73af\u200b\u795e\u7ecf\u200b\u7f51\u7edc\u5c42\u200b\uff0c\u200b\u4f8b\u5982\u200b\u957f\u77ed\u671f\u200b\u8bb0\u5fc6\u200b\uff08LSTM\uff09\u200b\u548c\u200b\u95e8\u63a7\u200b\u5faa\u73af\u200b\u5355\u5143\u200b\uff08GRU\uff09\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u901a\u5e38\u200b\u4f4d\u4e8e\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5c42\u200b\u4e0e\u200b\u5c42\u200b\u4e4b\u95f4\u200b\uff0c\u200b\u4e3a\u200b\u7ebf\u6027\u200b\uff08\u200b\u76f4\u7ebf\u200b\uff09\u200b\u51fd\u6570\u200b\u6dfb\u52a0\u200b\u975e\u7ebf\u6027\u200b\uff08\u200b\u975e\u200b\u76f4\u7ebf\u200b\uff09\u200b\u80fd\u529b\u200b\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u901a\u5e38\u200b\u7531\u200b\u5927\u91cf\u200b\u7684\u200b\u7ebf\u6027\u200b\u548c\u200b\u975e\u7ebf\u6027\u200b\u51fd\u6570\u200b\u7ec4\u6210\u200b\u3002</p> <p>PyTorch \u200b\u5728\u200b <code>torch.nn</code> \u200b\u4e2d\u200b\u5185\u7f6e\u200b\u4e86\u200b\u591a\u79cd\u200b\u975e\u7ebf\u6027\u200b\u6fc0\u6d3b\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u4e00\u4e9b\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li><code>nn.ReLU</code> - \u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u4fee\u6b63\u200b\u7ebf\u6027\u200b\u5355\u5143\u200b)\u3002</li> <li><code>nn.Sigmoid</code> - \u200b\u4e5f\u200b\u79f0\u4e3a\u200bSigmoid \u200b\u51fd\u6570\u200b\u3002</li> <li><code>nn.Softmax</code> - \u200b\u4e5f\u200b\u79f0\u4e3a\u200bSoftmax \u200b\u51fd\u6570\u200b\u3002</li> </ul> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u8bf7\u200b\u53c2\u89c1\u200b 02. PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b\u200b\u90e8\u5206\u200b 6\uff1a\u200b\u975e\u7ebf\u6027\u200b\uff0c\u200b\u7f3a\u5931\u200b\u7684\u200b\u4e00\u73af\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u635f\u5931\u200b\u51fd\u6570\u200b\u00b6","text":"<p>\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u8861\u91cf\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u6709\u200b\u591a\u200b\u9519\u8bef\u200b\u3002\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u7684\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u6b63\u786e\u200b\u503c\u200b\u76f8\u5dee\u200b\u591a\u8fdc\u200b\u3002</p> <p>\u200b\u7406\u60f3\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u901a\u8fc7\u200b\u8bad\u7ec3\u200b\u3001\u200b\u6570\u636e\u200b\u548c\u200b\u4f18\u5316\u200b\u51fd\u6570\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u635f\u5931\u200b\u503c\u4f1a\u200b\u5c3d\u53ef\u80fd\u200b\u964d\u4f4e\u200b\u3002</p> <p>\u200b\u5728\u200b PyTorch\uff08\u200b\u4ee5\u53ca\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e00\u822c\u200b\uff09\u200b\u4e2d\u200b\uff0c\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u4e5f\u200b\u5e38\u200b\u88ab\u200b\u79f0\u4e3a\u200b\uff1a\u200b\u51c6\u5219\u200b\u3001\u200b\u6210\u672c\u200b\u51fd\u6570\u200b\u3002</p> <p>PyTorch \u200b\u5728\u200b <code>torch.nn</code> \u200b\u4e2d\u200b\u5185\u7f6e\u200b\u4e86\u200b\u591a\u79cd\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5176\u4e2d\u200b\u4e00\u4e9b\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li><code>nn.L1Loss</code> - \u200b\u4e5f\u200b\u79f0\u4e3a\u200b MAE \u200b\u6216\u200b\u5e73\u5747\u200b\u7edd\u5bf9\u8bef\u5dee\u200b\uff08\u200b\u8fd9\u79cd\u200b\u635f\u5931\u200b\u5e38\u7528\u200b\u4e8e\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\u6216\u200b\u9884\u6d4b\u200b\u6570\u503c\u200b\uff0c\u200b\u5982\u200b\u623f\u4ef7\u200b\uff09\u3002</li> <li><code>nn.MSELoss</code> - \u200b\u4e5f\u200b\u79f0\u4e3a\u200b L2Loss \u200b\u6216\u5747\u65b9\u200b\u8bef\u5dee\u200b\uff08\u200b\u8fd9\u79cd\u200b\u635f\u5931\u200b\u5e38\u7528\u200b\u4e8e\u200b\u56de\u5f52\u200b\u95ee\u9898\u200b\u6216\u200b\u9884\u6d4b\u200b\u6570\u503c\u200b\uff0c\u200b\u5982\u200b\u623f\u4ef7\u200b\uff09\u3002</li> <li><code>nn.BCEWithLogitsLoss</code> - \u200b\u4e5f\u200b\u79f0\u4e3a\u200b\u4e8c\u5143\u200b\u4ea4\u53c9\u200b\u71b5\u200b\uff0c\u200b\u8fd9\u79cd\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5e38\u7528\u200b\u4e8e\u200b\u4e8c\u5143\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff08\u200b\u5c06\u200b\u4e8b\u7269\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u662f\u200b\u6216\u5426\u200b\uff09\u3002</li> <li><code>nn.CrossEntropyLoss</code> - \u200b\u8fd9\u79cd\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u5e38\u7528\u200b\u4e8e\u200b\u591a\u200b\u7c7b\u522b\u200b\u5206\u7c7b\u200b\u95ee\u9898\u200b\uff08\u200b\u5c06\u200b\u4e8b\u7269\u200b\u5206\u7c7b\u200b\u4e3a\u200b\u591a\u4e2a\u200b\u7c7b\u522b\u200b\u4e4b\u4e00\u200b\uff09\u3002</li> </ul>"},{"location":"pytorch_cheatsheet/","title":"\u4f18\u5316\u200b\u5668\u200b\u00b6","text":"<p>\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u5de5\u4f5c\u200b\u662f\u200b\u8c03\u6574\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u6743\u91cd\u200b\uff0c\u200b\u4ee5\u200b\u51cf\u5c11\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u7684\u200b\u503c\u200b\u3002</p> <p>PyTorch \u200b\u5728\u200b <code>torch.optim</code> \u200b\u6a21\u5757\u200b\u4e2d\u200b\u5185\u7f6e\u200b\u4e86\u200b\u591a\u79cd\u200b\u4f18\u5316\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u4e3b\u8981\u200b\u7684\u200b\u4f18\u5316\u200b\u5668\u200b\u51fd\u6570\u200b\u5305\u62ec\u200b\uff1a</p> <ul> <li><code>torch.optim.SGD(lr=0.1, params=model.parameters())</code> - SGD\uff0c\u200b\u5373\u200b\u968f\u673a\u200b\u68af\u5ea6\u200b\u4e0b\u964d\u200b\uff08<code>lr</code> \u200b\u8868\u793a\u200b\u201c\u200b\u5b66\u4e60\u200b\u7387\u200b\u201d\uff0c\u200b\u5373\u200b\u6bcf\u6b21\u200b\u8c03\u6574\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6743\u91cd\u200b\u7684\u200b\u500d\u6570\u200b\uff0c\u200b\u5c0f\u503c\u200b = \u200b\u5c0f\u200b\u8c03\u6574\u200b\uff0c\u200b\u5927\u200b\u503c\u200b = \u200b\u5927\u200b\u8c03\u6574\u200b\uff09\u3002</li> <li><code>torch.optim.Adam(lr=0.001, params=model.parameters())</code> - Adam \u200b\u4f18\u5316\u200b\u5668\u200b\uff08<code>params</code> \u200b\u8868\u793a\u200b\u201c\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b\u201d\uff0c\u200b\u5373\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u5e0c\u671b\u200b\u4f18\u5316\u200b\u51fd\u6570\u200b\u4f18\u5316\u200b\u7684\u200b\u6a21\u578b\u200b\u53c2\u6570\u200b/\u200b\u6743\u91cd\u200b\uff09\u3002</li> </ul>"},{"location":"pytorch_cheatsheet/","title":"\u7aef\u5230\u200b\u7aef\u200b\u793a\u4f8b\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u6574\u5408\u200b\u5230\u200b\u4e00\u4e2a\u200b\u5feb\u901f\u200b\u7684\u200b\u7aef\u200b\u5230\u200b\u7aef\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u4e2d\u200b\u3002</p> <p></p> <p>\u200b\u6b64\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u6458\u81ea\u200b 01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b\u3002</p>"},{"location":"pytorch_cheatsheet/","title":"\u521b\u5efa\u200b\u6570\u636e\u200b\u00b6","text":""},{"location":"pytorch_cheatsheet/","title":"\u521b\u5efa\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u521b\u5efa\u200b\u6a21\u578b\u200b\u7684\u200b\u4e24\u79cd\u200b\u4e3b\u8981\u200b\u65b9\u5f0f\u200b\uff1a</p> <ol> <li>\u200b\u5b50\u200b\u7c7b\u5316\u200b <code>torch.nn.Module</code> - \u200b\u4ee3\u7801\u200b\u8f83\u200b\u591a\u200b\u4f46\u200b\u975e\u5e38\u7075\u6d3b\u200b\uff0c\u200b\u5b50\u200b\u7c7b\u5316\u200b <code>torch.nn.Module</code> \u200b\u7684\u200b\u6a21\u578b\u200b\u5fc5\u987b\u200b\u5b9e\u73b0\u200b <code>forward()</code> \u200b\u65b9\u6cd5\u200b\u3002</li> <li>\u200b\u4f7f\u7528\u200b <code>torch.nn.Sequential</code> - \u200b\u4ee3\u7801\u200b\u8f83\u5c11\u200b\u4f46\u200b\u7075\u6d3b\u6027\u200b\u8f83\u200b\u4f4e\u200b\u3002</li> </ol>"},{"location":"pytorch_cheatsheet/","title":"\u8bbe\u7f6e\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u00b6","text":""},{"location":"pytorch_cheatsheet/","title":"\u521b\u5efa\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b\u76ee\u6807\u200b\u662f\u200b\u51cf\u5c11\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\uff08\u200b\u5373\u200b\u6a21\u578b\u200b\u7684\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u5b9e\u9645\u200b\u6570\u636e\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b/\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u5b9e\u73b0\u200b\u6b63\u786e\u200b\uff0c\u200b\u5e76\u4e14\u200b\u6a21\u578b\u200b\u80fd\u591f\u200b\u5b66\u4e60\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u635f\u5931\u200b\u5e94\u8be5\u200b\u4f1a\u200b\u4e0b\u964d\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff1a</p> <ul> <li>PyTorch\u200b\u4f18\u5316\u200b\u5faa\u73af\u200b\u6b4c\u66f2\u200b</li> <li>PyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b \u200b\u7b2c\u200b3\u200b\u8282\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b</li> <li>PyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b \u200b\u7b2c\u200b3\u200b\u8282\u200b\uff1a\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b</li> <li>PyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b \u200b\u7b2c\u200b6.3\u200b\u8282\u200b\uff1a\u200b\u8bad\u7ec3\u200b</li> </ul>"},{"location":"pytorch_cheatsheet/","title":"\u989d\u5916\u200b\u8d44\u6e90\u200b\u00b6","text":"<p>\u200b\u4e0a\u8ff0\u200b\u5217\u8868\u200b\u5e76\u200b\u4e0d\u200b\u8be6\u5c3d\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u662f\u200b\u4e00\u4e9b\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\u7684\u200b\u597d\u200b\u5730\u65b9\u200b\uff1a</p> <ul> <li>PyTorch \u200b\u5b98\u65b9\u200b\u901f\u67e5\u8868\u200b\u3002</li> <li>Zero to Mastery \u200b\u5b66\u4e60\u200b PyTorch \u200b\u8bfe\u7a0b\u200b - \u200b\u4e00\u4e2a\u200b\u5168\u9762\u200b\u4e14\u200b\u9002\u5408\u200b\u521d\u5b66\u8005\u200b\u7684\u200b\u6df1\u5165\u200b\u8bfe\u7a0b\u200b\uff0c\u200b\u4ece\u200b\u57fa\u7840\u77e5\u8bc6\u200b\u5230\u200b\u5c06\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\u5230\u200b\u73b0\u5b9e\u200b\u4e16\u754c\u200b\u4e2d\u4f9b\u200b\u4ed6\u4eba\u200b\u4f7f\u7528\u200b\u3002</li> <li>PyTorch \u200b\u6027\u80fd\u200b\u8c03\u4f18\u200b\u6307\u5357\u200b - PyTorch \u200b\u56e2\u961f\u200b\u63d0\u4f9b\u200b\u7684\u200b\u5173\u4e8e\u200b\u5982\u4f55\u200b\u8c03\u4f18\u200b PyTorch \u200b\u6a21\u578b\u200b\u6027\u80fd\u200b\u7684\u200b\u8d44\u6e90\u200b\u3002</li> <li>PyTorch \u200b\u989d\u5916\u200b\u8d44\u6e90\u200b - \u200b\u4e00\u4e2a\u200b\u7cbe\u5fc3\u200b\u6311\u9009\u200b\u7684\u200b\u6709\u52a9\u4e8e\u200b\u6269\u5c55\u200b PyTorch \u200b\u5e76\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\u65b9\u9762\u200b\u7684\u200b\u8d44\u6e90\u200b\u5217\u8868\u200b\u3002</li> <li>Effective PyTorch by vahidk - \u200b\u4e00\u4e2a\u200b GitHub \u200b\u4ed3\u5e93\u200b\uff0c\u200b\u4ee5\u200b\u76f4\u63a5\u200b\u7684\u200b\u65b9\u5f0f\u200b\u6982\u8ff0\u200b\u4e86\u200b PyTorch \u200b\u7684\u200b\u4e00\u4e9b\u200b\u4e3b\u8981\u200b\u529f\u80fd\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/","title":"PyTorch \u200b\u989d\u5916\u200b\u8d44\u6e90","text":"<p>\u200b\u5c3d\u7ba1\u200b\u300a\u200b\u96f6\u5230\u200b\u7cbe\u901a\u200b PyTorch\u300b\u200b\u8bfe\u7a0b\u200b\u603b\u65f6\u957f\u200b\u8d85\u8fc7\u200b 40 \u200b\u5c0f\u65f6\u200b\uff0c\u200b\u4f46\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u5728\u200b\u5b8c\u6210\u200b\u8bfe\u7a0b\u200b\u540e\u200b\u4ecd\u7136\u200b\u5145\u6ee1\u200b\u5b66\u4e60\u200b\u7684\u200b\u70ed\u60c5\u200b\u3002</p> <p>\u200b\u6bd5\u7adf\u200b\uff0c\u200b\u8fd9\u95e8\u200b\u8bfe\u7a0b\u200b\u662f\u200b\u6784\u5efa\u200b PyTorch \u200b\u52a8\u529b\u200b\u7684\u200b\u7edd\u4f73\u200b\u9014\u5f84\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u662f\u200b\u4e3a\u4e86\u200b\u6269\u5c55\u200b\u8bfe\u7a0b\u5185\u5bb9\u200b\u800c\u200b\u6536\u96c6\u200b\u7684\u200b\u3002</p> <p>\u200b\u4e0d\u8fc7\u200b\u8981\u200b\u63d0\u9192\u200b\u4e00\u4e0b\u200b\uff1a\u200b\u8fd9\u91cc\u200b\u7684\u200b\u5185\u5bb9\u200b\u975e\u5e38\u200b\u4e30\u5bcc\u200b\u3002</p> <p>\u200b\u6700\u597d\u200b\u662f\u4ece\u200b\u6bcf\u4e2a\u200b\u90e8\u5206\u200b\u4e2d\u200b\u9009\u62e9\u200b 1 \u200b\u5230\u200b 2 \u200b\u4e2a\u200b\u8d44\u6e90\u200b\uff08\u200b\u6216\u200b\u66f4\u5c11\u200b\uff09\u200b\u8fdb\u884c\u200b\u6df1\u5165\u200b\u63a2\u7d22\u200b\uff0c\u200b\u5176\u4f59\u200b\u7684\u200b\u53ef\u4ee5\u200b\u7559\u5f85\u200b\u4ee5\u540e\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u54ea\u200b\u4e00\u4e2a\u200b\u6700\u597d\u200b\u5462\u200b\uff1f</p> <p>\u200b\u55ef\u200b\uff0c\u200b\u5982\u679c\u200b\u5b83\u4eec\u200b\u80fd\u200b\u88ab\u200b\u5217\u5165\u200b\u8fd9\u4e2a\u200b\u6e05\u5355\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u8ba4\u4e3a\u200b\u5b83\u4eec\u200b\u90fd\u200b\u662f\u200b\u4f18\u8d28\u200b\u7684\u200b\u8d44\u6e90\u200b\u3002</p> <p>\u200b\u5927\u591a\u6570\u200b\u662f\u200b\u9488\u5bf9\u200b PyTorch \u200b\u7684\u200b\uff0c\u200b\u9002\u5408\u200b\u4f5c\u4e3a\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u5ef6\u4f38\u200b\uff0c\u200b\u4f46\u200b\u4e5f\u200b\u6709\u200b\u5c11\u6570\u200b\u4e0d\u662f\u200b\u4e13\u95e8\u200b\u9488\u5bf9\u200b PyTorch \u200b\u7684\u200b\uff0c\u200b\u4e0d\u8fc7\u200b\u5b83\u4eec\u200b\u5728\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u4ecd\u7136\u200b\u975e\u5e38\u200b\u6709\u200b\u4ef7\u503c\u200b\u3002</p>"},{"location":"pytorch_extra_resources/#pytorch_1","title":"\ud83d\udd25 \u200b\u7eaf\u200bPyTorch\u200b\u8d44\u6e90","text":"<ul> <li>PyTorch\u200b\u535a\u5ba2\u200b \u2014 \u200b\u4ece\u200b\u6e90\u5934\u200b\u4e0a\u200b\u4e86\u89e3\u200bPyTorch\u200b\u7684\u200b\u6700\u65b0\u200b\u52a8\u6001\u200b\u3002\u200b\u6211\u200b\u5927\u7ea6\u200b\u6bcf\u4e2a\u200b\u6708\u200b\u67e5\u770b\u200b\u4e00\u6b21\u200b\u535a\u5ba2\u200b\u66f4\u65b0\u200b\u3002</li> <li>PyTorch\u200b\u6587\u6863\u200b \u2014 \u200b\u6211\u4eec\u200b\u5c06\u200b\u5728\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\u591a\u6b21\u200b\u63a2\u7d22\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff0c\u200b\u4f46\u200b\u4ecd\u200b\u6709\u200b\u8bb8\u591a\u200b\u5185\u5bb9\u200b\u6211\u4eec\u200b\u672a\u66fe\u200b\u6d89\u53ca\u200b\u3002\u200b\u6ca1\u5173\u7cfb\u200b\uff0c\u200b\u7ecf\u5e38\u200b\u63a2\u7d22\u200b\u5e76\u200b\u5728\u200b\u5fc5\u8981\u200b\u65f6\u200b\u6df1\u5165\u200b\u4e86\u89e3\u200b\u3002</li> <li>PyTorch\u200b\u6027\u80fd\u200b\u8c03\u4f18\u200b\u6307\u5357\u200b \u2014 \u200b\u8bfe\u7a0b\u200b\u7ed3\u675f\u200b\u540e\u200b\uff0c\u200b\u60a8\u200b\u53ef\u80fd\u200b\u9996\u5148\u200b\u60f3\u200b\u505a\u200b\u7684\u200b\u5c31\u662f\u200b\u8ba9\u200b\u60a8\u200b\u7684\u200bPyTorch\u200b\u6a21\u578b\u200b\u66f4\u200b\u5feb\u200b\uff08\u200b\u8bad\u7ec3\u200b\u548c\u200b\u63a8\u7406\u200b\uff09\uff0cPyTorch\u200b\u6027\u80fd\u200b\u8c03\u4f18\u200b\u6307\u5357\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</li> <li>PyTorch\u200b\u98df\u8c31\u200b \u2014 PyTorch\u200b\u98df\u8c31\u200b\u662f\u200b\u4e00\u7cfb\u5217\u200b\u5c0f\u578b\u200b\u6559\u7a0b\u200b\uff0c\u200b\u5c55\u793a\u200b\u60a8\u200b\u53ef\u80fd\u200b\u60f3\u8981\u200b\u521b\u5efa\u200b\u7684\u200b\u5e38\u89c1\u200bPyTorch\u200b\u529f\u80fd\u200b\u548c\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\uff0c\u200b\u4f8b\u5982\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u548c\u200b\u5728\u200bPyTorch\u200b\u4e2d\u200b\u4fdd\u5b58\u200b\u548c\u200b\u52a0\u8f7d\u200b\u7528\u4e8e\u200b\u63a8\u7406\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> <li>PyTorch\u200b\u751f\u6001\u7cfb\u7edf\u200b - \u200b\u4e00\u7cfb\u5217\u200b\u57fa\u4e8e\u200b\u7eaf\u200bPyTorch\u200b\u6784\u5efa\u200b\u7684\u200b\u5de5\u5177\u200b\uff0c\u200b\u4e3a\u200b\u4e0d\u540c\u200b\u9886\u57df\u200b\u6dfb\u52a0\u200b\u4e13\u4e1a\u200b\u529f\u80fd\u200b\uff0c\u200b\u4ece\u200b\u7528\u4e8e\u200b3D\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200bPyTorch3D\u200b\u5230\u200b\u7528\u4e8e\u200b\u5feb\u901f\u200b\u6570\u636e\u200b\u589e\u5f3a\u200b\u7684\u200bAlbumentations\uff0c\u200b\u518d\u200b\u5230\u200b\u7528\u4e8e\u200b\u6a21\u578b\u200b\u8bc4\u4f30\u200b\u7684\u200bTorchMetrics\uff08\u200b\u611f\u8c22\u200bAlessandro\u200b\u7684\u200b\u63d0\u793a\u200b\uff09\u3002</li> <li>\u200b\u5728\u200bVSCode\u200b\u4e2d\u200b\u8bbe\u7f6e\u200bPyTorch \u2014 VSCode\u200b\u662f\u200b\u6700\u200b\u53d7\u6b22\u8fce\u200b\u7684\u200bIDE\u200b\u4e4b\u4e00\u200b\u3002\u200b\u5b83\u200b\u7684\u200bPyTorch\u200b\u652f\u6301\u200b\u8d8a\u6765\u8d8a\u200b\u597d\u200b\u3002\u200b\u5728\u200b\u6574\u4e2a\u200bZero to Mastery PyTorch\u200b\u8bfe\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200bGoogle Colab\u200b\u662f\u56e0\u4e3a\u200b\u5b83\u200b\u7684\u200b\u6613\u7528\u6027\u200b\u3002\u200b\u4f46\u200b\u5f88\u200b\u53ef\u80fd\u200b\u60a8\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u5728\u200bVSCode\u200b\u8fd9\u6837\u200b\u7684\u200bIDE\u200b\u4e2d\u200b\u8fdb\u884c\u200b\u5f00\u53d1\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/#pytorch_2","title":"\ud83d\udcc8\u00a0\u200b\u8ba9\u200b\u7eaf\u200bPyTorch\u200b\u66f4\u200b\u5f3a\u5927\u200b/\u200b\u589e\u52a0\u200b\u529f\u80fd\u200b\u7684\u200b\u5e93","text":"<p>\u200b\u672c\u200b\u8bfe\u7a0b\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u7eaf\u200bPyTorch\uff08\u200b\u4f7f\u7528\u200b\u6700\u5c11\u200b\u7684\u200b\u5916\u90e8\u200b\u5e93\u200b\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u5982\u679c\u200b\u4f60\u200b\u77e5\u9053\u200b\u5982\u4f55\u200b\u7f16\u5199\u200b\u7eaf\u200bPyTorch\uff0c\u200b\u4f60\u200b\u5c31\u200b\u80fd\u200b\u5b66\u4f1a\u200b\u4f7f\u7528\u200b\u5404\u79cd\u200b\u6269\u5c55\u200b\u5e93\u200b\u3002</p> <ul> <li>fast.ai \u2014 fastai\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5f00\u6e90\u200b\u5e93\u200b\uff0c\u200b\u8d1f\u8d23\u200b\u5904\u7406\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u8bb8\u591a\u200b\u7e41\u7410\u200b\u90e8\u5206\u200b\uff0c\u200b\u5e76\u200b\u4f7f\u200b\u521b\u5efa\u200b\u6700\u200b\u5148\u8fdb\u200b\u7684\u200b\u6a21\u578b\u200b\u53ea\u200b\u9700\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u6210\u4e3a\u200b\u53ef\u80fd\u200b\u3002\u200b\u4ed6\u4eec\u200b\u7684\u200b\u514d\u8d39\u200b\u5e93\u200b\u3001\u200b\u8bfe\u7a0b\u200b\u548c\u200b\u6587\u6863\u200b\u90fd\u200b\u662f\u200b\u4e16\u754c\u7ea7\u200b\u7684\u200b\u3002</li> <li>MosaicML \u200b\u63d0\u9ad8\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6548\u7387\u200b \u2014 \u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u7684\u200b\u901f\u5ea6\u200b\u8d8a\u200b\u5feb\u200b\uff0c\u200b\u4f60\u200b\u5c31\u200b\u80fd\u200b\u8d8a\u200b\u5feb\u200b\u5730\u200b\u627e\u51fa\u200b\u6709\u6548\u200b\u548c\u200b\u65e0\u6548\u200b\u7684\u200b\u65b9\u6cd5\u200b\u3002MosaicML\u200b\u7684\u200b\u5f00\u6e90\u200b<code>Composer</code>\u200b\u5e93\u200b\u901a\u8fc7\u200b\u5728\u200b\u540e\u53f0\u200b\u5b9e\u73b0\u200b\u52a0\u901f\u200b\u7b97\u6cd5\u200b\uff0c\u200b\u5e2e\u52a9\u200b\u4f60\u200b\u7528\u200bPyTorch\u200b\u66f4\u5feb\u200b\u5730\u200b\u8bad\u7ec3\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u66f4\u5feb\u200b\u5730\u200b\u4ece\u200b\u73b0\u6709\u200b\u7684\u200bPyTorch\u200b\u6a21\u578b\u200b\u4e2d\u200b\u83b7\u5f97\u200b\u66f4\u597d\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002\u200b\u4ed6\u4eec\u200b\u7684\u200b\u6240\u6709\u200b\u4ee3\u7801\u200b\u90fd\u200b\u662f\u200b\u5f00\u6e90\u200b\u7684\u200b\uff0c\u200b\u6587\u6863\u200b\u4e5f\u200b\u975e\u5e38\u200b\u51fa\u8272\u200b\u3002</li> <li>PyTorch Lightning \u200b\u51cf\u5c11\u200b\u6837\u677f\u200b\u4ee3\u7801\u200b \u2014 PyTorch Lightning\u200b\u8d1f\u8d23\u200b\u5904\u7406\u200b\u8bb8\u591a\u200b\u5728\u200b\u7eaf\u200bPyTorch\u200b\u4e2d\u200b\u7ecf\u5e38\u200b\u9700\u8981\u200b\u624b\u52a8\u200b\u5b8c\u6210\u200b\u7684\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u4f8b\u5982\u200b\u7f16\u5199\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u5faa\u73af\u200b\u3001\u200b\u6a21\u578b\u200b\u68c0\u67e5\u70b9\u200b\u3001\u200b\u65e5\u5fd7\u200b\u8bb0\u5f55\u200b\u7b49\u200b\u3002PyTorch Lightning\u200b\u5728\u200bPyTorch\u200b\u7684\u200b\u57fa\u7840\u200b\u4e0a\u200b\u6784\u5efa\u200b\uff0c\u200b\u5141\u8bb8\u200b\u4f60\u200b\u7528\u200b\u66f4\u200b\u5c11\u200b\u7684\u200b\u4ee3\u7801\u200b\u5236\u4f5c\u200bPyTorch\u200b\u6a21\u578b\u200b\u3002</li> </ul> <p></p> <p>\u200b\u6269\u5c55\u200b/\u200b\u589e\u5f3a\u200b\u7eaf\u200bPyTorch\u200b\u7684\u200b\u5e93\u200b\u3002</p>"},{"location":"pytorch_extra_resources/#pytorch_3","title":"\ud83d\udcd6 PyTorch \u200b\u4e66\u7c4d\u200b\u63a8\u8350","text":"<ul> <li>\u200b\u4f7f\u7528\u200b PyTorch \u200b\u548c\u200b Scikit-Learn \u200b\u8fdb\u884c\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff1a\u200b\u901a\u8fc7\u200b Sebastian Raschka \u200b\u7f16\u5199\u200b\u7684\u200b Python \u200b\u5f00\u53d1\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b \u2014 \u200b\u4e00\u672c\u200b\u6781\u4f73\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5165\u95e8\u200b\u4e66\u7c4d\u200b\u3002\u200b\u4ece\u200b\u4f7f\u7528\u200b Scikit-Learn \u200b\u8fdb\u884c\u200b\u4f20\u7edf\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7b97\u6cd5\u200b\u5f00\u59cb\u200b\uff0c\u200b\u89e3\u51b3\u200b\u7ed3\u6784\u5316\u200b\u6570\u636e\u200b\uff08\u200b\u8868\u683c\u200b\u6216\u884c\u200b\u548c\u200b\u5217\u200b\u6216\u200b Excel \u200b\u98ce\u683c\u200b\uff09\u200b\u95ee\u9898\u200b\uff0c\u200b\u7136\u540e\u200b\u5207\u6362\u200b\u5230\u200b\u5982\u4f55\u200b\u4f7f\u7528\u200b PyTorch \u200b\u8fdb\u884c\u200b\u975e\u200b\u7ed3\u6784\u5316\u200b\u6570\u636e\u200b\uff08\u200b\u5982\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u548c\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\uff09\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u3002</li> <li>Daniel Voigt Godoy \u200b\u7684\u200b PyTorch \u200b\u9010\u6b65\u200b\u7cfb\u5217\u200b \u2014 \u200b\u4e0e\u200b Zero to Mastery PyTorch \u200b\u8bfe\u7a0b\u200b\u4ece\u200b\u4ee3\u7801\u200b\u4f18\u5148\u200b\u7684\u200b\u89d2\u5ea6\u200b\u4e0d\u540c\u200b\uff0c\u200b\u9010\u6b65\u200b\u7cfb\u5217\u200b\u4ece\u200b\u6982\u5ff5\u200b\u4f18\u5148\u200b\u7684\u200b\u89d2\u5ea6\u200b\u6db5\u76d6\u200b PyTorch \u200b\u548c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\uff0c\u200b\u5e76\u200b\u9644\u6709\u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b\u3002\u200b\u8be5\u200b\u7cfb\u5217\u200b\u6709\u4e09\u7248\u200b\uff0c\u200b\u5206\u522b\u200b\u662f\u200b\u57fa\u7840\u200b\u3001\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u548c\u200b\u5e8f\u5217\u200b\uff08NLP\uff09\uff0c\u200b\u662f\u200b\u6211\u200b\u6700\u200b\u559c\u6b22\u200b\u7684\u200b\u4ece\u96f6\u5f00\u59cb\u200b\u5b66\u4e60\u200b PyTorch \u200b\u7684\u200b\u8d44\u6e90\u200b\u4e4b\u4e00\u200b\u3002</li> <li>\u200b\u6df1\u5165\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e66\u7c4d\u200b \u2014 \u200b\u53ef\u80fd\u200b\u662f\u200b\u4e92\u8054\u7f51\u200b\u4e0a\u200b\u6700\u200b\u5168\u9762\u200b\u7684\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u6982\u5ff5\u200b\u8d44\u6e90\u200b\uff0c\u200b\u9644\u6709\u200b PyTorch\u3001TensorFlow \u200b\u548c\u200b Gluon \u200b\u7684\u200b\u4ee3\u7801\u200b\u793a\u4f8b\u200b\u3002\u200b\u800c\u4e14\u200b\u5168\u90e8\u200b\u514d\u8d39\u200b\uff01\u200b\u4f8b\u5982\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u67e5\u770b\u200b\u4f5c\u8005\u200b\u5bf9\u200b\u6211\u4eec\u200b\u5728\u200b 08. PyTorch \u200b\u8bba\u6587\u200b\u590d\u73b0\u200b \u200b\u4e2d\u200b\u6d89\u53ca\u200b\u7684\u200b \u200b\u89c6\u89c9\u200b\u53d8\u6362\u5668\u200b \u200b\u7684\u200b\u89e3\u91ca\u200b\u3002</li> <li>\u200b\u989d\u5916\u200b\u63a8\u8350\u200b\uff1a fast.ai \u200b\u8bfe\u7a0b\u200b\uff08\u200b\u514d\u8d39\u200b\u5728\u7ebf\u200b\u63d0\u4f9b\u200b\uff09\u200b\u4e5f\u200b\u6709\u200b\u4e00\u672c\u200b\u514d\u8d39\u200b\u5728\u7ebf\u200b\u4e66\u7c4d\u200b\uff0c\u200b\u4f7f\u7528\u200b fastai \u200b\u548c\u200b PyTorch \u200b\u8fdb\u884c\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u3002</li> </ul> <p>\u200b\u5b66\u4e60\u200b PyTorch \u200b\u4ee5\u53ca\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u4e00\u822c\u200b\u77e5\u8bc6\u200b\u7684\u200b\u6559\u79d1\u4e66\u200b\u3002</p>"},{"location":"pytorch_extra_resources/#_1","title":"\ud83c\udfd7 \u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u4e0e\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\u8d44\u6e90","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\uff08\u200b\u4e5f\u200b\u79f0\u4e3a\u200b MLOps \u200b\u6216\u200b ML \u200b\u64cd\u4f5c\u200b\uff09\u200b\u662f\u200b\u5c06\u200b\u60a8\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u4ea4\u4ed8\u7ed9\u200b\u4ed6\u4eba\u200b\u7684\u200b\u5b9e\u8df5\u200b\u3002\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u610f\u5473\u7740\u200b\u901a\u8fc7\u200b\u516c\u5171\u200b\u5e94\u7528\u7a0b\u5e8f\u200b\u6216\u200b\u5e55\u540e\u200b\u5de5\u4f5c\u200b\u6765\u200b\u505a\u51fa\u200b\u5546\u4e1a\u200b\u51b3\u7b56\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u5c06\u200b\u5e2e\u52a9\u200b\u60a8\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u90e8\u7f72\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u6b65\u9aa4\u200b\u3002</p> <ul> <li>Chip Huyen \u200b\u7684\u200b\u300a\u200b\u8bbe\u8ba1\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u7cfb\u7edf\u200b\u300b \u2014 \u200b\u5982\u679c\u200b\u60a8\u200b\u60f3\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b ML \u200b\u7cfb\u7edf\u200b\uff0c\u200b\u4e86\u89e3\u200b\u5176\u4ed6\u4eba\u200b\u5982\u4f55\u200b\u505a\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u4f1a\u200b\u5f88\u200b\u6709\u200b\u5e2e\u52a9\u200b\u3002Chip \u200b\u7684\u200b\u4e66\u200b\u8f83\u200b\u5c11\u200b\u5173\u6ce8\u200b\u6784\u5efa\u200b\u5355\u4e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\uff08\u200b\u5c3d\u7ba1\u200b\u4e66\u4e2d\u200b\u6709\u200b\u5f88\u591a\u200b\u5173\u4e8e\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u5185\u5bb9\u200b\uff09\uff0c\u200b\u800c\u662f\u200b\u5173\u6ce8\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u8fde\u8d2f\u200b\u7684\u200b ML \u200b\u7cfb\u7edf\u200b\u3002\u200b\u5b83\u200b\u6db5\u76d6\u200b\u4e86\u200b\u4ece\u200b\u6570\u636e\u200b\u5de5\u7a0b\u200b\u5230\u200b\u6a21\u578b\u200b\u6784\u5efa\u200b\u3001\u200b\u6a21\u578b\u200b\u90e8\u7f72\u200b\uff08\u200b\u5728\u7ebf\u200b\u548c\u200b\u79bb\u7ebf\u200b\uff09\u200b\u5230\u200b\u6a21\u578b\u200b\u76d1\u63a7\u200b\u7684\u200b\u6240\u6709\u200b\u5185\u5bb9\u200b\u3002\u200b\u66f4\u68d2\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u8fd9\u200b\u672c\u4e66\u200b\u8bfb\u200b\u8d77\u6765\u200b\u5f88\u200b\u6109\u5feb\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u770b\u51fa\u200b\u8fd9\u200b\u672c\u4e66\u200b\u662f\u200b\u7531\u200b\u4e00\u4f4d\u200b\u4f5c\u5bb6\u200b\u5199\u200b\u7684\u200b\uff08Chip \u200b\u4e4b\u524d\u200b\u66fe\u200b\u5199\u200b\u8fc7\u200b\u51e0\u672c\u4e66\u200b\uff09\u3002</li> <li>Goku Mohandas \u200b\u7684\u200b Made With ML \u2014 \u200b\u6bcf\u5f53\u200b\u6211\u200b\u60f3\u8981\u200b\u5b66\u4e60\u200b\u6216\u200b\u53c2\u8003\u200b\u4e0e\u200b MLOps \u200b\u76f8\u5173\u200b\u7684\u200b\u5185\u5bb9\u200b\u65f6\u200b\uff0c\u200b\u6211\u200b\u90fd\u200b\u4f1a\u200b\u53bb\u200b madewithml.com/mlops \u200b\u770b\u770b\u200b\u662f\u5426\u200b\u6709\u200b\u76f8\u5173\u200b\u7684\u200b\u8bfe\u7a0b\u200b\u3002Made With ML \u200b\u4e0d\u4ec5\u200b\u6559\u200b\u4f60\u200b\u8bb8\u591a\u200b\u4e0d\u540c\u200b ML \u200b\u6a21\u578b\u200b\u7684\u200b\u57fa\u7840\u77e5\u8bc6\u200b\uff0c\u200b\u8fd8\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u7aef\u5230\u200b\u7aef\u7684\u200b ML \u200b\u7cfb\u7edf\u200b\uff0c\u200b\u5e76\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u5927\u91cf\u200b\u7684\u200b\u4ee3\u7801\u200b\u548c\u200b\u5de5\u5177\u200b\u793a\u4f8b\u200b\u3002</li> <li>Andriy Burkov \u200b\u7684\u200b\u300a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\u300b \u2014 \u200b\u5c3d\u7ba1\u200b\u8fd9\u200b\u672c\u4e66\u200b\u53ef\u4ee5\u200b\u5728\u7ebf\u200b\u514d\u8d39\u200b\u9605\u8bfb\u200b\uff0c\u200b\u4f46\u200b\u6211\u200b\u4e00\u200b\u51fa\u7248\u200b\u5c31\u200b\u4e70\u200b\u4e86\u200b\u3002\u200b\u6211\u200b\u591a\u6b21\u200b\u5c06\u200b\u5176\u200b\u4f5c\u4e3a\u200b\u53c2\u8003\u8d44\u6599\u200b\u548c\u200b\u5b66\u4e60\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b ML \u200b\u5de5\u7a0b\u200b\u7684\u200b\u5185\u5bb9\u200b\uff0c\u200b\u5b83\u200b\u57fa\u672c\u4e0a\u200b\u4e00\u76f4\u200b\u653e\u5728\u200b\u6211\u200b\u7684\u200b\u684c\u4e0a\u200b\u6216\u200b\u89e6\u624b\u53ef\u53ca\u200b\u7684\u200b\u5730\u65b9\u200b\u3002Burkov \u200b\u5f88\u200b\u597d\u200b\u5730\u200b\u6293\u4f4f\u200b\u4e86\u200b\u91cd\u70b9\u200b\uff0c\u200b\u5e76\u200b\u5728\u200b\u5fc5\u8981\u200b\u65f6\u200b\u5f15\u7528\u200b\u4e86\u200b\u8fdb\u4e00\u6b65\u200b\u7684\u200b\u6750\u6599\u200b\u3002</li> <li>Full Stack Deep Learning \u200b\u8bfe\u7a0b\u200b \u2014 \u200b\u6211\u200b\u7b2c\u4e00\u6b21\u200b\u53c2\u52a0\u200b\u8fd9\u4e2a\u200b\u8bfe\u7a0b\u200b\u662f\u200b\u5728\u200b 2021 \u200b\u5e74\u200b\u3002\u200b\u5b83\u200b\u4e0d\u65ad\u200b\u53d1\u5c55\u200b\uff0c\u200b\u6db5\u76d6\u200b\u4e86\u200b\u8be5\u200b\u9886\u57df\u200b\u6700\u65b0\u200b\u7684\u200b\u6700\u4f73\u200b\u5de5\u5177\u200b\u3002\u200b\u5b83\u200b\u5c06\u200b\u6559\u200b\u4f60\u200b\u5982\u4f55\u200b\u89c4\u5212\u200b\u4e00\u4e2a\u200b\u89e3\u51b3\u200b ML \u200b\u95ee\u9898\u200b\u7684\u200b\u9879\u76ee\u200b\uff0c\u200b\u5982\u4f55\u200b\u83b7\u53d6\u200b\u6216\u200b\u521b\u5efa\u200b\u6570\u636e\u200b\uff0c\u200b\u5982\u4f55\u200b\u5728\u200b ML \u200b\u9879\u76ee\u200b\u51fa\u9519\u200b\u65f6\u200b\u8fdb\u884c\u200b\u6545\u969c\u200b\u6392\u9664\u200b\uff0c\u200b\u6700\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u5982\u4f55\u200b\u6784\u5efa\u200b ML \u200b\u9a71\u52a8\u200b\u7684\u200b\u4ea7\u54c1\u200b\u3002</li> </ul> <p></p> <p>\u200b\u63d0\u5347\u200b\u60a8\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5de5\u7a0b\u200b\u6280\u80fd\u200b\u7684\u200b\u8d44\u6e90\u200b\uff08\u200b\u56f4\u7ed5\u200b\u6784\u5efa\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6a21\u578b\u200b\u7684\u200b\u6240\u6709\u200b\u6b65\u9aa4\u200b\uff09\u3002</p>"},{"location":"pytorch_extra_resources/#_2","title":"\ud83d\uddc3 \u200b\u5982\u4f55\u200b\u627e\u5230\u200b\u6570\u636e\u200b\u96c6","text":"<p>\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9879\u76ee\u200b\u59cb\u4e8e\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u6ca1\u6709\u200b\u6570\u636e\u200b\uff0c\u200b\u5c31\u200b\u6ca1\u6709\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u662f\u200b\u5bfb\u627e\u200b\u5404\u79cd\u200b\u4e3b\u9898\u200b\u548c\u200b\u95ee\u9898\u200b\u9886\u57df\u200b\u4e2d\u200b\u5f00\u6e90\u200b\u4e14\u200b\u901a\u5e38\u200b\u53ef\u200b\u76f4\u63a5\u200b\u4f7f\u7528\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u6700\u4f73\u200b\u9009\u62e9\u200b\u4e4b\u4e00\u200b\u3002</p> <ul> <li>Paperswithcode \u200b\u6570\u636e\u200b\u96c6\u200b \u2014 \u200b\u641c\u7d22\u200b\u6700\u200b\u5e38\u7528\u200b\u548c\u200b\u5e38\u89c1\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u57fa\u51c6\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u4e86\u89e3\u200b\u5b83\u4eec\u200b\u5305\u542b\u200b\u7684\u200b\u5185\u5bb9\u200b\u3001\u200b\u6765\u6e90\u200b\u4ee5\u53ca\u200b\u53ef\u200b\u627e\u5230\u200b\u7684\u200b\u4f4d\u7f6e\u200b\u3002\u200b\u901a\u5e38\u200b\u8fd8\u200b\u80fd\u200b\u770b\u5230\u200b\u6bcf\u4e2a\u200b\u6570\u636e\u200b\u96c6\u4e0a\u200b\u8868\u73b0\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</li> <li>HuggingFace \u200b\u6570\u636e\u200b\u96c6\u200b \u2014 \u200b\u4e0d\u4ec5\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8de8\u200b\u5e7f\u6cdb\u200b\u95ee\u9898\u200b\u9886\u57df\u200b\u67e5\u627e\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u8d44\u6e90\u200b\uff0c\u200b\u8fd8\u662f\u200b\u4e00\u4e2a\u200b\u5e93\u200b\uff0c\u200b\u53ef\u200b\u7528\u4e8e\u200b\u51e0\u884c\u200b\u4ee3\u7801\u200b\u5185\u200b\u4e0b\u8f7d\u200b\u5e76\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\u8fd9\u4e9b\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</li> <li>Kaggle \u200b\u6570\u636e\u200b\u96c6\u200b \u2014 \u200b\u627e\u5230\u200b\u901a\u5e38\u200b\u4f34\u968f\u200b Kaggle \u200b\u7ade\u8d5b\u200b\u7684\u200b\u5404\u79cd\u200b\u6570\u636e\u200b\u96c6\u200b\uff0c\u200b\u5176\u4e2d\u200b\u8bb8\u591a\u200b\u76f4\u63a5\u200b\u6765\u81ea\u200b\u884c\u4e1a\u200b\u3002</li> <li>Google \u200b\u6570\u636e\u200b\u96c6\u200b\u641c\u7d22\u200b \u2014 \u200b\u5c31\u200b\u50cf\u200b\u4f7f\u7528\u200b Google \u200b\u641c\u7d22\u200b\u4e00\u6837\u200b\uff0c\u200b\u4f46\u200b\u4e13\u95e8\u200b\u9488\u5bf9\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</li> </ul> <p>\u200b\u8fd9\u4e9b\u200b\u8d44\u6e90\u200b\u5e94\u8be5\u200b\u8db3\u591f\u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\uff0c\u200b\u4f46\u200b\u5bf9\u4e8e\u200b\u7279\u5b9a\u200b\u7684\u200b\u5177\u4f53\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u9700\u8981\u200b\u6784\u5efa\u200b\u81ea\u5df1\u200b\u7684\u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p> <p></p> <p>\u200b\u5404\u79cd\u200b\u95ee\u9898\u200b\u9886\u57df\u200b\u4e2d\u200b\u73b0\u6709\u200b\u548c\u200b\u5f00\u6e90\u200b\u6570\u636e\u200b\u96c6\u200b\u7684\u200b\u5bfb\u627e\u200b\u5730\u70b9\u200b\u3002</p>"},{"location":"pytorch_extra_resources/#_3","title":"\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u7684\u200b\u5de5\u5177","text":"<p>\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u7279\u5b9a\u200b\u95ee\u9898\u200b\u9886\u57df\u200b\u7684\u200b\u5e93\u200b\u548c\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff0c\u200b\u5982\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u548c\u200b\u63a8\u8350\u200b\u5f15\u64ce\u200b/\u200b\u7cfb\u7edf\u200b\u3002</p>"},{"location":"pytorch_extra_resources/#_4","title":"\ud83d\ude0e \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b 03. PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b \u200b\u4e2d\u200b\u4ecb\u7ecd\u200b\u4e86\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\uff0c\u200b\u4f46\u200b\u4f5c\u4e3a\u200b\u5feb\u901f\u200b\u56de\u987e\u200b\uff0c\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u662f\u200b\u8ba9\u200b\u8ba1\u7b97\u673a\u200b\u201c\u200b\u770b\u200b\u201d\u200b\u7684\u200b\u827a\u672f\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u89c6\u89c9\u200b\u7684\u200b\uff0c\u200b\u5982\u200b\u56fe\u50cf\u200b\u3001X\u200b\u5149\u7247\u200b\u3001\u200b\u751f\u4ea7\u7ebf\u200b\u89c6\u9891\u200b\u751a\u81f3\u200b\u624b\u5199\u200b\u6587\u6863\u200b\uff0c\u200b\u90a3\u4e48\u200b\u8fd9\u200b\u53ef\u80fd\u200b\u662f\u200b\u4e00\u4e2a\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u95ee\u9898\u200b\u3002</p> <ul> <li>TorchVision \u2014 PyTorch \u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u200b\u3002\u200b\u627e\u5230\u200b\u8bb8\u591a\u200b\u52a0\u8f7d\u200b\u89c6\u89c9\u200b\u6570\u636e\u200b\u7684\u200b\u65b9\u6cd5\u200b\u4ee5\u53ca\u200b\u8bb8\u591a\u200b\u53ef\u200b\u7528\u4e8e\u200b\u81ea\u5df1\u200b\u95ee\u9898\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</li> <li>timm (Torch Image Models) \u200b\u5e93\u200b \u2014 \u200b\u6700\u200b\u5168\u9762\u200b\u7684\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u5e93\u548c\u9884\u200b\u8bad\u7ec3\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u8d44\u6e90\u200b\u4e4b\u4e00\u200b\u3002\u200b\u51e0\u4e4e\u200b\u6240\u6709\u200b\u4f7f\u7528\u200b PyTorch \u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u7684\u200b\u65b0\u200b\u7814\u7a76\u200b\u90fd\u200b\u5728\u200b\u67d0\u79cd\u7a0b\u5ea6\u200b\u4e0a\u200b\u5229\u7528\u200b\u4e86\u200b <code>timm</code> \u200b\u5e93\u200b\u3002</li> <li>Yolov5 \u200b\u7528\u4e8e\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b \u2014 \u200b\u5982\u679c\u200b\u4f60\u200b\u60f3\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\u6784\u5efa\u200b\u76ee\u6807\u200b\u68c0\u6d4b\u200b\u6a21\u578b\u200b\uff0c<code>yolov5</code> GitHub \u200b\u4ed3\u5e93\u200b\u53ef\u80fd\u200b\u662f\u200b\u5feb\u901f\u200b\u5165\u95e8\u200b\u7684\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u3002</li> <li>VISSL (Vision Self-Supervised Learning) \u200b\u5e93\u200b \u2014 \u200b\u81ea\u200b\u76d1\u7763\u200b\u5b66\u4e60\u200b\u662f\u200b\u8ba9\u200b\u6570\u636e\u200b\u81ea\u5df1\u200b\u5b66\u4e60\u200b\u6a21\u5f0f\u200b\u7684\u200b\u827a\u672f\u200b\u3002\u200b\u4e0e\u200b\u63d0\u4f9b\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u6807\u7b7e\u200b\u5e76\u200b\u5b66\u4e60\u200b\u8868\u793a\u200b\u4e0d\u540c\u200b\uff0c\u200b\u81ea\u200b\u76d1\u7763\u200b\u5b66\u4e60\u200b\u8bd5\u56fe\u200b\u5728\u200b\u6ca1\u6709\u200b\u6807\u7b7e\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\u590d\u5236\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u7ed3\u679c\u200b\u3002VISSL \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u79cd\u200b\u6613\u4e8e\u200b\u4f7f\u7528\u200b\u7684\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u901a\u8fc7\u200b PyTorch \u200b\u5f00\u59cb\u200b\u4f7f\u7528\u200b\u81ea\u200b\u76d1\u7763\u200b\u5b66\u4e60\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u6a21\u578b\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/#nlp","title":"\ud83d\udcda \u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b (NLP)","text":"<p>\u200b\u81ea\u7136\u8bed\u8a00\u200b\u5904\u7406\u200b\u6d89\u53ca\u200b\u5728\u200b\u6587\u672c\u200b\u4e2d\u200b\u5bfb\u627e\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u53ef\u80fd\u200b\u60f3\u8981\u200b\u4ece\u200b\u652f\u6301\u200b\u5de5\u5355\u200b\u4e2d\u200b\u63d0\u53d6\u200b\u91cd\u8981\u200b\u5b9e\u4f53\u200b\u6216\u200b\u5c06\u200b\u6587\u6863\u200b\u5206\u7c7b\u200b\u5230\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u4e2d\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u6d89\u53ca\u200b\u5927\u91cf\u200b\u6587\u672c\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u67e5\u770b\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u3002</p> <ul> <li>TorchText \u2014 PyTorch \u200b\u5185\u7f6e\u200b\u7684\u200b\u6587\u672c\u200b\u9886\u57df\u200b\u5e93\u200b\u3002\u200b\u4e0e\u200b TorchVision \u200b\u7c7b\u4f3c\u200b\uff0c\u200b\u5b83\u200b\u5305\u542b\u200b\u8bb8\u591a\u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u52a0\u8f7d\u200b\u6570\u636e\u200b\u548c\u200b\u4e00\u7cfb\u5217\u200b\u53ef\u200b\u9002\u5e94\u200b\u81ea\u5df1\u200b\u95ee\u9898\u200b\u7684\u200b\u9884\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</li> <li>HuggingFace Transformers \u200b\u5e93\u200b \u2014 HuggingFace Transformers \u200b\u5e93\u200b\u5728\u200b GitHub \u200b\u4e0a\u200b\u7684\u200b\u661f\u6570\u200b\u6bd4\u200b PyTorch \u200b\u5e93\u200b\u672c\u8eab\u200b\u8fd8\u8981\u200b\u591a\u200b\u3002\u200b\u8fd9\u6709\u200b\u5176\u200b\u539f\u56e0\u200b\u3002\u200b\u5e76\u200b\u4e0d\u662f\u200b\u8bf4\u200b HuggingFace Transformers \u200b\u6bd4\u200b PyTorch \u200b\u66f4\u597d\u200b\uff0c\u200b\u800c\u662f\u200b\u56e0\u4e3a\u200b\u5b83\u200b\u5728\u200b\u5b83\u200b\u6240\u200b\u505a\u200b\u7684\u200b\u4e8b\u60c5\u200b\u4e0a\u200b\u505a\u200b\u5f97\u200b\u6700\u597d\u200b\uff1a\u200b\u4e3a\u200b NLP\uff08\u200b\u4ee5\u53ca\u200b\u66f4\u200b\u591a\u200b\uff09\u200b\u63d0\u4f9b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u548c\u9884\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6700\u65b0\u200b\u6a21\u578b\u200b\u3002</li> <li>\u200b\u989d\u5916\u200b\u63d0\u793a\u200b\uff1a \u200b\u60f3\u8981\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b HuggingFace Transformers \u200b\u5e93\u200b\u53ca\u5176\u200b\u5468\u8fb9\u200b\u7684\u200b\u4e00\u5207\u200b\uff0cHuggingFace \u200b\u56e2\u961f\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u514d\u8d39\u200b\u5728\u7ebf\u200b\u8bfe\u7a0b\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/#_5","title":"\ud83c\udfa4 \u200b\u8bed\u97f3\u200b\u548c\u200b\u97f3\u9891","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u95ee\u9898\u200b\u6d89\u53ca\u200b\u97f3\u9891\u6587\u4ef6\u200b\u6216\u200b\u8bed\u97f3\u200b\u6570\u636e\u200b\uff0c\u200b\u5982\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u58f0\u97f3\u200b\u8fdb\u884c\u200b\u5206\u7c7b\u200b\u6216\u200b\u5c06\u200b\u8bed\u97f3\u200b\u8f6c\u5f55\u200b\u4e3a\u200b\u6587\u672c\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u67e5\u770b\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u3002</p> <ul> <li>TorchAudio \u2014 PyTorch \u200b\u7684\u200b\u97f3\u9891\u200b\u9886\u57df\u200b\u5e93\u200b\u3002\u200b\u627e\u5230\u200b\u5185\u7f6e\u200b\u7684\u200b\u65b9\u6cd5\u200b\u6765\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u548c\u200b\u9884\u200b\u6784\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u67b6\u6784\u200b\u6765\u200b\u5bfb\u627e\u200b\u97f3\u9891\u200b\u6570\u636e\u200b\u4e2d\u200b\u7684\u200b\u6a21\u5f0f\u200b\u3002</li> <li>SpeechBrain \u2014 \u200b\u4e00\u4e2a\u200b\u57fa\u4e8e\u200b PyTorch \u200b\u7684\u200b\u5f00\u6e90\u200b\u5e93\u200b\uff0c\u200b\u7528\u4e8e\u200b\u5904\u7406\u200b\u8bed\u97f3\u200b\u95ee\u9898\u200b\uff0c\u200b\u5982\u200b\u8bc6\u522b\u200b\uff08\u200b\u5c06\u200b\u8bed\u97f3\u200b\u8f6c\u4e3a\u200b\u6587\u672c\u200b\uff09\u3001\u200b\u8bed\u97f3\u200b\u589e\u5f3a\u200b\u3001\u200b\u8bed\u97f3\u200b\u5904\u7406\u200b\u3001\u200b\u6587\u672c\u200b\u5230\u200b\u8bed\u97f3\u200b\u7b49\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b HuggingFace Hub \u200b\u4e0a\u200b\u5c1d\u8bd5\u200b\u4ed6\u4eec\u200b\u7684\u200b\u8bb8\u591a\u200b\u6a21\u578b\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/#_6","title":"\u2753\u200b\u63a8\u8350\u200b\u5f15\u64ce","text":"<p>\u200b\u4e92\u8054\u7f51\u200b\u662f\u200b\u7531\u200b\u63a8\u8350\u200b\u9a71\u52a8\u200b\u7684\u200b\u3002YouTube \u200b\u63a8\u8350\u200b\u89c6\u9891\u200b\uff0cNetflix \u200b\u63a8\u8350\u200b\u7535\u5f71\u200b\u548c\u200b\u7535\u89c6\u8282\u76ee\u200b\uff0c\u200b\u4e9a\u9a6c\u900a\u200b\u63a8\u8350\u200b\u4ea7\u54c1\u200b\uff0cMedium \u200b\u63a8\u8350\u200b\u6587\u7ae0\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u6b63\u5728\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u5728\u7ebf\u200b\u5546\u5e97\u200b\u6216\u200b\u5728\u7ebf\u200b\u5e02\u573a\u200b\uff0c\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u60f3\u200b\u5f00\u59cb\u200b\u5411\u200b\u4f60\u200b\u7684\u200b\u5ba2\u6237\u200b\u63a8\u8350\u200b\u4e1c\u897f\u200b\u3002</p> <p>\u200b\u4e3a\u6b64\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u63a8\u8350\u200b\u5f15\u64ce\u200b\u3002</p> <ul> <li>TorchRec \u2014 PyTorch \u200b\u6700\u65b0\u200b\u7684\u200b\u5185\u7f6e\u200b\u9886\u57df\u200b\u5e93\u200b\uff0c\u200b\u7528\u4e8e\u200b\u901a\u8fc7\u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u9a71\u52a8\u200b\u63a8\u8350\u200b\u5f15\u64ce\u200b\u3002TorchRec \u200b\u63d0\u4f9b\u200b\u4e86\u200b\u53ef\u4ee5\u200b\u5c1d\u8bd5\u200b\u548c\u200b\u4f7f\u7528\u200b\u7684\u200b\u63a8\u8350\u200b\u6570\u636e\u200b\u96c6\u200b\u548c\u200b\u6a21\u578b\u200b\u3002\u200b\u5c3d\u7ba1\u200b\u5982\u679c\u200b\u81ea\u5b9a\u4e49\u200b\u63a8\u8350\u200b\u5f15\u64ce\u200b\u4e0d\u200b\u7b26\u5408\u200b\u4f60\u200b\u7684\u200b\u8981\u6c42\u200b\uff08\u200b\u6216\u200b\u5de5\u4f5c\u91cf\u200b\u592a\u200b\u5927\u200b\uff09\uff0c\u200b\u8bb8\u591a\u200b\u4e91\u200b\u4f9b\u5e94\u5546\u200b\u63d0\u4f9b\u200b\u4e86\u200b\u63a8\u8350\u200b\u5f15\u64ce\u200b\u670d\u52a1\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/#_7","title":"\u23f3 \u200b\u65f6\u95f4\u200b\u5e8f\u5217","text":"<p>\u200b\u5982\u679c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u6709\u200b\u65f6\u95f4\u200b\u7ec4\u4ef6\u200b\uff0c\u200b\u5e76\u4e14\u200b\u4f60\u200b\u5e0c\u671b\u200b\u5229\u7528\u200b\u8fc7\u53bb\u200b\u7684\u200b\u6a21\u5f0f\u200b\u6765\u200b\u9884\u6d4b\u200b\u672a\u6765\u200b\uff0c\u200b\u4f8b\u5982\u200b\uff0c\u200b\u9884\u6d4b\u200b\u660e\u5e74\u200b\u6bd4\u7279\u200b\u5e01\u200b\u7684\u200b\u4ef7\u683c\u200b\uff08\u200b\u4e0d\u8981\u200b\u5c1d\u8bd5\u200b\u8fd9\u4e2a\u200b\uff0c\u200b\u80a1\u7968\u200b\u9884\u6d4b\u200b\u662f\u200b BS\uff09\u200b\u6216\u200b\u66f4\u200b\u5408\u7406\u200b\u7684\u200b\u9884\u6d4b\u200b\u4e0b\u5468\u200b\u57ce\u5e02\u200b\u7535\u529b\u200b\u9700\u6c42\u200b\u7684\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u60f3\u8981\u200b\u67e5\u770b\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u5e93\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u5e93\u200b\u4e0d\u200b\u4e00\u5b9a\u200b\u4f7f\u7528\u200b PyTorch\uff0c\u200b\u4f46\u200b\u7531\u4e8e\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u662f\u200b\u4e00\u4e2a\u200b\u5e38\u89c1\u95ee\u9898\u200b\uff0c\u200b\u6211\u200b\u5728\u200b\u8fd9\u91cc\u200b\u5305\u542b\u200b\u4e86\u200b\u5b83\u4eec\u200b\u3002</p> <ul> <li>Salesforce Merlion \u2014 \u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b Merlion \u200b\u7684\u200b\u6570\u636e\u200b\u52a0\u8f7d\u200b\u5668\u200b\u3001\u200b\u9884\u200b\u6784\u5efa\u200b\u6a21\u578b\u200b\u3001AutoML\uff08\u200b\u81ea\u52a8\u5316\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\uff09\u200b\u8d85\u200b\u53c2\u6570\u200b\u8c03\u6574\u200b\u7b49\u200b\uff0c\u200b\u5c06\u200b\u4f60\u200b\u7684\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u6570\u636e\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u60c5\u62a5\u200b\uff0c\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u90fd\u200b\u662f\u200b\u53d7\u200b\u5b9e\u9645\u200b\u7528\u4f8b\u200b\u542f\u53d1\u200b\u7684\u200b\uff0c\u200b\u7528\u4e8e\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u9884\u6d4b\u200b\u548c\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u5f02\u5e38\u200b\u68c0\u6d4b\u200b\u3002</li> <li>Facebook Kats \u2014 Facebook \u200b\u7684\u200b\u6574\u4e2a\u200b\u4e1a\u52a1\u200b\u4f9d\u8d56\u4e8e\u200b\u9884\u6d4b\u200b\uff1a\u200b\u4f55\u65f6\u200b\u662f\u200b\u653e\u7f6e\u200b\u5e7f\u544a\u200b\u7684\u200b\u6700\u4f73\u200b\u65f6\u95f4\u200b\uff1f\u200b\u6240\u4ee5\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u6253\u8d4c\u200b\u4ed6\u4eec\u200b\u5728\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u9884\u6d4b\u200b\u8f6f\u4ef6\u200b\u4e0a\u200b\u6295\u5165\u200b\u4e86\u200b\u5927\u91cf\u200b\u8d44\u91d1\u200b\u3002Kats\uff08Kit to Analyze Time Series data\uff09\u200b\u662f\u200b\u4ed6\u4eec\u200b\u7684\u200b\u5f00\u6e90\u200b\u5e93\u200b\uff0c\u200b\u7528\u4e8e\u200b\u65f6\u95f4\u200b\u5e8f\u5217\u200b\u9884\u6d4b\u200b\u3001\u200b\u68c0\u6d4b\u200b\u548c\u200b\u6570\u636e\u5904\u7406\u200b\u3002\u200b\u5f53\u7136\u200b\uff0c\u200b\u8bf7\u200b\u63d0\u4f9b\u200b\u60a8\u200b\u5e0c\u671b\u200b\u7ffb\u8bd1\u200b\u7684\u200b\u82f1\u6587\u200b Markdown \u200b\u5185\u5bb9\u200b\uff0c\u200b\u6211\u4f1a\u200b\u5c06\u200b\u5176\u200b\u7ffb\u8bd1\u200b\u4e3a\u200b\u4e2d\u6587\u200b\uff0c\u200b\u5e76\u200b\u4fdd\u6301\u200b\u539f\u6709\u200b\u683c\u5f0f\u200b\u3002</li> </ul>"},{"location":"pytorch_extra_resources/#_8","title":"\ud83d\udc69\u200d\ud83d\udcbb \u200b\u5982\u4f55\u200b\u627e\u5230\u200b\u5de5\u4f5c","text":"<p>\u200b\u5b8c\u6210\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u8bfe\u7a0b\u200b\u540e\u200b\uff0c\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u60f3\u200b\u8fd0\u7528\u200b\u4f60\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u6280\u80fd\u200b\u3002</p> <p>\u200b\u751a\u81f3\u200b\u66f4\u597d\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u7528\u200b\u5b83\u4eec\u200b\u6765\u200b\u8d5a\u94b1\u200b\u3002</p> <p>\u200b\u4ee5\u4e0b\u200b\u8d44\u6e90\u200b\u662f\u200b\u5173\u4e8e\u200b\u5982\u4f55\u200b\u627e\u5230\u200b\u5de5\u4f5c\u200b\u7684\u200b\u826f\u597d\u200b\u6307\u5357\u200b\u3002</p> <ul> <li>\"\u200b\u50cf\u200b\u6211\u200b\u8fd9\u6837\u200b\u7684\u200b\u521d\u5b66\u8005\u200b\u6570\u636e\u200b\u79d1\u5b66\u5bb6\u200b\u5982\u4f55\u200b\u83b7\u5f97\u200b\u7ecf\u9a8c\u200b\uff1f\" \u200b\u4f5c\u8005\u200b\uff1aDaniel Bourke \u2014 \u200b\u6211\u200b\u7ecf\u5e38\u200b\u88ab\u200b\u95ee\u5230\u200b\u201c\u200b\u5982\u4f55\u200b\u83b7\u5f97\u200b\u7ecf\u9a8c\u200b\uff1f\u201d\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u8bb8\u591a\u200b\u5de5\u4f5c\u200b\u8981\u6c42\u200b\u90fd\u200b\u5199\u200b\u7740\u200b\u201c\u200b\u9700\u8981\u200b\u7ecf\u9a8c\u200b\u201d\u3002\u200b\u4e8b\u5b9e\u8bc1\u660e\u200b\uff0c\u200b\u83b7\u5f97\u200b\u7ecf\u9a8c\u200b\uff08\u200b\u548c\u200b\u5de5\u4f5c\u200b\u7684\u200b\uff09\u200b\u6700\u4f73\u200b\u65b9\u5f0f\u200b\u4e4b\u4e00\u200b\u662f\u200b\uff1a\u200b\u5728\u200b\u62e5\u6709\u200b\u5de5\u4f5c\u200b\u4e4b\u524d\u200b\u5c31\u200b\u5f00\u59cb\u200b\u505a\u200b\u8fd9\u4efd\u200b\u5de5\u4f5c\u200b\u3002</li> <li>\u200b\u4f60\u200b\u5e76\u200b\u4e0d\u200b\u771f\u7684\u200b\u9700\u8981\u200b\u53e6\u200b\u4e00\u4e2a\u200bMOOC \u200b\u4f5c\u8005\u200b\uff1aEugene Yan \u2014 MOOC\u200b\u4ee3\u8868\u200b\u5927\u89c4\u6a21\u200b\u5728\u7ebf\u200b\u516c\u5f00\u200b\u8bfe\u7a0b\u200b\uff08\u200b\u6216\u200b\u7c7b\u4f3c\u200b\u7684\u200b\u4e1c\u897f\u200b\uff09\u3002MOOCs\u200b\u975e\u5e38\u200b\u7f8e\u597d\u200b\u3002\u200b\u5b83\u4eec\u200b\u8ba9\u200b\u4e16\u754c\u5404\u5730\u200b\u7684\u200b\u4eba\u4eec\u200b\u6309\u7167\u200b\u81ea\u5df1\u200b\u7684\u200b\u8282\u594f\u200b\u5b66\u4e60\u200b\u3002\u200b\u7136\u800c\u200b\uff0c\u200b\u4eba\u4eec\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u4e0d\u65ad\u200b\u91cd\u590d\u200b\u505a\u200bMOOC\uff0c\u200b\u8ba4\u4e3a\u200b\u201c\u200b\u5982\u679c\u200b\u6211\u200b\u518d\u200b\u505a\u200b\u4e00\u95e8\u200b\uff0c\u200b\u6211\u200b\u5c31\u200b\u4f1a\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u201d\u3002\u200b\u4e8b\u5b9e\u4e0a\u200b\uff0c\u200b\u51e0\u95e8\u200b\u5c31\u200b\u8db3\u591f\u200b\u4e86\u200b\uff0cMOOC\u200b\u7684\u200b\u56de\u62a5\u200b\u5f88\u5feb\u200b\u5c31\u200b\u4f1a\u200b\u5f00\u59cb\u200b\u51cf\u5c11\u200b\u3002\u200b\u76f8\u53cd\u200b\uff0c\u200b\u79bb\u5f00\u200b\u5e38\u89c4\u200b\u8def\u5f84\u200b\uff0c\u200b\u5f00\u59cb\u200b\u6784\u5efa\u200b\uff0c\u200b\u5f00\u59cb\u200b\u521b\u9020\u200b\uff0c\u200b\u5f00\u59cb\u200b\u5b66\u4e60\u200b\u65e0\u6cd5\u200b\u88ab\u200b\u6559\u6388\u200b\u7684\u200b\u6280\u80fd\u200b\u3002\u200b\u5c55\u793a\u200b\u8fd9\u4e9b\u200b\u6280\u80fd\u200b\u6765\u200b\u83b7\u5f97\u200b\u5de5\u4f5c\u200b\u3002</li> <li>\u200b\u989d\u5916\u200b\u63a8\u8350\u200b\uff1a \u200b\u5173\u4e8e\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9762\u8bd5\u200b\u6700\u200b\u5168\u9762\u200b\u7684\u200b\u8d44\u6e90\u200b\uff0c\u200b\u8bf7\u200b\u67e5\u770b\u200bChip Huyen\u200b\u7684\u200b\u514d\u8d39\u200b\u300a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9762\u8bd5\u200b\u6307\u5357\u200b\u300b\u3002</li> </ul>"},{"location":"pytorch_most_common_errors/","title":"PyTorch \u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e09\u79cd\u200b\u9519\u8bef","text":"In\u00a0[1]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") <pre>PyTorch version: 1.12.1+cu113\n</pre> In\u00a0[2]: Copied! <pre># Create two tensors\ntensor_1 = torch.rand(3, 4)\ntensor_2 = torch.rand(3, 4)\n\n# Check the shapes\nprint(tensor_1.shape)\nprint(tensor_2.shape)\n</pre> # Create two tensors tensor_1 = torch.rand(3, 4) tensor_2 = torch.rand(3, 4)  # Check the shapes print(tensor_1.shape) print(tensor_2.shape) <pre>torch.Size([3, 4])\ntorch.Size([3, 4])\n</pre> <p>\u200b\u8bf7\u200b\u6ce8\u610f\u200b\uff0c\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u5b83\u4eec\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u8fd0\u7b97\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u8fd0\u7b97\u200b\u4e0e\u200b\u6807\u51c6\u200b\u4e58\u6cd5\u200b\u8fd0\u7b97\u200b\u4e0d\u540c\u200b\u3002</p> <p>\u200b\u5bf9\u4e8e\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b\u5f20\u91cf\u200b\uff0c\u200b\u6807\u51c6\u200b\u4e58\u6cd5\u200b\u8fd0\u7b97\u200b\uff08<code>*</code> \u200b\u6216\u200b <code>torch.mul()</code>\uff09\u200b\u53ef\u4ee5\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\uff0c\u200b\u800c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u8fd0\u7b97\u200b\uff08<code>@</code> \u200b\u6216\u200b <code>torch.matmul()</code>\uff09\u200b\u5c06\u4f1a\u200b\u62a5\u9519\u200b\u3002</p> <p>\u200b\u6709\u5173\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u8fd0\u7b97\u200b\u7684\u200b\u8be6\u7ec6\u200b\u89e3\u91ca\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 00. PyTorch \u200b\u57fa\u7840\u200b\uff1a\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> In\u00a0[3]: Copied! <pre># Standard multiplication, the following lines perform the same operation (will work)\ntensor_3 = tensor_1 * tensor_2 # can do standard multiplication with \"*\"\ntensor_4 = torch.mul(tensor_1, tensor_2) # can also do standard multiplicaton with \"torch.mul()\" \n\n# Check for equality \ntensor_3 == tensor_4\n</pre> # Standard multiplication, the following lines perform the same operation (will work) tensor_3 = tensor_1 * tensor_2 # can do standard multiplication with \"*\" tensor_4 = torch.mul(tensor_1, tensor_2) # can also do standard multiplicaton with \"torch.mul()\"   # Check for equality  tensor_3 == tensor_4 Out[3]: <pre>tensor([[True, True, True, True],\n        [True, True, True, True],\n        [True, True, True, True]])</pre> <p>\u200b\u592a\u68d2\u4e86\u200b\uff01\u200b\u770b\u8d77\u6765\u200b\u6807\u51c6\u200b\u4e58\u6cd5\u200b\u5728\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b\u4e0b\u200b\u8fd0\u884c\u200b\u826f\u597d\u200b\u3002</p> <p>\u200b\u63a5\u4e0b\u6765\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> In\u00a0[4]: Copied! <pre># Try matrix multiplication (won't work)\ntensor_5 = tensor_1 @ tensor_2 # could also do \"torch.matmul(tensor_1, tensor_2)\"\n</pre> # Try matrix multiplication (won't work) tensor_5 = tensor_1 @ tensor_2 # could also do \"torch.matmul(tensor_1, tensor_2)\" <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [4], in &lt;cell line: 2&gt;()\n      1 # Try matrix multiplication (won't work)\n----&gt; 2 tensor_5 = tensor_1 @ tensor_2\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)</pre> <p>\u200b\u7cdf\u7cd5\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b\u7c7b\u4f3c\u200b\u4ee5\u4e0b\u200b\u7684\u200b\u9519\u8bef\u200b\uff1a</p> <pre><code>RuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-11-2ca2c90dbb42&gt; in &lt;module&gt;\n      1 # \u200b\u5c1d\u8bd5\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u4e0d\u200b\u5de5\u4f5c\u200b\uff09\n----&gt; 2 tensor_5 = tensor_1 @ tensor_2\n\nRuntimeError: mat1 \u200b\u548c\u200b mat2 \u200b\u7684\u200b\u5f62\u72b6\u200b\u65e0\u6cd5\u200b\u76f8\u4e58\u200b\uff083x4 \u200b\u548c\u200b 3x4\uff09\n</code></pre> <p>\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u4e24\u4e2a\u200b\u5f20\u91cf\u200b\uff08\u200b\u77e9\u9635\u200b\uff09\u200b\u65e0\u6cd5\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u5b83\u4eec\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u517c\u5bb9\u200b\u3002</p> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u6709\u200b\u7279\u5b9a\u200b\u7684\u200b\u89c4\u5219\u200b\uff1a</p> <ol> <li>\u200b\u5185\u200b\u7ef4\u5ea6\u200b\u5fc5\u987b\u200b\u5339\u914d\u200b\uff1a</li> </ol> <ul> <li><code>(3, 4) @ (3, 4)</code> \u200b\u4e0d\u200b\u5de5\u4f5c\u200b</li> <li><code>(4, 3) @ (3, 4)</code> \u200b\u5de5\u4f5c\u200b</li> <li><code>(3, 4) @ (4, 3)</code> \u200b\u5de5\u4f5c\u200b</li> </ul> <ol> <li>\u200b\u7ed3\u679c\u200b\u77e9\u9635\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b\u5916\u200b\u7ef4\u5ea6\u200b\uff1a</li> </ol> <ul> <li><code>(4, 3) @ (3, 4)</code> -&gt; <code>(4, 4)</code></li> <li><code>(3, 4) @ (4, 3)</code> -&gt; <code>(3, 3)</code></li> </ul> <p>\u200b\u90a3\u4e48\u200b\u6211\u4eec\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5462\u200b\uff1f</p> <p>\u200b\u8fd9\u65f6\u200b\u5c31\u200b\u9700\u8981\u200b\u7528\u5230\u200b\u8f6c\u7f6e\u200b\u6216\u200b\u91cd\u5851\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u5728\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u66f4\u200b\u666e\u904d\u200b\u7684\u200b\u662f\u200b\u8f6c\u7f6e\u200b\u64cd\u4f5c\u200b\u3002</p> <ul> <li>\u200b\u8f6c\u7f6e\u200b - \u200b\u8f6c\u7f6e\u200b\uff08<code>torch.transpose()</code>\uff09\u200b\u64cd\u4f5c\u200b\u4ea4\u6362\u200b\u7ed9\u5b9a\u200b\u5f20\u91cf\u200b\u7684\u200b\u7ef4\u5ea6\u200b\u3002<ul> <li>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4f60\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>tensor.T</code> \u200b\u7684\u200b\u5feb\u6377\u65b9\u5f0f\u200b\u6765\u200b\u8fdb\u884c\u200b\u8f6c\u7f6e\u200b\u3002</li> </ul> </li> <li>\u200b\u91cd\u5851\u200b - \u200b\u91cd\u5851\u200b\uff08<code>torch.reshape()</code>\uff09\u200b\u64cd\u4f5c\u200b\u8fd4\u56de\u200b\u4e00\u4e2a\u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u539f\u59cb\u200b\u5143\u7d20\u200b\u4f46\u200b\u5f62\u72b6\u200b\u4e0d\u540c\u200b\u7684\u200b\u5f20\u91cf\u200b\u3002</li> </ul> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b9e\u9645\u64cd\u4f5c\u200b\u3002</p> In\u00a0[5]: Copied! <pre># Perform a transpose on tensor_1 and then perform matrix multiplication \ntensor_6 = tensor_1.T @ tensor_2\nprint(f\"Shape of input tensors: {tensor_1.T.shape} and {tensor_2.shape}\")\nprint(f\"Shape of output tensor: {tensor_6.shape}\")\n</pre> # Perform a transpose on tensor_1 and then perform matrix multiplication  tensor_6 = tensor_1.T @ tensor_2 print(f\"Shape of input tensors: {tensor_1.T.shape} and {tensor_2.shape}\") print(f\"Shape of output tensor: {tensor_6.shape}\") <pre>Shape of input tensors: torch.Size([4, 3]) and torch.Size([3, 4])\nShape of output tensor: torch.Size([4, 4])\n</pre> <p>\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\u5230\u200b\u7531\u4e8e\u200b\u8f6c\u7f6e\u200b\u64cd\u4f5c\u200b\uff08<code>tensor_1.T</code>\uff09\uff0c<code>tensor_1</code>\u200b\u7684\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u4ece\u200b <code>(3, 4)</code> \u200b\u53d8\u4e3a\u200b\u4e86\u200b <code>(4, 3)</code>\u3002</p> <p>\u200b\u6b63\u200b\u56e0\u4e3a\u200b\u5982\u6b64\u200b\uff0c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u89c4\u5219\u200b1\uff0c\u200b\u5185\u200b\u7ef4\u5ea6\u200b\u5fc5\u987b\u200b\u5339\u914d\u200b\uff0c\u200b\u5f97\u5230\u200b\u4e86\u200b\u6ee1\u8db3\u200b\u3002</p> <p>\u200b\u6700\u540e\u200b\uff0c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u6ee1\u8db3\u200b\u4e86\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u89c4\u5219\u200b2\uff0c\u200b\u7ed3\u679c\u200b\u77e9\u9635\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u5916\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c<code>tensor_6</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(4, 4)</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fdb\u884c\u200b\u540c\u6837\u200b\u7684\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u6211\u4eec\u200b\u5c06\u200b\u8f6c\u7f6e\u200b <code>tensor_2</code> \u200b\u800c\u200b\u4e0d\u662f\u200b <code>tensor_1</code>\u3002</p> In\u00a0[6]: Copied! <pre># Perform a transpose on tensor_2 and then perform matrix multiplication\ntensor_7 = tensor_1 @ tensor_2.T\nprint(f\"Shape of input tensors: {tensor_1.shape} and {tensor_2.T.shape}\")\nprint(f\"Shape of output tensor: {tensor_7.shape}\")\n</pre> # Perform a transpose on tensor_2 and then perform matrix multiplication tensor_7 = tensor_1 @ tensor_2.T print(f\"Shape of input tensors: {tensor_1.shape} and {tensor_2.T.shape}\") print(f\"Shape of output tensor: {tensor_7.shape}\") <pre>Shape of input tensors: torch.Size([3, 4]) and torch.Size([4, 3])\nShape of output tensor: torch.Size([3, 3])\n</pre> <p>\u200b\u54c7\u200b\u54e6\u200b\uff01</p> <p>\u200b\u518d\u6b21\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\uff01</p> <p>\u200b\u770b\u770b\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u89c4\u5219\u200b1\u200b\u548c\u200b\u89c4\u5219\u200b2\u200b\u662f\u200b\u5982\u4f55\u200b\u518d\u6b21\u200b\u5f97\u5230\u200b\u6ee1\u8db3\u200b\u7684\u200b\u3002</p> <p>\u200b\u53ea\u4e0d\u8fc7\u200b\u8fd9\u6b21\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u8f6c\u7f6e\u200b\u4e86\u200b<code>tensor_2</code>\uff0c\u200b\u6240\u4ee5\u200b\u5f97\u5230\u200b\u7684\u200b\u8f93\u51fa\u200b\u5f20\u91cf\u200b\u5f62\u72b6\u200b\u662f\u200b<code>(3, 3)</code>\u3002</p> <p>\u200b\u597d\u6d88\u606f\u200b\u662f\u200b\uff0c\u200b\u5927\u591a\u6570\u200b\u65f6\u5019\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u4f7f\u7528\u200bPyTorch\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65f6\u200b\uff0c\u200b\u8fd9\u4e2a\u200b\u5e93\u4f1a\u200b\u4e3a\u200b\u4f60\u200b\u5904\u7406\u200b\u5927\u90e8\u5206\u200b\u4f60\u200b\u9700\u8981\u200b\u6267\u884c\u200b\u7684\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u64cd\u4f5c\u200b\u3002</p> <p>\u200b\u8bdd\u867d\u5982\u6b64\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u7528\u200bPyTorch\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u770b\u770b\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u5728\u200b\u54ea\u91cc\u200b\u3002</p> In\u00a0[7]: Copied! <pre>import torchvision\nfrom torchvision import datasets, transforms\n\n# Setup training data\ntrain_data = datasets.FashionMNIST(\n    root=\"data\", # where to download data to?\n    train=True, # get training data\n    download=True, # download data if it doesn't exist on disk\n    transform=transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n    target_transform=None # you can transform labels as well\n)\n\n# Setup testing data\ntest_data = datasets.FashionMNIST(\n    root=\"data\",\n    train=False, # get test data\n    download=True,\n    transform=transforms.ToTensor()\n)\n</pre> import torchvision from torchvision import datasets, transforms  # Setup training data train_data = datasets.FashionMNIST(     root=\"data\", # where to download data to?     train=True, # get training data     download=True, # download data if it doesn't exist on disk     transform=transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors     target_transform=None # you can transform labels as well )  # Setup testing data test_data = datasets.FashionMNIST(     root=\"data\",     train=False, # get test data     download=True,     transform=transforms.ToTensor() ) <pre>Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n</pre> <pre>  0%|          | 0/26421880 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n</pre> <pre>  0%|          | 0/29515 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n</pre> <pre>  0%|          | 0/4422102 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\nDownloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n</pre> <pre>  0%|          | 0/5148 [00:00&lt;?, ?it/s]</pre> <pre>Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n\n</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u83b7\u53d6\u200b\u4e00\u4e9b\u200b\u5173\u4e8e\u200b\u7b2c\u4e00\u4e2a\u200b\u8bad\u7ec3\u6837\u672c\u200b\u7684\u200b\u8be6\u7ec6\u4fe1\u606f\u200b\uff0c\u200b\u5305\u62ec\u200b\u6807\u7b7e\u200b\u3001\u200b\u7c7b\u522b\u200b\u540d\u79f0\u200b\u548c\u200b\u7c7b\u522b\u200b\u6570\u91cf\u200b\u3002</p> In\u00a0[8]: Copied! <pre># See first training sample\nimage, label = train_data[0]\nprint(f\"Image shape: {image.shape} -&gt; [batch, height, width]\") \nprint(f\"Label: {label}\") # label is an int rather than a tensor (it has no shape attribute)\n</pre> # See first training sample image, label = train_data[0] print(f\"Image shape: {image.shape} -&gt; [batch, height, width]\")  print(f\"Label: {label}\") # label is an int rather than a tensor (it has no shape attribute) <pre>Image shape: torch.Size([1, 28, 28]) -&gt; [batch, height, width]\nLabel: 9\n</pre> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[1, 28, 28]</code> \u200b\u6216\u200b <code>[batch_size, height, width]</code>\u3002</p> In\u00a0[9]: Copied! <pre># See class names and number of classes\nclass_names = train_data.classes\nnum_classes = len(class_names)\nclass_names, num_classes\n</pre> # See class names and number of classes class_names = train_data.classes num_classes = len(class_names) class_names, num_classes Out[9]: <pre>(['T-shirt/top',\n  'Trouser',\n  'Pullover',\n  'Dress',\n  'Coat',\n  'Sandal',\n  'Shirt',\n  'Sneaker',\n  'Bag',\n  'Ankle boot'],\n 10)</pre> In\u00a0[10]: Copied! <pre># Plot a sample\nimport matplotlib.pyplot as plt\nplt.imshow(image.squeeze(), cmap=\"gray\") # plot image as grayscale\nplt.axis(False)\nplt.title(class_names[label]);\n</pre> # Plot a sample import matplotlib.pyplot as plt plt.imshow(image.squeeze(), cmap=\"gray\") # plot image as grayscale plt.axis(False) plt.title(class_names[label]); In\u00a0[11]: Copied! <pre>from torch import nn\n\n# Create a two layer neural network\nmodel_0 = nn.Sequential(\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=10)\n)\n\n# Pass the image through the model (this will error)\nmodel_0(image)\n</pre> from torch import nn  # Create a two layer neural network model_0 = nn.Sequential(     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=10) )  # Pass the image through the model (this will error) model_0(image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [11], in &lt;cell line: 10&gt;()\n      4 model_0 = nn.Sequential(\n      5     nn.Linear(in_features=10, out_features=10),\n      6     nn.Linear(in_features=10, out_features=10)\n      7 )\n      9 # Pass the image through the model (this will error)\n---&gt; 10 model_0(image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)</pre> <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u4ee3\u7801\u200b\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff01</p> <p>\u200b\u7c7b\u4f3c\u200b\u4e8e\u200b\uff1a</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)\n</code></pre> <p>\u200b\u5173\u952e\u5728\u4e8e\u200b\u6700\u540e\u200b\u4e00\u884c\u200b <code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)</code>\u3002</p> <p>\u200b\u8fd9\u200b\u544a\u8bc9\u200b\u6211\u4eec\u200b\u6570\u636e\u200b\u5f62\u72b6\u200b\u6709\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u56e0\u4e3a\u200b\u5728\u200b\u5e55\u540e\u200b\uff0c<code>nn.Linear()</code> \u200b\u8bd5\u56fe\u200b\u8fdb\u884c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\uff1f</p> <p>\u200b\u6709\u200b\u51e0\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u9009\u9879\u200b\uff0c\u200b\u5177\u4f53\u200b\u53d6\u51b3\u4e8e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u54ea\u79cd\u200b\u5c42\u200b\u3002</p> <p>\u200b\u4f46\u200b\u7531\u4e8e\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4e13\u6ce8\u200b\u4e8e\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> <p><code>nn.Linear()</code> \u200b\u559c\u6b22\u200b\u63a5\u53d7\u200b\u5355\u7ef4\u200b\u5411\u91cf\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8f93\u5165\u200b <code>image</code> \u200b\u7684\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>[1, 28, 28]</code>\uff0c\u200b\u5b83\u200b\u66f4\u200b\u559c\u6b22\u200b <code>[1, 784]</code>\uff08<code>784 = 28*28</code>\uff09\u3002</p> <p>\u200b\u6362\u53e5\u8bdd\u8bf4\u200b\uff0c\u200b\u5b83\u200b\u5e0c\u671b\u200b\u6240\u6709\u200b\u4fe1\u606f\u200b\u90fd\u200b\u88ab\u200b\u5c55\u5e73\u200b\u6210\u200b\u5355\u4e00\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b PyTorch \u200b\u7684\u200b <code>nn.Flatten()</code> \u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u79cd\u200b\u5c55\u5e73\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u770b\u770b\u200b\u5b83\u200b\u662f\u200b\u5982\u4f55\u200b\u53d1\u751f\u200b\u7684\u200b\u3002</p> In\u00a0[12]: Copied! <pre># Create a flatten layer\nflatten = nn.Flatten()\n\n# Pass the image through the flatten layer\nflattened_image = flatten(image)\n\n# Print out the image shape before and after \nprint(f\"Before flatten shape: {image.shape} -&gt; [batch, height, width]\")\nprint(f\"After flatten shape: {flattened_image.shape} -&gt; [batch, height*width]\")\n</pre> # Create a flatten layer flatten = nn.Flatten()  # Pass the image through the flatten layer flattened_image = flatten(image)  # Print out the image shape before and after  print(f\"Before flatten shape: {image.shape} -&gt; [batch, height, width]\") print(f\"After flatten shape: {flattened_image.shape} -&gt; [batch, height*width]\") <pre>Before flatten shape: torch.Size([1, 28, 28]) -&gt; [batch, height, width]\nAfter flatten shape: torch.Size([1, 784]) -&gt; [batch, height*width]\n</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff0c\u200b\u56fe\u50cf\u200b\u6570\u636e\u200b\u5df2\u7ecf\u200b\u5c55\u5e73\u200b\u4e86\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u73b0\u6709\u200b\u6a21\u578b\u200b\u4e2d\u200b\u6dfb\u52a0\u200b <code>nn.Flatten()</code> \u200b\u5c42\u200b\u3002</p> In\u00a0[13]: Copied! <pre># Replicate model_0 except add a nn.Flatten() layer to begin with \nmodel_1 = nn.Sequential(\n    nn.Flatten(), # &lt;-- NEW: add nn.Flatten() layer\n    nn.Linear(in_features=10, out_features=10),\n    nn.Linear(in_features=10, out_features=10)\n)\n\n# Pass the image through the model\nmodel_1(image)\n</pre> # Replicate model_0 except add a nn.Flatten() layer to begin with  model_1 = nn.Sequential(     nn.Flatten(), # &lt;-- NEW: add nn.Flatten() layer     nn.Linear(in_features=10, out_features=10),     nn.Linear(in_features=10, out_features=10) )  # Pass the image through the model model_1(image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [13], in &lt;cell line: 9&gt;()\n      2 model_1 = nn.Sequential(\n      3     nn.Flatten(), # &lt;-- NEW: add nn.Flatten() layer\n      4     nn.Linear(in_features=10, out_features=10),\n      5     nn.Linear(in_features=10, out_features=10)\n      6 )\n      8 # Pass the image through the model\n----&gt; 9 model_1(image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)</pre> <p>\u200b\u54ce\u5440\u200b\uff01</p> <p>\u200b\u53c8\u200b\u51fa\u9519\u200b\u4e86\u200b...</p> <p>\u200b\u9519\u8bef\u4fe1\u606f\u200b\u5982\u4e0b\u200b\uff1a</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)\n</code></pre> <p>\u200b\u540c\u6837\u200b\uff0c\u200b\u5173\u952e\u200b\u4fe1\u606f\u200b\u5728\u200b\u6700\u540e\u200b\u4e00\u884c\u200b\u3002</p> <p><code>RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)</code></p> <p>\u200b\u55ef\u200b\uff0c\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b <code>(1x784)</code> \u200b\u80af\u5b9a\u200b\u6765\u81ea\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\uff08<code>image</code>\uff09\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5c06\u200b\u5176\u200b\u4ece\u200b <code>(1, 28, 28)</code> \u200b\u5c55\u5e73\u200b\u4e3a\u200b <code>(1, 784)</code>\u3002</p> <p>\u200b\u90a3\u4e48\u200b <code>(10x10)</code> \u200b\u5462\u200b\uff1f</p> <p>\u200b\u8fd9\u4e9b\u200b\u503c\u200b\u6765\u81ea\u200b\u6211\u4eec\u200b\u5728\u200b <code>nn.Linear()</code> \u200b\u5c42\u4e2d\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u53c2\u6570\u200b\uff0c<code>in_features=10</code> \u200b\u548c\u200b <code>out_features=10</code> \u200b\u6216\u8005\u200b <code>nn.Linear(in_features=10, out_features=10)</code>\u3002</p> <p>\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u7b2c\u4e00\u6761\u200b\u89c4\u5219\u200b\u662f\u200b\u4ec0\u4e48\u200b\u6765\u200b\u7740\u200b\uff1f</p> <ol> <li>\u200b\u5185\u7ef4\u200b\u5fc5\u987b\u200b\u5339\u914d\u200b\u3002</li> </ol> <p>\u200b\u5bf9\u200b\uff01</p> <p>\u200b\u90a3\u4e48\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u628a\u200b\u7b2c\u4e00\u5c42\u200b\u7684\u200b <code>in_features=10</code> \u200b\u6539\u4e3a\u200b <code>in_features=784</code> \u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u770b\u200b\uff01</p> In\u00a0[14]: Copied! <pre># Flatten the input as well as make sure the first layer can accept the flattened input shape\nmodel_2 = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(in_features=784, out_features=10), # &lt;-- NEW: change in_features=10 to in_features=784\n    nn.Linear(in_features=10, out_features=10)\n)\n\n# Pass the image through the model\nmodel_2(image)\n</pre> # Flatten the input as well as make sure the first layer can accept the flattened input shape model_2 = nn.Sequential(     nn.Flatten(),     nn.Linear(in_features=784, out_features=10), # &lt;-- NEW: change in_features=10 to in_features=784     nn.Linear(in_features=10, out_features=10) )  # Pass the image through the model model_2(image) Out[14]: <pre>tensor([[-0.2045,  0.2677, -0.0713, -0.3096, -0.0586,  0.3153, -0.3413,  0.2031,\n          0.4421,  0.1715]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>\u200b\u6210\u529f\u200b\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u4ece\u200b\u6a21\u578b\u200b\u4e2d\u200b\u5f97\u5230\u200b\u4e86\u200b\u8f93\u51fa\u200b\uff01</p> <p>\u200b\u867d\u7136\u200b\u8fd9\u4e2a\u200b\u8f93\u51fa\u200b\u76ee\u524d\u200b\u53ef\u80fd\u200b\u6ca1\u6709\u200b\u592a\u200b\u591a\u200b\u610f\u4e49\u200b\uff0c\u200b\u4f46\u200b\u81f3\u5c11\u200b\u6211\u4eec\u200b\u77e5\u9053\u200b\u6240\u6709\u200b\u5f62\u72b6\u200b\u90fd\u200b\u5339\u914d\u200b\uff0c\u200b\u6570\u636e\u200b\u80fd\u591f\u200b\u5b8c\u5168\u200b\u6d41\u7ecf\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p><code>nn.Flatten()</code> \u200b\u5c42\u200b\u5c06\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u56fe\u50cf\u200b\u4ece\u200b <code>(1, 28, 28)</code> \u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>(1, 784)</code>\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b <code>nn.Linear(in_features=784, out_features=10)</code> \u200b\u5c42\u200b\u80fd\u591f\u200b\u5c06\u200b\u5176\u200b\u4f5c\u4e3a\u200b\u8f93\u5165\u200b\u3002</p> In\u00a0[15]: Copied! <pre># Create a model with incorrect input and output shapes between layers\nmodel_3 = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(in_features=784, out_features=10), # out_features=10 \n    nn.Linear(in_features=5, out_features=10) # &lt;-- NEW: in_features does not match the out_features of the previous layer\n)\n\n# Pass the image through the model (this will error)\nmodel_3(image)\n</pre> # Create a model with incorrect input and output shapes between layers model_3 = nn.Sequential(     nn.Flatten(),     nn.Linear(in_features=784, out_features=10), # out_features=10      nn.Linear(in_features=5, out_features=10) # &lt;-- NEW: in_features does not match the out_features of the previous layer )  # Pass the image through the model (this will error) model_3(image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [15], in &lt;cell line: 9&gt;()\n      2 model_3 = nn.Sequential(\n      3     nn.Flatten(),\n      4     nn.Linear(in_features=784, out_features=10), # out_features=10 \n      5     nn.Linear(in_features=5, out_features=10) # &lt;-- NEW: in_features does not match the out_features of the previous layer\n      6 )\n      8 # Pass the image through the model (this will error)\n----&gt; 9 model_3(image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)</pre> <p>\u200b\u8fd0\u884c\u200b\u4e0a\u8ff0\u200b\u6a21\u578b\u200b\u65f6\u200b\uff0c\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b\u4ee5\u4e0b\u200b\u9519\u8bef\u200b\uff1a</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)\n</code></pre> <p>\u200b\u518d\u6b21\u200b\u8fdd\u53cd\u200b\u4e86\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u89c4\u5219\u200b1\uff0c\u200b\u5373\u200b\u5185\u7ef4\u200b\u5fc5\u987b\u200b\u5339\u914d\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e00\u4e2a\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u4e3a\u200b <code>(1, 10)</code>\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u7b2c\u4e8c\u4e2a\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u671f\u671b\u200b\u7684\u200b\u5f62\u72b6\u200b\u662f\u200b <code>(1, 5)</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8be5\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u5462\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u624b\u52a8\u200b\u5c06\u200b\u7b2c\u4e8c\u4e2a\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u7684\u200b <code>in_features</code> \u200b\u8bbe\u7f6e\u200b\u4e3a\u200b 10\uff0c\u200b\u6216\u8005\u200b\u5c1d\u8bd5\u200b\u4f7f\u7528\u200b PyTorch \u200b\u7684\u200b\u8f83\u200b\u65b0\u200b\u7279\u6027\u200b\u2014\u2014\u201c\u200b\u60f0\u6027\u200b\u201d\u200b\u5c42\u200b\u3002</p> In\u00a0[16]: Copied! <pre># Try nn.LazyLinear() as the second layer\nmodel_4 = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(in_features=784, out_features=10),\n    nn.LazyLinear(out_features=10) # &lt;-- NEW: no in_features parameter as this is inferred from the previous layer's output\n)\n\n# Pass the image through the model\nmodel_4(image)\n</pre> # Try nn.LazyLinear() as the second layer model_4 = nn.Sequential(     nn.Flatten(),     nn.Linear(in_features=784, out_features=10),     nn.LazyLinear(out_features=10) # &lt;-- NEW: no in_features parameter as this is inferred from the previous layer's output )  # Pass the image through the model model_4(image) <pre>/home/daniel/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n  warnings.warn('Lazy modules are a new feature under heavy development '\n</pre> Out[16]: <pre>tensor([[ 0.4282,  0.2492, -0.2045, -0.4943, -0.1639,  0.1166,  0.3828, -0.1283,\n         -0.1771, -0.2277]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>\u200b\u8fd9\u200b\u53ef\u4ee5\u200b\u6b63\u5e38\u200b\u5de5\u4f5c\u200b\uff08\u200b\u4e0d\u8fc7\u200b\u6839\u636e\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b PyTorch \u200b\u7248\u672c\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6709\u200b\u4e00\u6761\u200b\u8b66\u544a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u5982\u679c\u200b\u662f\u200b\u8fd9\u6837\u200b\uff0c\u200b\u4e0d\u7528\u200b\u62c5\u5fc3\u200b\uff0c\u200b\u8fd9\u200b\u53ea\u662f\u200b\u8868\u660e\u200b <code>Lazy</code> \u200b\u5c42\u200b\u4ecd\u200b\u5728\u200b\u5f00\u53d1\u200b\u4e2d\u200b\uff09\uff01</p> <p>\u200b\u6211\u4eec\u200b\u8bd5\u8bd5\u200b\u628a\u200b\u6240\u6709\u200b\u7684\u200b <code>nn.Linear()</code> \u200b\u5c42\u66ff\u6362\u200b\u6210\u200b <code>nn.LazyLinear()</code> \u200b\u5c42\u200b\u600e\u4e48\u6837\u200b\uff1f</p> <p>\u200b\u8fd9\u6837\u200b\u6211\u4eec\u200b\u53ea\u200b\u9700\u8981\u200b\u4e3a\u200b\u6bcf\u4e2a\u200b\u5c42\u200b\u8bbe\u7f6e\u200b <code>out_features</code> \u200b\u503c\u200b\u5373\u53ef\u200b\u3002</p> In\u00a0[18]: Copied! <pre># Replace all nn.Linear() layers with nn.LazyLinear()\nmodel_5 = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=10),\n    nn.LazyLinear(out_features=10) # &lt;-- NEW \n)\n\n# Pass the image through the model\nmodel_5(image)\n</pre> # Replace all nn.Linear() layers with nn.LazyLinear() model_5 = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=10),     nn.LazyLinear(out_features=10) # &lt;-- NEW  )  # Pass the image through the model model_5(image) Out[18]: <pre>tensor([[ 0.1375, -0.2175, -0.1054,  0.1424, -0.1406, -0.1180, -0.0896, -0.4285,\n         -0.0077, -0.3188]], grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u56fe\u50cf\u200b\u518d\u6b21\u200b\u987a\u5229\u200b\u901a\u8fc7\u200b\u7f51\u7edc\u200b\uff0c\u200b\u6ca1\u6709\u200b\u4efb\u4f55\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4e0a\u8ff0\u200b\u793a\u4f8b\u200b\u4ec5\u200b\u6d89\u53ca\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b\u4e00\u79cd\u200b\u5c42\u200b\u7c7b\u578b\u200b <code>nn.Linear()</code>\uff0c\u200b\u7136\u800c\u200b\uff0c\u200b\u65e0\u8bba\u662f\u200b\u5728\u200b\u6240\u6709\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u8fd8\u662f\u200b\u4e0d\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\u4e2d\u200b\uff0c\u200b\u786e\u4fdd\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u4e0e\u200b\u6bcf\u200b\u4e00\u5c42\u200b\u76f8\u5339\u914d\u200b\u7684\u200b\u539f\u5219\u200b\u662f\u200b\u4e00\u81f4\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5377\u79ef\u200b\u795e\u7ecf\u7f51\u7edc\u200b (CNN) \u200b\u4e2d\u200b\u4f7f\u7528\u200b\u7684\u200b <code>nn.Conv2d()</code> \u200b\u5c42\u200b\u751a\u81f3\u200b\u53ef\u4ee5\u200b\u63a5\u53d7\u200b\u4e0d\u200b\u9700\u8981\u200b\u4f7f\u7528\u200b <code>nn.Flatten()</code> \u200b\u7684\u200b\u8f93\u5165\u200b\u3002\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u5728\u200b 03. PyTorch \u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b \u200b\u7b2c\u200b7\u200b\u8282\u200b\uff1a\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b CNN \u200b\u4e2d\u200b\u4e86\u89e3\u200b\u66f4\u200b\u591a\u200b\u76f8\u5173\u200b\u4fe1\u606f\u200b\u3002</p> In\u00a0[19]: Copied! <pre>import torch\n\n# Set device to \"cuda\" if it's available otherwise default to \"cpu\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Current device: {device}\")\n</pre> import torch  # Set device to \"cuda\" if it's available otherwise default to \"cpu\" device = \"cuda\" if torch.cuda.is_available() else \"cpu\" print(f\"Current device: {device}\") <pre>Current device: cuda\n</pre> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u4e0e\u200b <code>model_5</code> \u200b\u5177\u6709\u200b\u76f8\u540c\u200b\u5c42\u200b\u7684\u200b\u6a21\u578b\u200b\u3002</p> <p>\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u6a21\u578b\u200b\u548c\u200b\u5f20\u91cf\u200b\u9ed8\u8ba4\u200b\u662f\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\u521b\u5efa\u200b\u7684\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u68c0\u67e5\u200b\u6211\u4eec\u200b\u521b\u5efa\u200b\u7684\u200b\u6a21\u578b\u200b\u7684\u200b <code>device</code> \u200b\u5c5e\u6027\u200b\u6765\u200b\u6d4b\u8bd5\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p> In\u00a0[20]: Copied! <pre>from torch import nn\n\n# Create a model (similar to model_5 above)\nmodel_6 = nn.Sequential(\n    nn.Flatten(),\n    nn.LazyLinear(out_features=10), \n    nn.LazyLinear(out_features=10)\n)\n\n# All models and tensors are created on the CPU by default (unless explicitly set otherwise)\nprint(f\"Model is on device: {next(model_6.parameters()).device}\")\n</pre> from torch import nn  # Create a model (similar to model_5 above) model_6 = nn.Sequential(     nn.Flatten(),     nn.LazyLinear(out_features=10),      nn.LazyLinear(out_features=10) )  # All models and tensors are created on the CPU by default (unless explicitly set otherwise) print(f\"Model is on device: {next(model_6.parameters()).device}\") <pre>Model is on device: cpu\n</pre> In\u00a0[21]: Copied! <pre>from torch.utils.data import DataLoader, RandomSampler\n\n# Only sample 10% of the data\ntrain_sampler = RandomSampler(train_data, \n                              num_samples=int(0.1*len(train_data)))\n\ntest_sampler = RandomSampler(test_data, \n                             num_samples=int(0.1*len(test_data)))\n\nprint(f\"Number of random training samples selected: {len(train_sampler)}/{len(train_data)}\")\nprint(f\"Number of random testing samples selected: {len(test_sampler)}/{len(test_data)}\")\n\n# Create DataLoaders and turn data into batches\nBATCH_SIZE = 32\ntrain_dataloader = DataLoader(dataset=train_data,\n                              batch_size=BATCH_SIZE,\n                              sampler=train_sampler)\n\ntest_dataloader = DataLoader(dataset=test_data,\n                             batch_size=BATCH_SIZE,\n                             sampler=test_sampler)\n\nprint(f\"Number of batches in train_dataloader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\nprint(f\"Number of batches in test_dataloader: {len(test_dataloader)} batch of size {BATCH_SIZE}\")\n\n# Create loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# Create optimizer\noptimizer = torch.optim.SGD(lr=0.01, \n                            params=model_6.parameters())\n</pre> from torch.utils.data import DataLoader, RandomSampler  # Only sample 10% of the data train_sampler = RandomSampler(train_data,                                num_samples=int(0.1*len(train_data)))  test_sampler = RandomSampler(test_data,                               num_samples=int(0.1*len(test_data)))  print(f\"Number of random training samples selected: {len(train_sampler)}/{len(train_data)}\") print(f\"Number of random testing samples selected: {len(test_sampler)}/{len(test_data)}\")  # Create DataLoaders and turn data into batches BATCH_SIZE = 32 train_dataloader = DataLoader(dataset=train_data,                               batch_size=BATCH_SIZE,                               sampler=train_sampler)  test_dataloader = DataLoader(dataset=test_data,                              batch_size=BATCH_SIZE,                              sampler=test_sampler)  print(f\"Number of batches in train_dataloader: {len(train_dataloader)} batches of size {BATCH_SIZE}\") print(f\"Number of batches in test_dataloader: {len(test_dataloader)} batch of size {BATCH_SIZE}\")  # Create loss function loss_fn = nn.CrossEntropyLoss()  # Create optimizer optimizer = torch.optim.SGD(lr=0.01,                              params=model_6.parameters()) <pre>Number of random training samples selected: 6000/60000\nNumber of random testing samples selected: 1000/10000\nNumber of batches in train_dataloader: 188 batches of size 32\nNumber of batches in test_dataloader: 32 batch of size 32\n</pre> In\u00a0[22]: Copied! <pre>from tqdm.auto import tqdm\n\n# Set the number of epochs\nepochs = 5\n\n# Train the model\nfor epoch in tqdm(range(epochs)):\n\n    # Set loss to 0 every epoch\n    train_loss = 0\n\n    # Get images (X) and labels (y)\n    for X, y in train_dataloader:\n\n        # Forward pass\n        y_pred = model_6(X)\n\n        # Calculate the loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss\n\n        # Optimizer zero grad\n        optimizer.zero_grad()\n\n        # Loss backward\n        loss.backward()\n\n        # Optimizer step\n        optimizer.step()\n  \n    # Print loss in the epoch loop only\n    print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")\n</pre> from tqdm.auto import tqdm  # Set the number of epochs epochs = 5  # Train the model for epoch in tqdm(range(epochs)):      # Set loss to 0 every epoch     train_loss = 0      # Get images (X) and labels (y)     for X, y in train_dataloader:          # Forward pass         y_pred = model_6(X)          # Calculate the loss         loss = loss_fn(y_pred, y)         train_loss += loss          # Optimizer zero grad         optimizer.zero_grad()          # Loss backward         loss.backward()          # Optimizer step         optimizer.step()        # Print loss in the epoch loop only     print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0 | Training loss: 334.65\nEpoch: 1 | Training loss: 215.44\nEpoch: 2 | Training loss: 171.15\nEpoch: 3 | Training loss: 154.72\nEpoch: 4 | Training loss: 142.22\n</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01\u200b\u770b\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u8fd0\u884c\u200b\u6b63\u5e38\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u6a21\u578b\u200b\u7684\u200b\u635f\u5931\u200b\u6b63\u5728\u200b\u4e0b\u964d\u200b\uff08\u200b\u635f\u5931\u200b\u8d8a\u4f4e\u200b\u8d8a\u200b\u597d\u200b\uff09\u3002</p> In\u00a0[23]: Copied! <pre># Send model_6 to the target device (\"cuda\")\nmodel_6.to(device)\n\n# Print out what device the model is on\nprint(f\"Model is on device: {next(model_6.parameters()).device}\")\n</pre> # Send model_6 to the target device (\"cuda\") model_6.to(device)  # Print out what device the model is on print(f\"Model is on device: {next(model_6.parameters()).device}\") <pre>Model is on device: cuda:0\n</pre> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_6</code> \u200b\u4f4d\u4e8e\u200b <code>\"cuda:0\"</code> \u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff08\u200b\u5176\u4e2d\u200b <code>0</code> \u200b\u662f\u200b\u8bbe\u5907\u200b\u7684\u200b\u7d22\u5f15\u200b\uff0c\u200b\u4ee5\u9632\u200b\u6709\u200b\u591a\u5757\u200b GPU\uff09\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8fd0\u884c\u200b\u4e0e\u200b\u4e0a\u9762\u200b\u76f8\u540c\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u4ee3\u7801\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p> <p>\u200b\u4f60\u200b\u80fd\u200b\u731c\u200b\u5230\u200b\u5417\u200b\uff1f</p> In\u00a0[24]: Copied! <pre>from tqdm.auto import tqdm\n\n# Set the number of epochs\nepochs = 5\n\n# Train the model\nfor epoch in tqdm(range(epochs)):\n\n  # Set loss to 0 every epoch\n  train_loss = 0\n\n  # Get images (X) and labels (y)\n  for X, y in train_dataloader:\n\n    # Forward pass\n    y_pred = model_6(X) # model is on GPU, data is on CPU (will error)\n\n    # Calculate the loss\n    loss = loss_fn(y_pred, y)\n    train_loss += loss\n    \n    # Optimizer zero grad\n    optimizer.zero_grad()\n\n    # Loss backward\n    loss.backward()\n\n    # Optimizer step\n    optimizer.step()\n  \n  # Print loss in the epoch loop only\n  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")\n</pre> from tqdm.auto import tqdm  # Set the number of epochs epochs = 5  # Train the model for epoch in tqdm(range(epochs)):    # Set loss to 0 every epoch   train_loss = 0    # Get images (X) and labels (y)   for X, y in train_dataloader:      # Forward pass     y_pred = model_6(X) # model is on GPU, data is on CPU (will error)      # Calculate the loss     loss = loss_fn(y_pred, y)     train_loss += loss          # Optimizer zero grad     optimizer.zero_grad()      # Loss backward     loss.backward()      # Optimizer step     optimizer.step()      # Print loss in the epoch loop only   print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [24], in &lt;cell line: 7&gt;()\n     12 # Get images (X) and labels (y)\n     13 for X, y in train_dataloader:\n     14 \n     15   # Forward pass\n---&gt; 16   y_pred = model_6(X) # model is on GPU, data is on CPU (will error)\n     18   # Calculate the loss\n     19   loss = loss_fn(y_pred, y)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre> <p>\u200b\u54ce\u5440\u200b\uff01</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b\u4e00\u4e2a\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\uff1a</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u770b\u5230\u200b\u9519\u8bef\u4fe1\u606f\u200b\u6307\u51fa\u200b\uff1a<code>Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!</code>\u3002</p> <p>\u200b\u672c\u8d28\u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u4f4d\u4e8e\u200b <code>cuda:0</code> \u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u6570\u636e\u200b\u5f20\u91cf\u200b\uff08<code>X</code> \u200b\u548c\u200b <code>y</code>\uff09\u200b\u4ecd\u7136\u200b\u5728\u200b <code>cpu</code> \u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u4f46\u200b PyTorch \u200b\u671f\u671b\u200b \u200b\u6240\u6709\u200b \u200b\u5f20\u91cf\u200b\u90fd\u200b\u5728\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p> In\u00a0[25]: Copied! <pre># Send the model to the target device (we don't need to do this again but we will for completeness)\nmodel_6.to(device)\n\n# Set the number of epochs\nepochs = 5\n\n# Train the model\nfor epoch in tqdm(range(epochs)):\n\n  # Set loss to 0 every epoch\n  train_loss = 0\n\n  # Get images (X) and labels (y)\n  for X, y in train_dataloader:\n\n    # Put target data on target device  &lt;-- NEW\n    X, y = X.to(device), y.to(device) # &lt;-- NEW: send data to target device\n\n    # Forward pass\n    y_pred = model_6(X)\n\n    # Calculate the loss\n    loss = loss_fn(y_pred, y)\n    train_loss += loss\n    \n    # Optimizer zero grad\n    optimizer.zero_grad()\n\n    # Loss backward\n    loss.backward()\n\n    # Optimizer step\n    optimizer.step()\n  \n  # Print loss in the epoch loop only\n  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")\n</pre> # Send the model to the target device (we don't need to do this again but we will for completeness) model_6.to(device)  # Set the number of epochs epochs = 5  # Train the model for epoch in tqdm(range(epochs)):    # Set loss to 0 every epoch   train_loss = 0    # Get images (X) and labels (y)   for X, y in train_dataloader:      # Put target data on target device  &lt;-- NEW     X, y = X.to(device), y.to(device) # &lt;-- NEW: send data to target device      # Forward pass     y_pred = model_6(X)      # Calculate the loss     loss = loss_fn(y_pred, y)     train_loss += loss          # Optimizer zero grad     optimizer.zero_grad()      # Loss backward     loss.backward()      # Optimizer step     optimizer.step()      # Print loss in the epoch loop only   print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\") <pre>  0%|          | 0/5 [00:00&lt;?, ?it/s]</pre> <pre>Epoch: 0 | Training loss: 134.76\nEpoch: 1 | Training loss: 127.76\nEpoch: 2 | Training loss: 120.85\nEpoch: 3 | Training loss: 120.50\nEpoch: 4 | Training loss: 116.29\n</pre> <p>\u200b\u975e\u5e38\u200b\u597d\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u4e0e\u200b\u4e4b\u524d\u200b\u4e00\u6837\u200b\u5b8c\u6210\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u73b0\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u5f20\u91cf\u200b\u90fd\u200b\u5728\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u50cf\u200b HuggingFace Accelerate \u200b\u8fd9\u6837\u200b\u7684\u200b\u5e93\u200b\u662f\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6a21\u578b\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u7edd\u4f73\u200b\u65b9\u5f0f\u200b\uff0c\u200b\u51e0\u4e4e\u200b\u4e0d\u200b\u9700\u8981\u200b\u663e\u5f0f\u200b\u8bbe\u7f6e\u200b\u8bbe\u5907\u200b\uff08\u200b\u5b83\u4eec\u200b\u4f1a\u200b\u81ea\u52a8\u200b\u53d1\u73b0\u200b\u6700\u4f73\u200b\u8bbe\u5907\u200b\u5e76\u200b\u4e3a\u200b\u60a8\u200b\u8bbe\u7f6e\u200b\u597d\u200b\u4e00\u5207\u200b\uff09\u3002</p> <p>\u200b\u60a8\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u7f16\u5199\u200b\u51fd\u6570\u200b\u6765\u200b\u786e\u4fdd\u60a8\u200b\u7684\u200b\u8bad\u7ec3\u200b\u4ee3\u7801\u200b\u90fd\u200b\u5728\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fd0\u884c\u200b\uff0c\u200b\u66f4\u200b\u591a\u200b\u5185\u5bb9\u200b\u8bf7\u53c2\u9605\u200b 05. PyTorch \u200b\u6a21\u5757\u5316\u200b\u8fdb\u9636\u200b \u200b\u7b2c\u200b4\u200b\u8282\u200b\uff1a\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u51fd\u6570\u200b\u3002</p> In\u00a0[26]: Copied! <pre># Get a single sample from the test dataset\ntest_image, test_label = test_data.data[0], test_data.targets[0]\nprint(f\"Test image shape: {test_image.shape}\")\nprint(f\"Test image label: {test_label}\")\n</pre> # Get a single sample from the test dataset test_image, test_label = test_data.data[0], test_data.targets[0] print(f\"Test image shape: {test_image.shape}\") print(f\"Test image label: {test_label}\") <pre>Test image shape: torch.Size([28, 28])\nTest image label: 9\n</pre> In\u00a0[27]: Copied! <pre># Plot test image\nimport matplotlib.pyplot as plt\nplt.imshow(test_image, cmap=\"gray\")\nplt.axis(False)\nplt.title(class_names[test_label]);\n</pre> # Plot test image import matplotlib.pyplot as plt plt.imshow(test_image, cmap=\"gray\") plt.axis(False) plt.title(class_names[test_label]); <p>\u200b\u770b\u8d77\u6765\u200b\u4e0d\u9519\u200b\uff01</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u901a\u8fc7\u200b\u5c06\u200b\u5176\u200b\u4f20\u9012\u200b\u7ed9\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_6</code> \u200b\u6765\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[28]: Copied! <pre># Pass the test image through model_6 to make a prediction\nmodel_6(test_image)\n</pre> # Pass the test image through model_6 to make a prediction model_6(test_image) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [28], in &lt;cell line: 2&gt;()\n      1 # Pass the test image through model_6 to make a prediction\n----&gt; 2 model_6(test_image)\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)</pre> <p>\u200b\u8be5\u6b7b\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u53c8\u200b\u9047\u5230\u200b\u4e86\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\u3002</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n</code></pre> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_6</code> \u200b\u4f4d\u4e8e\u200b GPU\uff08\u200b\u5373\u200b <code>\"cuda\"</code>\uff09\u200b\u4e0a\u200b\uff0c\u200b\u800c\u200b\u6211\u4eec\u200b\u7684\u200b <code>test_image</code> \u200b\u4f4d\u4e8e\u200b CPU \u200b\u4e0a\u200b\uff08\u200b\u5728\u200b PyTorch \u200b\u4e2d\u200b\uff0c\u200b\u6240\u6709\u200b\u5f20\u91cf\u200b\u9ed8\u8ba4\u200b\u4f4d\u4e8e\u200b CPU \u200b\u4e0a\u200b\uff09\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b <code>test_image</code> \u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b <code>device</code>\uff0c\u200b\u7136\u540e\u200b\u518d\u200b\u5c1d\u8bd5\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[30]: Copied! <pre># Send test_image to target device\nmodel_6(test_image.to(device))\n</pre> # Send test_image to target device model_6(test_image.to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [30], in &lt;cell line: 2&gt;()\n      1 # Send test_image to target device\n----&gt; 2 model_6(test_image.to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)</pre> <p>\u200b\u7cdf\u7cd5\u200b\uff01\u200b\u53c8\u200b\u662f\u200b\u9519\u8bef\u200b...</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)\n</code></pre> <p>\u200b\u8fd9\u6b21\u200b\u662f\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u4e4b\u524d\u200b\u4e5f\u200b\u9047\u5230\u200b\u8fc7\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u7684\u200b <code>test_image</code> \u200b\u5f62\u72b6\u200b\u51fa\u200b\u4e86\u200b\u4ec0\u4e48\u200b\u95ee\u9898\u200b\uff1f</p> <p>\u200b\u4e5f\u8bb8\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b\u5e26\u6709\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff1f</p> <p>\u200b\u800c\u200b\u6211\u4eec\u200b\u5f53\u524d\u200b\u7684\u200b <code>test_image</code> \u200b\u6ca1\u6709\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\uff1f</p> <p>\u200b\u8fd9\u91cc\u200b\u6709\u200b\u4e00\u4e2a\u200b\u6709\u7528\u200b\u7684\u200b\u7ecf\u9a8c\u200b\u6cd5\u5219\u200b\u9700\u8981\u200b\u8bb0\u4f4f\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u559c\u6b22\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u5b83\u4eec\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u76f8\u540c\u200b\u683c\u5f0f\u200b\u548c\u200b\u5f62\u72b6\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\uff0c\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b\u5e26\u6709\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u56fe\u50cf\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u5b83\u200b\u503e\u5411\u200b\u4e8e\u200b\u559c\u6b22\u200b\u9884\u6d4b\u200b\u5e26\u6709\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u56fe\u50cf\u200b\uff0c\u200b\u5373\u4f7f\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u53ea\u6709\u200b1\uff08\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u662f\u200b\u5728\u200b <code>torch.float32</code> \u200b\u683c\u5f0f\u200b\uff08\u200b\u6216\u200b\u5176\u4ed6\u200b\u683c\u5f0f\u200b\uff09\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u5b83\u200b\u4e5f\u200b\u4f1a\u200b\u559c\u6b22\u200b\u9884\u6d4b\u200b\u76f8\u540c\u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\uff08\u200b\u6211\u4eec\u200b\u7a0d\u540e\u200b\u4f1a\u200b\u770b\u5230\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>torch.unsqueeze()</code> \u200b\u65b9\u6cd5\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>test_image</code> \u200b\u6dfb\u52a0\u200b\u4e00\u4e2a\u200b\u5355\u4e00\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u3002</p> In\u00a0[31]: Copied! <pre># Changing the input size to be the same as what the model was trained on\noriginal_input_shape = test_image.shape\nupdated_input_shape = test_image.unsqueeze(dim=0).shape # adding a batch dimension on the \"0th\" dimension\n\n# Print out shapes of original tensor and updated tensor\nprint(f\"Original input data shape: {original_input_shape} -&gt; [height, width]\")\nprint(f\"Updated input data shape (with added batch dimension): {updated_input_shape} -&gt; [batch, height, width]\")\n</pre> # Changing the input size to be the same as what the model was trained on original_input_shape = test_image.shape updated_input_shape = test_image.unsqueeze(dim=0).shape # adding a batch dimension on the \"0th\" dimension  # Print out shapes of original tensor and updated tensor print(f\"Original input data shape: {original_input_shape} -&gt; [height, width]\") print(f\"Updated input data shape (with added batch dimension): {updated_input_shape} -&gt; [batch, height, width]\") <pre>Original input data shape: torch.Size([28, 28]) -&gt; [height, width]\nUpdated input data shape (with added batch dimension): torch.Size([1, 28, 28]) -&gt; [batch, height, width]\n</pre> <p>\u200b\u592a\u597d\u4e86\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u627e\u5230\u200b\u4e86\u200b\u4e00\u79cd\u200b\u65b9\u6cd5\u200b\u6765\u200b\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>test_image</code> \u200b\u6dfb\u52a0\u200b\u6279\u5904\u7406\u200b\u7ef4\u5ea6\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\u8ba9\u200b\u6211\u4eec\u200b\u518d\u200b\u5c1d\u8bd5\u200b\u5bf9\u200b\u5b83\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u9884\u6d4b\u200b\u3002</p> In\u00a0[32]: Copied! <pre># Make prediction on test image with additional batch size dimension and with it on the target device\nmodel_6(test_image.unsqueeze(dim=0).to(device))\n</pre> # Make prediction on test image with additional batch size dimension and with it on the target device model_6(test_image.unsqueeze(dim=0).to(device)) <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nInput In [32], in &lt;cell line: 2&gt;()\n      1 # Make prediction on test image with additional batch size dimension and with it on the target device\n----&gt; 2 model_6(test_image.unsqueeze(dim=0).to(device))\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139, in Sequential.forward(self, input)\n    137 def forward(self, input):\n    138     for module in self:\n--&gt; 139         input = module(input)\n    140     return input\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130, in Module._call_impl(self, *input, **kwargs)\n   1126 # If we don't have any hooks, we want to skip the rest of the logic in\n   1127 # this function, and just call forward.\n   1128 if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n   1129         or _global_forward_hooks or _global_forward_pre_hooks):\n-&gt; 1130     return forward_call(*input, **kwargs)\n   1131 # Do not call functions when jit is used\n   1132 full_backward_hooks, non_full_backward_hooks = [], []\n\nFile ~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114, in Linear.forward(self, input)\n    113 def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114     return F.linear(input, self.weight, self.bias)\n\nRuntimeError: expected scalar type Float but found Byte</pre> <p>\u200b\u4ec0\u4e48\u200b\uff1f</p> <p>\u200b\u53c8\u200b\u51fa\u9519\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8fd9\u6b21\u200b\u662f\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b\uff1a</p> <pre><code>/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n    112 \n    113     def forward(self, input: Tensor) -&gt; Tensor:\n--&gt; 114         return F.linear(input, self.weight, self.bias)\n    115 \n    116     def extra_repr(self) -&gt; str:\n\nRuntimeError: expected scalar type Float but found Byte\n</code></pre> <p>\u200b\u6211\u4eec\u200b\u9047\u5230\u200b\u4e86\u200b PyTorch \u200b\u4e2d\u200b\u7b2c\u4e09\u200b\u5e38\u89c1\u200b\u7684\u200b\u9519\u8bef\u200b\u2014\u2014\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5728\u200b\u4e0b\u200b\u4e00\u8282\u200b\u4e2d\u200b\u63a2\u8ba8\u200b\u5982\u4f55\u200b\u4fee\u590d\u200b\u5b83\u4eec\u200b\u3002</p> In\u00a0[33]: Copied! <pre># Get a single sample from the train_dataloader and print the dtype\ntrain_image_batch, train_label_batch = next(iter(train_dataloader))\ntrain_image_single, train_label_single = train_image_batch[0], train_label_batch[0]\n\n# Print the datatype of the train_image_single\nprint(f\"Datatype of training data: {train_image_single.dtype}\")\n</pre> # Get a single sample from the train_dataloader and print the dtype train_image_batch, train_label_batch = next(iter(train_dataloader)) train_image_single, train_label_single = train_image_batch[0], train_label_batch[0]  # Print the datatype of the train_image_single print(f\"Datatype of training data: {train_image_single.dtype}\") <pre>Datatype of training data: torch.float32\n</pre> <p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6211\u4eec\u200b\u786e\u8ba4\u200b\u4e86\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u662f\u200b <code>torch.float32</code> \u200b\u7c7b\u578b\u200b\u7684\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_6</code> \u200b\u5e0c\u671b\u200b\u5728\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u662f\u200b\u5408\u7406\u200b\u7684\u200b\u3002</p> <p>\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u662f\u200b\u5982\u4f55\u200b\u53d8\u6210\u200b\u8fd9\u4e2a\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u5462\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u53d1\u751f\u200b\u5728\u200b\u7b2c\u200b 1.3 \u200b\u8282\u200b\uff0c\u200b\u5f53\u200b\u6211\u4eec\u200b\u4e0b\u8f7d\u200b Fashion MNIST \u200b\u6570\u636e\u200b\u96c6\u200b\u5e76\u200b\u4f7f\u7528\u200b <code>torchvision.transforms.ToTensor()</code> \u200b\u7684\u200b <code>transform</code> \u200b\u53c2\u6570\u200b\u65f6\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b <code>transform</code> \u200b\u4f1a\u200b\u5c06\u200b\u4f20\u9012\u200b\u7ed9\u200b\u5b83\u200b\u7684\u200b\u4efb\u4f55\u200b\u6570\u636e\u200b\u8f6c\u6362\u200b\u4e3a\u200b <code>torch.Tensor</code>\uff0c\u200b\u5e76\u200b\u4f7f\u7528\u200b\u9ed8\u8ba4\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b <code>torch.float32</code>\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u53e6\u200b\u4e00\u4e2a\u200b\u7ecf\u9a8c\u200b\u6cd5\u5219\u200b\u662f\u200b\uff1a\u200b\u5728\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u65e0\u8bba\u200b\u5bf9\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u6267\u884c\u200b\u4e86\u200b\u54ea\u4e9b\u200b\u8f6c\u6362\u200b\uff0c\u200b\u90fd\u200b\u5e94\u8be5\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u6267\u884c\u200b\u76f8\u540c\u200b\u7684\u200b\u8f6c\u6362\u200b\u3002</p> In\u00a0[34]: Copied! <pre># Print out the original datatype of test_image\nprint(f\"Original datatype: {test_image.unsqueeze(dim=0).dtype}\")\n\n# Change the datatype of test_image and see the change\nprint(f\"Changing the datatype: {test_image.unsqueeze(dim=0).type(torch.float32).dtype}\")\n</pre> # Print out the original datatype of test_image print(f\"Original datatype: {test_image.unsqueeze(dim=0).dtype}\")  # Change the datatype of test_image and see the change print(f\"Changing the datatype: {test_image.unsqueeze(dim=0).type(torch.float32).dtype}\") <pre>Original datatype: torch.uint8\nChanging the datatype: torch.float32\n</pre> In\u00a0[35]: Copied! <pre># Make a prediction with model_6 on the transformed test_image\npred_on_gpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n                      .type(torch.float32) # convert the datatype to torch.float32\n                      .to(device)) # send the tensor to the target device\npred_on_gpu\n</pre> # Make a prediction with model_6 on the transformed test_image pred_on_gpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension                       .type(torch.float32) # convert the datatype to torch.float32                       .to(device)) # send the tensor to the target device pred_on_gpu Out[35]: <pre>tensor([[ -963.8352, -1658.8182,  -735.9952, -1285.2964,  -550.3845,   949.4190,\n          -538.1960,  1123.0616,   552.7371,  1413.8110]], device='cuda:0',\n       grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>\u200b\u54c7\u547c\u200b\uff01\uff01\uff01</p> <p>\u200b\u867d\u7136\u200b\u6b65\u9aa4\u200b\u4e0d\u5c11\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_6</code> \u200b\u6210\u529f\u200b\u5bf9\u200b <code>test_image</code> \u200b\u8fdb\u884c\u200b\u4e86\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b <code>test_image</code> \u200b\u9ed8\u8ba4\u200b\u4f4d\u4e8e\u200b CPU \u200b\u4e0a\u200b\uff0c\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>.cpu()</code> \u200b\u65b9\u6cd5\u200b \u200b\u5c06\u200b\u6a21\u578b\u200b\u653e\u200b\u56de\u200b CPU\uff0c\u200b\u5e76\u200b\u5728\u200b CPU \u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u76f8\u540c\u200b\u7684\u200b\u9884\u6d4b\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u5728\u200b GPU \u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</p> In\u00a0[36]: Copied! <pre># Put model back on CPU\nmodel_6.cpu()\n \n# Make a prediction on the CPU device (no need to put test_image on the CPU as it's already there)\npred_on_cpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n                      .type(torch.float32)) # convert the datatype to torch.float32 \npred_on_cpu\n</pre> # Put model back on CPU model_6.cpu()   # Make a prediction on the CPU device (no need to put test_image on the CPU as it's already there) pred_on_cpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension                       .type(torch.float32)) # convert the datatype to torch.float32  pred_on_cpu Out[36]: <pre>tensor([[ -963.8351, -1658.8182,  -735.9953, -1285.2964,  -550.3845,   949.4189,\n          -538.1960,  1123.0615,   552.7371,  1413.8110]],\n       grad_fn=&lt;AddmmBackward0&gt;)</pre> <p>\u200b\u9884\u6d4b\u200b\u518d\u6b21\u200b\u594f\u6548\u200b\u4e86\u200b\uff01</p> <p>\u200b\u8fd9\u662f\u200b\u6b63\u786e\u200b\u7684\u200b\u5417\u200b\uff1f</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u83b7\u53d6\u200b\u6a21\u578b\u200b\u7684\u200b\u539f\u59cb\u200b\u8f93\u51fa\u200b\uff0c\u200b\u5e76\u200b\u5c06\u200b\u5176\u200b\u4ece\u200b <code>\u200b\u539f\u59cb\u200b\u5bf9\u6570\u200b -&gt; \u200b\u9884\u6d4b\u200b\u6982\u7387\u200b -&gt; \u200b\u9884\u6d4b\u200b\u6807\u7b7e\u200b</code> \u200b\u8fdb\u884c\u200b\u8f6c\u6362\u200b\u6765\u200b\u9a8c\u8bc1\u200b\uff08\u200b\u66f4\u200b\u591a\u200b\u5173\u4e8e\u200b\u6b64\u200b\u8f6c\u6362\u200b\u7684\u200b\u5185\u5bb9\u200b\u8bf7\u200b\u53c2\u89c1\u200b 02. PyTorch \u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5206\u7c7b\u200b 3.1 \u200b\u8282\u200b\uff09\u3002</p> In\u00a0[37]: Copied! <pre># Convert raw logits to prediction probabilities\npred_probs = torch.softmax(pred_on_cpu, dim=1)\n\n# Convert prediction probabilities to prediction label\npred_label = torch.argmax(pred_probs, dim=1)\n\n# Check if it's correct\nprint(f\"Test label: {test_label}\")\nprint(f\"Pred label: {pred_label}\")\nprint(f\"Is the prediction correct? {pred_label.item() == test_label}\")\n</pre> # Convert raw logits to prediction probabilities pred_probs = torch.softmax(pred_on_cpu, dim=1)  # Convert prediction probabilities to prediction label pred_label = torch.argmax(pred_probs, dim=1)  # Check if it's correct print(f\"Test label: {test_label}\") print(f\"Pred label: {pred_label}\") print(f\"Is the prediction correct? {pred_label.item() == test_label}\") <pre>Test label: 9\nPred label: tensor([9])\nIs the prediction correct? True\n</pre> <p>\u200b\u5728\u200b\u8fdb\u884c\u200b\u6d4b\u8bd5\u200b\u6216\u200b\u81ea\u5b9a\u4e49\u200b\u6837\u672c\u200b\u7684\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u6d89\u53ca\u200b\u76f8\u5f53\u200b\u591a\u200b\u7684\u200b\u6b65\u9aa4\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u907f\u514d\u200b\u91cd\u590d\u200b\u6240\u6709\u200b\u8fd9\u4e9b\u200b\u6b65\u9aa4\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u5c06\u200b\u5b83\u4eec\u200b\u8f6c\u5316\u200b\u4e3a\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u3002</p> <p>\u200b\u5728\u200b04. PyTorch \u200b\u81ea\u200b\u5b9a\u4e49\u6570\u636e\u200b\u96c6\u200b \u200b\u7b2c\u200b11.3\u200b\u8282\u200b\uff1a\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u51fd\u6570\u200b\u6765\u200b\u9884\u6d4b\u200b\u81ea\u5b9a\u4e49\u200b\u56fe\u50cf\u200b\u4e2d\u6709\u200b\u4e00\u4e2a\u200b\u8fd9\u65b9\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#pytorch","title":"PyTorch \u200b\u4e2d\u200b\u6700\u200b\u5e38\u89c1\u200b\u7684\u200b\u4e09\u79cd\u200b\u9519\u8bef\u200b\u00b6","text":"<p>PyTorch \u200b\u662f\u200b\u5f53\u524d\u200b\u53ef\u7528\u200b\u7684\u200b\u6700\u5927\u200b\u7684\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u5e93\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u56e0\u6b64\u200b\uff0c\u200b\u5728\u200b\u4f7f\u7528\u200b\u5b83\u200b\u65f6\u200b\uff0c\u200b\u4f60\u200b\u5f88\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u5404\u79cd\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u7531\u4e8e\u200b\u5e93\u200b\u7684\u200b\u521b\u5efa\u8005\u200b\u8fdb\u884c\u200b\u4e86\u200b\u5404\u79cd\u200b\u7ef4\u62a4\u200b\u548c\u200b\u68c0\u67e5\u200b\uff0c\u200b\u56e0\u6b64\u200b\u9519\u8bef\u200b\u5f88\u5c11\u200b\u662f\u56e0\u4e3a\u200b\u5e93\u200b\u672c\u8eab\u200b\u7684\u200b\u95ee\u9898\u200b\u3002</p> <p>\u200b\u8fd9\u200b\u610f\u5473\u7740\u200b\u4f60\u200b\u9047\u5230\u200b\u7684\u200b\u5927\u591a\u6570\u200b\u9519\u8bef\u200b\u90fd\u200b\u662f\u200b\u7528\u6237\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u66f4\u200b\u5177\u4f53\u5730\u8bf4\u200b\uff0c\u200b\u4f60\u200b\u5199\u9519\u200b\u4e86\u200b\u4ee3\u7801\u200b\u3002</p> <p>\u200b\u4e0d\u8981\u200b\u89c9\u5f97\u200b\u88ab\u200b\u5192\u72af\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7a0b\u5e8f\u5458\u200b\u90fd\u200b\u4f1a\u200b\u9047\u5230\u200b\u8fd9\u79cd\u200b\u60c5\u51b5\u200b\u3002</p> <p>\u200b\u5728\u200b\u7528\u6237\u200b\u9519\u8bef\u200b\u4e2d\u200b\uff0c\u200b\u5f88\u200b\u53ef\u80fd\u200b\u662f\u200b\u4ee5\u4e0b\u200b\u4e09\u79cd\u200b\u4e4b\u4e00\u200b\uff1a</p> <ol> <li>\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b - \u200b\u4f60\u200b\u8bd5\u56fe\u200b\u5bf9\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\u7684\u200b\u77e9\u9635\u200b/\u200b\u5f20\u91cf\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u5f62\u72b6\u200b\u662f\u200b <code>[1, 28, 28]</code>\uff0c\u200b\u4f46\u200b\u4f60\u200b\u7684\u200b\u7b2c\u4e00\u5c42\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u662f\u200b <code>[10]</code>\u3002</li> <li>\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b - \u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002\u200b\u4f8b\u5982\u200b\uff0c\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\uff08\u200b\u4f8b\u5982\u200b <code>\"cuda\"</code>\uff09\uff0c\u200b\u800c\u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u5728\u200b CPU \u200b\u4e0a\u200b\uff08\u200b\u4f8b\u5982\u200b <code>\"cpu\"</code>\uff09\u3002</li> <li>\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b - \u200b\u4f60\u200b\u7684\u200b\u6570\u636e\u200b\u662f\u200b\u4e00\u79cd\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08\u200b\u4f8b\u5982\u200b <code>torch.float32</code>\uff09\uff0c\u200b\u4f46\u200b\u4f60\u200b\u8981\u200b\u6267\u884c\u200b\u7684\u200b\u64cd\u4f5c\u200b\u9700\u8981\u200b\u53e6\u200b\u4e00\u79cd\u200b\u6570\u636e\u7c7b\u578b\u200b\uff08\u200b\u4f8b\u5982\u200b <code>torch.int64</code>\uff09\u3002</li> </ol> <p></p> <p>\u200b\u6ce8\u610f\u200b\u8fd9\u91cc\u200b\u7684\u200b\u91cd\u590d\u200b\u4e3b\u9898\u200b\u3002</p> <p>\u200b\u4f60\u200b\u7684\u200b\u5f62\u72b6\u200b\u3001\u200b\u8bbe\u5907\u200b\u548c\u200b/\u200b\u6216\u200b\u6570\u636e\u7c7b\u578b\u200b\u4e4b\u95f4\u200b\u5b58\u5728\u200b\u67d0\u79cd\u200b\u4e0d\u200b\u5339\u914d\u200b\u3002</p> <p>\u200b\u672c\u200b\u7b14\u8bb0\u672c\u200b/\u200b\u535a\u5ba2\u200b\u6587\u7ae0\u200b\u5c06\u200b\u901a\u8fc7\u200b\u793a\u4f8b\u200b\u4ecb\u7ecd\u200b\u4e0a\u8ff0\u200b\u6bcf\u79cd\u200b\u9519\u8bef\u200b\u53ca\u5176\u200b\u89e3\u51b3\u200b\u65b9\u6cd5\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u8fd9\u200b\u4e0d\u4f1a\u200b\u963b\u6b62\u200b\u4f60\u200b\u5c06\u200b\u6765\u72af\u200b\u8fd9\u4e9b\u200b\u9519\u8bef\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u4f1a\u200b\u8ba9\u200b\u4f60\u200b\u8db3\u591f\u200b\u610f\u8bc6\u200b\u5230\u200b\u8fd9\u4e9b\u200b\u9519\u8bef\u200b\uff0c\u200b\u4ece\u800c\u200b\u51cf\u5c11\u200b\u5b83\u4eec\u200b\uff0c\u200b\u66f4\u200b\u91cd\u8981\u200b\u7684\u200b\u662f\u200b\uff0c\u200b\u77e5\u9053\u200b\u5982\u4f55\u200b\u89e3\u51b3\u200b\u5b83\u4eec\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ee5\u4e0b\u200b\u6240\u6709\u200b\u793a\u4f8b\u200b\u5747\u200b\u6539\u7f16\u81ea\u200b learnpytorch.io\uff0c\u200b\u8fd9\u662f\u200b Zero to Mastery: PyTorch for Deep Learning \u200b\u89c6\u9891\u200b\u8bfe\u7a0b\u200b\u7684\u200b\u4e66\u7c4d\u200b\u7248\u672c\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#1-pytorch","title":"1. PyTorch\u200b\u4e2d\u200b\u7684\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u00b6","text":""},{"location":"pytorch_most_common_errors/#11","title":"1.1 \u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u7684\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u00b6","text":"<p>PyTorch \u200b\u662f\u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6a21\u578b\u200b\u7684\u200b\u6700\u4f73\u200b\u6846\u67b6\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u57fa\u672c\u64cd\u4f5c\u200b\u4e4b\u4e00\u200b\u5c31\u662f\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u3002</p> <p>\u200b\u7136\u800c\u200b\uff0c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u6709\u200b\u975e\u5e38\u200b\u5177\u4f53\u200b\u7684\u200b\u89c4\u5b9a\u200b\u3002</p> <p>\u200b\u5982\u679c\u200b\u4e0d\u200b\u9075\u5b88\u200b\u8fd9\u4e9b\u200b\u89c4\u5219\u200b\uff0c\u200b\u4f60\u200b\u4f1a\u200b\u9047\u5230\u200b\u8457\u540d\u200b\u7684\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u3002</p> <pre><code>RuntimeError: mat1 \u200b\u548c\u200b mat2 \u200b\u7684\u200b\u5f62\u72b6\u200b\u65e0\u6cd5\u200b\u76f8\u4e58\u200b (3x4 \u200b\u548c\u200b 3x4)\n</code></pre> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u7b80\u77ed\u200b\u7684\u200b\u4f8b\u5b50\u200b\u5f00\u59cb\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5c3d\u7ba1\u200b\u5b83\u200b\u88ab\u200b\u79f0\u4e3a\u200b\u201c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u201d\uff0c\u200b\u4f46\u200b PyTorch \u200b\u4e2d\u200b\u7684\u200b\u51e0\u4e4e\u200b\u6240\u6709\u200b\u6570\u636e\u200b\u5f62\u5f0f\u200b\u90fd\u200b\u662f\u200b\u4ee5\u200b\u5f20\u91cf\u200b\u7684\u200b\u5f62\u5f0f\u200b\u51fa\u73b0\u200b\u7684\u200b\u3002\u200b\u5f20\u91cf\u200b\u662f\u200b\u4e00\u4e2a\u200b n \u200b\u7ef4\u200b\u6570\u7ec4\u200b\uff08n \u200b\u53ef\u4ee5\u200b\u662f\u200b\u4efb\u4f55\u200b\u6570\u5b57\u200b\uff09\u3002\u200b\u56e0\u6b64\u200b\uff0c\u200b\u867d\u7136\u200b\u6211\u200b\u4f7f\u7528\u200b\u201c\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\u201d\u200b\u8fd9\u4e2a\u200b\u672f\u8bed\u200b\uff0c\u200b\u4f46\u200b\u8fd9\u200b\u540c\u6837\u200b\u9002\u7528\u200b\u4e8e\u200b\u201c\u200b\u5f20\u91cf\u200b\u4e58\u6cd5\u200b\u201d\u3002\u200b\u6709\u5173\u200b\u77e9\u9635\u200b\u548c\u200b\u5f20\u91cf\u200b\u4e4b\u95f4\u200b\u5dee\u5f02\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 00. PyTorch \u200b\u57fa\u7840\u200b\uff1a\u200b\u5f20\u91cf\u200b\u4ecb\u7ecd\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#12-pytorch","title":"1.2 PyTorch\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u5728\u200b\u4f7f\u7528\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u6216\u200b\u77e9\u9635\u200b\u4e58\u4ee5\u200b\u5f20\u91cf\u200b\uff09\u200b\u65f6\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u7684\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u3002</p> <p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200bPyTorch\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u770b\u770b\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u53ef\u80fd\u200b\u51fa\u73b0\u200b\u5728\u200b\u54ea\u4e9b\u5730\u65b9\u200b\u3002</p> <p>\u200b\u5728\u200b\u4ee5\u4e0b\u200b\u4efb\u4e00\u200b\u60c5\u51b5\u200b\u4e0b\u200b\uff0c\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u4e2d\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff1a</p> <ul> <li>\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u6b63\u786e\u200b - \u200b\u60a8\u200b\u7684\u200b\u6570\u636e\u200b\u5177\u6709\u200b\u67d0\u79cd\u200b\u5f62\u72b6\u200b\uff0c\u200b\u4f46\u200b\u6a21\u578b\u200b\u7684\u200b\u7b2c\u4e00\u5c42\u200b\u671f\u671b\u200b\u4e0d\u540c\u200b\u7684\u200b\u5f62\u72b6\u200b\u3002</li> <li>\u200b\u5c42\u200b\u4e0e\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b - \u200b\u6a21\u578b\u200b\u4e2d\u200b\u7684\u200b\u67d0\u200b\u4e00\u5c42\u200b\u8f93\u51fa\u200b\u67d0\u79cd\u200b\u5f62\u72b6\u200b\uff0c\u200b\u4f46\u200b\u4e0b\u200b\u4e00\u5c42\u200b\u671f\u671b\u200b\u4e0d\u540c\u200b\u7684\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u3002</li> <li>\u200b\u5728\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\u8f93\u5165\u200b\u6570\u636e\u200b\u4e2d\u200b\u6ca1\u6709\u200b\u6279\u6b21\u200b\u5927\u5c0f\u200b\u7ef4\u5ea6\u200b - \u200b\u60a8\u200b\u7684\u200b\u6a21\u578b\u200b\u5728\u200b\u5177\u6709\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u6837\u672c\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u56e0\u6b64\u200b\u5f53\u200b\u60a8\u200b\u5c1d\u8bd5\u200b\u5728\u200b\u6ca1\u6709\u200b\u6279\u6b21\u200b\u7ef4\u5ea6\u200b\u7684\u200b\u5355\u4e2a\u200b\u6837\u672c\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u65f6\u200b\uff0c\u200b\u4f1a\u200b\u51fa\u73b0\u200b\u9519\u8bef\u200b\u3002</li> </ul> <p>\u200b\u4e3a\u4e86\u200b\u5c55\u793a\u200b\u8fd9\u4e9b\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u7b80\u5355\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff08\u200b\u65e0\u8bba\u200b\u7f51\u7edc\u200b\u5927\u5c0f\u200b\u5982\u4f55\u200b\uff0c\u200b\u9519\u8bef\u200b\u90fd\u200b\u662f\u200b\u76f8\u540c\u200b\u7684\u200b\uff09\uff0c\u200b\u5c1d\u8bd5\u200b\u5728\u200bFashion MNIST\u200b\u6570\u636e\u200b\u96c6\u200b\uff0810\u200b\u79cd\u200b\u4e0d\u540c\u200b\u7c7b\u522b\u200b\u7684\u200b\u670d\u88c5\u200b\u7684\u200b\u9ed1\u767d\u200b\u56fe\u50cf\u200b\uff09\u200b\u4e2d\u200b\u5bfb\u627e\u200b\u6a21\u5f0f\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u4ee5\u4e0b\u200b\u793a\u4f8b\u200b\u7279\u522b\u200b\u5173\u6ce8\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u6784\u5efa\u200b\u6700\u4f73\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002\u200b\u60a8\u200b\u53ef\u4ee5\u200b\u5728\u200b03. PyTorch\u200b\u8ba1\u7b97\u673a\u200b\u89c6\u89c9\u200b\u4e2d\u200b\u770b\u5230\u200b\u8fd9\u4e2a\u200b\u95ee\u9898\u200b\u7684\u200b\u5b8c\u6574\u200b\u5de5\u4f5c\u200b\u793a\u4f8b\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#13","title":"1.3 \u200b\u4e0b\u8f7d\u200b\u6570\u636e\u200b\u96c6\u200b\u00b6","text":"<p>\u200b\u9996\u5148\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b <code>torchvision.datasets</code> \u200b\u83b7\u53d6\u200b Fashion MNIST \u200b\u6570\u636e\u200b\u96c6\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#14","title":"1.4 \u200b\u6784\u5efa\u200b\u5177\u6709\u200b\u4e0d\u540c\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7cfb\u5217\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u7684\u200b\u95ee\u9898\u200b\u662f\u200b\uff1a\u200b\u6784\u5efa\u200b\u4e00\u4e2a\u200b\u80fd\u591f\u200b\u8bc6\u522b\u200b\u670d\u88c5\u200b\u7070\u5ea6\u200b\u56fe\u50cf\u200b\u4e2d\u200b\u6a21\u5f0f\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u3002</p> <p>\u200b\u8fd9\u4e2a\u200b\u9648\u8ff0\u200b\u53ef\u4ee5\u200b\u975e\u5e38\u200b\u6df1\u5165\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u201c\u200b\u54ea\u200b\u79cd\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u6700\u597d\u200b\uff1f\u201d\u200b\u662f\u200b\u6574\u4e2a\u200b\u673a\u5668\u200b\u5b66\u4e60\u200b\u9886\u57df\u200b\u7684\u200b\u4e3b\u8981\u200b\u7814\u7a76\u200b\u95ee\u9898\u200b\u4e4b\u4e00\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c3d\u53ef\u80fd\u200b\u7b80\u5355\u200b\u5730\u200b\u5f00\u59cb\u200b\uff0c\u200b\u4ee5\u200b\u5c55\u793a\u200b\u4e0d\u540c\u200b\u7684\u200b\u9519\u8bef\u200b\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200bPyTorch\u200b\u6784\u5efa\u200b\u51e0\u4e2a\u200b\u4e24\u5c42\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u7f51\u7edc\u200b\u5c55\u793a\u200b\u4e00\u79cd\u200b\u4e0d\u540c\u200b\u7684\u200b\u9519\u8bef\u200b\uff1a</p> \u200b\u6a21\u578b\u200b\u7f16\u53f7\u200b \u200b\u5c42\u200b \u200b\u9519\u8bef\u200b\u5c55\u793a\u200b 0 2\u200b\u4e2a\u200b\u5e26\u6709\u200b10\u200b\u4e2a\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u7684\u200b<code>nn.Linear()</code> \u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b 1 \u200b\u4e0e\u200b\u6a21\u578b\u200b1\u200b\u76f8\u540c\u200b + 1\u200b\u4e2a\u200b<code>nn.Flatten()</code> \u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\uff08\u200b\u4ecd\u7136\u200b\uff09 2 1\u200b\u4e2a\u200b<code>nn.Flatten()</code>\uff0c1\u200b\u4e2a\u200b\u5177\u6709\u200b\u6b63\u786e\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u7684\u200b<code>nn.Linear()</code>\u200b\u548c\u200b1\u200b\u4e2a\u200b\u5e26\u6709\u200b10\u200b\u4e2a\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u7684\u200b<code>nn.Linear()</code> \u200b\u65e0\u200b\uff08\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u6b63\u786e\u200b\uff09 3 \u200b\u4e0e\u200b\u6a21\u578b\u200b2\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b<code>nn.Linear()</code>\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5f62\u72b6\u200b\u4e0d\u540c\u200b \u200b\u5c42\u200b\u4e4b\u95f4\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b 4 \u200b\u4e0e\u200b\u6a21\u578b\u200b3\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u6700\u540e\u200b\u4e00\u5c42\u200b\u66ff\u6362\u200b\u4e3a\u200b<code>nn.LazyLinear()</code> \u200b\u65e0\u200b\uff08\u200b\u5c55\u793a\u200b<code>nn.LazyX()</code>\u200b\u5c42\u200b\u5982\u4f55\u200b\u63a8\u65ad\u200b\u6b63\u786e\u200b\u5f62\u72b6\u200b\uff09 5 \u200b\u4e0e\u200b\u6a21\u578b\u200b4\u200b\u76f8\u540c\u200b\uff0c\u200b\u4f46\u200b\u6240\u6709\u200b<code>nn.Linear()</code>\u200b\u66ff\u6362\u200b\u4e3a\u200b<code>nn.LazyLinear()</code> \u200b\u65e0\u200b\uff08\u200b\u5c55\u793a\u200b<code>nn.LazyX()</code>\u200b\u5c42\u200b\u5982\u4f55\u200b\u63a8\u65ad\u200b\u6b63\u786e\u200b\u5f62\u72b6\u200b\uff09"},{"location":"pytorch_most_common_errors/#15","title":"1.5 \u200b\u8f93\u5165\u200b\u5c42\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4ece\u200b\u4e00\u4e2a\u200b\u5305\u542b\u200b\u4e24\u4e2a\u200b <code>nn.Linear()</code> \u200b\u5c42\u200b\u7684\u200b\u7f51\u7edc\u200b\u5f00\u59cb\u200b\uff0c\u200b\u6bcf\u4e2a\u200b\u5c42\u6709\u200b 10 \u200b\u4e2a\u200b\u9690\u85cf\u200b\u5355\u5143\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u5173\u4e8e\u200b <code>nn.Linear()</code> \u200b\u5185\u90e8\u200b\u7684\u200b\u5de5\u4f5c\u200b\u539f\u7406\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u7b2c\u200b 6 \u200b\u8282\u200b\uff1a\u200b\u7efc\u5408\u200b\u5e94\u7528\u200b\u3002</p> <p>\u200b\u7136\u540e\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u628a\u200b <code>image</code> \u200b\u4f20\u9012\u200b\u7ed9\u200b\u8fd9\u4e2a\u200b\u7f51\u7edc\u200b\uff0c\u200b\u770b\u770b\u200b\u4f1a\u200b\u53d1\u751f\u200b\u4ec0\u4e48\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#16","title":"1.6 \u200b\u9690\u85cf\u200b\u5c42\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\u00b6","text":"<p>\u200b\u5982\u679c\u200b\u6211\u4eec\u200b\u7684\u200b\u8f93\u5165\u200b\u5c42\u200b\u5177\u6709\u200b\u6b63\u786e\u200b\u7684\u200b\u5f62\u72b6\u200b\uff0c\u200b\u4f46\u200b\u8fde\u63a5\u200b\u7684\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u5b58\u5728\u200b\u4e0d\u200b\u5339\u914d\u200b\u4f1a\u200b\u600e\u6837\u200b\uff1f</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u7b2c\u4e00\u4e2a\u200b <code>nn.Linear()</code> \u200b\u7684\u200b <code>out_features=10</code>\uff0c\u200b\u4f46\u200b\u4e0b\u200b\u4e00\u4e2a\u200b <code>nn.Linear()</code> \u200b\u7684\u200b <code>in_features=5</code>\u3002</p> <p>\u200b\u8fd9\u200b\u5c31\u662f\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u8f93\u5165\u200b\u548c\u200b\u8f93\u51fa\u200b\u5f62\u72b6\u200b\u4e0d\u200b\u5339\u914d\u200b\u7684\u200b\u4f8b\u5b50\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#17-pytorch","title":"1.7 PyTorch \u200b\u60f0\u6027\u200b\u5c42\u200b\uff08\u200b\u81ea\u52a8\u200b\u63a8\u65ad\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\uff09\u00b6","text":"<p>PyTorch \u200b\u4e2d\u200b\u7684\u200b\u60f0\u6027\u200b\u5c42\u200b\u901a\u5e38\u200b\u4ee5\u200b <code>nn.LazyX</code> \u200b\u7684\u200b\u5f62\u5f0f\u200b\u51fa\u73b0\u200b\uff0c\u200b\u5176\u4e2d\u200b <code>X</code> \u200b\u662f\u200b\u73b0\u6709\u200b\u975e\u200b\u60f0\u6027\u200b\u5c42\u200b\u7684\u200b\u5bf9\u5e94\u200b\u5f62\u5f0f\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c<code>nn.Linear()</code> \u200b\u7684\u200b\u60f0\u6027\u200b\u7b49\u6548\u200b\u5c42\u200b\u662f\u200b <code>nn.LazyLinear()</code>\u3002</p> <p><code>Lazy</code> \u200b\u5c42\u200b\u7684\u200b\u4e3b\u8981\u200b\u7279\u70b9\u200b\u662f\u200b\u80fd\u591f\u200b\u63a8\u65ad\u200b <code>in_features</code> \u200b\u6216\u200b\u4ece\u524d\u200b\u4e00\u5c42\u200b\u7684\u200b\u8f93\u5165\u200b\u5f62\u72b6\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u622a\u81f3\u200b 2022 \u200b\u5e74\u200b 11 \u200b\u6708\u200b\uff0cPyTorch \u200b\u4e2d\u200b\u7684\u200b <code>Lazy</code> \u200b\u5c42\u200b\u4ecd\u200b\u5904\u4e8e\u200b\u5b9e\u9a8c\u200b\u9636\u6bb5\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u53d1\u751f\u53d8\u5316\u200b\uff0c\u200b\u4f46\u200b\u5b83\u4eec\u200b\u7684\u200b\u7528\u6cd5\u200b\u5e94\u8be5\u200b\u4e0e\u200b\u4e0b\u9762\u200b\u7684\u200b\u793a\u4f8b\u200b\u4e0d\u4f1a\u200b\u6709\u592a\u5927\u200b\u5dee\u5f02\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5982\u679c\u200b\u524d\u200b\u4e00\u5c42\u200b\u7684\u200b <code>out_features=10</code>\uff0c\u200b\u90a3\u4e48\u200b\u540e\u7eed\u200b\u7684\u200b <code>Lazy</code> \u200b\u5c42\u200b\u5e94\u8be5\u200b\u63a8\u65ad\u51fa\u200b <code>in_features=10</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u6d4b\u8bd5\u200b\u4e00\u4e0b\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#2-pytorch","title":"2. PyTorch \u200b\u4e2d\u200b\u7684\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\u00b6","text":"<p>PyTorch \u200b\u7684\u200b\u4e3b\u8981\u200b\u4f18\u52bf\u200b\u4e4b\u4e00\u200b\u662f\u200b\u5176\u200b\u5185\u7f6e\u200b\u7684\u200b\u80fd\u591f\u200b\u5728\u200b GPU\uff08\u200b\u56fe\u5f62\u200b\u5904\u7406\u5355\u5143\u200b\uff09\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u7684\u200b\u80fd\u529b\u200b\u3002</p> <p>GPU \u200b\u901a\u5e38\u200b\u80fd\u591f\u200b\u6bd4\u200b CPU\uff08\u200b\u4e2d\u592e\u200b\u5904\u7406\u5355\u5143\u200b\uff09\u200b\u66f4\u5feb\u200b\u5730\u200b\u6267\u884c\u200b\u64cd\u4f5c\u200b\uff0c\u200b\u7279\u522b\u200b\u662f\u200b\u77e9\u9635\u200b\u4e58\u6cd5\u200b\uff08\u200b\u8fd9\u662f\u200b\u6784\u6210\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u7684\u200b\u5927\u90e8\u5206\u200b\u64cd\u4f5c\u200b\uff09\u3002</p> <p>\u200b\u5982\u679c\u200b\u4f60\u200b\u4f7f\u7528\u200b\u7684\u200b\u662f\u200b\u539f\u751f\u200b PyTorch\uff08\u200b\u6ca1\u6709\u200b\u5176\u4ed6\u200b\u5916\u90e8\u200b\u5e93\u200b\uff09\uff0cPyTorch \u200b\u8981\u6c42\u200b\u4f60\u200b\u660e\u786e\u200b\u8bbe\u7f6e\u200b\u5728\u200b\u54ea\u4e2a\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u8981\u200b\u5c06\u200b\u6a21\u578b\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\uff0c\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>to()</code> \u200b\u65b9\u6cd5\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>model.to(device)</code>\u3002</p> <p>\u200b\u540c\u6837\u200b\u5730\u200b\uff0c\u200b\u5bf9\u4e8e\u200b\u6570\u636e\u200b <code>some_dataset.to(device)</code>\u3002</p> <p>\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b \u200b\u53d1\u751f\u200b\u5728\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200b\u4e0d\u540c\u200b\u8bbe\u5907\u200b\u4e0a\u65f6\u200b\u3002</p> <p>\u200b\u4f8b\u5982\u200b\uff0c\u200b\u5f53\u200b\u4f60\u200b\u5c06\u200b\u6a21\u578b\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b GPU \u200b\u8bbe\u5907\u200b\uff0c\u200b\u4f46\u200b\u6570\u636e\u200b\u4ecd\u7136\u200b\u5728\u200b CPU \u200b\u4e0a\u65f6\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#21","title":"2.1 \u200b\u8bbe\u7f6e\u200b\u76ee\u6807\u200b\u8bbe\u5907\u200b\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b\u5f53\u524d\u200b\u8bbe\u5907\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b <code>\"cuda\"</code>\uff08\u200b\u5982\u679c\u200b\u53ef\u7528\u200b\uff09\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6709\u5173\u200b\u5982\u4f55\u200b\u83b7\u53d6\u200b GPU \u200b\u5e76\u200b\u4f7f\u7528\u200b PyTorch \u200b\u8fdb\u884c\u200b\u8bbe\u7f6e\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 00. PyTorch \u200b\u57fa\u7840\u200b\uff1a\u200b\u5728\u200b GPU \u200b\u4e0a\u200b\u8fd0\u884c\u200b\u5f20\u91cf\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#22","title":"2.2 \u200b\u4e3a\u200b\u5efa\u6a21\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u00b6","text":"<p>\u200b\u4e3a\u4e86\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u5efa\u6a21\u200b\uff0c\u200b\u6211\u4eec\u200b\u6765\u200b\u521b\u5efa\u200b\u4e00\u4e9b\u200b PyTorch \u200b\u7684\u200b <code>DataLoader</code>\u3002</p> <p>\u200b\u4e3a\u4e86\u200b\u52a0\u5feb\u901f\u5ea6\u200b\uff0c\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b <code>torch.utils.data.RandomSampler</code> \u200b\u5b9e\u4f8b\u200b\u6765\u200b\u968f\u673a\u200b\u9009\u62e9\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u6837\u672c\u200b\u7684\u200b 10%\uff08\u200b\u6211\u4eec\u200b\u5e76\u200b\u4e0d\u200b\u592a\u200b\u5173\u5fc3\u200b\u6027\u80fd\u200b\u6700\u4f73\u200b\u7684\u200b\u6a21\u578b\u200b\uff0c\u200b\u800c\u662f\u200b\u66f4\u200b\u5173\u6ce8\u200b\u5c55\u793a\u200b\u6f5c\u5728\u200b\u7684\u200b\u9519\u8bef\u200b\uff09\u3002</p> <p>\u200b\u6211\u4eec\u200b\u8fd8\u200b\u5c06\u200b\u8bbe\u7f6e\u200b\u4e00\u4e2a\u200b\u635f\u5931\u200b\u51fd\u6570\u200b <code>torch.nn.CrossEntropyLoss()</code> \u200b\u548c\u200b\u4e00\u4e2a\u200b\u4f18\u5316\u200b\u5668\u200b <code>torch.optim.SGD(lr=0.01)</code>\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6709\u5173\u200b\u4e3a\u200b\u8bad\u7ec3\u200b PyTorch \u200b\u6a21\u578b\u200b\u51c6\u5907\u200b\u6570\u636e\u200b\u3001\u200b\u635f\u5931\u200b\u51fd\u6570\u200b\u548c\u200b\u4f18\u5316\u200b\u5668\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b 01. PyTorch \u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u57fa\u7840\u200b \u200b\u7b2c\u200b3\u200b\u8282\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#23-cpu","title":"2.3 \u200b\u5728\u200bCPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u00b6","text":"<p>\u200b\u6570\u636e\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u6a21\u578b\u200b\u4e5f\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5f00\u59cb\u200b\u8bad\u7ec3\u200b\u5427\u200b\uff01</p> <p>\u200b\u6211\u4eec\u200b\u5c06\u200b\u4f7f\u7528\u200b\u6807\u51c6\u200b\u7684\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\uff0c\u200b\u5bf9\u200b<code>model_6</code>\u200b\u8fdb\u884c\u200b\u4e94\u8f6e\u200b\u8bad\u7ec3\u200b\uff0c\u200b\u4f7f\u7528\u200b10%\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u5728\u200b\u8fd9\u91cc\u200b\u4e0d\u5fc5\u200b\u8fc7\u4e8e\u200b\u62c5\u5fc3\u200b\u635f\u5931\u200b\u503c\u200b\u662f\u5426\u200b\u5c3d\u53ef\u80fd\u200b\u4f4e\u200b\uff0c\u200b\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u66f4\u200b\u5173\u6ce8\u200b\u7684\u200b\u662f\u200b\u786e\u4fdd\u200b\u6ca1\u6709\u200b\u9519\u8bef\u200b\uff0c\u200b\u800c\u200b\u4e0d\u662f\u200b\u8ffd\u6c42\u200b\u6700\u4f4e\u200b\u53ef\u80fd\u200b\u7684\u200b\u635f\u5931\u200b\u503c\u200b\u3002</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u6709\u5173\u200bPyTorch\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u6b65\u9aa4\u200b\u7684\u200b\u66f4\u200b\u591a\u200b\u4fe1\u606f\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b01. PyTorch\u200b\u5de5\u4f5c\u200b\u6d41\u7a0b\u200b\u7b2c\u200b3\u200b\u8282\u200b\uff1aPyTorch\u200b\u8bad\u7ec3\u200b\u5faa\u73af\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#24-gpu","title":"2.4 \u200b\u5c1d\u8bd5\u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff08\u200b\u542b\u200b\u9519\u8bef\u200b\uff09\u00b6","text":"<p>\u200b\u73b0\u5728\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c06\u200b <code>model_6</code> \u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b <code>device</code>\uff08\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u8fd9\u662f\u200b\u4e00\u4e2a\u200b <code>\"cuda\"</code> GPU\uff09\u3002</p>"},{"location":"pytorch_most_common_errors/#25-gpu","title":"2.5 \u200b\u5728\u200bGPU\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\uff08\u200b\u65e0\u200b\u9519\u8bef\u200b\uff09\u00b6","text":"<p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u89e3\u51b3\u200b\u8fd9\u4e2a\u200b\u9519\u8bef\u200b\uff0c\u200b\u5c06\u200b\u6570\u636e\u200b\u5f20\u91cf\u200b\uff08<code>X</code>\u200b\u548c\u200b<code>y</code>\uff09\u200b\u4e5f\u200b\u53d1\u9001\u5230\u200b\u76ee\u6807\u200b<code>device</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4f7f\u7528\u200b<code>X.to(device)</code>\u200b\u548c\u200b<code>y.to(device)</code>\u200b\u6765\u200b\u5b9e\u73b0\u200b\u8fd9\u200b\u4e00\u70b9\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#26","title":"2.6 \u200b\u5728\u200b\u9884\u6d4b\u200b\u65f6\u200b\u51fa\u73b0\u200b\u7684\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5728\u200b\u8bad\u7ec3\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\u5df2\u7ecf\u200b\u89c1\u8fc7\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b\uff0c\u200b\u4f46\u200b\u540c\u6837\u200b\u7684\u200b\u9519\u8bef\u200b\u4e5f\u200b\u53ef\u80fd\u200b\u5728\u200b\u6d4b\u8bd5\u200b\u6216\u200b\u63a8\u7406\u200b\uff08\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff09\u200b\u65f6\u200b\u53d1\u751f\u200b\u3002</p> <p>\u200b\u8bad\u7ec3\u200b\u6a21\u578b\u200b\u5e76\u200b\u5728\u200b\u67d0\u4e9b\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u8bad\u7ec3\u200b\u7684\u200b\u6574\u4e2a\u200b\u60f3\u6cd5\u200b\u662f\u200b\u4e3a\u4e86\u200b\u5229\u7528\u200b\u5b83\u200b\u5bf9\u200b\u672a\u200b\u89c1\u200b\u8fc7\u200b\u7684\u200b\u6570\u636e\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u4f7f\u7528\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b <code>model_6</code>\uff0c\u200b\u5e76\u200b\u5229\u7528\u200b\u5b83\u200b\u5bf9\u200b\u6d4b\u8bd5\u6570\u636e\u200b\u96c6\u4e2d\u200b\u7684\u200b\u4e00\u4e2a\u200b\u6837\u672c\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#3-pytorch","title":"3. PyTorch \u200b\u4e2d\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b\u00b6","text":"<p>\u200b\u56de\u987e\u200b\u4e00\u4e0b\u200b\u7ecf\u9a8c\u200b\u6cd5\u5219\u200b\uff1a\u200b\u8bad\u7ec3\u200b\u597d\u200b\u7684\u200b\u6a21\u578b\u200b\u559c\u6b22\u200b\u9884\u6d4b\u200b\u4e0e\u200b\u5b83\u4eec\u200b\u8bad\u7ec3\u200b\u65f6\u200b\u5f62\u72b6\u200b\u548c\u200b\u683c\u5f0f\u200b\u76f8\u540c\u200b\u7684\u200b\u6570\u636e\u200b\u3002</p> <p>\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u7684\u200b\u6a21\u578b\u200b\u671f\u671b\u200b\u4e00\u4e2a\u200b <code>Float</code> \u200b\u6570\u636e\u7c7b\u578b\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u7684\u200b <code>test_image</code> \u200b\u662f\u200b <code>Byte</code> \u200b\u6570\u636e\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u4e4b\u524d\u200b\u7684\u200b\u9519\u8bef\u4fe1\u606f\u200b\u4e2d\u200b\u7684\u200b\u6700\u540e\u200b\u4e00\u884c\u200b\u5f97\u77e5\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1a</p> <pre><code>RuntimeError: expected scalar type Float but found Byte\n</code></pre> <p>\u200b\u4e3a\u4ec0\u4e48\u200b\u4f1a\u200b\u8fd9\u6837\u200b\uff1f</p> <p>\u200b\u8fd9\u200b\u662f\u56e0\u4e3a\u200b\u6211\u4eec\u200b\u7684\u200b <code>model_6</code> \u200b\u662f\u200b\u5728\u200b <code>Float</code> \u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\u6837\u672c\u200b\u4e0a\u200b\u8bad\u7ec3\u200b\u7684\u200b\uff0c\u200b\u5177\u4f53\u6765\u8bf4\u200b\uff0c\u200b\u662f\u200b <code>torch.float32</code>\u3002</p> <p>\u200b\u6211\u4eec\u200b\u600e\u4e48\u200b\u77e5\u9053\u200b\u8fd9\u200b\u4e00\u70b9\u200b\uff1f</p> <p>\u200b\u55ef\u200b\uff0c<code>torch.float32</code> \u200b\u662f\u200b PyTorch \u200b\u4e2d\u200b\u8bb8\u591a\u200b\u5f20\u91cf\u200b\u7684\u200b\u9ed8\u8ba4\u503c\u200b\uff0c\u200b\u9664\u975e\u200b\u660e\u786e\u200b\u8bbe\u7f6e\u200b\u4e3a\u200b\u5176\u4ed6\u200b\u7c7b\u578b\u200b\u3002</p> <p>\u200b\u4f46\u200b\u8ba9\u200b\u6211\u4eec\u200b\u68c0\u67e5\u4e00\u4e0b\u200b\u4ee5\u200b\u786e\u4fdd\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#31","title":"3.1 \u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u7c7b\u578b\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u67e5\u770b\u200b <code>train_dataloader</code> \u200b\u4e2d\u200b\u6837\u672c\u200b\u7684\u200b <code>dtype</code> \u200b\u5c5e\u6027\u200b\u6765\u200b\u68c0\u67e5\u200b\u6a21\u578b\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u7684\u200b\u7c7b\u578b\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#32","title":"3.2 \u200b\u6539\u53d8\u200b\u5f20\u91cf\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\u00b6","text":"<p>\u200b\u5728\u200b\u6211\u4eec\u200b\u7684\u200b\u4f8b\u5b50\u200b\u4e2d\u200b\uff0c\u200b\u6211\u4eec\u200b\u53ef\u4ee5\u200b\u521b\u5efa\u200b\u4e00\u4e2a\u200b\u72ec\u7acb\u200b\u7684\u200b\u53d8\u6362\u200b\u6765\u200b\u8f6c\u6362\u200b\u6211\u4eec\u200b\u7684\u200b\u6d4b\u8bd5\u6570\u636e\u200b\uff0c\u200b\u4f46\u200b\u6211\u4eec\u200b\u4e5f\u200b\u53ef\u4ee5\u200b\u4f7f\u7528\u200b <code>tensor.type(some_type_here)</code> \u200b\u6765\u200b\u6539\u53d8\u200b\u76ee\u6807\u200b\u5f20\u91cf\u200b\u7684\u200b\u6570\u636e\u7c7b\u578b\u200b\uff0c\u200b\u4f8b\u5982\u200b <code>tensor_1.type(torch.float32)</code>\u3002</p> <p>\u200b\u8ba9\u200b\u6211\u4eec\u200b\u8bd5\u4e00\u8bd5\u200b\u3002</p>"},{"location":"pytorch_most_common_errors/#33","title":"3.3 \u200b\u5bf9\u200b\u6d4b\u8bd5\u200b\u56fe\u50cf\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u5e76\u200b\u786e\u4fdd\u200b\u5176\u200b\u683c\u5f0f\u200b\u6b63\u786e\u200b\u00b6","text":"<p>\u200b\u597d\u200b\u4e86\u200b\uff0c\u200b\u770b\u8d77\u6765\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u51c6\u5907\u200b\u597d\u200b\u4e86\u200b\u6240\u6709\u200b\u7684\u200b\u62fc\u56fe\u200b\u788e\u7247\u200b\uff1a\u200b\u5f62\u72b6\u200b\u3001\u200b\u8bbe\u5907\u200b\u548c\u200b\u6570\u636e\u7c7b\u578b\u200b\uff0c\u200b\u8ba9\u200b\u6211\u4eec\u200b\u5c1d\u8bd5\u200b\u8fdb\u884c\u200b\u4e00\u6b21\u200b\u9884\u6d4b\u200b\u5427\u200b\uff01</p> <p>\u200b\u6ce8\u610f\u200b\uff1a \u200b\u8bb0\u4f4f\u200b\u6a21\u578b\u200b\u559c\u6b22\u200b\u5728\u200b\u4e0e\u5176\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u76f8\u540c\u200b\uff08\u200b\u6216\u200b\u76f8\u4f3c\u200b\uff09\u200b\u683c\u5f0f\u200b\u7684\u200b\u6570\u636e\u200b\u4e0a\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\uff08\u200b\u5f62\u72b6\u200b\u3001\u200b\u8bbe\u5907\u200b\u548c\u200b\u6570\u636e\u7c7b\u578b\u200b\uff09\u3002</p>"},{"location":"pytorch_most_common_errors/","title":"\u7efc\u5408\u200b\u8fd0\u7528\u200b\u00b6","text":"<p>\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u5b9e\u9645\u200b\u63a5\u89e6\u200b\u4e86\u200b\u5728\u200b\u4f7f\u7528\u200b PyTorch \u200b\u6784\u5efa\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u65f6\u4f1a\u200b\u9047\u5230\u200b\u7684\u200b\u4e09\u5927\u200b\u4e3b\u8981\u200b\u9519\u8bef\u200b\uff1a</p> <ol> <li>\u200b\u5f62\u72b6\u200b\u9519\u8bef\u200b - \u200b\u4f60\u200b\u6b63\u5728\u200b\u5904\u7406\u200b\u7684\u200b\u6570\u636e\u200b\u4e0e\u200b\u6784\u5efa\u200b\u7684\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5bfb\u627e\u200b\u6a21\u5f0f\u200b\u7684\u200b\u65b9\u5f0f\u200b\u4e0d\u200b\u5339\u914d\u200b\uff0c\u200b\u6216\u8005\u200b\u795e\u7ecf\u7f51\u7edc\u200b\u5404\u200b\u8fde\u63a5\u200b\u5c42\u200b\u4e4b\u95f4\u200b\u4e0d\u200b\u5339\u914d\u200b\u3002</li> <li>\u200b\u8bbe\u5907\u200b\u9519\u8bef\u200b - \u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u4f4d\u4e8e\u200b\u4e0d\u540c\u200b\u7684\u200b\u8bbe\u5907\u200b\u4e0a\u200b\uff0cPyTorch \u200b\u671f\u671b\u200b\u6240\u6709\u200b\u5f20\u91cf\u200b\u548c\u200b\u5bf9\u8c61\u200b\u4f4d\u4e8e\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</li> <li>\u200b\u6570\u636e\u7c7b\u578b\u200b\u9519\u8bef\u200b - \u200b\u4f60\u200b\u8bd5\u56fe\u200b\u4f7f\u7528\u200b\u4e00\u79cd\u200b\u6570\u636e\u7c7b\u578b\u200b\u8fdb\u884c\u200b\u8ba1\u7b97\u200b\uff0c\u200b\u800c\u200b\u6a21\u578b\u200b\u671f\u671b\u200b\u53e6\u200b\u4e00\u79cd\u200b\u6570\u636e\u7c7b\u578b\u200b\u3002</li> </ol> <p>\u200b\u5e76\u4e14\u200b\u6211\u4eec\u200b\u5df2\u7ecf\u200b\u4e86\u89e3\u200b\u4e86\u200b\u5b83\u4eec\u200b\u53d1\u751f\u200b\u7684\u200b\u539f\u56e0\u200b\u4ee5\u53ca\u200b\u5982\u4f55\u200b\u4fee\u590d\u200b\u5b83\u4eec\u200b\uff1a</p> <ul> <li>\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u5e0c\u671b\u200b\u5bf9\u200b\u4e0e\u5176\u200b\u8bad\u7ec3\u200b\u6570\u636e\u200b\u76f8\u540c\u200b\u7c7b\u578b\u200b\u7684\u200b\u6570\u636e\u200b\uff08\u200b\u5f62\u72b6\u200b\u3001\u200b\u8bbe\u5907\u200b\u548c\u200b\u6570\u636e\u7c7b\u578b\u200b\uff09\u200b\u8fdb\u884c\u200b\u9884\u6d4b\u200b\u3002</li> <li>\u200b\u4f60\u200b\u7684\u200b\u6a21\u578b\u200b\u548c\u200b\u6570\u636e\u200b\u5e94\u200b\u5728\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u200b\u65f6\u200b\u4f4d\u4e8e\u200b\u540c\u4e00\u200b\u8bbe\u5907\u200b\u4e0a\u200b\u3002</li> <li>\u200b\u4f60\u200b\u53ef\u4ee5\u200b\u901a\u8fc7\u200b\u521b\u5efa\u200b\u5b9a\u4e49\u200b <code>device</code> \u200b\u548c\u200b\u6570\u636e\u7c7b\u578b\u200b\u7684\u200b\u53ef\u200b\u91cd\u7528\u200b\u51fd\u6570\u200b\u6765\u200b\u89e3\u51b3\u200b\u8bb8\u591a\u200b\u8fd9\u4e9b\u200b\u95ee\u9898\u200b\uff0c\u200b\u4f8b\u5982\u200b\u5728\u200b04. PyTorch \u200b\u6a21\u5757\u5316\u200b\u7ae0\u8282\u200b 4\uff1a\u200b\u521b\u5efa\u200b\u8bad\u7ec3\u200b\u548c\u200b\u6d4b\u8bd5\u51fd\u6570\u200b\u4e2d\u200b\u3002</li> </ul> <p>\u200b\u4e86\u89e3\u200b\u8fd9\u4e9b\u200b\u9519\u8bef\u200b\u5e76\u200b\u4e0d\u80fd\u200b\u9632\u6b62\u200b\u4f60\u200b\u5c06\u6765\u200b\u72af\u9519\u200b\uff0c\u200b\u4f46\u200b\u5b83\u200b\u4f1a\u200b\u7ed9\u200b\u4f60\u200b\u4e00\u4e2a\u200b\u89e3\u51b3\u95ee\u9898\u200b\u7684\u200b\u65b9\u5411\u200b\u3002</p> <p>\u200b\u8981\u200b\u83b7\u53d6\u200b\u8fd9\u4e9b\u200b\u9519\u8bef\u200b\u7684\u200b\u66f4\u200b\u6df1\u5165\u200b\u793a\u4f8b\u200b\uff0c\u200b\u5305\u62ec\u200b\u5b9e\u9645\u200b\u5236\u4f5c\u200b\u548c\u200b\u4fee\u590d\u200b\u5b83\u4eec\u200b\u7684\u200b\u65b9\u6cd5\u200b\uff0c\u200b\u8bf7\u53c2\u9605\u200b\u4ece\u200b\u96f6\u5230\u200b\u7cbe\u901a\u200b\uff1aPyTorch \u200b\u6df1\u5ea6\u200b\u5b66\u4e60\u200b\u8bfe\u7a0b\u200b\u3002</p>"}]}