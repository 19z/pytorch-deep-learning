{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/19z/pytorch-deep-learning/blob/main/00_pytorch_fundamentals.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a> \n",
    "\n",
    "[查看源代码](https://github.com/19z/pytorch-deep-learning/blob/main/00_pytorch_fundamentals.ipynb) | [查看幻灯片](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/slides/00_pytorch_and_deep_learning_fundamentals.pdf) | [观看视频讲解](https://youtu.be/Z_ikDlimN6A?t=76)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jSNK7duj5SeU"
   },
   "source": [
    "# 00. PyTorch 基础\n",
    "\n",
    "## 什么是 PyTorch？\n",
    "\n",
    "[PyTorch](https://pytorch.org/) 是一个开源的机器学习和深度学习框架。\n",
    "\n",
    "## PyTorch 可以用来做什么？\n",
    "\n",
    "PyTorch 允许你使用 Python 代码操作和处理数据，并编写机器学习算法。\n",
    "\n",
    "## 谁在使用 PyTorch？\n",
    "\n",
    "许多世界最大的科技公司，如 [Meta (Facebook)](https://ai.facebook.com/blog/pytorch-builds-the-future-of-ai-and-machine-learning-at-facebook/)、特斯拉和微软，以及人工智能研究公司如 [OpenAI](https://openai.com/blog/openai-pytorch/)，都使用 PyTorch 来支持研究和将机器学习引入他们的产品。\n",
    "\n",
    "![PyTorch 在工业和研究领域的应用](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-being-used-across-research-and-industry.png)\n",
    "\n",
    "例如，Andrej Karpathy（特斯拉 AI 负责人）在多个演讲中（[PyTorch DevCon 2019](https://youtu.be/oBklltKXtDE)，[Tesla AI Day 2021](https://youtu.be/j0z4FweCy4M?t=2904)）讲述了特斯拉如何使用 PyTorch 来支持他们的自动驾驶计算机视觉模型。\n",
    "\n",
    "PyTorch 还在其他行业中使用，如农业，用于[为拖拉机提供计算机视觉](https://medium.com/pytorch/ai-for-ag-production-machine-learning-for-agriculture-e8cfdb9849a1)。\n",
    "\n",
    "## 为什么要使用 PyTorch？\n",
    "\n",
    "机器学习研究人员非常喜欢使用 PyTorch。截至 2022 年 2 月，PyTorch 是 [Papers With Code](https://paperswithcode.com/trends) 上使用最多的深度学习框架，这是一个追踪机器学习研究论文及其相关代码库的网站。\n",
    "\n",
    "PyTorch 还能在后台处理许多事情，如 GPU 加速（使你的代码运行更快）。\n",
    "\n",
    "因此，你可以专注于操作数据和编写算法，而 PyTorch 会确保其运行速度。\n",
    "\n",
    "如果像特斯拉和 Meta (Facebook) 这样的公司使用它来构建模型，并将其部署到数百个应用程序、数千辆汽车和数十亿用户的内容中，那么它在开发方面显然也是强大的。\n",
    "\n",
    "## 本模块我们将涵盖的内容\n",
    "\n",
    "本课程分为不同的部分（笔记本）。\n",
    "\n",
    "每个笔记本涵盖了 PyTorch 中的重要概念和思想。\n",
    "\n",
    "后续笔记本建立在前面笔记本的知识基础上（编号从 00、01、02 开始，一直延续到结束）。\n",
    "\n",
    "本笔记本涉及机器学习和深度学习的基本构建块——张量。\n",
    "\n",
    "具体来说，我们将涵盖：\n",
    "\n",
    "| **主题** | **内容** |\n",
    "| ----- | ----- |\n",
    "| **张量介绍** | 张量是所有机器学习和深度学习的基本构建块。 |\n",
    "| **创建张量** | 张量可以表示几乎任何类型的数据（图像、文字、数字表格）。 |\n",
    "| **从张量获取信息** | 如果你能把信息放入张量，你也会想要取出它。 |\n",
    "| **操作张量** | 机器学习算法（如神经网络）涉及以多种方式操作张量，如加法、乘法、组合。 |\n",
    "| **处理张量形状** | 机器学习中最常见的问题之一是处理形状不匹配（尝试将错误形状的张量与其他张量混合）。 |\n",
    "| **张量索引** | 如果你对 Python 列表或 NumPy 数组进行过索引，那么对张量进行索引非常相似，只是张量可以有更多的维度。 |\n",
    "| **混合 PyTorch 张量和 NumPy** | PyTorch 使用张量（[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html)），NumPy 喜欢数组（[`np.ndarray`](https://numpy.org/doc/stable/reference/generated/numpy.ndarray.html)），有时你会想要混合使用它们。 |\n",
    "| **可重复性** | 机器学习非常实验性，因为它使用大量的随机性来工作，有时你会希望这种随机性不那么随机。 |\n",
    "| **在 GPU 上运行张量** | GPU（图形处理单元）使你的代码运行更快，PyTorch 使在 GPU 上运行代码变得容易。 |\n",
    "\n",
    "## 你可以在哪里获得帮助？\n",
    "\n",
    "本课程的所有材料都可以在 [GitHub](https://github.com/mrdbourke/pytorch-deep-learning) 上找到。\n",
    "\n",
    "如果你遇到问题，也可以在 [Discussions 页面](https://github.com/mrdbourke/pytorch-deep-learning/discussions) 上提问。\n",
    "\n",
    "还有 [PyTorch 开发者论坛](https://discuss.pytorch.org/)，一个非常有用的 PyTorch 相关问题的交流平台。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v3iRCRUTGeu"
   },
   "source": [
    "## 导入 PyTorch\n",
    "\n",
    "> **注意：** 在运行本笔记本中的任何代码之前，您应该已经完成了 [PyTorch 安装步骤](https://pytorch.org/get-started/locally/)。\n",
    ">\n",
    "> 然而，**如果您在 Google Colab 上运行**，一切应该都能正常工作（Google Colab 自带 PyTorch 和其他库的安装）。\n",
    "\n",
    "让我们从导入 PyTorch 并检查我们正在使用的版本开始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "1VxEOik46Y4i",
    "outputId": "f3141076-29bc-4600-c1c3-1586b1fe2292"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu116'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SqvI4S9TGew"
   },
   "source": [
    "太好了，看来我们已经安装了 PyTorch 1.10.0 或更高版本。\n",
    "\n",
    "这意味着如果你正在学习这些资料，你会发现它们与 PyTorch 1.10.0 及以上版本的大部分内容是兼容的。不过，如果你的版本号远高于此，你可能会注意到一些不一致之处。\n",
    "\n",
    "如果你遇到任何问题，请在课程的 [GitHub 讨论页面](https://github.com/mrdbourke/pytorch-deep-learning/discussions) 上发帖。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-33BKR16iWc"
   },
   "source": [
    "## 张量简介\n",
    "\n",
    "现在我们已经导入了 PyTorch，是时候学习张量了。\n",
    "\n",
    "张量是机器学习的基本构建块。\n",
    "\n",
    "它们的作用是以数值方式表示数据。\n",
    "\n",
    "例如，你可以将一张图像表示为一个形状为 `[3, 224, 224]` 的张量，这意味着 `[颜色通道, 高度, 宽度]`，即图像有 `3` 个颜色通道（红、绿、蓝），高度为 `224` 像素，宽度为 `224` 像素。\n",
    "\n",
    "![从输入图像到图像张量表示的示例，图像被分解为 3 个颜色通道以及表示高度和宽度的数值](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-tensor-shape-example-of-image.png)\n",
    "\n",
    "在张量术语（用于描述张量的语言）中，该张量具有三个维度，分别对应 `颜色通道`、`高度` 和 `宽度`。\n",
    "\n",
    "但我们有点超前了。\n",
    "\n",
    "让我们通过编码来进一步了解张量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gFF0N2TU7S7Q"
   },
   "source": [
    "### 创建张量\n",
    "\n",
    "PyTorch 热爱张量。以至于有一个完整的文档页面专门介绍 [`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html) 类。\n",
    "\n",
    "你的第一项家庭作业是 [阅读 `torch.Tensor` 的文档](https://pytorch.org/docs/stable/tensors.html) 10 分钟。但你可以稍后再做。\n",
    "\n",
    "让我们开始编写代码。\n",
    "\n",
    "我们要创建的第一个东西是 **标量**。\n",
    "\n",
    "标量是一个单一的数字，在张量术语中，它是一个零维张量。\n",
    "\n",
    "> **注意：** 这是本课程的一个趋势。我们将专注于编写特定的代码。但通常我会设置一些练习，涉及阅读和熟悉 PyTorch 文档。毕竟，一旦你完成了这门课程，你无疑会想学习更多。而文档是你经常会去的地方。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YUDgG2zk7Us5",
    "outputId": "0ac22bd2-16bc-4307-f312-31ae89d6c375"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scalar\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqSuhW7rTGey"
   },
   "source": [
    "看看上面的输出结果是`tensor(7)`？\n",
    "\n",
    "这意味着尽管`scalar`是一个单独的数值，但它的类型是`torch.Tensor`。\n",
    "\n",
    "我们可以使用`ndim`属性来检查张量的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lV98Yz868bav",
    "outputId": "502a625e-ff3c-4fc4-b523-f7634ea82128"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scalar.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZO2YW_QGTGez"
   },
   "source": [
    "如果我们想要从张量中提取数值呢？\n",
    "\n",
    "也就是说，将 `torch.Tensor` 转换为 Python 整数。\n",
    "\n",
    "为此，我们可以使用 `item()` 方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-k4cyKumPfbE",
    "outputId": "1f6a7916-0c7c-403f-8ebd-875454a94470"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the Python number within a tensor (only works with one-element tensors)\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qYs7ulrATGe0"
   },
   "source": [
    "好的，现在让我们来看一个**向量**。\n",
    "\n",
    "向量是一个单维的张量，但它可以包含许多数字。\n",
    "\n",
    "比如，你可以用向量 `[3, 2]` 来描述你家的 `[卧室, 浴室]`。或者你可以用 `[3, 2, 2]` 来描述你家的 `[卧室, 浴室, 车位]`。\n",
    "\n",
    "这里重要的趋势是，向量在它能表示的内容上具有灵活性（张量也是如此）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IZF6ASs8QH9",
    "outputId": "e556ed2a-e58a-440f-b103-0f06c91bc75c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector\n",
    "vector = torch.tensor([7, 7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mXxRUUW2TGe1"
   },
   "source": [
    "太好了，`vector` 现在包含了两个 7，这是我最喜欢的数字。\n",
    "\n",
    "你觉得它会有多少维呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "03hm3VVv8kr4",
    "outputId": "2035bb26-0189-4b28-fa02-34220d44677f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the number of dimensions of vector\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0VYvSGbTGe1"
   },
   "source": [
    "嗯，这有点奇怪，`vector` 包含了两个数字，却只有一维。\n",
    "\n",
    "我来告诉你一个小窍门。\n",
    "\n",
    "你可以通过外侧方括号（`[`）的数量来判断一个 PyTorch 张量有多少维，你只需要数一边的方括号即可。\n",
    "\n",
    "`vector` 有多少个方括号呢？\n",
    "\n",
    "另一个关于张量的重要概念是它们的 `shape` 属性。`shape` 告诉你这些元素是如何排列的。\n",
    "\n",
    "让我们来看看 `vector` 的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6zREV1bDTGe2",
    "outputId": "2a6e7ceb-7eb2-422b-b006-2c6e4825272f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of vector\n",
    "vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aWKppNyTGe2"
   },
   "source": [
    "上述代码返回 `torch.Size([2])`，这意味着我们的向量形状为 `[2]`。这是因为我们在方括号内放置了两个元素（`[7, 7]`）。\n",
    "\n",
    "现在让我们来看一个**矩阵**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D5iNwCYL8QO9",
    "outputId": "88fc63a7-4130-4c7a-a574-c61e85d2e99e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix\n",
    "MATRIX = torch.tensor([[7, 8], \n",
    "                       [9, 10]])\n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3U1bCdjTGe3"
   },
   "source": [
    "哇！更多数字！矩阵和向量一样灵活，只不过它们多了一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8LREUbeb8r8j",
    "outputId": "636246b0-b109-472a-c6d5-8601a9e08654"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions\n",
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhXXgq-dTGe3"
   },
   "source": [
    "`MATRIX` 有两个维度（你是否数过一侧外面的方括号数量？）。\n",
    "\n",
    "你认为它会有什么样的 `shape`？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_TL26I31TGe3",
    "outputId": "f05ec0b6-0bc1-4381-9474-56cbe6c67139"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvLpUvrKTGe4"
   },
   "source": [
    "我们得到输出 `torch.Size([2, 2])`，因为 `MATRIX` 有两层元素且每层有两行。\n",
    "\n",
    "那我们如何创建一个**张量**呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEMDQr188QWW",
    "outputId": "4230e6bd-1844-4210-eea8-245bb8b8b265"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [3, 6, 9],\n",
       "         [2, 4, 5]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor\n",
    "TENSOR = torch.tensor([[[1, 2, 3],\n",
    "                        [3, 6, 9],\n",
    "                        [2, 4, 5]]])\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UmJKkXD7TGe4"
   },
   "source": [
    "哇！这个张量看起来真不错。\n",
    "\n",
    "我要强调的是，张量几乎可以表示任何东西。\n",
    "\n",
    "我们刚刚创建的这个张量，可以是牛排和杏仁黄油店的销售数据（这两样是我最喜欢的食物）。\n",
    "\n",
    "![一个在Google表格中展示的简单张量，显示了星期几、牛排销售量和杏仁黄油销售量](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_simple_tensor.png)\n",
    "\n",
    "你觉得它有多少个维度？（提示：使用方括号计数法）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8dhuEsjS8QcT",
    "outputId": "7a45df1b-fc32-4cc5-e330-527c6ef7ba5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check number of dimensions for TENSOR\n",
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ln9dys5VTGe4"
   },
   "source": [
    "至于它的形状呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hdVv4iNRTGe5",
    "outputId": "d8ac706c-020b-4926-b145-d44e41f35e90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of TENSOR\n",
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxk8GU7oTGe5"
   },
   "source": [
    "好的，它输出了 `torch.Size([1, 3, 3])`。\n",
    "\n",
    "维度是从外到内的。\n",
    "\n",
    "这意味着有一个 3x3 的维度。\n",
    "\n",
    "![不同张量维度的示例](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-pytorch-different-tensor-dimensions.png)\n",
    "\n",
    "> **注意：** 你可能注意到我用小写字母表示 `scalar` 和 `vector`，用大写字母表示 `MATRIX` 和 `TENSOR`。这是有意为之的。在实践中，你通常会看到标量和向量用小写字母表示，如 `y` 或 `a`。而矩阵和张量用大写字母表示，如 `X` 或 `W`。\n",
    ">\n",
    "> 你可能还会注意到矩阵和张量这两个词可以互换使用。这是常见的做法。因为在 PyTorch 中，你通常处理的是 `torch.Tensor`（因此得名张量），然而，其内部的形状和维度将决定它实际上是什么。\n",
    "\n",
    "让我们总结一下。\n",
    "\n",
    "| 名称 | 是什么？ | 维度数量 | 通常/示例（小写或大写） |\n",
    "| ----- | ----- | ----- | ----- |\n",
    "| **标量** | 一个数字 | 0 | 小写 (`a`) | \n",
    "| **向量** | 带有方向的数字（例如风速和方向），但也可以包含许多其他数字 | 1 | 小写 (`y`) |\n",
    "| **矩阵** | 二维数字数组 | 2 | 大写 (`Q`) |\n",
    "| **张量** | 多维数字数组 | 可以是任意数量，0 维张量是标量，1 维张量是向量 | 大写 (`X`) | \n",
    "\n",
    "![标量、向量、矩阵和张量的外观](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00-scalar-vector-matrix-tensor.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dms7G4nkTGe5"
   },
   "source": [
    "### 随机张量\n",
    "\n",
    "我们已经确立了张量代表某种形式的数据。\n",
    "\n",
    "而诸如神经网络之类的机器学习模型则会对张量进行操作并从中寻找模式。\n",
    "\n",
    "但在使用 PyTorch 构建机器学习模型时，手动创建张量（就像我们一直在做的那样）的情况非常罕见。\n",
    "\n",
    "相反，机器学习模型通常从包含大量随机数的大张量开始，并在处理数据的过程中调整这些随机数，使其更好地表示数据。\n",
    "\n",
    "本质上：\n",
    "\n",
    "`从随机数开始 -> 观察数据 -> 更新随机数 -> 观察数据 -> 更新随机数...`\n",
    "\n",
    "作为数据科学家，你可以定义机器学习模型如何开始（初始化）、如何观察数据（表示）以及如何更新（优化）其随机数。\n",
    "\n",
    "我们将在后续实践中亲自体验这些步骤。\n",
    "\n",
    "现在，让我们看看如何创建一个包含随机数的张量。\n",
    "\n",
    "我们可以使用 [`torch.rand()`](https://pytorch.org/docs/stable/generated/torch.rand.html) 并传入 `size` 参数来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EOJEtDx--GnK",
    "outputId": "2680d44b-e31c-4ab1-d5b1-c0cd76706a0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.6541, 0.4807, 0.2162, 0.6168],\n",
       "         [0.4428, 0.6608, 0.6194, 0.8620],\n",
       "         [0.2795, 0.6055, 0.4958, 0.5483]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor of size (3, 4)\n",
    "random_tensor = torch.rand(size=(3, 4))\n",
    "random_tensor, random_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-wB1c_cXTGe5"
   },
   "source": [
    "`torch.rand()` 的灵活性在于我们可以将 `size` 调整为任何我们想要的形状。\n",
    "\n",
    "例如，假设你想要一个常见的图像形状的随机张量 `[224, 224, 3]`（即 `[高度, 宽度, 颜色通道]`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xMF_NUp3Ym__",
    "outputId": "8346b853-0b1e-481a-d9ee-a410ee21bab0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a random tensor of size (224, 224, 3)\n",
    "random_image_size_tensor = torch.rand(size=(224, 224, 3))\n",
    "random_image_size_tensor.shape, random_image_size_tensor.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0MQNTY0eTGe6"
   },
   "source": [
    "### 零和一\n",
    "\n",
    "有时，你只想用零或一填充张量。\n",
    "\n",
    "这在掩码操作中很常见（例如，用零掩码张量中的某些值，让模型知道不要学习这些值）。\n",
    "\n",
    "让我们使用 [`torch.zeros()`](https://pytorch.org/docs/stable/generated/torch.zeros.html) 创建一个充满零的张量。\n",
    "\n",
    "同样，`size` 参数在这里起作用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCzhd0hl9Vp6",
    "outputId": "9c8ec87f-d8c9-4751-a13e-6a5e986daaa9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.],\n",
       "         [0., 0., 0., 0.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all zeros\n",
    "zeros = torch.zeros(size=(3, 4))\n",
    "zeros, zeros.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDQBZJRUZWTN"
   },
   "source": [
    "我们可以采用类似的方法，使用[`torch.ones()`](https://pytorch.org/docs/stable/generated/torch.ones.html)来创建一个全为1的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HRe6sSXiTGe6",
    "outputId": "3f45b0b8-7f65-423d-c664-f5b5f7866fd2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.],\n",
       "         [1., 1., 1., 1.]]),\n",
       " torch.float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of all ones\n",
    "ones = torch.ones(size=(3, 4))\n",
    "ones, ones.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hib1NYrSarL2"
   },
   "source": [
    "### 创建一个范围和类似张量\n",
    "\n",
    "有时候你可能需要一个数字范围，例如从1到10或从0到100。\n",
    "\n",
    "你可以使用 `torch.arange(start, end, step)` 来实现这一点。\n",
    "\n",
    "其中：\n",
    "* `start` = 范围的起始值（例如 0）\n",
    "* `end` = 范围的结束值（例如 10）\n",
    "* `step` = 每两个值之间的步长（例如 1）\n",
    "\n",
    "> **注意：** 在 Python 中，你可以使用 `range()` 来创建一个范围。然而在 PyTorch 中，`torch.range()` 已被弃用，未来可能会显示错误。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IqUs81d9W4W",
    "outputId": "2a6f0c08-052e-4b36-b4eb-6a537239026f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3695928/193451495.py:2: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
      "  zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use torch.arange(), torch.range() is deprecated \n",
    "zero_to_ten_deprecated = torch.range(0, 10) # Note: this may return an error in the future\n",
    "\n",
    "# Create a range of values 0 to 10\n",
    "zero_to_ten = torch.arange(start=0, end=10, step=1)\n",
    "zero_to_ten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-bXf0Ugbh-D"
   },
   "source": [
    "有时你可能需要一个特定类型的张量，其形状与另一个张量相同。\n",
    "\n",
    "例如，一个与先前张量形状相同的、所有元素均为零的张量。\n",
    "\n",
    "为此，你可以使用 [`torch.zeros_like(input)`](https://pytorch.org/docs/stable/generated/torch.zeros_like.html) 或 [`torch.ones_like(input)`](https://pytorch.org/docs/1.9.1/generated/torch.ones_like.html)，这两个函数分别返回一个形状与 `input` 相同、元素全为零或全为1的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvXwUut5BhHq",
    "outputId": "096b2f8e-8c21-4ace-97b9-c36b92b2fe77"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also create a tensor of zeros similar to another tensor\n",
    "ten_zeros = torch.zeros_like(input=zero_to_ten) # will have same shape\n",
    "ten_zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "huKZ6QlYTGe7"
   },
   "source": [
    "### 张量数据类型\n",
    "\n",
    "在 PyTorch 中，有许多不同的 [张量数据类型](https://pytorch.org/docs/stable/tensors.html#data-types)。\n",
    "\n",
    "有些专用于 CPU，有些则更适合 GPU。\n",
    "\n",
    "了解它们之间的区别需要一些时间。\n",
    "\n",
    "通常，如果你看到 `torch.cuda` 字样，那么这个张量是用于 GPU 的（因为 Nvidia GPU 使用名为 CUDA 的计算工具包）。\n",
    "\n",
    "最常见的类型（通常也是默认类型）是 `torch.float32` 或 `torch.float`。\n",
    "\n",
    "这被称为“32 位浮点数”。\n",
    "\n",
    "但也有 16 位浮点数（`torch.float16` 或 `torch.half`）和 64 位浮点数（`torch.float64` 或 `torch.double`）。\n",
    "\n",
    "更复杂的是，还有 8 位、16 位、32 位和 64 位整数。\n",
    "\n",
    "还有更多！\n",
    "\n",
    "> **注意：** 整数是像 `7` 这样的平滑圆整数，而浮点数有小数点，如 `7.0`。\n",
    "\n",
    "所有这些数据类型的存在都与 **计算精度** 有关。\n",
    "\n",
    "精度是指描述一个数字时使用的细节量。\n",
    "\n",
    "精度值越高（8、16、32），表示一个数字所需的细节和数据就越多。\n",
    "\n",
    "这在深度学习和数值计算中很重要，因为你需要进行如此多的运算，计算所需的细节越多，使用的计算资源就越多。\n",
    "\n",
    "因此，较低精度的数据类型通常计算速度更快，但在评估指标（如准确性）上会牺牲一些性能（计算速度快但精度较低）。\n",
    "\n",
    "> **资源：**\n",
    "  * 查看 [PyTorch 文档中所有可用张量数据类型的列表](https://pytorch.org/docs/stable/tensors.html#data-types)。\n",
    "  * 阅读 [Wikipedia 页面了解计算中的精度概述](https://en.wikipedia.org/wiki/Precision_(computer_science))。\n",
    "\n",
    "让我们看看如何创建具有特定数据类型的张量。我们可以使用 `dtype` 参数来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q3MoGnpw9XaF",
    "outputId": "61070939-8c52-4ac6-bed7-e64b3ce24615"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3]), torch.float32, device(type='cpu'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default datatype for tensors is float32\n",
    "float_32_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=None, # defaults to None, which is torch.float32 or whatever datatype is passed\n",
    "                               device=None, # defaults to None, which uses the default tensor type\n",
    "                               requires_grad=False) # if True, operations performed on the tensor are recorded \n",
    "\n",
    "float_32_tensor.shape, float_32_tensor.dtype, float_32_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MhP8kzDfe_ty"
   },
   "source": [
    "除了形状问题（张量形状不匹配），在PyTorch中你还会经常遇到另外两种常见问题：数据类型和设备问题。\n",
    "\n",
    "例如，一个张量是 `torch.float32` 而另一个是 `torch.float16`（PyTorch通常希望张量具有相同的格式）。\n",
    "\n",
    "或者一个张量在CPU上，而另一个在GPU上（PyTorch希望张量之间的计算在同一设备上进行）。\n",
    "\n",
    "我们稍后会更多地讨论设备问题。\n",
    "\n",
    "现在，让我们创建一个 `dtype=torch.float16` 的张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKSuajld_09s",
    "outputId": "cbac29d9-3371-4fe1-b47c-3af4623b5fbf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float16"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor = torch.tensor([3.0, 6.0, 9.0],\n",
    "                               dtype=torch.float16) # torch.half would also work\n",
    "\n",
    "float_16_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gUjkB2AX7Upz"
   },
   "source": [
    "## 获取张量的信息\n",
    "\n",
    "一旦你创建了张量（或者由他人或 PyTorch 模块为你创建），你可能希望从中获取一些信息。\n",
    "\n",
    "我们之前已经见过这些属性，但最常见的三个张量属性是：\n",
    "* `shape` - 张量的形状是什么？（某些操作需要特定的形状规则）\n",
    "* `dtype` - 张量中的元素存储为什么数据类型？\n",
    "* `device` - 张量存储在哪个设备上？（通常是 GPU 或 CPU）\n",
    "\n",
    "让我们创建一个随机张量，并找出它的详细信息。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hd_X4D0j7Umq",
    "outputId": "86045713-ab36-4c8e-840c-e788f80c5266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4688, 0.0055, 0.8551, 0.0646],\n",
      "        [0.6538, 0.5157, 0.4071, 0.2109],\n",
      "        [0.9960, 0.3061, 0.9369, 0.7008]])\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "some_tensor = torch.rand(3, 4)\n",
    "\n",
    "# Find out details about it\n",
    "print(some_tensor)\n",
    "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {some_tensor.device}\") # will default to CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45K-E5uPg6cj"
   },
   "source": [
    "> **注意：** 在 PyTorch 中遇到问题时，很可能是与上述三个属性之一有关。因此，当错误信息出现时，给自己唱一首小歌，叫做“什么，什么，哪里”：\n",
    "  * “*我的张量是什么形状？它们的类型是什么，存储在哪里？什么形状，什么类型，哪里哪里哪里*”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdiWvoAi7UjL"
   },
   "source": [
    "## 操作张量（张量运算）\n",
    "\n",
    "在深度学习中，数据（图像、文本、视频、音频、蛋白质结构等）被表示为张量。\n",
    "\n",
    "模型通过研究这些张量并对其执行一系列操作（可能是数百万次）来学习，从而创建输入数据中模式的表示。\n",
    "\n",
    "这些操作通常是以下几种基本运算的精彩组合：\n",
    "* 加法\n",
    "* 减法\n",
    "* 乘法（逐元素）\n",
    "* 除法\n",
    "* 矩阵乘法\n",
    "\n",
    "仅此而已。当然，还有一些其他的操作，但这些是神经网络的基本构建块。\n",
    "\n",
    "通过以正确的方式堆叠这些构建块，你可以创建最复杂的神经网络（就像乐高积木一样！）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sk_6Dd7L7Uce"
   },
   "source": [
    "### 基本操作\n",
    "\n",
    "我们从一些基本的操作开始，包括加法（`+`）、减法（`-`）和乘法（`*`）。\n",
    "\n",
    "它们的工作方式正如你所想的那样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X71WpQoPD7a4",
    "outputId": "ab30f13e-fc67-4ae4-c5ce-1006410dba07"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11, 12, 13])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor of values and add a number to it\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor + 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sp4TlTWWEFeO",
    "outputId": "ce7d2296-881f-4eb3-802e-fd12bc25d6ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply it by 10\n",
    "tensor * 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1VEHnuRkn8Q"
   },
   "source": [
    "注意上面张量的值并没有变成 `tensor([110, 120, 130])`，这是因为张量内部的值不会改变，除非它们被重新赋值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuB1UjCIEJIA",
    "outputId": "57cae862-c145-4681-d74b-fe6d77f2125a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensors don't change unless reassigned\n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYvqGpUTk1o6"
   },
   "source": [
    "我们来减去一个数，这次我们将重新赋值给 `tensor` 变量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4iWKoLsENry",
    "outputId": "14d6771d-eb57-4b11-88a7-b1bb308ddc6e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-9, -8, -7])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract and reassign\n",
    "tensor = tensor - 10\n",
    "tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tFgZY-PaFNXa",
    "outputId": "3536ea54-a056-444c-cd5d-6d438ddda965"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add and reassign\n",
    "tensor = tensor + 10\n",
    "tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "CYXDoIOzk-6I"
   },
   "source": [
    "PyTorch 也内置了许多函数，如 [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul)（乘法简写）和 [`torch.add()`](https://pytorch.org/docs/stable/generated/torch.add.html)，用于执行基本运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uVysdk3kFWbY",
    "outputId": "3a5bf687-cf24-4224-9e76-975f84638ca8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use torch functions\n",
    "torch.multiply(tensor, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxuPJIpNFbqO",
    "outputId": "f04cafd9-eaea-4254-df1a-5ab3b524d74e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Original tensor is still unchanged \n",
    "tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70UNL33AlVQq"
   },
   "source": [
    "然而，更常见的是使用运算符符号，如 `*` 而不是 `torch.mul()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5v3RkR0F2Jq",
    "outputId": "0137caab-5ea1-4d95-f4c5-a0baa0fd652d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) * tensor([1, 2, 3])\n",
      "Equals: tensor([1, 4, 9])\n"
     ]
    }
   ],
   "source": [
    "# Element-wise multiplication (each element multiplies its equivalent, index 0->0, 1->1, 2->2)\n",
    "print(tensor, \"*\", tensor)\n",
    "print(\"Equals:\", tensor * tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TT5fVuyu7q5z"
   },
   "source": [
    "### 矩阵乘法（一切尽在矩阵乘法中）\n",
    "\n",
    "在机器学习和深度学习算法（如神经网络）中，最常见的操作之一就是[矩阵乘法](https://www.mathsisfun.com/algebra/matrix-multiplying.html)。\n",
    "\n",
    "PyTorch 在 [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html) 方法中实现了矩阵乘法功能。\n",
    "\n",
    "矩阵乘法需要记住的两个主要规则是：\n",
    "\n",
    "1. **内维度**必须匹配：\n",
    "   * `(3, 2) @ (3, 2)` 无法进行\n",
    "   * `(2, 3) @ (3, 2)` 可以进行\n",
    "   * `(3, 2) @ (2, 3)` 可以进行\n",
    "2. 结果矩阵的形状是**外维度**：\n",
    "   * `(2, 3) @ (3, 2)` -> `(2, 2)`\n",
    "   * `(3, 2) @ (2, 3)` -> `(3, 3)`\n",
    "\n",
    "> **注意：** 在 Python 中，\"`@`\" 符号表示矩阵乘法。\n",
    "\n",
    "> **资源：** 你可以在 [PyTorch 文档](https://pytorch.org/docs/stable/generated/torch.matmul.html)中查看所有关于 `torch.matmul()` 的矩阵乘法规则。\n",
    "\n",
    "让我们创建一个张量，并对其实现逐元素乘法和矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZE7loucmDlEM",
    "outputId": "44032bf9-c1f7-42fc-c842-dbe7a5c1221a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VUAZ3_b0vOKv"
   },
   "source": [
    "元素间乘法与矩阵乘法的区别在于值的相加。\n",
    "\n",
    "对于值为 `[1, 2, 3]` 的 `tensor` 变量：\n",
    "\n",
    "| 操作 | 计算 | 代码 |\n",
    "| ----- | ----- | ----- |\n",
    "| **元素间乘法** | `[1*1, 2*2, 3*3]` = `[1, 4, 9]` | `tensor * tensor` |\n",
    "| **矩阵乘法** | `[1*1 + 2*2 + 3*3]` = `[14]` | `tensor.matmul(tensor)` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i42gkUeHvI_1",
    "outputId": "18a630ce-bb56-4c40-81b4-9fdbb2ed7a4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise matrix multiplication\n",
    "tensor * tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PvCBiiTTDk8y",
    "outputId": "cf623247-8f1b-49f1-e788-16da3ed1e59c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4E_pROBDk2r",
    "outputId": "a09af00f-277b-479e-b0a2-ad6311ee5413"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can also use the \"@\" symbol for matrix multiplication, though not recommended\n",
    "tensor @ tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obbginUMv43A"
   },
   "source": [
    "你可以手动进行矩阵乘法，但并不推荐这样做。\n",
    "\n",
    "内置的 `torch.matmul()` 方法速度更快。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6qMSaLOoJscL",
    "outputId": "8bcad8a2-c900-4966-e13c-ff2cc02b9207"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 773 µs, sys: 0 ns, total: 773 µs\n",
      "Wall time: 499 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Matrix multiplication by hand \n",
    "# (avoid doing operations with for loops at all cost, they are computationally expensive)\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "  value += tensor[i] * tensor[i]\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVWiKB0KwH74",
    "outputId": "fce58235-5c09-49ec-f34b-a90e5640281e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 146 µs, sys: 83 µs, total: 229 µs\n",
      "Wall time: 171 µs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ4DDmo1TGe-"
   },
   "source": [
    "## 深度学习中最常见的错误之一（形状错误）\n",
    "\n",
    "由于深度学习很大一部分涉及矩阵的乘法和运算，而矩阵对于形状和大小的组合有严格的规则，因此你在深度学习中会遇到的最常见错误之一就是形状不匹配。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "rN5RcoD4Jo6y",
    "outputId": "20f6c65b-86f4-4903-d253-f6cbf0583934"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 75\u001B[0m in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001B[0m tensor_A \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mtensor([[\u001B[39m1\u001B[39m, \u001B[39m2\u001B[39m],\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001B[0m                          [\u001B[39m3\u001B[39m, \u001B[39m4\u001B[39m],\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001B[0m                          [\u001B[39m5\u001B[39m, \u001B[39m6\u001B[39m]], dtype\u001B[39m=\u001B[39mtorch\u001B[39m.\u001B[39mfloat32)\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001B[0m tensor_B \u001B[39m=\u001B[39m torch\u001B[39m.\u001B[39mtensor([[\u001B[39m7\u001B[39m, \u001B[39m10\u001B[39m],\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001B[0m                          [\u001B[39m8\u001B[39m, \u001B[39m11\u001B[39m], \n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001B[0m                          [\u001B[39m9\u001B[39m, \u001B[39m12\u001B[39m]], dtype\u001B[39m=\u001B[39mtorch\u001B[39m.\u001B[39mfloat32)\n\u001B[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y134sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001B[0m torch\u001B[39m.\u001B[39;49mmatmul(tensor_A, tensor_B)\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (3x2 and 3x2)"
     ]
    }
   ],
   "source": [
    "# Shapes need to be in the right way  \n",
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)\n",
    "\n",
    "torch.matmul(tensor_A, tensor_B) # (this will error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNA6MZEFxWVt"
   },
   "source": [
    "我们可以通过使 `tensor_A` 和 `tensor_B` 的内维匹配来实现它们之间的矩阵乘法。\n",
    "\n",
    "实现这一点的方法之一是使用**转置**（交换给定张量的维度）。\n",
    "\n",
    "在 PyTorch 中，你可以使用以下任一方法进行转置：\n",
    "* `torch.transpose(input, dim0, dim1)` - 其中 `input` 是需要转置的张量，`dim0` 和 `dim1` 是要交换的维度。\n",
    "* `tensor.T` - 其中 `tensor` 是需要转置的张量。\n",
    "\n",
    "我们尝试后者。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lUqgaANiy1wq",
    "outputId": "e48bbf0c-8008-434e-d372-caa658b2f36b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7., 10.],\n",
      "        [ 8., 11.],\n",
      "        [ 9., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B\n",
    "print(tensor_A)\n",
    "print(tensor_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DveqxO7iy_Fi",
    "outputId": "1bd2e85b-ea4d-4948-c408-8eb46ef3534c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[ 7.,  8.,  9.],\n",
      "        [10., 11., 12.]])\n"
     ]
    }
   ],
   "source": [
    "# View tensor_A and tensor_B.T\n",
    "print(tensor_A)\n",
    "print(tensor_B.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "35rEIu-NKtVE",
    "outputId": "0b32c7f1-556e-45d4-de22-388419e93dc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: tensor_A = torch.Size([3, 2]), tensor_B = torch.Size([3, 2])\n",
      "\n",
      "New shapes: tensor_A = torch.Size([3, 2]) (same as above), tensor_B.T = torch.Size([2, 3])\n",
      "\n",
      "Multiplying: torch.Size([3, 2]) * torch.Size([2, 3]) <- inner dimensions match\n",
      "\n",
      "Output:\n",
      "\n",
      "tensor([[ 27.,  30.,  33.],\n",
      "        [ 61.,  68.,  75.],\n",
      "        [ 95., 106., 117.]])\n",
      "\n",
      "Output shape: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# The operation works when tensor_B is transposed\n",
    "print(f\"Original shapes: tensor_A = {tensor_A.shape}, tensor_B = {tensor_B.shape}\\n\")\n",
    "print(f\"New shapes: tensor_A = {tensor_A.shape} (same as above), tensor_B.T = {tensor_B.T.shape}\\n\")\n",
    "print(f\"Multiplying: {tensor_A.shape} * {tensor_B.T.shape} <- inner dimensions match\\n\")\n",
    "print(\"Output:\\n\")\n",
    "output = torch.matmul(tensor_A, tensor_B.T)\n",
    "print(output) \n",
    "print(f\"\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfcFEqfLjN24"
   },
   "source": [
    "你也可以使用 [`torch.mm()`](https://pytorch.org/docs/stable/generated/torch.mm.html)，它是 `torch.matmul()` 的简写形式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "x3rJvW_TTGe_",
    "outputId": "2c501972-20bf-4a83-ad4a-b5f1b2424097"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27.,  30.,  33.],\n",
       "        [ 61.,  68.,  75.],\n",
       "        [ 95., 106., 117.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.mm is a shortcut for matmul\n",
    "torch.mm(tensor_A, tensor_B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXKozI4T0hFi"
   },
   "source": [
    "如果没有转置，矩阵乘法的规则就无法满足，我们会得到如上所示的错误。\n",
    "\n",
    "来个视觉演示怎么样？\n",
    "\n",
    "![矩阵乘法的视觉演示](https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/00-matrix-multiply-crop.gif)\n",
    "\n",
    "你可以在[这里](http://matrixmultiplication.xyz/)创建自己的矩阵乘法视觉演示。\n",
    "\n",
    "> **注意：** 这种矩阵乘法也被称为两个矩阵的[**点积**](https://www.mathsisfun.com/algebra/vectors-dot-product.html)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA64Z4DmkB31"
   },
   "source": [
    "神经网络中充满了矩阵乘法和点积运算。\n",
    "\n",
    "[`torch.nn.Linear()`](https://pytorch.org/docs/1.9.1/generated/torch.nn.Linear.html) 模块（我们稍后会看到它的实际应用），也称为前馈层或全连接层，实现了输入 `x` 与权重矩阵 `A` 之间的矩阵乘法。\n",
    "\n",
    "$$\n",
    "y = x \\cdot A^T + b\n",
    "$$\n",
    "\n",
    "其中：\n",
    "* `x` 是层的输入（深度学习是由多个层（如 `torch.nn.Linear()` 和其他层）堆叠而成的）。\n",
    "* `A` 是由层创建的权重矩阵，初始时为随机数，随着神经网络学习更好地表示数据中的模式而调整（注意“`T`”，这是因为权重矩阵被转置了）。\n",
    "  * **注意：** 你可能经常看到用 `W` 或其他字母如 `X` 来表示权重矩阵。\n",
    "* `b` 是用于稍微偏移权重和输入的偏置项。\n",
    "* `y` 是输出（通过对输入进行操作以期发现其中的模式）。\n",
    "\n",
    "这是一个线性函数（你可能在高中或其他地方见过类似 $y = mx + b$ 的形式），可以用来绘制一条直线！\n",
    "\n",
    "让我们来玩一玩线性层。\n",
    "\n",
    "尝试更改下面 `in_features` 和 `out_features` 的值，看看会发生什么。\n",
    "\n",
    "你注意到形状方面有什么变化吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC_MjKW1LX7T",
    "outputId": "768f75d2-c978-4df3-e18a-4684d46bdfa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "# Since the linear layer starts with a random weights matrix, let's make it reproducible (more on this later)\n",
    "torch.manual_seed(42)\n",
    "# This uses matrix multiplication\n",
    "linear = torch.nn.Linear(in_features=2, # in_features = matches inner dimension of input \n",
    "                         out_features=6) # out_features = describes outer value \n",
    "x = tensor_A\n",
    "output = linear(x)\n",
    "print(f\"Input shape: {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zIGrP5j1pN7j"
   },
   "source": [
    "> **问题：** 如果在上述代码中将 `in_features` 从 2 改为 3，会发生什么情况？是否会出现错误？如何更改输入 (`x`) 的形状以适应这种错误？提示：我们在上面的 `tensor_B` 上做了什么？\n",
    "\n",
    "如果在上述代码中将 `in_features` 从 2 改为 3，通常会导致错误，因为输入张量 `x` 的形状与新的 `in_features` 不匹配。具体来说，如果 `x` 的形状是 `(batch_size, 2)`，而 `in_features` 改为 3，那么 `x` 的列数（即特征数）与 `in_features` 不一致，从而引发错误。\n",
    "\n",
    "为了解决这个问题，你需要调整输入张量 `x` 的形状，使其与新的 `in_features` 匹配。具体来说，你需要将 `x` 的形状从 `(batch_size, 2)` 改为 `(batch_size, 3)`。这可以通过对 `x` 进行适当的变换来实现，类似于我们在上面的 `tensor_B` 上所做的操作。\n",
    "\n",
    "例如，如果你有一个形状为 `(batch_size, 2)` 的输入张量 `x`，你可以通过添加一个额外的列来将其形状改为 `(batch_size, 3)`。这可以通过以下方式实现：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 假设 x 是一个形状为 (batch_size, 2) 的张量\n",
    "x = torch.randn(3, 2)  # 示例输入\n",
    "\n",
    "# 添加一个额外的列，使其形状变为 (batch_size, 3)\n",
    "x_expanded = torch.cat((x, torch.zeros(3, 1)), dim=1)\n",
    "\n",
    "# 现在 x_expanded 的形状为 (3, 3)，可以与 in_features=3 的线性层匹配\n",
    "```\n",
    "\n",
    "通过这种方式，你可以调整输入张量的形状以适应新的 `in_features` 值，从而避免错误。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPNF0nMWoGEj"
   },
   "source": [
    "如果你以前从未接触过矩阵乘法，一开始可能会觉得这个话题有些令人困惑。\n",
    "\n",
    "但当你多次尝试并深入研究一些神经网络后，你会发现它无处不在。\n",
    "\n",
    "记住，矩阵乘法就是你所需要的一切。\n",
    "\n",
    "![矩阵乘法就是你所需要的一切](https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/00_matrix_multiplication_is_all_you_need.jpeg)\n",
    "\n",
    "*当你开始深入研究神经网络层并构建自己的模型时，你会发现矩阵乘法无处不在。**来源：** https://marksaroufim.substack.com/p/working-class-deep-learner*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjMmrJOOPv5e"
   },
   "source": [
    "### 查找最小值、最大值、均值、总和等（聚合操作）\n",
    "\n",
    "我们已经了解了几种操作张量的方法，接下来让我们探讨几种聚合张量的方式（从多个值变为较少值）。\n",
    "\n",
    "首先，我们将创建一个张量，然后找出它的最大值、最小值、均值和总和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrFQbe5fP1Rk",
    "outputId": "034013c1-b384-4a0d-edf8-295ed3a456f1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(0, 100, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-J-wfMdlsEco"
   },
   "source": [
    "现在让我们进行一些聚合操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e5wSP9YKP3Lb",
    "outputId": "3aa238c7-646f-434f-a55c-292aabef7227"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0\n",
      "Maximum: 90\n",
      "Mean: 45.0\n",
      "Sum: 450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "# print(f\"Mean: {x.mean()}\") # this will error\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") # won't work without float datatype\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHoKpsg3sKQE"
   },
   "source": [
    "> **注意：** 你可能会发现一些方法，如 `torch.mean()`，需要张量处于 `torch.float32`（最常见）或其他特定数据类型，否则操作将失败。\n",
    "\n",
    "你也可以使用 `torch` 方法来实现上述操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Cr23Y9uP3HO",
    "outputId": "9c86d805-eef2-465c-e2c8-2bccd515e6d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(0), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7ApCaZjDkvp"
   },
   "source": [
    "### 位置最小值/最大值\n",
    "\n",
    "你还可以分别使用 [`torch.argmax()`](https://pytorch.org/docs/stable/generated/torch.argmax.html) 和 [`torch.argmin()`](https://pytorch.org/docs/stable/generated/torch.argmin.html) 找到张量中最大值或最小值出现的位置索引。\n",
    "\n",
    "这在只需要知道最高（或最低）值所在位置而不需要实际值本身时非常有用（我们将在后面的部分看到这一点，例如在使用 [softmax 激活函数](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html)时）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FzNBl9JSGlHi",
    "outputId": "01e0740e-c34f-469b-9c8f-9e6e5f0363af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Index where max value occurs: 8\n",
      "Index where min value occurs: 0\n"
     ]
    }
   ],
   "source": [
    "# Create a tensor\n",
    "tensor = torch.arange(10, 100, 10)\n",
    "print(f\"Tensor: {tensor}\")\n",
    "\n",
    "# Returns index of max and min values\n",
    "print(f\"Index where max value occurs: {tensor.argmax()}\")\n",
    "print(f\"Index where min value occurs: {tensor.argmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QBu33WihOXBk"
   },
   "source": [
    "### 改变张量数据类型\n",
    "\n",
    "如前所述，深度学习操作中常见的问题之一是张量具有不同的数据类型。\n",
    "\n",
    "如果一个张量是 `torch.float64` 类型，而另一个是 `torch.float32` 类型，你可能会遇到一些错误。\n",
    "\n",
    "但有一个解决方法。\n",
    "\n",
    "你可以使用 [`torch.Tensor.type(dtype=None)`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html) 方法来改变张量的数据类型，其中 `dtype` 参数是你希望使用的数据类型。\n",
    "\n",
    "首先，我们将创建一个张量并检查其数据类型（默认是 `torch.float32`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rY2FEsCAOaLu",
    "outputId": "507f1ade-7c7a-4172-fa48-60c9ac4831c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor and check its datatype\n",
    "tensor = torch.arange(10., 100., 10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jR30FHEc92of"
   },
   "source": [
    "现在，我们将创建另一个张量，与之前一样，但将其数据类型更改为 `torch.float16`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cac8gRYjOeab",
    "outputId": "96e5ce12-bc29-4a2b-f81c-bfc89ea2d075"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10., 20., 30., 40., 50., 60., 70., 80., 90.], dtype=torch.float16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a float16 tensor\n",
    "tensor_float16 = tensor.type(torch.float16)\n",
    "tensor_float16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndVlKJZ4-7_5"
   },
   "source": [
    "我们可以采用类似的方法来创建一个 `torch.int8` 张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Yqovld2Oj6s",
    "outputId": "667da17f-e38f-404a-bd2d-63683e45c99a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a int8 tensor\n",
    "tensor_int8 = tensor.type(torch.int8)\n",
    "tensor_int8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44GxVabar-xe"
   },
   "source": [
    "> **注意：** 不同的数据类型一开始可能会让人感到困惑。但可以这样想，数字越小（例如 32、16、8），计算机存储的值就越不精确。而存储量越少，通常会导致计算速度更快，模型整体更小。基于移动设备的神经网络通常使用 8 位整数进行运算，这些网络更小、运行更快，但精确度不如使用 float32 的网络。更多关于这方面的内容，建议阅读关于[计算精度](https://en.wikipedia.org/wiki/Precision_(computer_science))的资料。\n",
    "\n",
    "> **练习：** 到目前为止，我们已经介绍了不少张量方法，但在 [`torch.Tensor` 文档](https://pytorch.org/docs/stable/tensors.html)中还有更多内容。我建议花 10 分钟时间浏览一下，看看有没有什么吸引你的方法。点击它们，然后自己动手写代码看看会发生什么。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7CkCtAYmGsHY"
   },
   "source": [
    "### 重塑、堆叠、压缩和解压缩\n",
    "\n",
    "很多时候，你会希望在不改变张量内部值的情况下，重塑或改变张量的维度。\n",
    "\n",
    "为此，一些常用的方法包括：\n",
    "\n",
    "| 方法 | 一行描述 |\n",
    "| ----- | ----- |\n",
    "| [`torch.reshape(input, shape)`](https://pytorch.org/docs/stable/generated/torch.reshape.html#torch.reshape) | 将 `input` 重塑为 `shape`（如果兼容），也可以使用 `torch.Tensor.reshape()`。 |\n",
    "| [`Tensor.view(shape)`](https://pytorch.org/docs/stable/generated/torch.Tensor.view.html) | 返回一个不同 `shape` 的原始张量视图，但与原始张量共享相同的数据。 |\n",
    "| [`torch.stack(tensors, dim=0)`](https://pytorch.org/docs/1.9.1/generated/torch.stack.html) | 沿新维度 (`dim`) 连接一系列 `tensors`，所有 `tensors` 必须具有相同的大小。 |\n",
    "| [`torch.squeeze(input)`](https://pytorch.org/docs/stable/generated/torch.squeeze.html) | 压缩 `input` 以移除所有值为 `1` 的维度。 |\n",
    "| [`torch.unsqueeze(input, dim)`](https://pytorch.org/docs/1.9.1/generated/torch.unsqueeze.html) | 在 `dim` 处添加一个值为 `1` 的维度返回 `input`。 |\n",
    "| [`torch.permute(input, dims)`](https://pytorch.org/docs/stable/generated/torch.permute.html) | 返回原始 `input` 的一个视图，其维度按 `dims` 重新排列。 |\n",
    "\n",
    "为什么要使用这些方法？\n",
    "\n",
    "因为深度学习模型（神经网络）都是以某种方式操纵张量的。由于矩阵乘法的规则，如果形状不匹配，你会遇到错误。这些方法帮助你确保你的张量的正确元素与其他张量的正确元素混合。\n",
    "\n",
    "让我们尝试一下这些方法。\n",
    "\n",
    "首先，我们将创建一个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EYjRTLOzG4Ev",
    "outputId": "f7f2719c-15ce-406b-dc8f-4477046cd5d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "import torch\n",
    "x = torch.arange(1., 8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3_VarMO9CoT8"
   },
   "source": [
    "现在让我们通过 `torch.reshape()` 增加一个额外的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "US4WjpQ3SG-8",
    "outputId": "c519d59e-85f1-4a10-eaaa-acb487028e3a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add an extra dimension\n",
    "x_reshaped = x.reshape(1, 7)\n",
    "x_reshaped, x_reshaped.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tig5xm0jCxuU"
   },
   "source": [
    "我们还可以使用 `torch.view()` 来改变视图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WDN2BNe5TGfB",
    "outputId": "3df1b0d6-2548-4ecc-ca25-0c4e28a6e536"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change view (keeps same data as original but changes view)\n",
    "# See more: https://stackoverflow.com/a/54507446/7900723\n",
    "z = x.view(1, 7)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m8joAaUEC2NX"
   },
   "source": [
    "请记住，使用 `torch.view()` 改变张量的视图实际上只是创建了同一张量的一个新的视图。\n",
    "\n",
    "因此，改变视图也会改变原始张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2DxURVvXTGfC",
    "outputId": "668d194d-dd0a-4db1-da00-9c3fd8849186"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Changing z changes x\n",
    "z[:, 0] = 5\n",
    "z, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxnqDBlpDDJ_"
   },
   "source": [
    "如果我们想将新创建的张量在自身上堆叠五次，可以使用 `torch.stack()` 来实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pX5Adf3ORiTK",
    "outputId": "703e8568-61df-4ebd-f4d3-a6366dc265c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stack tensors on top of each other\n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) # try changing dim to dim=1 and see what happens\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET56QzNHDuOI"
   },
   "source": [
    "如何从张量中移除所有单一维度？\n",
    "\n",
    "你可以使用 `torch.squeeze()` 来实现这一点（我记得这就像是“挤压”张量，使其只保留大于1的维度）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w2Y2HEoDRxJZ",
    "outputId": "dd0645a6-1cdd-46bc-a3a2-433d9cd09336"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "Previous shape: torch.Size([1, 7])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "New shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "# Remove extra dimension from x_reshaped\n",
    "x_squeezed = x_reshaped.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "acjDLk8WD8NC"
   },
   "source": [
    "要实现与 `torch.squeeze()` 相反的操作，可以使用 `torch.unsqueeze()` 在特定索引处添加一个维度值为 1 的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUC-DEEwSYv7",
    "outputId": "da60e019-3ea6-42f8-8e47-ba037ead737f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Previous shape: torch.Size([7])\n",
      "\n",
      "New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "New shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_squeezed}\")\n",
    "print(f\"Previous shape: {x_squeezed.shape}\")\n",
    "\n",
    "## Add an extra dimension with unsqueeze\n",
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)\n",
    "print(f\"\\nNew tensor: {x_unsqueezed}\")\n",
    "print(f\"New shape: {x_unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9DuJzXgFbM5"
   },
   "source": [
    "你还可以使用 `torch.permute(input, dims)` 来重新排列轴的顺序，其中 `input` 会被转换成一个具有新 `dims` 的*视图*。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fCRGCX8DTGfC",
    "outputId": "6853328b-a1cf-4470-f366-106a231a189c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Create tensor with specific shape\n",
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "# Permute the original tensor to rearrange the axis order\n",
    "x_permuted = x_original.permute(2, 0, 1) # shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06LKaFemGBoE"
   },
   "source": [
    "> **注意**：由于置换返回的是一个*视图*（与原始数据共享相同的数据），因此置换后的张量中的值将与原始张量中的值相同，如果你更改了视图中的值，它也会改变原始张量中的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEPqVL7fTGfC"
   },
   "source": [
    "## 索引（从张量中选择数据）\n",
    "\n",
    "有时，您可能希望从张量中选择特定数据（例如，仅选择第一列或第二行）。\n",
    "\n",
    "为此，您可以使用索引。\n",
    "\n",
    "如果您曾经在 Python 列表或 NumPy 数组上进行过索引操作，那么在 PyTorch 中对张量进行索引操作与此非常相似。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oSXzdxCQTGfD",
    "outputId": "05a72c08-5f8c-433a-cd31-46065686f825"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor \n",
    "import torch\n",
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xQG5krnKG43B"
   },
   "source": [
    "索引值的顺序是从外维度到内维度（请参考方括号的使用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zv_Z3IAzTGfD",
    "outputId": "cf6c0936-7600-4af4-9b6f-f6b8ac9b4c05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First square bracket:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "Second square bracket: tensor([1, 2, 3])\n",
      "Third square bracket: 1\n"
     ]
    }
   ],
   "source": [
    "# Let's index bracket by bracket\n",
    "print(f\"First square bracket:\\n{x[0]}\") \n",
    "print(f\"Second square bracket: {x[0][0]}\") \n",
    "print(f\"Third square bracket: {x[0][0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XaLjaIFxHe89"
   },
   "source": [
    "你也可以使用 `:` 来指定“该维度中的所有值”，然后使用逗号（`,`）来添加另一个维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gCT09pqeTGfD",
    "outputId": "a91f9b73-f8f0-476a-9c69-fcd03b042f6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th dimension and the 0 index of 1st dimension\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dwDx_gMsTGfD",
    "outputId": "8165cfd9-a88d-4212-8c45-1eb84ef5be83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of 0th & 1st dimensions but only index 1 of 2nd dimension\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xiw3_1E3TGfD",
    "outputId": "12fa4749-cf52-4e88-c2c0-44d26aeb633c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all values of the 0 dimension but only the 1 index value of the 1st and 2nd dimension\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XFVEgrKhTGfD",
    "outputId": "69eadeb9-11b3-4b48-cb95-0b3305c1274c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get index 0 of 0th and 1st dimension and all values of 2nd dimension \n",
    "x[0, 0, :] # same as x[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Ik0r11RIxtm"
   },
   "source": [
    "索引一开始可能会让人感到相当困惑，特别是对于较大的张量（我仍然需要尝试多次索引才能正确操作）。但只要稍加练习，并遵循数据探索者的座右铭（**可视化，可视化，再可视化**），你就会开始掌握其中的窍门。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8ZaW0Bq7rCm"
   },
   "source": [
    "## PyTorch 张量与 NumPy\n",
    "\n",
    "由于 NumPy 是 Python 中流行的数值计算库，PyTorch 提供了与 NumPy 良好交互的功能。\n",
    "\n",
    "从 NumPy 到 PyTorch（以及反过来）的两个主要方法如下：\n",
    "* [`torch.from_numpy(ndarray)`](https://pytorch.org/docs/stable/generated/torch.from_numpy.html) - NumPy 数组 -> PyTorch 张量。\n",
    "* [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) - PyTorch 张量 -> NumPy 数组。\n",
    "\n",
    "让我们尝试一下这些方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yDrDCnvY7rKS",
    "outputId": "86155a63-01f9-4372-e889-61a65ebf0fb1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy array to tensor\n",
    "import torch\n",
    "import numpy as np\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "16JG6cONLPnO"
   },
   "source": [
    "> **注意：** 默认情况下，NumPy 数组会以 `float64` 数据类型创建，如果你将其转换为 PyTorch 张量，它会保持相同的数据类型（如上所述）。\n",
    ">\n",
    "> 然而，许多 PyTorch 计算默认使用 `float32`。\n",
    ">\n",
    "> 因此，如果你想将你的 NumPy 数组（float64）转换为 PyTorch 张量（float64），然后再转换为 PyTorch 张量（float32），可以使用 `tensor = torch.from_numpy(array).type(torch.float32)`。\n",
    "\n",
    "由于我们在上面重新赋值了 `tensor`，如果你改变张量，数组将保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ovwl7VCREv8L",
    "outputId": "efd21eb9-0010-436a-dc29-f851e3d7d77a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the array, keep the tensor\n",
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "geVvu1p0MTWc"
   },
   "source": [
    "如果你想从 PyTorch 张量转换为 NumPy 数组，可以调用 `tensor.numpy()`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xw_7ZyVaTKxQ",
    "outputId": "54d6f347-d3f6-44df-9155-83d980c31780"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor to NumPy array\n",
    "tensor = torch.ones(7) # create a tensor of ones with dtype=float32\n",
    "numpy_tensor = tensor.numpy() # will be dtype=float32 unless changed\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dt8yEV1jMfi2"
   },
   "source": [
    "同样地，遵循上述规则，如果你修改了原始的 `tensor`，新的 `numpy_tensor` 将保持不变。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mMp6ZSkET4_Y",
    "outputId": "100678a4-c220-4a44-e4a5-0542359cb9de"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2., 2., 2., 2., 2., 2., 2.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change the tensor, keep the array the same\n",
    "tensor = tensor + 1\n",
    "tensor, numpy_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gU3ubCrUkI-"
   },
   "source": [
    "## 可重复性（试图从随机中去除随机性）\n",
    "\n",
    "随着你对神经网络和机器学习的了解加深，你会发现随机性在其中扮演了多么重要的角色。\n",
    "\n",
    "嗯，确切地说是伪随机性。毕竟，从设计角度来看，计算机本质上是非随机的（每一步都是可预测的），所以它们产生的随机性是模拟出来的（尽管对此也有争议，但既然我不是计算机科学家，就让你自己去探索更多吧）。\n",
    "\n",
    "那么，这与神经网络和深度学习有什么关系呢？\n",
    "\n",
    "我们讨论过，神经网络从随机数开始来描述数据中的模式（这些随机数是糟糕的描述），并尝试使用张量操作（以及我们尚未讨论的其他一些方法）来改进这些随机数，以更好地描述数据中的模式。\n",
    "\n",
    "简而言之：\n",
    "\n",
    "``从随机数开始 -> 张量操作 -> 尝试变得更好（一次又一次）``\n",
    "\n",
    "虽然随机性既美好又强大，但有时你希望随机性少一些。\n",
    "\n",
    "为什么？\n",
    "\n",
    "这样你就可以进行可重复的实验。\n",
    "\n",
    "例如，你创建了一个能够达到X性能的算法。\n",
    "\n",
    "然后你的朋友尝试验证你并没有发疯。\n",
    "\n",
    "他们如何做到这一点？\n",
    "\n",
    "这就是**可重复性**的作用。\n",
    "\n",
    "换句话说，你能在你的计算机上运行相同的代码，得到与我一样的（或非常相似的）结果吗？\n",
    "\n",
    "让我们看一个PyTorch中可重复性的简短示例。\n",
    "\n",
    "我们将从创建两个随机张量开始，既然它们是随机的，你可能会期望它们是不同的，对吧？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eSwxnwEbTGfF",
    "outputId": "73b34154-734f-496f-9b55-b6aaa137e854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor A:\n",
      "tensor([[0.8016, 0.3649, 0.6286, 0.9663],\n",
      "        [0.7687, 0.4566, 0.5745, 0.9200],\n",
      "        [0.3230, 0.8613, 0.0919, 0.3102]])\n",
      "\n",
      "Tensor B:\n",
      "tensor([[0.9536, 0.6002, 0.0351, 0.6826],\n",
      "        [0.3743, 0.5220, 0.1336, 0.9666],\n",
      "        [0.9754, 0.8474, 0.8988, 0.1105]])\n",
      "\n",
      "Does Tensor A equal Tensor B? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False, False],\n",
       "        [False, False, False, False],\n",
       "        [False, False, False, False]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create two random tensors\n",
    "random_tensor_A = torch.rand(3, 4)\n",
    "random_tensor_B = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor A:\\n{random_tensor_A}\\n\")\n",
    "print(f\"Tensor B:\\n{random_tensor_B}\\n\")\n",
    "print(f\"Does Tensor A equal Tensor B? (anywhere)\")\n",
    "random_tensor_A == random_tensor_B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPU6mDKJnr8M"
   },
   "source": [
    "正如你可能预料的那样，这些张量的值各不相同。\n",
    "\n",
    "但如果你想创建两个具有**相同**值的随机张量呢？\n",
    "\n",
    "也就是说，张量仍然包含随机值，但它们的“风味”相同。\n",
    "\n",
    "这时，[`torch.manual_seed(seed)`](https://pytorch.org/docs/stable/generated/torch.manual_seed.html) 就派上用场了，其中 `seed` 是一个整数（比如 `42`，但可以是任何值），用于给随机性“调味”。\n",
    "\n",
    "让我们尝试创建一些更具“风味”的随机张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sB6d1GfYTGfF",
    "outputId": "4d11d38e-4406-4aff-9a81-cf13aa89ee5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "# # Set the random seed\n",
    "RANDOM_SEED=42 # try changing this to different values and see what happens to the numbers below\n",
    "torch.manual_seed(seed=RANDOM_SEED) \n",
    "random_tensor_C = torch.rand(3, 4)\n",
    "\n",
    "# Have to reset the seed every time a new rand() is called \n",
    "# Without this, tensor_D would be different to tensor_C \n",
    "torch.random.manual_seed(seed=RANDOM_SEED) # try commenting this line out and seeing what happens\n",
    "random_tensor_D = torch.rand(3, 4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uct53Xr5QRC_"
   },
   "source": [
    "太好了！\n",
    "\n",
    "看起来设置种子起作用了。\n",
    "\n",
    "> **资源：** 我们刚刚涉及的内容只是 PyTorch 中可重复性的冰山一角。如果你想了解更多关于可重复性以及随机种子的知识，我建议查看以下资料：\n",
    "> * [PyTorch 可重复性文档](https://pytorch.org/docs/stable/notes/randomness.html)（一个好的练习是花 10 分钟阅读这个文档，即使你现在不理解它，但意识到它的存在是很重要的）。\n",
    "> * [维基百科随机种子页面](https://en.wikipedia.org/wiki/Random_seed)（这将为你提供随机种子和伪随机性的一般概述）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxIIM7t27rQ-"
   },
   "source": [
    "## 在GPU上运行张量（并加快计算速度）\n",
    "\n",
    "深度学习算法需要大量的数值运算。\n",
    "\n",
    "默认情况下，这些运算通常在CPU（中央处理单元）上进行。\n",
    "\n",
    "然而，还有另一种常见的硬件——GPU（图形处理单元），它通常在执行神经网络所需特定类型的运算（矩阵乘法）方面比CPU快得多。\n",
    "\n",
    "你的电脑可能就有一块GPU。\n",
    "\n",
    "如果是这样，你应该尽可能地利用它来训练神经网络，因为它很可能会大幅缩短训练时间。\n",
    "\n",
    "有几种方法可以首先获取GPU的访问权限，然后让PyTorch使用GPU。\n",
    "\n",
    "> **注意：** 在本课程中，当我提到“GPU”时，除非另有说明，我指的是启用了[Nvidia GPU with CUDA](https://developer.nvidia.com/cuda-gpus)（CUDA是一种计算平台和API，有助于使GPU能够用于通用计算，而不仅仅是图形处理）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0UiR6QpoYQH_"
   },
   "source": [
    "### 1. 获取GPU\n",
    "\n",
    "当你听到GPU这个词时，你可能已经知道是怎么回事了。但如果你还不清楚，有几种方法可以获取GPU的使用权限。\n",
    "\n",
    "| **方法** | **设置难度** | **优点** | **缺点** | **设置方法** |\n",
    "| ----- | ----- | ----- | ----- | ----- |\n",
    "| Google Colab | 简单 | 免费使用，几乎无需设置，可以轻松通过链接分享工作 | 不保存数据输出，计算资源有限，可能会超时 | [遵循Google Colab指南](https://colab.research.google.com/notebooks/gpu.ipynb) |\n",
    "| 使用自己的 | 中等 | 在本地机器上运行所有内容 | GPU不是免费的，需要前期成本 | 遵循[PyTorch安装指南](https://pytorch.org/get-started/locally/) |\n",
    "| 云计算（AWS、GCP、Azure） | 中等至困难 | 前期成本小，几乎可以无限使用计算资源 | 如果持续运行可能会很昂贵，需要一些时间来正确设置 | 遵循[PyTorch安装指南](https://pytorch.org/get-started/cloud-partners/) |\n",
    "\n",
    "还有更多使用GPU的选项，但上述三种方法目前足够使用。\n",
    "\n",
    "就我个人而言，我结合使用Google Colab和自己的个人电脑进行小规模实验（以及创建这个课程），并在需要更多计算能力时转向云资源。\n",
    "\n",
    "> **资源：** 如果你想购买自己的GPU但不确定选择哪种，[Tim Dettmers有一个很棒的指南](https://timdettmers.com/2020/09/07/which-gpu-for-deep-learning/)。\n",
    "\n",
    "要检查你是否可以访问Nvidia GPU，可以运行`!nvidia-smi`，其中`!`（也称为bang）表示“在命令行上运行此命令”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vEMcO-9zYc-w",
    "outputId": "77405db7-3494-4add-cfc7-8415e52a0412"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jan 21 08:34:23 2023       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 515.48.07    Driver Version: 515.48.07    CUDA Version: 11.7     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA TITAN RTX    On   | 00000000:01:00.0 Off |                  N/A |\n",
      "| 40%   30C    P8     7W / 280W |    177MiB / 24576MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      1061      G   /usr/lib/xorg/Xorg                 53MiB |\n",
      "|    0   N/A  N/A   2671131      G   /usr/lib/xorg/Xorg                 97MiB |\n",
      "|    0   N/A  N/A   2671256      G   /usr/bin/gnome-shell                9MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvkB9p5zYf8E"
   },
   "source": [
    "如果你没有可用的 Nvidia GPU，上述命令将输出类似以下内容：\n",
    "\n",
    "```\n",
    "NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n",
    "```\n",
    "\n",
    "在这种情况下，请返回并按照安装步骤操作。\n",
    "\n",
    "如果你确实有 GPU，上述命令将输出类似以下内容：\n",
    "\n",
    "```\n",
    "Wed Jan 19 22:09:08 2022       \n",
    "+-----------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 495.46       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
    "|-------------------------------+----------------------+----------------------+\n",
    "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
    "|                               |                      |               MIG M. |\n",
    "|===============================+======================+======================|\n",
    "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
    "| N/A   35C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
    "|                               |                      |                  N/A |\n",
    "+-------------------------------+----------------------+----------------------+\n",
    "                                                                               \n",
    "+-----------------------------------------------------------------------------+\n",
    "| Processes:                                                                  |\n",
    "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
    "|        ID   ID                                                   Usage      |\n",
    "|=============================================================================|\n",
    "|  No running processes found                                                 |\n",
    "+-----------------------------------------------------------------------------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvibZ6e0YcDk"
   },
   "source": [
    "### 2. 让 PyTorch 在 GPU 上运行\n",
    "\n",
    "一旦你准备好了一个可访问的 GPU，下一步就是让 PyTorch 使用它来存储数据（张量）和处理数据（对张量执行操作）。\n",
    "\n",
    "为此，你可以使用 [`torch.cuda`](https://pytorch.org/docs/stable/cuda.html) 包。\n",
    "\n",
    "与其谈论它，不如让我们尝试一下。\n",
    "\n",
    "你可以使用 [`torch.cuda.is_available()`](https://pytorch.org/docs/stable/generated/torch.cuda.is_available.html#torch.cuda.is_available) 来测试 PyTorch 是否可以访问 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OweDLgwjEvZ2",
    "outputId": "3a278a24-3ec3-4b1f-8f96-298086fa6ea6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for GPU\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jedZcx2PZFpL"
   },
   "source": [
    "如果上述输出为 `True`，则表示 PyTorch 可以识别并使用 GPU；如果输出为 `False`，则表示 PyTorch 无法识别 GPU，此时你需要重新检查安装步骤。\n",
    "\n",
    "现在，假设你希望设置代码，使其能够在 CPU 或可用 GPU 上运行。\n",
    "\n",
    "这样一来，无论你或其他人使用何种计算设备运行你的代码，它都能正常工作。\n",
    "\n",
    "让我们创建一个 `device` 变量来存储可用设备的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "j92HBCKB7rYa",
    "outputId": "8cca1643-645c-4b67-f1f5-37066f6b9549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FjFyPP2WaCch"
   },
   "source": [
    "如果上述输出为 `\"cuda\"`，这意味着我们可以将所有 PyTorch 代码设置为使用可用的 CUDA 设备（即 GPU），而如果输出为 `\"cpu\"`，则我们的 PyTorch 代码将继续使用 CPU。\n",
    "\n",
    "> **注意：** 在 PyTorch 中，编写[**设备无关的代码**](https://pytorch.org/docs/master/notes/cuda.html#device-agnostic-code)是最佳实践。这意味着代码将在 CPU（始终可用）或 GPU（如果可用）上运行。\n",
    "\n",
    "如果你想进行更快的计算，可以使用 GPU，但如果你想进行*更*快的计算，可以使用多个 GPU。\n",
    "\n",
    "你可以使用 [`torch.cuda.device_count()`](https://pytorch.org/docs/stable/generated/torch.cuda.device_count.html#torch.cuda.device_count) 来计算 PyTorch 可以访问的 GPU 数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MArsn0DFTGfG",
    "outputId": "de717df5-bb67-4900-805e-a6f00ad0b409"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of devices\n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVNf1hiqa-gO"
   },
   "source": [
    "了解PyTorch可以访问的GPU数量是有帮助的，以防你希望在一个GPU上运行某个进程，而在另一个GPU上运行另一个进程（PyTorch还具备让你在*所有*GPU上运行进程的功能）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 在苹果 M系芯片上运行 PyTorch\n",
    "\n",
    "为了在苹果的 M1/M2/M3 GPU 上运行 PyTorch，可以使用 [`torch.backends.mps`](https://pytorch.org/docs/stable/notes/mps.html) 模块。\n",
    "\n",
    "请确保 macOS 和 PyTorch 的版本已更新。\n",
    "\n",
    "你可以使用 `torch.backends.mps.is_available()` 来测试 PyTorch 是否可以访问 GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for Apple Silicon GPU\n",
    "import torch\n",
    "torch.backends.mps.is_available() # Note this will print false if you're not running on a Mac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set device type\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如前所述，如果上述输出为 `\"mps\"`，这意味着我们可以将所有 PyTorch 代码设置为使用可用的 Apple Silicon GPU。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\" # Use NVIDIA GPU (if available)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\" # Use Apple Silicon GPU (if available)\n",
    "else:\n",
    "    device = \"cpu\" # Default to CPU if no GPU is available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XqQLcuj68OA-"
   },
   "source": [
    "### 3. 将张量（和模型）置于GPU上\n",
    "\n",
    "你可以通过调用 [`to(device)`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) 方法将张量（以及模型，我们稍后会看到这一点）置于特定的设备上。其中，`device` 是你希望张量（或模型）所要放置的目标设备。\n",
    "\n",
    "为什么要这样做？\n",
    "\n",
    "GPU 提供的数值计算速度远快于 CPU，并且如果 GPU 不可用，由于我们的 **设备无关代码**（见上文），它将运行在 CPU 上。\n",
    "\n",
    "> **注意：** 使用 `to(device)` 方法将张量置于 GPU 上（例如 `some_tensor.to(device)`）会返回该张量的副本，即相同的张量将同时存在于 CPU 和 GPU 上。要覆盖张量，需要重新赋值：\n",
    ">\n",
    "> `some_tensor = some_tensor.to(device)`\n",
    "\n",
    "让我们尝试创建一个张量并将其置于 GPU 上（如果 GPU 可用）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FhI3srFXEHfP",
    "outputId": "2f4f6435-fdc4-4e99-e87c-9421c2100f36"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='mps:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create tensor (default on CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# Tensor not on GPU\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "# Move tensor to GPU (if available)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxXeRKO0TGfG"
   },
   "source": [
    "如果你有可用的GPU，上述代码将输出类似以下内容：\n",
    "\n",
    "```\n",
    "tensor([1, 2, 3]) cpu\n",
    "tensor([1, 2, 3], device='cuda:0')\n",
    "```\n",
    "\n",
    "注意第二个张量带有 `device='cuda:0'`，这意味着它存储在可用的第0号GPU上（GPU的索引从0开始，如果有两个GPU可用，它们将分别是 `'cuda:0'` 和 `'cuda:1'`，以此类推，最多到 `'cuda:n'`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4puyUX4Bci5D"
   },
   "source": [
    "### 4. 将张量移回CPU\n",
    "\n",
    "如果我们想将张量移回CPU，应该怎么做呢？\n",
    "\n",
    "例如，如果你想使用NumPy与张量进行交互（NumPy不利用GPU），你可能需要这样做。\n",
    "\n",
    "让我们尝试对我们的`tensor_on_gpu`使用[`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html)方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "3ChSLJgPTGfG",
    "outputId": "32e92f62-db28-4dc7-ce93-c2ab33229252"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[1;32m/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb Cell 157\u001B[0m in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001B[0m \u001B[39m# If tensor is on GPU, can't transform it to NumPy (this will error)\u001B[39;00m\n\u001B[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22544954414e2d525458227d/home/daniel/code/pytorch/pytorch-course/pytorch-deep-learning/00_pytorch_fundamentals.ipynb#Y312sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001B[0m tensor_on_gpu\u001B[39m.\u001B[39;49mnumpy()\n",
      "\u001B[0;31mTypeError\u001B[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    }
   ],
   "source": [
    "# If tensor is on GPU, can't transform it to NumPy (this will error)\n",
    "tensor_on_gpu.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhymtkRDTGfG"
   },
   "source": [
    "相反，为了将张量返回到 CPU 并使其可与 NumPy 一起使用，我们可以使用 [`Tensor.cpu()`](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html)。\n",
    "\n",
    "这会将张量复制到 CPU 内存中，以便 CPU 可以使用它。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gN15s-NdTGfG",
    "outputId": "9fffb6f2-c200-4f9c-d987-d9ab5d9cba49"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instead, copy the tensor back to cpu\n",
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qyzNH5lrTGfH"
   },
   "source": [
    "上述操作返回的是位于CPU内存中的GPU张量副本，因此原始张量仍然保留在GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S5u83PCRTGfH",
    "outputId": "4cb931e2-7c8d-49b9-a7de-db3d3c6589b5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlmBpnuPTGfH"
   },
   "source": [
    "## 练习\n",
    "\n",
    "所有练习都旨在练习上述代码。\n",
    "\n",
    "你应该能够通过参考每个部分或遵循所链接的资源来完成这些练习。\n",
    "\n",
    "**资源：**\n",
    "\n",
    "* [00练习模板笔记本](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/exercises/00_pytorch_fundamentals_exercises.ipynb)。\n",
    "* [00练习示例解决方案笔记本](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/extras/solutions/00_pytorch_fundamentals_exercise_solutions.ipynb)（在查看此内容之前尝试练习）。\n",
    "\n",
    "1. **文档阅读** - 深度学习（以及一般编程学习）的一个很大部分是熟悉你正在使用的某个框架的文档。在本课程的其余部分中，我们将大量使用PyTorch文档。因此，我建议你花10分钟阅读以下内容（如果你现在不明白某些内容也没关系，重点还不是完全理解，而是意识）。请参阅[`torch.Tensor`](https://pytorch.org/docs/stable/tensors.html#torch-tensor)和[`torch.cuda`](https://pytorch.org/docs/master/notes/cuda.html#cuda-semantics)的文档。\n",
    "2. 创建一个形状为`(7, 7)`的随机张量。\n",
    "3. 对第2题中的张量与另一个形状为`(1, 7)`的随机张量进行矩阵乘法（提示：你可能需要转置第二个张量）。\n",
    "4. 将随机种子设置为`0`，然后重新做第2题和第3题。\n",
    "5. 说到随机种子，我们看到了如何用`torch.manual_seed()`设置它，但有没有GPU的等效方法？（提示：你需要查阅`torch.cuda`的文档）。如果有，将GPU随机种子设置为`1234`。\n",
    "6. 创建两个形状为`(2, 3)`的随机张量，并将它们都发送到GPU（你需要有GPU访问权限）。在创建张量时设置`torch.manual_seed(1234)`（这不一定是GPU随机种子）。\n",
    "7. 对第6题中创建的张量进行矩阵乘法（再次提醒，你可能需要调整其中一个张量的形状）。\n",
    "8. 找出第7题输出中的最大值和最小值。\n",
    "9. 找出第7题输出中的最大值和最小值的索引。\n",
    "10. 创建一个形状为`(1, 1, 1, 10)`的随机张量，然后创建一个新的张量，移除所有`1`维度，留下形状为`(10)`的张量。设置种子为`7`，并打印出第一个张量及其形状以及第二个张量及其形状。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlmBpnuPTGfH"
   },
   "source": [
    "## 课外学习\n",
    "\n",
    "* 花1小时浏览[PyTorch基础教程](https://pytorch.org/tutorials/beginner/basics/intro.html)（推荐阅读[快速入门](https://pytorch.org/tutorials/beginner/basics/quickstart_tutorial.html)和[张量](https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html)部分）。\n",
    "* 想了解更多关于张量如何表示数据的信息，请观看此视频：[什么是张量？](https://youtu.be/f5liqUk0ZTw)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "00_pytorch_fundamentals.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "3fbe1355223f7b2ffc113ba3ade6a2b520cadace5d5ec3e828c83ce02eb221bf"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
