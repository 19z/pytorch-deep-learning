{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c62b709-eea8-4e47-b263-73a944a6fa3f",
   "metadata": {},
   "source": "<a href=\"https://colab.research.google.com/github/19z/pytorch-deep-learning/blob/main/extras/pytorch_most_common_errors.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"在 Colab 中打开\"/></a>"
  },
  {
   "cell_type": "markdown",
   "id": "fd5762fa-e6ff-488b-b088-3a5ca69d56a5",
   "metadata": {},
   "source": [
    "# PyTorch 中最常见的三种错误\n",
    "\n",
    "PyTorch 是当前可用的最大的机器学习库之一。\n",
    "\n",
    "因此，在使用它时，你很可能会遇到各种错误。\n",
    "\n",
    "由于库的创建者进行了各种维护和检查，因此错误很少是因为库本身的问题。\n",
    "\n",
    "这意味着你遇到的大多数错误都是用户错误。\n",
    "\n",
    "更具体地说，你写错了代码。\n",
    "\n",
    "不要觉得被冒犯，每个程序员都会遇到这种情况。\n",
    "\n",
    "在用户错误中，很可能是以下三种之一：\n",
    "\n",
    "1. **形状错误** - 你试图对形状不匹配的矩阵/张量执行操作。例如，你的数据形状是 `[1, 28, 28]`，但你的第一层输入形状是 `[10]`。\n",
    "2. **设备错误** - 你的模型和数据位于不同的设备上。例如，你的模型在 GPU 上（例如 `\"cuda\"`），而你的数据在 CPU 上（例如 `\"cpu\"`）。\n",
    "3. **数据类型错误** - 你的数据是一种数据类型（例如 `torch.float32`），但你要执行的操作需要另一种数据类型（例如 `torch.int64`）。\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/misc-three-main-errors-in-pytorch.png\" width=750 alt=\"PyTorch 中最常见的三种错误\"/>\n",
    "\n",
    "注意这里的重复主题。\n",
    "\n",
    "你的形状、设备和/或数据类型之间存在某种不匹配。\n",
    "\n",
    "本笔记本/博客文章将通过示例介绍上述每种错误及其解决方法。\n",
    "\n",
    "虽然这不会阻止你将来犯这些错误，但它会让你足够意识到这些错误，从而减少它们，更重要的是，知道如何解决它们。\n",
    "\n",
    "> **注意：** 以下所有示例均改编自 [learnpytorch.io](https://learnpytorch.io)，这是 [Zero to Mastery: PyTorch for Deep Learning](https://dbourke.link/ZTMPyTorch) 视频课程的书籍版本。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f0b33b-6a43-4c10-8363-e7b8cac6ceef",
   "metadata": {},
   "source": [
    "## 1. PyTorch中的形状错误"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b999de2b-88e1-4b7e-a669-928e58b5148d",
   "metadata": {},
   "source": [
    "### 1.1 矩阵乘法的形状错误\n",
    "\n",
    "PyTorch 是构建神经网络模型的最佳框架之一。\n",
    "\n",
    "神经网络的基本操作之一就是矩阵乘法。\n",
    "\n",
    "然而，矩阵乘法有非常具体的规定。\n",
    "\n",
    "如果不遵守这些规则，你会遇到著名的形状错误。\n",
    "\n",
    "```\n",
    "RuntimeError: mat1 和 mat2 的形状无法相乘 (3x4 和 3x4)\n",
    "```\n",
    "\n",
    "让我们从一个简短的例子开始。\n",
    "\n",
    "> **注意：** 尽管它被称为“矩阵乘法”，但 PyTorch 中的几乎所有数据形式都是以张量的形式出现的。张量是一个 n 维数组（n 可以是任何数字）。因此，虽然我使用“矩阵乘法”这个术语，但这同样适用于“张量乘法”。有关矩阵和张量之间差异的更多信息，请参阅 [00. PyTorch 基础：张量介绍](https://www.learnpytorch.io/00_pytorch_fundamentals/#introduction-to-tensors)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b50eccb-017f-49ac-a274-0cde7d724138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.12.1+cu113\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c218bcbd-d31d-45b6-8f83-8ccb43a5abf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "# Create two tensors\n",
    "tensor_1 = torch.rand(3, 4)\n",
    "tensor_2 = torch.rand(3, 4)\n",
    "\n",
    "# Check the shapes\n",
    "print(tensor_1.shape)\n",
    "print(tensor_2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827859d9-d392-4a30-8467-39c8a40268fa",
   "metadata": {},
   "source": [
    "请注意，这两个张量的形状是相同的。\n",
    "\n",
    "让我们尝试对它们进行矩阵乘法运算。\n",
    "\n",
    "> **注意：** 矩阵乘法运算与标准乘法运算不同。\n",
    ">\n",
    "> 对于我们当前的张量，标准乘法运算（`*` 或 [`torch.mul()`](https://pytorch.org/docs/stable/generated/torch.mul.html)）可以正常工作，而矩阵乘法运算（`@` 或 [`torch.matmul()`](https://pytorch.org/docs/stable/generated/torch.matmul.html)）将会报错。\n",
    ">\n",
    "> 有关矩阵乘法运算的详细解释，请参阅 [00. PyTorch 基础：矩阵乘法](https://www.learnpytorch.io/00_pytorch_fundamentals/#matrix-multiplication-is-all-you-need)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90589c0c-1038-4016-b4ea-e51f6c24f6e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Standard multiplication, the following lines perform the same operation (will work)\n",
    "tensor_3 = tensor_1 * tensor_2 # can do standard multiplication with \"*\"\n",
    "tensor_4 = torch.mul(tensor_1, tensor_2) # can also do standard multiplicaton with \"torch.mul()\" \n",
    "\n",
    "# Check for equality \n",
    "tensor_3 == tensor_4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d51eacf-87b0-4ea0-becd-feeb95e437ce",
   "metadata": {},
   "source": [
    "太棒了！看起来标准乘法在我们当前的张量形状下运行良好。\n",
    "\n",
    "接下来让我们尝试矩阵乘法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f996537-6d21-41fd-bcc5-f1c563df34d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [4]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Try matrix multiplication (won't work)\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m tensor_5 \u001B[38;5;241m=\u001B[39m \u001B[43mtensor_1\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m@\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtensor_2\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (3x4 and 3x4)"
     ]
    }
   ],
   "source": [
    "# Try matrix multiplication (won't work)\n",
    "tensor_5 = tensor_1 @ tensor_2 # could also do \"torch.matmul(tensor_1, tensor_2)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34574baa-9c23-4e6a-bf45-33ad3da0c7a6",
   "metadata": {},
   "source": [
    "糟糕！\n",
    "\n",
    "我们遇到了类似以下的错误：\n",
    "\n",
    "```\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-11-2ca2c90dbb42> in <module>\n",
    "      1 # 尝试矩阵乘法（不工作）\n",
    "----> 2 tensor_5 = tensor_1 @ tensor_2\n",
    "\n",
    "RuntimeError: mat1 和 mat2 的形状无法相乘（3x4 和 3x4）\n",
    "```\n",
    "\n",
    "这是一个**形状错误**，我们的两个张量（矩阵）无法进行*矩阵*乘法，因为它们的形状不兼容。\n",
    "\n",
    "为什么？\n",
    "\n",
    "这是因为矩阵乘法有特定的规则：\n",
    "\n",
    "1. **内维度**必须匹配：\n",
    "* `(3, 4) @ (3, 4)` 不工作\n",
    "* `(4, 3) @ (3, 4)` 工作\n",
    "* `(3, 4) @ (4, 3)` 工作\n",
    "2. 结果矩阵的形状是**外维度**：\n",
    "* `(4, 3) @ (3, 4)` -> `(4, 4)`\n",
    "* `(3, 4) @ (4, 3)` -> `(3, 3)`\n",
    "\n",
    "那么我们如何解决这个问题呢？\n",
    "\n",
    "这时就需要用到*转置*或*重塑*操作。\n",
    "\n",
    "在神经网络的情况下，更普遍的是转置操作。\n",
    "\n",
    "* **转置** - 转置（[`torch.transpose()`](https://pytorch.org/docs/stable/generated/torch.transpose.html)）操作交换给定张量的维度。\n",
    "  * **注意：** 你也可以使用 `tensor.T` 的快捷方式来进行转置。\n",
    "* **重塑** - 重塑（[`torch.reshape()`](https://pytorch.org/docs/stable/generated/torch.reshape.html)）操作返回一个具有相同原始元素但形状不同的张量。\n",
    "\n",
    "让我们看看实际操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba5c1b7f-542b-4163-a094-34ec7f0c4976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensors: torch.Size([4, 3]) and torch.Size([3, 4])\n",
      "Shape of output tensor: torch.Size([4, 4])\n"
     ]
    }
   ],
   "source": [
    "# Perform a transpose on tensor_1 and then perform matrix multiplication \n",
    "tensor_6 = tensor_1.T @ tensor_2\n",
    "print(f\"Shape of input tensors: {tensor_1.T.shape} and {tensor_2.shape}\")\n",
    "print(f\"Shape of output tensor: {tensor_6.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d145eff9-cfcb-4d91-a732-de7c9b662cf7",
   "metadata": {},
   "source": [
    "没有错误！\n",
    "\n",
    "注意到由于转置操作（`tensor_1.T`），`tensor_1`的输入形状从 `(3, 4)` 变为了 `(4, 3)`。\n",
    "\n",
    "正因为如此，矩阵乘法的规则1，**内维度必须匹配**，得到了满足。\n",
    "\n",
    "最后，输出形状满足了矩阵乘法的规则2，**结果矩阵的形状为外维度**。\n",
    "\n",
    "在我们的例子中，`tensor_6` 的形状为 `(4, 4)`。\n",
    "\n",
    "让我们进行同样的操作，只不过这次我们将转置 `tensor_2` 而不是 `tensor_1`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4533e23b-0e9d-4bce-aa0c-dae1c2879e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of input tensors: torch.Size([3, 4]) and torch.Size([4, 3])\n",
      "Shape of output tensor: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "# Perform a transpose on tensor_2 and then perform matrix multiplication\n",
    "tensor_7 = tensor_1 @ tensor_2.T\n",
    "print(f\"Shape of input tensors: {tensor_1.shape} and {tensor_2.T.shape}\")\n",
    "print(f\"Shape of output tensor: {tensor_7.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87e9014-583a-4db5-ae81-c804c2cfe274",
   "metadata": {},
   "source": [
    "哇哦！\n",
    "\n",
    "再次没有错误！\n",
    "\n",
    "看看矩阵乘法的规则1和规则2是如何再次得到满足的。\n",
    "\n",
    "只不过这次因为我们转置了`tensor_2`，所以得到的输出张量形状是`(3, 3)`。\n",
    "\n",
    "好消息是，大多数时候，当你使用PyTorch构建神经网络时，这个库会为你处理大部分你需要执行的矩阵乘法操作。\n",
    "\n",
    "话虽如此，让我们用PyTorch构建一个神经网络，看看形状错误可能会出现在哪里。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cca226-4ea2-408e-a05d-52f54cabb62c",
   "metadata": {},
   "source": [
    "### 1.2 PyTorch神经网络形状错误\n",
    "\n",
    "我们已经了解了在使用矩阵乘法（或矩阵乘以张量）时可能出现的形状错误。\n",
    "\n",
    "现在，让我们构建一个PyTorch神经网络，看看形状错误可能出现在哪些地方。\n",
    "\n",
    "在以下任一情况下，神经网络中会出现形状错误：\n",
    "* **输入形状不正确** - 您的数据具有某种形状，但模型的第一层期望不同的形状。\n",
    "* **层与层之间的输入和输出形状不匹配** - 模型中的某一层输出某种形状，但下一层期望不同的输入形状。\n",
    "* **在进行预测时输入数据中没有批次大小维度** - 您的模型在具有批次维度的样本上进行训练，因此当您尝试在没有批次维度的单个样本上进行预测时，会出现错误。\n",
    "\n",
    "为了展示这些形状错误，让我们构建一个简单的神经网络（无论网络大小如何，错误都是相同的），尝试在Fashion MNIST数据集（10种不同类别的服装的黑白图像）中寻找模式。\n",
    "\n",
    "> **注意：** 以下示例特别关注形状错误，而不是构建*最佳*神经网络。您可以在[03. PyTorch计算机视觉](https://www.learnpytorch.io/03_pytorch_computer_vision/)中看到这个问题的完整工作示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c370b97e-9731-496b-be6a-d2d955dc3bc9",
   "metadata": {},
   "source": [
    "### 1.3 下载数据集\n",
    "\n",
    "首先，我们将从 [`torchvision.datasets`](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html) 获取 Fashion MNIST 数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8932b2c6-49b0-4ce5-80ed-c8118eddd07f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "966b1ed47e214e189c79c2d1e1c1f2bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/26421880 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4d501453aba4c43bf5ea3e9a12ffe80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29515 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a57edd30fc643c0baba05f3595fbe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4422102 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f795a425a8d04dd59215f4414997a416",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to data/FashionMNIST/raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Setup training data\n",
    "train_data = datasets.FashionMNIST(\n",
    "    root=\"data\", # where to download data to?\n",
    "    train=True, # get training data\n",
    "    download=True, # download data if it doesn't exist on disk\n",
    "    transform=transforms.ToTensor(), # images come as PIL format, we want to turn into Torch tensors\n",
    "    target_transform=None # you can transform labels as well\n",
    ")\n",
    "\n",
    "# Setup testing data\n",
    "test_data = datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False, # get test data\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a17253b-aa2b-4ad5-bd52-289da2ac934d",
   "metadata": {},
   "source": [
    "现在让我们获取一些关于第一个训练样本的详细信息，包括标签、类别名称和类别数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec308d1f-3b54-48c9-9169-3fe25077b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([1, 28, 28]) -> [batch, height, width]\n",
      "Label: 9\n"
     ]
    }
   ],
   "source": [
    "# See first training sample\n",
    "image, label = train_data[0]\n",
    "print(f\"Image shape: {image.shape} -> [batch, height, width]\") \n",
    "print(f\"Label: {label}\") # label is an int rather than a tensor (it has no shape attribute)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40897bb1-1c7e-434f-822d-7473e03077a2",
   "metadata": {},
   "source": [
    "我们的图像形状为 `[1, 28, 28]` 或 `[batch_size, height, width]`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eaf83774-5995-4ddf-8394-d956e25fb85a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['T-shirt/top',\n",
       "  'Trouser',\n",
       "  'Pullover',\n",
       "  'Dress',\n",
       "  'Coat',\n",
       "  'Sandal',\n",
       "  'Shirt',\n",
       "  'Sneaker',\n",
       "  'Bag',\n",
       "  'Ankle boot'],\n",
       " 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See class names and number of classes\n",
    "class_names = train_data.classes\n",
    "num_classes = len(class_names)\n",
    "class_names, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85a4b016-5a02-46b9-ba4d-3a3fe144190d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a sample\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image.squeeze(), cmap=\"gray\") # plot image as grayscale\n",
    "plt.axis(False)\n",
    "plt.title(class_names[label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d8f1db-d8c6-4d2f-9cac-2a19594ad3bb",
   "metadata": {},
   "source": [
    "### 1.4 构建具有不同形状错误的神经网络系列\n",
    "\n",
    "我们的问题是：构建一个能够识别服装灰度图像中模式的神经网络。\n",
    "\n",
    "这个陈述可以非常深入，因为“哪种神经网络最好？”是整个机器学习领域的主要研究问题之一。\n",
    "\n",
    "但让我们尽可能简单地开始，以展示不同的错误类型。\n",
    "\n",
    "我们将使用PyTorch构建几个两层的神经网络，每个网络展示一种不同的错误：\n",
    "\n",
    "| **模型编号** | **层** | **错误展示** | \n",
    "| ----- | ----- | ----- |\n",
    "| 0 | 2个带有10个隐藏单元的`nn.Linear()` | 输入形状错误 |\n",
    "| 1 | 与模型1相同 + 1个`nn.Flatten()` | 输入形状错误（仍然） |\n",
    "| 2 | 1个`nn.Flatten()`，1个具有正确输入形状的`nn.Linear()`和1个带有10个隐藏单元的`nn.Linear()` | 无（输入形状正确） |\n",
    "| 3 | 与模型2相同，但`nn.Linear()`层之间的形状不同 | 层之间形状错误 |\n",
    "| 4 | 与模型3相同，但最后一层替换为`nn.LazyLinear()` | 无（展示`nn.LazyX()`层如何推断正确形状） |\n",
    "| 5 | 与模型4相同，但所有`nn.Linear()`替换为`nn.LazyLinear()` | 无（展示`nn.LazyX()`层如何推断正确形状） |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1bf8f-95be-4f80-8701-11a34e7dee3e",
   "metadata": {},
   "source": [
    "### 1.5 输入层形状错误\n",
    "\n",
    "我们将从一个包含两个 [`nn.Linear()`](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) 层的网络开始，每个层有 10 个隐藏单元。\n",
    "\n",
    "> **注意：** 关于 `nn.Linear()` 内部的工作原理，请参阅 [01. PyTorch 工作流程第 6 节：综合应用](https://www.learnpytorch.io/01_pytorch_workflow/#6-putting-it-all-together)。\n",
    "\n",
    "然后，我们将把 `image` 传递给这个网络，看看会发生什么。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2aaf7f7a-6100-4d09-9d05-e35d985a773c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 10>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      4\u001B[0m model_0 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m      5\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, out_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m),\n\u001B[1;32m      6\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, out_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      7\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m# Pass the image through the model (this will error)\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[43mmodel_0\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a two layer neural network\n",
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=10)\n",
    ")\n",
    "\n",
    "# Pass the image through the model (this will error)\n",
    "model_0(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e39356d-1e12-40c2-9003-03f268078ed2",
   "metadata": {},
   "source": [
    "运行上述代码后，我们遇到了另一个形状错误！\n",
    "\n",
    "类似于：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)\n",
    "```\n",
    "\n",
    "关键在于最后一行 `RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 10x10)`。\n",
    "\n",
    "这告诉我们数据形状有问题。\n",
    "\n",
    "因为在幕后，`nn.Linear()` 试图进行矩阵乘法。\n",
    "\n",
    "我们该如何解决这个问题？\n",
    "\n",
    "有几种不同的选项，具体取决于你使用的是哪种层。\n",
    "\n",
    "但由于我们使用的是 `nn.Linear()` 层，让我们专注于这一点。\n",
    "\n",
    "`nn.Linear()` 喜欢接受单维向量的数据。\n",
    "\n",
    "例如，输入 `image` 的形状为 `[1, 28, 28]`，它更喜欢 `[1, 784]`（`784 = 28*28`）。\n",
    "\n",
    "换句话说，它希望所有信息都被*展平*成单一维度。\n",
    "\n",
    "我们可以使用 PyTorch 的 [`nn.Flatten()`](https://pytorch.org/docs/stable/generated/torch.nn.Flatten.html) 来实现这种展平。\n",
    "\n",
    "让我们看看它是如何发生的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3cd2b1eb-a355-4d87-b895-10a6f3f6ae75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before flatten shape: torch.Size([1, 28, 28]) -> [batch, height, width]\n",
      "After flatten shape: torch.Size([1, 784]) -> [batch, height*width]\n"
     ]
    }
   ],
   "source": [
    "# Create a flatten layer\n",
    "flatten = nn.Flatten()\n",
    "\n",
    "# Pass the image through the flatten layer\n",
    "flattened_image = flatten(image)\n",
    "\n",
    "# Print out the image shape before and after \n",
    "print(f\"Before flatten shape: {image.shape} -> [batch, height, width]\")\n",
    "print(f\"After flatten shape: {flattened_image.shape} -> [batch, height*width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd85d38f-5016-4314-85b1-83e79786fc71",
   "metadata": {},
   "source": [
    "太好了，图像数据已经展平了！\n",
    "\n",
    "现在让我们尝试在我们的现有模型中添加 `nn.Flatten()` 层。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54446b26-438e-45b8-96bd-bd3967d35902",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m model_1 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m      3\u001B[0m     nn\u001B[38;5;241m.\u001B[39mFlatten(), \u001B[38;5;66;03m# <-- NEW: add nn.Flatten() layer\u001B[39;00m\n\u001B[1;32m      4\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, out_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m),\n\u001B[1;32m      5\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, out_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Pass the image through the model\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mmodel_1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)"
     ]
    }
   ],
   "source": [
    "# Replicate model_0 except add a nn.Flatten() layer to begin with \n",
    "model_1 = nn.Sequential(\n",
    "    nn.Flatten(), # <-- NEW: add nn.Flatten() layer\n",
    "    nn.Linear(in_features=10, out_features=10),\n",
    "    nn.Linear(in_features=10, out_features=10)\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_1(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592ac09-b222-4601-bd46-3cb093f81318",
   "metadata": {},
   "source": [
    "哎呀！\n",
    "\n",
    "又出错了...\n",
    "\n",
    "错误信息如下：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)\n",
    "```\n",
    "\n",
    "同样，关键信息在最后一行。\n",
    "\n",
    "`RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x784 and 10x10)`\n",
    "\n",
    "嗯，我们知道 `(1x784)` 肯定来自我们的输入数据（`image`），因为我们已经将其从 `(1, 28, 28)` 展平为 `(1, 784)`。\n",
    "\n",
    "那么 `(10x10)` 呢？\n",
    "\n",
    "这些值来自我们在 `nn.Linear()` 层中设置的参数，`in_features=10` 和 `out_features=10` 或者 `nn.Linear(in_features=10, out_features=10)`。\n",
    "\n",
    "矩阵乘法的第一条规则是什么来着？\n",
    "\n",
    "1. **内维**必须匹配。\n",
    "\n",
    "对！\n",
    "\n",
    "那么如果我们把第一层的 `in_features=10` 改为 `in_features=784` 会发生什么？\n",
    "\n",
    "让我们试试看！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a241a720-8d9a-4308-8ba2-b2b7c92b6240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2045,  0.2677, -0.0713, -0.3096, -0.0586,  0.3153, -0.3413,  0.2031,\n",
       "          0.4421,  0.1715]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the input as well as make sure the first layer can accept the flattened input shape\n",
    "model_2 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=784, out_features=10), # <-- NEW: change in_features=10 to in_features=784\n",
    "    nn.Linear(in_features=10, out_features=10)\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_2(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c7bea3-cd05-4c34-9ea0-594429cdf20d",
   "metadata": {},
   "source": [
    "成功了！\n",
    "\n",
    "我们从模型中得到了输出！\n",
    "\n",
    "虽然这个输出目前可能没有太多意义，但至少我们知道所有形状都匹配，数据能够完全流经我们的模型。\n",
    "\n",
    "`nn.Flatten()` 层将我们的输入图像从 `(1, 28, 28)` 转换为 `(1, 784)`，而我们的第一个 `nn.Linear(in_features=784, out_features=10)` 层能够将其作为输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3161e428-22ce-476d-b19f-5867c77e137e",
   "metadata": {},
   "source": [
    "### 1.6 隐藏层输入和输出形状不匹配\n",
    "\n",
    "如果我们的输入层具有正确的形状，但连接的层之间存在不匹配会怎样？\n",
    "\n",
    "例如，第一个 `nn.Linear()` 的 `out_features=10`，但下一个 `nn.Linear()` 的 `in_features=5`。\n",
    "\n",
    "这就是**层之间输入和输出形状不匹配**的例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ae5ea9d8-9f7f-4341-b2db-a23649460400",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [15]\u001B[0m, in \u001B[0;36m<cell line: 9>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m model_3 \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mSequential(\n\u001B[1;32m      3\u001B[0m     nn\u001B[38;5;241m.\u001B[39mFlatten(),\n\u001B[1;32m      4\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m784\u001B[39m, out_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m), \u001B[38;5;66;03m# out_features=10 \u001B[39;00m\n\u001B[1;32m      5\u001B[0m     nn\u001B[38;5;241m.\u001B[39mLinear(in_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m, out_features\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m) \u001B[38;5;66;03m# <-- NEW: in_features does not match the out_features of the previous layer\u001B[39;00m\n\u001B[1;32m      6\u001B[0m )\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# Pass the image through the model (this will error)\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[43mmodel_3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimage\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)"
     ]
    }
   ],
   "source": [
    "# Create a model with incorrect input and output shapes between layers\n",
    "model_3 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=784, out_features=10), # out_features=10 \n",
    "    nn.Linear(in_features=5, out_features=10) # <-- NEW: in_features does not match the out_features of the previous layer\n",
    ")\n",
    "\n",
    "# Pass the image through the model (this will error)\n",
    "model_3(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5bb6f61-02ba-4065-b6eb-c920c198816c",
   "metadata": {},
   "source": [
    "运行上述模型时，我们遇到了以下错误：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x10 and 5x10)\n",
    "```\n",
    "\n",
    "再次违反了矩阵乘法的规则1，即**内维**必须匹配。\n",
    "\n",
    "我们的第一个 `nn.Linear()` 层输出形状为 `(1, 10)`，但我们的第二个 `nn.Linear()` 层期望的形状是 `(1, 5)`。\n",
    "\n",
    "我们该如何解决这个问题呢？\n",
    "\n",
    "我们可以手动将第二个 `nn.Linear()` 层的 `in_features` 设置为 10，或者尝试使用 PyTorch 的较新特性——“惰性”层。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f41b4e-dd31-44f2-a22f-960d8a0d51a2",
   "metadata": {},
   "source": [
    "### 1.7 PyTorch 惰性层（自动推断输入形状）\n",
    "\n",
    "PyTorch 中的惰性层通常以 `nn.LazyX` 的形式出现，其中 `X` 是现有非惰性层的对应形式。\n",
    "\n",
    "例如，`nn.Linear()` 的惰性等效层是 [`nn.LazyLinear()`](https://pytorch.org/docs/stable/generated/torch.nn.LazyLinear.html)。\n",
    "\n",
    "`Lazy` 层的主要特点是能够*推断* `in_features` 或从前一层的输入形状。\n",
    "\n",
    "> **注意：** 截至 2022 年 11 月，PyTorch 中的 `Lazy` 层仍处于实验阶段，可能会发生变化，但它们的用法应该与下面的示例不会有太大差异。\n",
    "\n",
    "例如，如果前一层的 `out_features=10`，那么后续的 `Lazy` 层应该推断出 `in_features=10`。\n",
    "\n",
    "让我们测试一下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e615051b-93ac-4dd9-a480-8e24619aeaa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/lazy.py:178: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4282,  0.2492, -0.2045, -0.4943, -0.1639,  0.1166,  0.3828, -0.1283,\n",
       "         -0.1771, -0.2277]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try nn.LazyLinear() as the second layer\n",
    "model_4 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(in_features=784, out_features=10),\n",
    "    nn.LazyLinear(out_features=10) # <-- NEW: no in_features parameter as this is inferred from the previous layer's output\n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_4(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d421bf-e58d-4a5d-b1db-19616dfe9f65",
   "metadata": {},
   "source": [
    "这可以正常工作（不过根据你使用的 PyTorch 版本，可能会有一条警告信息，如果是这样，不用担心，这只是表明 `Lazy` 层仍在开发中）！\n",
    "\n",
    "我们试试把所有的 `nn.Linear()` 层替换成 `nn.LazyLinear()` 层怎么样？\n",
    "\n",
    "这样我们只需要为每个层设置 `out_features` 值即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2230de11-f765-495b-8e9f-0c680d9902f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1375, -0.2175, -0.1054,  0.1424, -0.1406, -0.1180, -0.0896, -0.4285,\n",
       "         -0.0077, -0.3188]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Replace all nn.Linear() layers with nn.LazyLinear()\n",
    "model_5 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.LazyLinear(out_features=10),\n",
    "    nn.LazyLinear(out_features=10) # <-- NEW \n",
    ")\n",
    "\n",
    "# Pass the image through the model\n",
    "model_5(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1cdcbc-0f57-4371-b753-655f36bbea3a",
   "metadata": {},
   "source": [
    "太好了！\n",
    "\n",
    "我们的图像再次顺利通过网络，没有任何问题。\n",
    "\n",
    "> **注意：** 上述示例仅涉及 PyTorch 中的一种层类型 `nn.Linear()`，然而，无论是在所有神经网络还是不同类型的数据中，确保输入和输出形状与每一层相匹配的原则是一致的。\n",
    ">\n",
    "> 例如，卷积神经网络 (CNN) 中使用的 [`nn.Conv2d()`](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) 层甚至可以接受不需要使用 `nn.Flatten()` 的输入。你可以在 [03. PyTorch 计算机视觉 第7节：构建一个 CNN](https://www.learnpytorch.io/03_pytorch_computer_vision/#7-model-2-building-a-convolutional-neural-network-cnn) 中了解更多相关信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bb1210-564f-404b-b5d3-a149972cc22b",
   "metadata": {},
   "source": [
    "## 2. PyTorch 中的设备错误\n",
    "\n",
    "PyTorch 的主要优势之一是其内置的能够在 GPU（图形处理单元）上进行计算的能力。\n",
    "\n",
    "GPU 通常能够比 CPU（中央处理单元）更快地执行操作，特别是矩阵乘法（这是构成神经网络的大部分操作）。\n",
    "\n",
    "如果你使用的是原生 PyTorch（没有其他外部库），PyTorch 要求你明确设置在哪个设备上进行计算。\n",
    "\n",
    "例如，要将模型发送到目标设备，你可以使用 [`to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html) 方法，例如 `model.to(device)`。\n",
    "\n",
    "同样地，对于数据 `some_dataset.to(device)`。\n",
    "\n",
    "**设备错误** 发生在模型和数据位于不同设备上时。\n",
    "\n",
    "例如，当你将模型发送到目标 GPU 设备，但数据仍然在 CPU 上时。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9ade20-6f1b-4fc6-944d-553f2112834d",
   "metadata": {},
   "source": [
    "### 2.1 设置目标设备\n",
    "\n",
    "让我们将当前设备设置为 `\"cuda\"`（如果可用）。\n",
    "\n",
    "> **注意：** 有关如何获取 GPU 并使用 PyTorch 进行设置的更多信息，请参阅 [00. PyTorch 基础：在 GPU 上运行张量](https://www.learnpytorch.io/00_pytorch_fundamentals/#running-tensors-on-gpus-and-making-faster-computations)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aff03038-c2f0-4e51-a46e-9d1e8fbde893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set device to \"cuda\" if it's available otherwise default to \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Current device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494deffc-46a7-49e1-8a52-a73636982162",
   "metadata": {},
   "source": [
    "现在让我们创建一个与 `model_5` 具有相同层的模型。\n",
    "\n",
    "在 PyTorch 中，模型和张量默认是在 CPU 上创建的。\n",
    "\n",
    "我们可以通过检查我们创建的模型的 `device` 属性来测试这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cff50c25-5e81-43d6-816d-8816c58cbb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cpu\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "# Create a model (similar to model_5 above)\n",
    "model_6 = nn.Sequential(\n",
    "    nn.Flatten(),\n",
    "    nn.LazyLinear(out_features=10), \n",
    "    nn.LazyLinear(out_features=10)\n",
    ")\n",
    "\n",
    "# All models and tensors are created on the CPU by default (unless explicitly set otherwise)\n",
    "print(f\"Model is on device: {next(model_6.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c0e5f9-fcc1-4abf-a81d-496aa2f8b5cc",
   "metadata": {},
   "source": [
    "### 2.2 为建模准备数据\n",
    "\n",
    "为了准备数据进行建模，我们来创建一些 PyTorch 的 `DataLoader`。\n",
    "\n",
    "为了加快速度，我们将使用 [`torch.utils.data.RandomSampler`](https://pytorch.org/docs/stable/data.html#torch.utils.data.RandomSampler) 实例来随机选择训练和测试样本的 10%（我们并不太关心性能最佳的模型，而是更关注展示潜在的错误）。\n",
    "\n",
    "我们还将设置一个损失函数 `torch.nn.CrossEntropyLoss()` 和一个优化器 `torch.optim.SGD(lr=0.01)`。\n",
    "\n",
    "> **注意：** 有关为训练 PyTorch 模型准备数据、损失函数和优化器的更多信息，请参阅 [01. PyTorch 工作流程基础 第3节：训练模型](https://www.learnpytorch.io/01_pytorch_workflow/#3-train-model)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b3873224-cf4e-4ec9-aa2d-713166ace3cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of random training samples selected: 6000/60000\n",
      "Number of random testing samples selected: 1000/10000\n",
      "Number of batches in train_dataloader: 188 batches of size 32\n",
      "Number of batches in test_dataloader: 32 batch of size 32\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "\n",
    "# Only sample 10% of the data\n",
    "train_sampler = RandomSampler(train_data, \n",
    "                              num_samples=int(0.1*len(train_data)))\n",
    "\n",
    "test_sampler = RandomSampler(test_data, \n",
    "                             num_samples=int(0.1*len(test_data)))\n",
    "\n",
    "print(f\"Number of random training samples selected: {len(train_sampler)}/{len(train_data)}\")\n",
    "print(f\"Number of random testing samples selected: {len(test_sampler)}/{len(test_data)}\")\n",
    "\n",
    "# Create DataLoaders and turn data into batches\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(dataset=train_data,\n",
    "                              batch_size=BATCH_SIZE,\n",
    "                              sampler=train_sampler)\n",
    "\n",
    "test_dataloader = DataLoader(dataset=test_data,\n",
    "                             batch_size=BATCH_SIZE,\n",
    "                             sampler=test_sampler)\n",
    "\n",
    "print(f\"Number of batches in train_dataloader: {len(train_dataloader)} batches of size {BATCH_SIZE}\")\n",
    "print(f\"Number of batches in test_dataloader: {len(test_dataloader)} batch of size {BATCH_SIZE}\")\n",
    "\n",
    "# Create loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.SGD(lr=0.01, \n",
    "                            params=model_6.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d9fb5e-0ff8-4869-ba04-b62bab59f02c",
   "metadata": {},
   "source": [
    "### 2.3 在CPU上训练模型\n",
    "\n",
    "数据准备好了，模型也准备好了，让我们开始训练吧！\n",
    "\n",
    "我们将使用标准的PyTorch训练循环，对`model_6`进行五轮训练，使用10%的数据。\n",
    "\n",
    "在这里不必过于担心损失值是否尽可能低，因为我们更关注的是确保没有错误，而不是追求最低可能的损失值。\n",
    "\n",
    "<img src=\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/images/01-pytorch-training-loop-annotated.png\" alt=\"PyTorch训练循环步骤注解\" width=750/>\n",
    "\n",
    "> **注意：** 有关PyTorch训练循环步骤的更多信息，请参阅[01. PyTorch工作流程第3节：PyTorch训练循环](https://www.learnpytorch.io/01_pytorch_workflow/#pytorch-training-loop)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "03d6f92e-d4aa-40bf-b3ed-c97a98c72e3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28ec173850c84277a11540fc3cd0350f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 334.65\n",
      "Epoch: 1 | Training loss: 215.44\n",
      "Epoch: 2 | Training loss: 171.15\n",
      "Epoch: 3 | Training loss: 154.72\n",
      "Epoch: 4 | Training loss: 142.22\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "    # Set loss to 0 every epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    # Get images (X) and labels (y)\n",
    "    for X, y in train_dataloader:\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model_6(X)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        train_loss += loss\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimizer step\n",
    "        optimizer.step()\n",
    "  \n",
    "    # Print loss in the epoch loop only\n",
    "    print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d3079-a360-44fb-a955-05f7f74edaec",
   "metadata": {},
   "source": [
    "太好了！看来我们的训练循环运行正常！\n",
    "\n",
    "我们模型的损失正在下降（损失越低越好）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f79817-30ff-4f06-a201-954acba34abb",
   "metadata": {},
   "source": [
    "### 2.4 尝试在GPU上训练模型（含错误）\n",
    "\n",
    "现在，让我们将 `model_6` 发送到目标 `device`（在我们的例子中，这是一个 `\"cuda\"` GPU）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4ce89bb-2624-4427-9de1-7570154bb8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Send model_6 to the target device (\"cuda\")\n",
    "model_6.to(device)\n",
    "\n",
    "# Print out what device the model is on\n",
    "print(f\"Model is on device: {next(model_6.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c9434-9a9a-4089-98fd-32196036504b",
   "metadata": {},
   "source": [
    "我们的 `model_6` 位于 `\"cuda:0\"` 设备上（其中 `0` 是设备的索引，以防有多块 GPU）。\n",
    "\n",
    "现在，让我们运行与上面相同的训练循环代码，看看会发生什么。\n",
    "\n",
    "你能猜到吗？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7552567-8d35-4570-8dd9-f984d02cd2e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75606965bc224a6a8b223db95612e24e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [24]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Get images (X) and labels (y)\u001B[39;00m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m X, y \u001B[38;5;129;01min\u001B[39;00m train_dataloader:\n\u001B[1;32m     14\u001B[0m \n\u001B[1;32m     15\u001B[0m   \u001B[38;5;66;03m# Forward pass\u001B[39;00m\n\u001B[0;32m---> 16\u001B[0m   y_pred \u001B[38;5;241m=\u001B[39m \u001B[43mmodel_6\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# model is on GPU, data is on CPU (will error)\u001B[39;00m\n\u001B[1;32m     18\u001B[0m   \u001B[38;5;66;03m# Calculate the loss\u001B[39;00m\n\u001B[1;32m     19\u001B[0m   loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, y)\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "  # Set loss to 0 every epoch\n",
    "  train_loss = 0\n",
    "\n",
    "  # Get images (X) and labels (y)\n",
    "  for X, y in train_dataloader:\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model_6(X) # model is on GPU, data is on CPU (will error)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss\n",
    "    \n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "  \n",
    "  # Print loss in the epoch loop only\n",
    "  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dde9a2-8c87-44d2-9d2c-3257589e37c3",
   "metadata": {},
   "source": [
    "哎呀！\n",
    "\n",
    "看起来我们遇到了一个设备错误：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n",
    "```\n",
    "\n",
    "我们可以看到错误信息指出：`Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`。\n",
    "\n",
    "本质上，我们的模型位于 `cuda:0` 设备上，但我们的数据张量（`X` 和 `y`）仍然在 `cpu` 设备上。\n",
    "\n",
    "但 **PyTorch 期望 *所有* 张量都在同一设备上**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c50ba5-d2ae-4b1b-98f2-5b8c9b0d366a",
   "metadata": {},
   "source": [
    "### 2.5 在GPU上训练模型（无错误）\n",
    "\n",
    "让我们解决这个错误，将数据张量（`X`和`y`）也发送到目标`device`。\n",
    "\n",
    "我们可以通过使用`X.to(device)`和`y.to(device)`来实现这一点。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "505b45f9-abd3-4abe-bd34-17052312dee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f94f47483b84c0ca344c1c3545a9d04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 | Training loss: 134.76\n",
      "Epoch: 1 | Training loss: 127.76\n",
      "Epoch: 2 | Training loss: 120.85\n",
      "Epoch: 3 | Training loss: 120.50\n",
      "Epoch: 4 | Training loss: 116.29\n"
     ]
    }
   ],
   "source": [
    "# Send the model to the target device (we don't need to do this again but we will for completeness)\n",
    "model_6.to(device)\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 5\n",
    "\n",
    "# Train the model\n",
    "for epoch in tqdm(range(epochs)):\n",
    "\n",
    "  # Set loss to 0 every epoch\n",
    "  train_loss = 0\n",
    "\n",
    "  # Get images (X) and labels (y)\n",
    "  for X, y in train_dataloader:\n",
    "\n",
    "    # Put target data on target device  <-- NEW\n",
    "    X, y = X.to(device), y.to(device) # <-- NEW: send data to target device\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = model_6(X)\n",
    "\n",
    "    # Calculate the loss\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    train_loss += loss\n",
    "    \n",
    "    # Optimizer zero grad\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Loss backward\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "  \n",
    "  # Print loss in the epoch loop only\n",
    "  print(f\"Epoch: {epoch} | Training loss: {train_loss:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4866ae09-785a-4ddb-b2f3-a9e224bf7929",
   "metadata": {},
   "source": [
    "非常好！\n",
    "\n",
    "我们的训练循环与之前一样完成，因为现在我们的模型和数据张量都在同一设备上。\n",
    "\n",
    "> **注意：** 像 [HuggingFace Accelerate](https://github.com/huggingface/accelerate) 这样的库是使用 PyTorch 模型进行训练的绝佳方式，几乎不需要显式设置设备（它们会自动发现最佳设备并为您设置好一切）。\n",
    "> \n",
    "> 您也可以编写函数来确保您的训练代码都在同一设备上运行，更多内容请参阅 [05. PyTorch 模块化进阶 第4节：创建训练函数](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098b8ac3-192e-4299-8545-a2fd389c36a0",
   "metadata": {},
   "source": [
    "### 2.6 在预测时出现的设备错误\n",
    "\n",
    "我们在训练过程中已经见过设备错误，但同样的错误也可能在测试或推理（进行预测）时发生。\n",
    "\n",
    "训练模型并在某些数据上进行训练的整个想法是为了利用它对*未见过的*数据进行预测。\n",
    "\n",
    "让我们使用训练好的 `model_6`，并利用它对测试数据集中的一个样本进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "38082d32-4d43-43e9-974e-f00f4f2724fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test image shape: torch.Size([28, 28])\n",
      "Test image label: 9\n"
     ]
    }
   ],
   "source": [
    "# Get a single sample from the test dataset\n",
    "test_image, test_label = test_data.data[0], test_data.targets[0]\n",
    "print(f\"Test image shape: {test_image.shape}\")\n",
    "print(f\"Test image label: {test_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e64a5ebb-058b-4b3f-ba0f-ccf041a580c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot test image\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(test_image, cmap=\"gray\")\n",
    "plt.axis(False)\n",
    "plt.title(class_names[test_label]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f98c15-6b4a-457a-a7c9-bfe62692c97c",
   "metadata": {},
   "source": [
    "看起来不错！\n",
    "\n",
    "现在让我们尝试通过将其传递给我们的 `model_6` 来进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f77c721-c9f5-4f65-bacb-899a06e72148",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [28]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Pass the test image through model_6 to make a prediction\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel_6\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_image\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)"
     ]
    }
   ],
   "source": [
    "# Pass the test image through model_6 to make a prediction\n",
    "model_6(test_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79aeb66d-e84d-4b95-b1ed-052a023a50d5",
   "metadata": {},
   "source": [
    "该死！\n",
    "\n",
    "我们又遇到了设备错误。\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat1 in method wrapper_addmm)\n",
    "```\n",
    "\n",
    "这是因为我们的 `model_6` 位于 GPU（即 `\"cuda\"`）上，而我们的 `test_image` 位于 CPU 上（在 PyTorch 中，所有张量默认位于 CPU 上）。\n",
    "\n",
    "让我们将 `test_image` 发送到目标 `device`，然后再尝试进行预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1a3d722-24e5-4df0-907d-4553050fd2f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [30]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Send test_image to target device\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel_6\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_image\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)"
     ]
    }
   ],
   "source": [
    "# Send test_image to target device\n",
    "model_6(test_image.to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccfecaf-d7b2-49ba-a98f-016d4078bf36",
   "metadata": {},
   "source": [
    "糟糕！又是错误...\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: mat1 and mat2 shapes cannot be multiplied (28x28 and 784x10)\n",
    "```\n",
    "\n",
    "这次是形状错误。\n",
    "\n",
    "我们之前也遇到过这种情况。\n",
    "\n",
    "我们的 `test_image` 形状出了什么问题？\n",
    "\n",
    "也许是因为我们的模型是在带有批次维度的图像上训练的？\n",
    "\n",
    "而我们当前的 `test_image` 没有批次维度？\n",
    "\n",
    "这里有一个有用的经验法则需要记住：**训练好的模型喜欢预测与它们训练时相同格式和形状的数据**。\n",
    "\n",
    "这意味着，如果我们的模型是在带有批次维度的图像上训练的，它倾向于喜欢预测带有批次维度的图像，即使批次维度只有1（单个样本）。\n",
    "\n",
    "如果我们的模型是在 `torch.float32` 格式（或其他格式）的数据上训练的，它也会喜欢预测相同格式的数据（我们稍后会看到这一点）。\n",
    "\n",
    "我们可以使用 [`torch.unsqueeze()`](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html) 方法为我们的 `test_image` 添加一个单一批次维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e8dc1ba7-9156-44ee-b6b3-35771106a1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original input data shape: torch.Size([28, 28]) -> [height, width]\n",
      "Updated input data shape (with added batch dimension): torch.Size([1, 28, 28]) -> [batch, height, width]\n"
     ]
    }
   ],
   "source": [
    "# Changing the input size to be the same as what the model was trained on\n",
    "original_input_shape = test_image.shape\n",
    "updated_input_shape = test_image.unsqueeze(dim=0).shape # adding a batch dimension on the \"0th\" dimension\n",
    "\n",
    "# Print out shapes of original tensor and updated tensor\n",
    "print(f\"Original input data shape: {original_input_shape} -> [height, width]\")\n",
    "print(f\"Updated input data shape (with added batch dimension): {updated_input_shape} -> [batch, height, width]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745679ec-251e-4cb3-9732-d9954dbe0e2c",
   "metadata": {},
   "source": [
    "太好了！\n",
    "\n",
    "我们已经找到了一种方法来为我们的 `test_image` 添加批处理维度。\n",
    "\n",
    "现在让我们再尝试对它进行一次预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3a25cef5-e0db-44dd-bbc7-17ff104ab846",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Byte",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [32]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Make prediction on test image with additional batch size dimension and with it on the target device\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mmodel_6\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtest_image\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/code/pytorch/env/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: expected scalar type Float but found Byte"
     ]
    }
   ],
   "source": [
    "# Make prediction on test image with additional batch size dimension and with it on the target device\n",
    "model_6(test_image.unsqueeze(dim=0).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a30c81-4118-471e-8672-7897257e8709",
   "metadata": {},
   "source": [
    "什么？\n",
    "\n",
    "又出错了！\n",
    "\n",
    "这次是数据类型错误：\n",
    "\n",
    "```\n",
    "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py in forward(self, input)\n",
    "    112 \n",
    "    113     def forward(self, input: Tensor) -> Tensor:\n",
    "--> 114         return F.linear(input, self.weight, self.bias)\n",
    "    115 \n",
    "    116     def extra_repr(self) -> str:\n",
    "\n",
    "RuntimeError: expected scalar type Float but found Byte\n",
    "```\n",
    "\n",
    "我们遇到了 PyTorch 中第三常见的错误——数据类型错误。\n",
    "\n",
    "让我们在下一节中探讨如何修复它们。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4053b033-862b-4c67-b2e4-dc8de4d0fd6e",
   "metadata": {},
   "source": [
    "## 3. PyTorch 中的数据类型错误\n",
    "\n",
    "回顾一下经验法则：**训练好的模型喜欢预测与它们训练时形状和格式相同的数据**。\n",
    "\n",
    "看起来我们的模型期望一个 `Float` 数据类型，但我们的 `test_image` 是 `Byte` 数据类型。\n",
    "\n",
    "我们可以通过之前的错误信息中的最后一行得知这一点：\n",
    "\n",
    "```\n",
    "RuntimeError: expected scalar type Float but found Byte\n",
    "```\n",
    "\n",
    "为什么会这样？\n",
    "\n",
    "这是因为我们的 `model_6` 是在 `Float` 格式的数据样本上训练的，具体来说，是 `torch.float32`。\n",
    "\n",
    "我们怎么知道这一点？\n",
    "\n",
    "嗯，`torch.float32` 是 PyTorch 中许多张量的默认值，除非明确设置为其他类型。\n",
    "\n",
    "但让我们检查一下以确保。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa086a9-d8ef-4438-9e86-512cfd86dd6e",
   "metadata": {},
   "source": [
    "### 3.1 检查模型训练数据的类型\n",
    "\n",
    "我们可以通过查看 `train_dataloader` 中样本的 `dtype` 属性来检查模型训练数据的类型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9145327-5d47-48fa-a6c1-5763896bed90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datatype of training data: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Get a single sample from the train_dataloader and print the dtype\n",
    "train_image_batch, train_label_batch = next(iter(train_dataloader))\n",
    "train_image_single, train_label_single = train_image_batch[0], train_label_batch[0]\n",
    "\n",
    "# Print the datatype of the train_image_single\n",
    "print(f\"Datatype of training data: {train_image_single.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d881ad2-8aeb-4742-8823-8dbaab7f4e18",
   "metadata": {},
   "source": [
    "好了，我们确认了训练数据样本是 `torch.float32` 类型的。\n",
    "\n",
    "因此，我们的 `model_6` 希望在这个数据类型上进行预测是合理的。\n",
    "\n",
    "但我们的训练数据是如何变成这个数据类型的呢？\n",
    "\n",
    "这发生在第 1.3 节，当我们下载 Fashion MNIST 数据集并使用 [`torchvision.transforms.ToTensor()`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ToTensor.html) 的 `transform` 参数时。\n",
    "\n",
    "这个 `transform` 会将传递给它的任何数据转换为 `torch.Tensor`，并使用默认的数据类型 `torch.float32`。\n",
    "\n",
    "因此，另一个经验法则是：**在进行预测时，无论对训练数据执行了哪些转换，都应该对测试数据执行相同的转换**。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d2a74-4c28-4c9a-a9ad-05cc81c71a14",
   "metadata": {},
   "source": [
    "### 3.2 改变张量的数据类型\n",
    "\n",
    "在我们的例子中，我们可以创建一个独立的变换来转换我们的测试数据，但我们也可以使用 `tensor.type(some_type_here)` 来改变目标张量的数据类型，例如 `tensor_1.type(torch.float32)`。\n",
    "\n",
    "让我们试一试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9e031504-824a-4ea1-b159-dbf54846af26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original datatype: torch.uint8\n",
      "Changing the datatype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Print out the original datatype of test_image\n",
    "print(f\"Original datatype: {test_image.unsqueeze(dim=0).dtype}\")\n",
    "\n",
    "# Change the datatype of test_image and see the change\n",
    "print(f\"Changing the datatype: {test_image.unsqueeze(dim=0).type(torch.float32).dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a01303-8a2f-40cf-8a39-077acc4063cd",
   "metadata": {},
   "source": [
    "### 3.3 对测试图像进行预测并确保其格式正确\n",
    "\n",
    "好了，看起来我们已经准备好了所有的拼图碎片：形状、设备和数据类型，让我们尝试进行一次预测吧！\n",
    "\n",
    "> **注意：** 记住模型喜欢在与其训练数据相同（或相似）格式的数据上进行预测（形状、设备和数据类型）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c2db9993-0148-4881-9f68-fcea63946c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -963.8352, -1658.8182,  -735.9952, -1285.2964,  -550.3845,   949.4190,\n",
       "          -538.1960,  1123.0616,   552.7371,  1413.8110]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make a prediction with model_6 on the transformed test_image\n",
    "pred_on_gpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n",
    "                      .type(torch.float32) # convert the datatype to torch.float32\n",
    "                      .to(device)) # send the tensor to the target device\n",
    "pred_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef441646-1d24-4fde-9fa5-0b54ce9de2fe",
   "metadata": {},
   "source": [
    "哇呼！！！\n",
    "\n",
    "虽然步骤不少，但我们的 `model_6` 成功对 `test_image` 进行了预测。\n",
    "\n",
    "由于 `test_image` 默认位于 CPU 上，我们也可以使用 [`.cpu()` 方法](https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html) 将模型放回 CPU，并在 CPU 设备上进行相同的预测，而不是在 GPU 设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e98bd335-43ca-4c76-ac66-f3ffccf79dc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -963.8351, -1658.8182,  -735.9953, -1285.2964,  -550.3845,   949.4189,\n",
       "          -538.1960,  1123.0615,   552.7371,  1413.8110]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put model back on CPU\n",
    "model_6.cpu()\n",
    " \n",
    "# Make a prediction on the CPU device (no need to put test_image on the CPU as it's already there)\n",
    "pred_on_cpu = model_6(test_image.unsqueeze(dim=0) # add a batch dimension\n",
    "                      .type(torch.float32)) # convert the datatype to torch.float32 \n",
    "pred_on_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2269862-f9d9-4985-9483-1821132b8605",
   "metadata": {},
   "source": [
    "预测再次奏效了！\n",
    "\n",
    "这是正确的吗？\n",
    "\n",
    "我们可以通过获取模型的原始输出，并将其从 `原始对数 -> 预测概率 -> 预测标签` 进行转换来验证（更多关于此转换的内容请参见 [02. PyTorch 神经网络分类 3.1 节](https://www.learnpytorch.io/02_pytorch_classification/#31-going-from-raw-model-outputs-to-predicted-labels-logits-prediction-probabilities-prediction-labels)）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afbaa8ae-3954-43d4-92e1-3a0074428056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test label: 9\n",
      "Pred label: tensor([9])\n",
      "Is the prediction correct? True\n"
     ]
    }
   ],
   "source": [
    "# Convert raw logits to prediction probabilities\n",
    "pred_probs = torch.softmax(pred_on_cpu, dim=1)\n",
    "\n",
    "# Convert prediction probabilities to prediction label\n",
    "pred_label = torch.argmax(pred_probs, dim=1)\n",
    "\n",
    "# Check if it's correct\n",
    "print(f\"Test label: {test_label}\")\n",
    "print(f\"Pred label: {pred_label}\")\n",
    "print(f\"Is the prediction correct? {pred_label.item() == test_label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8a3073-6c98-4137-aa3b-f2df832d1a67",
   "metadata": {},
   "source": [
    "在进行测试或自定义样本的预测时，可能会涉及相当多的步骤。\n",
    "\n",
    "因此，为了避免重复所有这些步骤，可以将它们转化为一个函数。\n",
    "\n",
    "在[04. PyTorch 自定义数据集 第11.3节：构建一个函数来预测自定义图像](https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function)中有一个这方面的示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2740aaa9-e3a7-42ea-9ca0-d4dc5be67766",
   "metadata": {},
   "source": [
    "## 综合运用\n",
    "\n",
    "我们已经实际接触了在使用 PyTorch 构建神经网络时会遇到的三大主要错误：\n",
    "\n",
    "1. **形状错误** - 你正在处理的数据与构建的神经网络寻找模式的方式不匹配，或者神经网络各连接层之间不匹配。\n",
    "2. **设备错误** - 你的模型和数据位于不同的设备上，PyTorch 期望*所有*张量和对象位于*同一*设备上。\n",
    "3. **数据类型错误** - 你试图使用一种数据类型进行计算，而模型期望另一种数据类型。\n",
    "\n",
    "并且我们已经了解了它们发生的原因以及如何修复它们：\n",
    "\n",
    "* 你的模型希望对与其训练数据相同类型的数据（形状、设备和数据类型）进行预测。\n",
    "* 你的模型和数据应在训练和测试时位于同一设备上。\n",
    "* 你可以通过创建定义 `device` 和数据类型的可重用函数来解决许多这些问题，例如在[04. PyTorch 模块化章节 4：创建训练和测试函数](https://www.learnpytorch.io/05_pytorch_going_modular/#4-creating-train_step-and-test_step-functions-and-train-to-combine-them)中。\n",
    "\n",
    "了解这些错误并不能防止你将来犯错，但它会给你一个解决问题的方向。\n",
    "\n",
    "要获取这些错误的更深入示例，包括实际制作和修复它们的方法，请参阅[从零到精通：PyTorch 深度学习课程](https://dbourke.link/ZTMPyTorch)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
